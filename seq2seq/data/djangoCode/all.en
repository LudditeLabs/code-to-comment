Collect all locale directories
Filter by resources, if any
Output changed stats
TODO: merge first with the latest en catalog  msgfmt output stats on stderr
Transifex pull
msgcat to wrap lines and msgfmt for compilation of .mo file
Warn if we are installing over top of an existing installation. This can  cause issues where files that were deleted from a more recent Django are  still present in site-packages. See 18115.  We have to try also with an explicit prefix of /usr/local in order to  catch Debian's custom user site-packages directory.  We note the need for the warning here, but present it after the  command is run, so it's more likely to be seen.
Dynamically calculate the version based on django.VERSION.
RE for option descriptions without a '--' prefix
register the snippet directive  register a node for snippet directive so that the xml parser  knows how to handle the enter/exit parsing event
code-block directives
code-block directives
Some filenames have '_', which is special in latex.
Prevent rawsource from appearing in output a second time.
Don't use border=1, which docutils does by default.
Turn the "new in version" stuff (versionadded/versionchanged) into a  better callout -- the Sphinx default is just a little span,  which is a bit less obvious that I'd like.  FIXME: these messages are all hardcoded in English. We need to change  that to accommodate other language docs, but I can't work out how to make  that work.
Give each section a unique ID -- nice for custom CSS hooks
Workaround for sphinx-build recursion limit overflow:  pickle.dump(doctree, f, pickle.HIGHEST_PROTOCOL)   RuntimeError: maximum recursion depth exceeded while pickling an object  Python's default allowed recursion depth is 1000 but this isn't enough for  building docs/ref/settings.txt sometimes.  https://groups.google.com/d/topic/sphinx-dev/MtRf64eGtv4/discussion
Make sure we get the version of this copy of Django
If extensions (or modules to document with autodoc) are in another directory,  add these directories to sys.path here. If the directory is relative to the  documentation root, use os.path.abspath to make it absolute, like shown here.
Add any Sphinx extension module names here, as strings. They can be extensions  coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
Spelling check needs an additional module that is not installed by default.  Add it only if spelling check is requested so docs can be generated without it.
Spelling language.
Location of word list.
The suffix of source filenames.
The master toctree document.
General substitutions.
The version info for the project you're documenting, acts as replacement for  |version| and |release|, also used in various other places throughout the  built documents.  The short X.Y version.  The full version, including alpha/beta/rc tags.
The "development version" of Django
Location for .po/.mo translation files used when language is set
There are two options for replacing |today|: either, you set today to some  non-false value, then it is used:  today = ''  Else, today_fmt is used as the format for a strftime call.
List of patterns, relative to source directory, that match files and  directories to ignore when looking for source files.
If true, '()' will be appended to :func: etc. cross-reference text.
If true, the current module name will be prepended to all description  unit titles (such as .. function::).
If true, sectionauthor and moduleauthor directives will be shown in the  output. They are ignored by default.
The name of the Pygments (syntax highlighting) style to use.
Links to Python's docs should reference the most recent version of the 3.x  branch, which is located at this URL.
The theme to use for HTML and HTML Help pages.  See the documentation for  a list of builtin themes.
Add any paths that contain custom themes here, relative to this directory.
If not '', a 'Last updated on:' timestamp is inserted at every page bottom,  using the given strftime format.
If true, SmartyPants will be used to convert quotes and dashes to  typographically correct entities.
HTML translator class for the builder
Additional templates that should be rendered to pages, maps page names to  template names.
Output file base name for HTML help builder.
Grouping the document tree into LaTeX files. List of tuples  (source start file, target name, title, author, document class [howto/manual]).  latex_documents = []
One entry per manual page. List of tuples  (source start file, name, description, authors, manual section).
List of tuples (startdocname, targetname, title, author, dir_entry,  description, category, toctree_only)
Bibliographic Dublin Core info.
The HTML theme for the epub output. Since the default themes are not optimized  for small screen space, using the same theme for HTML and epub output is  usually not wise. This defaults to 'epub', a theme designed to save visual  space.
A tuple containing the cover image and cover page html template filenames.
-- ticket options ------------------------------------------------------------
Fail loudly when content has control chars (unsupported in XML 1.0)  See http://www.w3.org/International/questions/qa-controls
create a dummy class for Python 3.5+ where it's been removed
Support the sections of ISO 8601 date representation that are accepted by  timedelta
If there's already a max-age header but we're being asked to set a new  max-age, use the minimum of the two ages. In practice this happens when  a decorator and a piece of middleware both operate on a given view.
Allow overriding private caching and vice versa
We need to keep the cookies, see ticket 4994.
Get HTTP request headers  There can be more than one ETag in the request, so we  consider the list of values.  In case of an invalid ETag, ignore all ETag headers.  Apparently Opera sends invalidly quoted headers at times  (we should be returning a 400 response, but that's a  little extreme) -- this is bug 10681.
If-None-Match must be ignored if original result would be anything  other than a 2XX or 304 status. 304 status would result in no change.  http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.htmlsec14.26
If-Modified-Since must be ignored if the original result was not a 200.  http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.htmlsec14.25
We only get here if no undefined combinations of headers are  specified.
Note that we need to keep the original order intact, because cache  implementations may rely on the order of the Vary contents in, say,  computing an MD5 hash.  Use .lower() here so we treat headers as case-insensitive.
first check if LocaleMiddleware or another middleware added  LANGUAGE_CODE to request, then fall back to the active language  which in turn can also fall back to settings.LANGUAGE_CODE
The datetime module doesn't restrict the output of tzname().  Windows is known to use non-standard, locale-dependent names.  User-defined tzinfo classes may return absolutely anything.  Hence this paranoid conversion to create a valid cache key.
If i18n or l10n are used, the generated cache key will be suffixed  with the current locale. Adding the raw value of Accept-Language is  redundant in that case and would result in storing the same content  under multiple keys in the cache. See 18191 for details.
if there is no Vary header, we still need a cache key  for the request.build_absolute_uri()
Default logging for Django. This sends an email to the site admins on every  HTTP 500 error. Depending on DEBUG, all other log records are either sent to  the console (DEBUG=True) or discarded (DEBUG=False) by means of the  require_debug_true filter.
First find the logging configuration function ...
... then invoke it with the logging settings
Since we add a nicely formatted traceback on our own, create a copy  of the log record without the exception data.
Put 2XX first, since it should be the common case
Any 5XX, or any other response
Configuration for urlize() function.
List of possible strings used for bullets in bulleted lists.
Escape every ASCII character with a value less than 32.
Note: in typical case this loop executes _strip_once once. Loop condition  is redundant, but helps to reduce number of executions of _strip_once.  _strip_once was not able to detect more tags or length increased  due to http://bugs.python.org/issue20288  (affects Python 2 < 2.7.7 and Python 3 < 3.3.5)
Tilde is part of RFC3986 Unreserved Characters  http://tools.ietf.org/html/rfc3986section-2.3  See also http://bugs.python.org/issue16285
Handle IDN before quoting.  invalid IPv6 URL (normally square brackets in hostname part).
Separately unquoting key/value, so as to not mix querystring separators  included in query values. See 22267.  urlencode will take care of quoting
Remove trail for unescaped if it was not consumed by unescape
Trail was consumed by unescape (as end-of-entity marker), move it to text
Continue trimming until middle remains unchanged.
Trim trailing punctuation.
Trim wrapping punctuation.  Keep parentheses at the end only if they're balanced.
lead: Current punctuation trimmed from the beginning of the word.  middle: Current state of the word.  trail: Current punctuation trimmed from the end of the word.  Deal with punctuation.
Make URL we want to point to.
Make link.
Ensure final_path starts with base_path (using normcase to ensure we  don't false-negative on case insensitive operating systems like Windows),  further, one of the following conditions must be true:   a) The next character is the path separator (to prevent conditions like      safe_join("/dir", "/../d"))   b) The final path must be the same as the base path.   c) The base path must be the most root path (meaning either "/" or "C:\\")
Mapping of an escape character to a representative of that class. So, e.g.,  "\w" is replaced by "x" in a reverse URL. A value of None means to ignore  this sequence. Any missing key is mapped to itself.
Do a linear scan to work out the special features of this pattern. The  idea is that we scan once here and collect all the information we need to  make future decisions.
A "while" loop is used here because later on we need to be able to peek  at the next character and possibly go around without consuming another  one at the top of the loop.
Replace "any character" with an arbitrary representative.
FIXME: One day we'll should do this, but not in 1.0.
This can only be the end of a non-capturing group, since all  other unescaped parentheses are handled by the grouping  section later (and the full group is handled there).  We regroup everything inside the capturing group so that it  can be quantified, if necessary.
Replace ranges with the first character in the range.
Some kind of group.  A positional group
All of these are ignorable. Walk to the end of the  group.
Non-capturing group
Anything else, other than a named group, is something  we cannot reverse.
We are in a named capturing group. Extra the name and  then skip to the end.  We are in a named backreference.  Named backreferences have already consumed the  parenthesis.
Quantifiers affect the previous item in the result list.  We had to look ahead, but it wasn't need to compute the  quantifier, so use this character next time around the  main loop.
If we are quantifying a capturing group (or  something containing such a group) and the minimum is  zero, we must also handle the case of one occurrence  being present. All the quantifiers (except {0,0},  which we conveniently ignore) that have a 0 minimum  also allow a single occurrence.
Anything else is a literal.
A case of using the disjunctive form. No results for you!
Consume the trailing '?', if necessary.
Split args into two lists depending on whether they have default value  Join the two lists and combine it with default values  Add possible *args and **kwargs and prepend them with '*' or '**'
Ignore 'self'
Not all callables are inspectable with getargspec, so we'll  try a couple different ways but in the end fall back on assuming  it is -- we don't want to prevent registration of valid but weird  callables.
-*- encoding: utf-8 -*-
For backwards compatibility. (originally in Django, then added to six 1.9)
The input is the result of a gettext_lazy() call.
Handle the common case first for performance reasons.  Note: We use .decode() here, instead of six.text_type(s, encoding,  errors), so that if s is a SafeBytes, it ends up being a  SafeText at the end.  If we get to here, the caller has passed in an Exception  subclass populated with non-ASCII bytestring data without a  working unicode method. Try to handle this without raising a  further exception by individually forcing the exception args  to unicode.
The input is the result of a gettext_lazy() call.
Handle the common case first for performance reasons.  An Exception subclass containing non-ASCII data that doesn't  know how to print itself properly. We shouldn't raise a  further exception.
backwards compatibility for Python 2
The list of safe characters here is constructed from the "reserved" and  "unreserved" characters specified in sections 2.2 and 2.3 of RFC 3986:      reserved    = gen-delims / sub-delims      gen-delims  = ":" / "/" / "?" / "" / "[" / "]" / "@"      sub-delims  = "!" / "$" / "&" / "'" / "(" / ")"                    / "*" / "+" / "," / ";" / "="      unreserved  = ALPHA / DIGIT / "-" / "." / "_" / "~"  Of the unreserved characters, urllib.quote already considers all but  the ~ safe.  The % character is also added to the list of safe characters here, as the  end of section 3.1 of RFC 3987 specifically mentions that % must not be  converted.
These are the "reserved" and "unreserved" characters specified in  sections 2.2 and 2.3 of RFC 2396:    reserved    = ";" | "/" | "?" | ":" | "@" | "&" | "=" | "+" | "$" | ","    unreserved  = alphanum | mark    mark        = "-" | "_" | "." | "!" | "~" | "*" | "'" | "(" | ")"  The list of safe characters here is constructed subtracting ";", "=",  and "?" according to section 3.3 of RFC 2396.  The reason for not subtracting and escaping "/" is that we are escaping  the entire path, not a path segment.
I know about `os.sep` and `os.altsep` but I want to leave  some flexibility for hardcoding separators.
Do not return default here because __setitem__() may store  another value -- QueryDict.__setitem__() does. Look it up.
Do not return default_list here because setlist() may store  another value -- QueryDict.setlist() does. Look it up.
All list mutation functions complain.
Useful for very coarse version differentiation.
Jython always uses 32 bits.
It's possible to have sizeof(long) != sizeof(Py_ssize_t).
32-bit
64-bit
This is a bit ugly, but it avoids running this again by  removing this descriptor.
Subclasses should override this
in case of a reload
Add windows specific modules.
If the file has an encoding, encode unicode with it.
This requires a bit of explanation: the basic idea is to make a dummy  metaclass for one level of class instantiation that replaces itself with  the actual metaclass.
Remove other six meta path importers, since they cause problems. This can  happen if six is removed from sys.modules and then reloaded. (Setuptools does  this for some reason.)  Here's some real nastiness: Another "instance" of the six module might  be floating around. Therefore, we can't use isinstance() to check for  the six meta path importer, since the other six instance will have  inserted an importer with different class.  Finally, add the importer to the meta path import hook.
memoryview and buffer are not strictly equivalent, but should be fine for  django core usage (mainly BinaryField). However, Jython doesn't support  buffer (see http://bugs.jython.org/issue1521), so we have to be careful.
We can't use strftime() because it produces locale-dependent results, so  we have to map english month and day names manually  Support datetime objects older than 1900  We do this ourselves to be timezone aware, email.Utils is not tz aware.
Historically, this function assumes that naive datetimes are in UTC.
Support datetime objects older than 1900
Historically, this function assumes that naive datetimes are in UTC.
Force ints to unicode
Force ints to unicode
datetime.now(tz=utc) is slower, as documented in django.utils.timezone.now
Spec: http://blogs.law.harvard.edu/tech/rss
Author information.
Enclosure.
Categories.
Spec: https://tools.ietf.org/html/rfc4287
Author information.
Unique ID.
Summary.
Enclosures.
Categories.
Rights.
This isolates the decision of what the system default is, so calling code can  do "feedgenerator.DefaultFeed" instead of "feedgenerator.Rss201rev2Feed".
We only support timezone when formatting datetime objects,  not date objects (timezone information not appropriate),  or time objects (against established django policy).
Have to use tzinfo.tzname and not datetime.tzname  because datatime.tzname does not expect Unicode
pytz raises AmbiguousTimeError during the autumn DST change.  This happens mainly when __init__ receives a naive datetime  and sets self.timezone = get_default_timezone().
pytz raises AmbiguousTimeError during the autumn DST change.  This happens mainly when __init__ receives a naive datetime  and sets self.timezone = get_default_timezone().
`offset` is a datetime.timedelta. For negative values (to the west of  UTC) only days can be negative (days=-1) and seconds are always  positive. e.g. UTC-1 -> timedelta(days=-1, seconds=82800, microseconds=0)  Positive offsets have days=0
pytz raises AmbiguousTimeError during the autumn DST change.  This happens mainly when __init__ receives a naive datetime  and sets self.timezone = get_default_timezone().
Algorithm from http://www.personal.ecu.edu/mccartyr/ISOwdALG.txt
Make the common case fast  sign  decimal part  grouping
format_cache is a mapping from (format_type, lang) to the format string.  By using the cache, it is possible to avoid running get_format_modules  repeatedly.
Return the general setting by default
Special case where we suspect a dot meant decimal separator (see 22171)
This relies on os.environ['TZ'] being set to settings.TIME_ZONE.
for pytz timezones
for regular tzinfo objects
If `value` is naive, astimezone() will raise a ValueError,  so we don't need to perform a redundant check.  This method is available for pytz time zones.
timeit shows that datetime.now(tz=utc) is 24% slower
This method is available for pytz time zones.
Check that we won't overwrite the timezone of an aware datetime.  This may be wrong around DST changes!
If `value` is naive, astimezone() will raise a ValueError,  so we don't need to perform a redundant check.  This method is available for pytz time zones.
make an integer out of the number
create the result in base 'len(to_digits)'
Determine the number of comma-separated sections and number of words in  each section for this sentence.  Convert to sentence case and add end punctuation.
Standard connector type. Clients usually won't use this at all and  subclasses will usually override the value.
We can reuse self.children to append or squash the node other.  We can squash the other node's children directly into this  node. We are just doing (AB)(CD) == (ABCD) here, with the  addition that if the length of the other node is 1 the  connector doesn't matter. However, for the len(self) == 1  case we don't want to do the squashing, as it would alter  self.connector.  We could use perhaps additional logic here to see if some  children could be used for pushdown here.
Attempt to import the app's module.
Reset the registry to the state before the last import  as this import will have to reoccur on the next request and  this could raise NotRegistered and AlreadyRegistered  exceptions (see 8245).
Decide whether to bubble up this error. If the app just  doesn't have the module in question, we can ignore the error  attempting to import it, otherwise we want it to bubble up.
package isn't a package.
None indicates a cached miss; see mark_miss() in Python/import.c.
Since the remainder of this function assumes that we're dealing with  a package (module with a __path__), so if it's not, then bail here.
Try the cached finder.  Implicit import machinery should be used.  Else see if the finder knows of a loader.
No cached finder, so try and make one.  XXX Could cache in sys.path_importer_cache  Once a finder is found, stop the search.  Continue the search for a finder.  No finder found.  Try the implicit import machinery if searching a directory.  XXX Could insert None or NullImporter
Exhausted the search, so the module cannot be found.
Convert to list because _NamespacePath does not support indexing on 3.3.
Capitalizes the first letter of a string.
Set up regular expressions
The truncation text didn't contain the %(truncated_text)s string  replacement argument so just append it to the text.  But don't append the truncation text if the current text already  ends in this.
Calculate the length to truncate to (max length - end_text length)
Don't consider combining characters  as adding to the string length
Return the truncated string
Return the original string since no truncation was necessary
Count non-HTML chars/words and keep note of open tags
Checked through whole string
It's an actual non-HTML word or char  Check for tag
Don't worry about non tags or tags after our truncate point
Element names are always case-insensitive  Check for match in open tags list  SGML: An end tag closes, back to the matching start tag,  all unclosed intervening start tags with omitted end tags  Add it to the start of the open tags list
Close any tags still open  Return string
Translators: This string is used as a separator between list elements
From http://www.xhaus.com/alan/python/httpcomp.htmlgzip  Used with permission.
Like compress_string, but for iterators of strings.  Output headers...
note: python<=2.5 doesn't seem to know about pax headers, filter them  Some corrupt tar files seem to produce this  (specifically bad symlinks)
A directory
This code was mostly based on ipaddr-py  Copyright 2007 Google Inc. https://github.com/google/ipaddr-py  Licensed under the Apache License, Version 2.0 (the "License").
This algorithm can only handle fully exploded  IP strings
If needed, unpack the IPv4 and return straight away  - no need in running the rest of the algorithm
Remove leading zeroes
Determine best hextet to compress  Start of a sequence of zeros.  This is the longest sequence of zeros so far.
Compress the most suitable hextet  For zeros at the end of the address.  For zeros at the beginning of the address.
not an ipv4 mapping
already sanitized
We need to have at least one ':'.
We can only have one '::' shortener.
'::' should be encompassed by start, digits or end.
A single colon can neither start nor end an address.
We can never have more than 7 ':' (1::2:3:4:5:6:7:8 is invalid)
If we have no concatenation, we need to have 8 fields with 7 ':'.  We might have an IPv4 mapped address.
Now that we have that all squared away, let's check that each of the  hextets are between 0x0 and 0xFFFF.  If we have an IPv4 mapped address, the IPv4 portion has to  be at the end of the IPv6 portion.  a value error here means that we got a bad hextet,  something like 0xzzzz
We've already got a longhand ip_str.
If there is a ::, we need to expand it with zeroes  to get to 8 hextets - unless there is a dot in the last hextet,  meaning we're doing v4-mapping
Now need to make sure every hextet is 4 lower case characters.  If a hextet is < 4 characters, we've got missing leading 0's.
Convert datetime.date to datetime.datetime for comparison.
Deal with leapyears by subtracing the number of leapdays
ignore microseconds  d is in the future compared to now, stop processing.  Now get the second item
Split the color configuration into parts  A default palette has been specified  Process a palette defining string
Break the definition into the role,  plus the list of specific instructions.  The role must be in upper case
The first instruction can contain a slash  to break apart fg/bg.
All remaining instructions are options
The nocolor palette has all available roles.  Use that palette as the basis for determining  if the role is valid.
If there are no colors specified, return the empty palette.
'obj' can be a class or a function. If 'obj' is a function at the time it  is passed to _dec,  it will eventually be a method of the class it is  defined on. If 'obj' is a class, the 'name' is required to be the name  of the method that will be decorated.
bound_func has the signature that 'decorator' expects i.e.  no  'self' argument, but it is a closure over self so it can call  'func' correctly.  In case 'decorator' adds attributes to the function it decorates, we  want to copy those. We don't have access to bound_func in this scope,  but we can cheat by using it on a dummy function.
Need to preserve any existing attributes of 'func', including the name.
Don't worry about making _dec look similar to a list/tuple as it's rather  meaningless.  Change the name to aid debugging.
Defer running of process_response until after the template  has been rendered:
emails.Util.parsedate does the job for RFC1123 dates; unfortunately  RFC7231 makes it mandatory to support RFC850 dates too. So we roll  our own RFC-compliant parsing.
To prevent overconsumption of server resources, reject any  base36 string that is long than 13 base36 digits (13 digits  is sufficient to base36-encode any 64-bit integer)  ... then do a final check that the value will fit into an int to avoid  returning a long (15067). The long type was removed in Python 3.
etag_str has wrong format, treat it as an opaque string then
Chrome treats \ completely as / in paths but it could be part of some  basic auth credentials so we need to check both URLs.
Chrome considers any URL with more than two slashes to be absolute, but  urlparse is not so flexible. Treat any url with three slashes as unsafe.  Forbid URLs like http:///example.com - with a scheme, but without a hostname.  In that URL, example.com is not the hostname but, a path component. However,  Chrome will still consider example.com to be the hostname, so we must not  allow this syntax.  Forbid URLs that start with control characters. Some browsers (like  Chrome) ignore quite a few control characters at the start of a  URL and might consider the URL as scheme relative.
Handle case of a control-name with no equal sign
The Trans class is no more needed, so remove it from the namespace.
String doesn't contain a placeholder for the number
Translations are cached in a dictionary for every language.  The active translations are stored by threadid to make them thread local.
The default translation is based on the settings file.
magic gettext number to separate context from message
Get correct locale for sr-latn
A module-level cache is used for caching 'django' translations
default lang should have at least one translation file available.
No catalogs found for this language, set an empty catalog.
Don't set a fallback for the default language or any English variant  (as it's empty, so it'll ALWAYS fall back to the default language)  Get from cache
Take plural and _info from first catalog found (generally Django's).
If we don't have a real translation object, assume it's the default language.
str() is allowing a bytestring message to remain bytestring on Python 2
Returns an empty value of the corresponding type if an empty message  is given, instead of metadata, which is the default gettext behavior.
Translation not found  force unicode, because lazy version expects unicode
Translation not found
First, a quick check to make sure lang_code is well-formed (21458)
If 'fr-ca' is not supported, try special fallback or language-only 'fr'.
if fr-fr is not supported, try fr-ca.
Adding the u prefix allows gettext to recognize the Unicode string  (26093).
Handle comment tokens (`{ ... }`) plus other constructs on  the same line:
A context is provided
A context is provided
This library does not support strftime's "%s" or "%y" format strings.  Allowed if there's an even number of "%"s because they are escaped.
Also finds overlaps
For every non-leap year century, advance by  6 years to get into the 28-year repeat cycle
Move to around the year 2000
You can't trivially replace this with `functools.partial` because this binds  to classes and returns bound instances, whereas functools.partial (on  CPython) is a type and its instances don't bind.
All __promise__ return the same wrapper method, they  look up the correct implementation when called.
Builds a wrapper around some magic method  Automatically triggers the evaluation of a lazy value and  applies the given magic method of the result type.
object defines __str__(), so __prepare_class__() won't overload  a __str__() method from the proxied class.
Instances of this class are effectively immutable. It's just a  collection of functions. So we don't need to do anything  complicated for copying.
Creates the proxy object, instead of the actual value.
Avoid infinite recursion when tracing __init__ (19456).
Note: if a subclass overrides __init__(), it will likely need to  override __copy__() and __deepcopy__() as well.
Assign to __dict__ to avoid infinite __setattr__ loops.
Because we have messed with __class__ below, we confuse pickle as to what  class we are pickling. We're going to have to initialize the wrapped  object to successfully pickle it, so we might as well just pickle the  wrapped object since they're supposed to act the same way.  Unfortunately, if we try to simply act like the wrapped object, the ruse  will break down when pickle gets our id(). Thus we end up with pickle  thinking, in effect, that we are a distinct object from the wrapped  object, but with the same __dict__. This can cause problems (see 25389).  So instead, we define our own __reduce__ method and custom unpickler. We  pickle the wrapped object as the unpickler's argument, so that pickle  will pickle it normally, and then the unpickler simply returns its  argument.
We have to explicitly override __getstate__ so that older versions of  pickle don't try to pickle the __dict__ (which in the case of a  SimpleLazyObject may contain a lambda). The value will end up being  ignored by our __reduce__ and custom unpickler.
If uninitialized, copy the wrapper. Use type(self), not  self.__class__, because the latter is proxied.
If initialized, return a copy of the wrapped object.
We have to use type(self), not self.__class__, because the  latter is proxied.
Introspection support
Need to pretend to be the wrapped class, for the sake of objects that  care about this (especially in equality tests)
List/Tuple/Dictionary methods support
Return a meaningful representation of the lazy object for debugging  without evaluating the wrapped object.
If uninitialized, copy the wrapper. Use SimpleLazyObject, not  self.__class__, because the latter is proxied.
If initialized, return a copy of the wrapped object.
We have to use SimpleLazyObject, not self.__class__, because the  latter is proxied.
This import does nothing, but it's necessary to avoid some race conditions  in the threading module. See http://code.djangoproject.com/ticket/2330 .
Test whether inotify is enabled and likely to work
N.B. ``list(...)`` is needed, because this runs in parallel with  application code which might be mutating ``sys.modules``, and this will  fail with RuntimeError: cannot mutate dictionary while iterating  No changes in module list, short-circuit the function
Add the names of the .mo files that can be generated  by compilemessages management command to the list of files watched.
No need to update watches when request serves files.  (sender is supposed to be a django.core.handlers.BaseHandler subclass)
New modules may get imported when a request is processed.
Block until an event happens.
If we are here the code must have changed.
get the filename from the last item in the stack
Use the system PRNG if possible
We need to generate a derived key from our base key.  We can do this by  passing the key_salt and our base key through a pseudo-random function and  SHA1 works nicely.
If len(key_salt + secret) > sha_constructor().block_size, the above  line is redundant and could be replaced by key = key_salt + secret, since  the hmac module does the same thing for keys longer than the block size.  However, we need to ensure that we *always* do this.
This is ugly, and a hack, but it makes things better than  the alternative of predictability. This re-seeds the PRNG  using a value that is hard for an attacker to predict, every  time a random string is required. This may change the  properties of the chosen random sequence slightly, but this  is better than absolute predictability.
Prefer the stdlib implementation, when available.
Define the new method if missing and complain about it
Define the old method as a wrapped call to the new method.
Originally from https://bitbucket.org/ned/jslex
slash will mean division
C doesn't grok regexes, and they aren't needed for gettext,  so just output a string instead.
C doesn't have single-quoted strings, so make all strings  double-quoted.
C can't deal with Unicode escapes in identifiers.  We don't  need them for gettext anyway, so replace them with something  innocuous
no caching, just do a statistics update after a successful call
simple caching without ordering or size limit
size limited caching that tracks accesses by recency  record recent use of the key by moving it to the front of the list  getting here means that this same key was added to the  cache while the lock was released.  since the link  update is already done, we need only return the  computed result and update the count of misses.  use the old root to store the new key and result  empty the oldest link and make it the new root  now update the cache dictionary for the new links  put result in a new link at the front of the list
We capture the arguments to make returning them trivial
Python 2/fallback version  Make sure it's actually there and not an inner class
backwards compatibility for Python 2
backwards compatibility for Python 2
If a URLRegexResolver doesn't have a namespace or app_name, it passes  in an empty value.
A class-based view
A function-based view
Build a namespaced resolver for the given parent URLconf pattern.  This makes it possible to have captured parameters in the parent  URLconf pattern.
regex is either a string representing a regular expression, or a  translatable string (using ugettext_lazy) representing a regular  expression.
If there are any named groups, use those as kwargs, ignoring  non-named groups. Otherwise, pass all non-named arguments as  positional arguments.  In both cases, pass any extra_kwargs as **kwargs.
Python 3.5 collapses nested partials, so can change "while" to "if"  when it's the minimum supported version.
urlconf_name is the dotted Python path to the module defining  urlpatterns. It may also be an object with an urlpatterns attribute  or urlpatterns itself.  set of dotted paths to all functions and classes that are used in  urlpatterns
Don't bother to output the whole list, it can be huge
Merge captured arguments in match with submatch
If there are *any* named groups, ignore all non-named groups.  Otherwise, pass all non-named arguments as positional arguments.
urlconf_module might be a valid set of patterns, so we default to it
No handler specified in file; use lazy import, since  django.conf.urls imports this file.
WSGI provides decoded URLs, without %xx escapes, and the URL  resolver operates on such URLs. First substitute arguments  without quoting to build a decoded URL and look for a match.  Then, if we have a match, redo the substitution with quoted  arguments in order to return a properly encoded URL.  safe characters from `pchar` definition of RFC 3986  Don't allow construction of scheme relative urls.  lookup_view can be URL name or callable, but callables are not  friendly in error messages.
SCRIPT_NAME prefixes for each thread are stored here. If there's no entry for  the current thread (which is the only one we ever access), it is assumed to  be empty.
Overridden URLconfs for each thread are stored here.
Lookup the name to see if it could be an app identifier.  Yes! Path part matches an app in the current Resolver.  If we are reversing for a particular app, use that  namespace.  The name isn't shared by one of the instances (i.e.,  the default) so pick the first instance as the default.
Convert 'django.views.news.stories.story_detail' to  ['django.views.news.stories', 'story_detail']
If it is a model class or anything else with ._default_manager
If it's a model, use get_absolute_url()
Expand the lazy instance, as it can cause issues when it is passed  further to some Python functions like urlparse.
Handle relative URLs
Next try a reverse URL resolution.  If this is a callable, re-raise.  If this doesn't "feel" like a URL, re-raise.
Finally, fall back and assume it's a URL
Parse the header to get the boundary to split the parts.
Content-Length should contain the length of the body we are about  to receive.
This means we shouldn't continue...raise an error.
For compatibility with low-level network APIs (with 32-bit integers),  the chunk size should be < 2^31, but still divisible by 4.
We have to import QueryDict down here to avoid a circular import.
HTTP spec says that Content-Length >= 0 is valid  handling content-length == 0 before continuing
See if any of the handlers take care of the parsing.  This allows overriding everything if need be.  Check to see if it was handled
Create the data structures to be used later.
Instantiate the parser and stream:
Whether or not to signal a file-completion at the beginning of the loop.
Number of bytes that have been read.  To count the number of keys in the request.  To limit the amount of data read from the request.
We run this at the beginning of the next loop  since we cannot be sure a file is complete until  we hit the next boundary/part of the multipart content.
Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.
Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.
This is a post field, we can just set it in the post
Add two here to make the check consistent with the  x-www-form-urlencoded check that includes '&='.
This is a file, use the handler...
Since this is only a chunk, any error is an unfixable error.
If the chunk received by the handler is None, then don't continue.
Just use up the rest of this file...
Handle file upload completions on next iteration.
If this is neither a FIELD or a FILE, just exhaust the stream.
Make sure that the request data is all fed
Signal that the upload has completed.
If it returns a file object, then set the files dict.
Free up all file handles.  FIXME: this currently assumes that upload handlers store the file as 'file'  We should document that... (Maybe add handler.free_file to complement new_file)
do the whole thing in one shot if no limit was provided.
otherwise do some bookkeeping to return exactly enough  of the stream and stashing any extra content we get from  the producer
rollback an additional six bytes because the format is like  this: CRLF<boundary>[--CRLF]
Try to use mx fast string search if available. Otherwise  use Python find. Wrap the latter for consistency.
There's nothing left, we should just return and mark as done.
backup over CRLF
Stream at beginning of header, look for end of header  and parse it if found. The header must fit within one  chunk.
'find' returns the top of these four bytes, so we'll  need to munch them later to prevent them from polluting  the payload.
we find no header, so we just mark this fact and pass on  the stream verbatim
here we place any excess chunk back onto the stream, as  well as throwing away the CRLFCRLF bytes from above.
Eliminate blank lines  This terminology ("main value" and "dictionary of  parameters") is from the Python docs.
Iterate over each part
Lang/encoding embedded in the value (like "filename*=UTF-8''file.ext")  http://tools.ietf.org/html/rfc2231section-4
http://bugs.python.org/issue2193 is fixed in Python 3.3+.
Cookie pickling bug is fixed in Python 2.7.9 and Python 3.4.3+  http://bugs.python.org/issue22775
Apply the fix from http://bugs.python.org/issue22775 where  it's not fixed in Python itself  allow assignment of constructed Morsels (e.g. for pickling)
override private __set() method:  (needed for using our Morsel, and for laxness with CookieError
Assume an empty name per  https://bugzilla.mozilla.org/show_bug.cgi?id=169091
unquote using Python's algorithm.
_headers is a mapping of the lower-case name to the original case of  the header (required for working with legacy systems) and the header  value. Both the name of the header and its value are ASCII strings.  This parameter is set by the handler. It's necessary to preserve the  historical behavior of request_finished.
Leave self._reason_phrase unset in order to use the default  reason phrase for status code.
Extract the charset and strip its double quotes
Ensure string is valid in given charset
Convert bytestring using given charset
Ensure string is valid in given charset
Convert unicode string to given charset
Wrapping in str() is a workaround for 12422 under Python 2.
Add one second so the date matches exactly (a fraction of  time gets lost between converting to a timedelta and  then the date string).  Just set max_age - the max_age logic will set expires.
IE requires expires, so set it if hasn't been already.
Handle string types -- we can't rely on force_bytes here because:  - under Python 3 it attempts str conversion first  - when self._charset != 'utf-8' it re-encodes the content
Handle non-string types (16494)
The WSGI server must call this method upon completion of the request.  See http://blog.dscpl.com.au/2012/10/obligations-for-calling-close-on.html
Content is a bytestring. See the `content` property methods.
Consume iterators upon assignment to allow repeated iteration.  Create a list of properly encoded bytestrings to support write().
`streaming_content` should be an iterable of bytestrings.  See the `streaming_content` property methods.
Ensure we can never iterate on "value" more than once.
The encoding used in GET/POST dicts. None means use default setting.
We try three options, in order of decreasing preference.  Reconstruct the host using the algorithm from PEP 333.
There is no hostname validation when DEBUG=True
RFC 3986 requires query string arguments to be in the ASCII range.  Rather than crash if this doesn't happen, we encode defensively.
Make it an absolute url (but schemeless and domainless) for the  edge case that the path starts with '//'.
Join the constructed URL with the provided location, which will  allow the provided ``location`` to apply query strings to the  base path as well as override the host, if it begins with //
If there are no upload handlers defined, initialize them from settings.
Limit the maximum request data size that will be handled in-memory.
Use already read data
An error occurred while parsing POST data. Since when  formatting the error the request handler might access  self.POST, set self._post and self._file to prevent  attempts to parse POST data again.  Mark that an error occurred. This allows self.__repr__ to  be explicit about it instead of simply representing an  empty POST
These are both reset in __init__, but is specified here at the class  level so that unpickling will have valid values
query_string normally contains URL-encoded data, a subset of ASCII.  ... but some user agents are misbehaving :-(
It's an IPv6 address without a port.
List of browsers to dynamically create test classes for.  Sentinel value to differentiate browser-specific instances.
If the test class is either browser-specific or a test base, return it.  Reuse the created test class to make it browser-specific.  We can't rename it to include the browser name or create a  subclass like we do with the remaining browsers as it would  either duplicate tests or prevent pickling of its instances.  Create subclasses for each of the remaining browsers and expose  them through the test's module namespace.  If no browsers were specified, skip this class (it'll still be discovered).
quit() the WebDriver before attempting to terminate and join the  single-threaded LiveServerThread to avoid a dead lock if the browser  kept a connection alive.
If tblib isn't installed, pickling the traceback will always fail.  However we don't want tblib to be required for running the tests  when they pass or fail as expected. Drop the traceback when an  expected failure occurs.
The current implementation of the parallel test runner requires  multiprocessing to start subprocesses with fork().  On Python 3.4+: if multiprocessing.get_start_method() != 'fork':
connection.settings_dict must be updated in place for changes to be  reflected in django.db.connections. If the following line assigned  connection.settings_dict = settings_dict, new threads would connect  to the default database instead of the appropriate clone.
In case someone wants to modify these in a subclass.
if a module, or "module.ClassName[.method_name]", just run those  Try to be a bit smarter than unittest about finding the  default top-level for a given directory path, to avoid  breaking relative imports. (Unittest's default is to set  top-level equal to the path, which means relative imports  will result in "Attempted relative import in non-package.").
__init__.py all the way down? give up.
Try discovery if path is a package or directory
Make unittest forget the top-level dir it calculated from this  run, to support running tests from two different top-levels.
Since tests are distributed across processes on a per-TestCase  basis, there's no need for more processes than TestCases.
If there's only one TestCase, parallelization isn't needed.
Maps db signature to dependencies of all it's aliases
sanity check - no DB can depend on its own alias
Try to find a DB that has all it's dependencies met
If the database is marked as a test mirror, save the alias.
Store a tuple with DB parameters that uniquely identify it.  If we have two aliases with the same values for that tuple,  we only need to create the test database once.
Actually create the database for the first connection  Configure all other connections as mirrors of the first one
Configure the test mirrors.
removing last children if it is only whitespace  this can result in incorrect dom representations since  whitespace between inline tags like <span> is significant
attributes without a value is same as attribute with value that  equals the attributes name:  <input checked> == <input checked="checked">
child is text content and element is also text content, then  make a simple "text" in "text"
Special case handling of 'class' attribute, so that comparisons of DOM  instances are not sensitive to ordering of classes.
Removing ROOT element if it's not necessary
To simplify Django's test suite; not meant as a public API
Settings that may not work well when using 'override_settings' (19031)
Rebuild any AppDirectoriesFinder instance.  Rebuild management commands cache  Rebuild get_app_template_dirs cache.  Rebuild translations cache.
Reset process time zone
Reset local time zone cache
Reset the database connections' time zone
Considering the current implementation of the signals framework,  stacklevel=5 shows the line containing the override_settings call.
Set up middleware if needed. We couldn't do this earlier, because  settings weren't available.
sneaky little hack so that we can easily get round  CsrfViewMiddleware.  This makes life easier, and is probably  required for backwards compatibility with external tests against  admin views.
Request goes through middleware.
Simulate behaviors of most Web servers.
Attach the originating request to the response so that it could be  later retrieved.
We're emulating a WSGI server; we must call the close method  on completion.
Not by any means perfect, but good enough for our purposes.
Each bit of the multipart form data could be either a form value or a  file, or a *list* of form values and/or files. Remember that HTTP field  names can be duplicated!
This is a minimal valid WSGI environ dictionary, plus:  - HTTP_COOKIE: for cookie support,  - REMOTE_ADDR: often useful, see 8551.  See http://www.python.org/dev/peps/pep-3333/environ-variables
Encode the content so that the byte representation is correct.
If there are parameters, add them  Under Python 3, non-ASCII values in the WSGI environ are arbitrarily  decoded with ISO-8859-1. We replicate this behavior here.  Refs comment in `get_bytes_from_wsgi()`.
If QUERY_STRING is absent or empty, we want to extract it from the URL.  WSGI requires latin-1 encoded strings. See get_path_info().
Curry a data dictionary into an instance of the template renderer  callback function.  Capture exceptions created by the handler.  If the view raises an exception, Django will attempt to show  the 500.html template. If that template is not available,  we should ignore the error in favor of re-raising the  underlying exception that caused the 500 error. Any other  template found to be missing during view error handling  should be reported as-is.
Look for a signalled exception, clear the current context  exception data, then re-raise the signalled exception.  Also make sure that the signalled exception is cleared from  the local cache!
Save the client and request that stimulated the response.
Add any rendered template detail to the response.
Attach the ResolverMatch instance to the response
Flatten a single context. Not really necessary anymore thanks to  the __getattr__ flattening in ContextList, but has some edge-case  backwards-compatibility implications.
Update persistent cookie data.
Create a fake request to store login details.
Save the session values.
Set the cookie to represent the session.
Prepend the request path to handle relative path redirects
Check that we're not redirecting to somewhere we've already  been to, to prevent loops.
Such a lengthy chain likely also means a loop, but one with  a growing path, changing view, or changing query argument;  20 is the value of "network.http.redirection-limit" from Firefox.
Allow for Python 2.5 relative paths
Keep this code at the beginning to leave the settings unchanged  in case it raises an exception because INSTALLED_APPS is invalid.
Duplicate dict to prevent subclasses from altering their parent.
Hack used when instantiating from SimpleTestCase.setUpClass.
Duplicate list to prevent subclasses from altering their parent.
When called from SimpleTestCase.setUpClass, values may be  overridden several times; cumulate changes.
items my be a single value or an iterable.
If the string is not a complete xml document, we may need to add a  root element. This allow us to compare fragments, like "<foo/><bar/>"
Parse the want and got strings, and compare the parsings.
call test code that consumes from sys.stdin
The class we'll use for the test client self.client.  Can be overridden in derived classes.
Tests shouldn't be allowed to query the database since  this base class doesn't enforce any isolation.
The request was a followed redirect
Not a followed redirect
Prepend the request path to handle relative path redirects.
Get the redirection page, using the same client that was used  to obtain the original response.
For temporary backwards compatibility, try to compare with a relative url
If the response supports deferred rendering and hasn't been rendered  yet, then ensure that it does get rendered before proceeding further.
Put context(s) into a list to simplify processing.
Put error(s) into a list to simplify processing.
Search all contexts for the error.
Add punctuation to msg_prefix
Put context(s) into a list to simplify processing.
Put error(s) into a list to simplify processing.
Search all contexts for the error.
use this template with context manager
Use assertTemplateUsed as context manager.
Use assertTemplateNotUsed as context manager.
callable_obj was a documented kwarg in Django 1.8 and older.
Assertion used in context manager fashion.  Assertion was passed a callable.
test valid inputs  test invalid inputs
test required inputs  test that max_length and min_length are always accepted
Subclasses can ask for resetting of auto increment sequence before each  test case
Subclasses can enable only a subset of apps for faster tests
Subclasses can define fixtures which will be automatically installed.
If transactions aren't available, Django will serialize the database  contents into a fixture during setup and flush and reload them  during teardown (as flush does not restore data from migrations).  This can be slow; this flag allows enabling on a per-case basis.
Since tests will be wrapped in a transaction, or serialized if they  are not available, we allow queries to be run.
If the test case has a multi_db=True flag, act on all databases,  including mirrors or not. Otherwise, just on the default DB.
Reset sequences
If we need to provide replica initial data from migrated apps,  then do so.
We have to use this slightly awkward syntax due to the fact  that we're using *args and **kwargs together.
Some DB cursors include SQL statements as part of cursor  creation. If you have a test that does a rollback, the effect  of these statements is lost, which can affect the operation of  tests (e.g., losing a timezone setting causing objects to be  created with the wrong time). To make sure this doesn't  happen, get a clean connection at the start of every test.
Allow TRUNCATE ... CASCADE and don't emit the post_migrate signal  when flushing only a subset of the apps  Flush the database
rollback to avoid trying to recreate the serialized data.
For example qs.iterator() could be passed as qs, but it does not  have 'ordered' attribute.
If the backend does not support transactions, we should reload  class data before each test
Assume a class is decorated
Emulate behavior of django.contrib.staticfiles.views.serve() when it  invokes staticfiles' finders functionality.  TODO: Modify if/when that internal API is refactored
Override this thread's database connections with the ones  provided by the main thread.
Create the handler for serving static and media files
Go through the list of possible ports, hoping that we can find  one that is free to use for the WSGI server.  This port is already in use, so we go on and try with  the next one in the list.  Either none of the given ports are free or the error  is something else than "Address already in use". So  we let that error bubble up to the main thread.  A free port was found.
Stop the WSGI server
If using in-memory sqlite databases, pass the connections to  the server thread.  Explicitly enable thread-shareability for this connection
Launch the live server's thread
The specified ports may be of the form '8000-8010,8080,9200-9300'  i.e. a comma-separated list of ports or ranges of ports, so we break  it down into a detailed list of all possible ports.  A port range can be of either form: '8000' or '8000-8010'.  Port range of the form '8000'  Port range of the form '8000-8010'
Wait for the live server to be ready  Clean up behind ourselves, since tearDownClass won't get called in  case of errors.
There may not be a 'server_thread' attribute if setUpClass() for some  reasons has raised an exception.  Terminate the live server's thread
Restore sqlite in-memory database connections' non-shareability
Defer saving file-type fields until after the other fields, so a  callable upload_to can use the values from other fields.
Avoid circular import
We check if a string was passed to `fields` or `exclude`,  which is likely to be a mistake where the user typed ('foo') instead  of ('foo',)
If a model is defined, extract form fields from it.
Sentinel for fields_for_model to indicate "get the list of  fields from the model"
make sure opts.fields doesn't specify an invalid field  Override default model fields with any custom declared ones  (plus, include all the other declared fields).
if we didn't get an instance, instantiate a new one
if initial was provided, it should override the values from instance  self._validate_unique will be set to True by BaseModelForm.clean().  It is False by default so overriding self.clean() and failing to call  super will stop validate_unique from being called.  Apply ``limit_choices_to`` to each field.
Build up a list of fields that should be excluded from model field  validation and unique checks.  Exclude fields that aren't on the form. The developer may be  adding these values to the model after form validation.
Don't perform model validation on fields that were defined  manually on the form and excluded via the ModelForm's Meta  class. See 12901.
Exclude fields that failed form validation. There's no need for  the model fields to validate them as well.
Exclude empty fields that are not required by the form, if the  underlying model field is required. This keeps the model field  from raising a required error. Note: don't exclude the field from  validation if the model field allows blanks. If it does, the blank  value may be included in a unique check, so cannot be excluded  from validation.
Override any validation error messages defined at the model level  with those defined at the form level.
Allow the model generated by construct_instance() to raise  ValidationError and have them handled in the same way as others.
Foreign Keys being used to represent inline relationships  are excluded from basic field value validation. This is for two  reasons: firstly, the value may not be supplied (12507; the  case of providing new values to the admin); secondly the  object being referred to may not yet fully exist (12749).  However, these fields *must* be included in uniqueness checks,  so this can't be part of _get_validation_exclusions().
Validate uniqueness if needed.
Note that for historical reasons we want to include also  private_fields here. (GenericRelation was previously a fake  m2m field).
If committing, save the instance and the m2m data immediately.
If not committing, add a method to the form to allow deferred  saving of m2m data.
Build up a list of attributes that the Meta object will have.
If parent form class already has an inner Meta, the Meta we're  creating needs to inherit from the parent's inner meta.  Give this new form class a reasonable name.
Class attributes for the new form class.
Instantiate type(form) in order to use the same metaclass as form.
Set of fields that must be unique among forms of this set.
Set initial values for extra forms
If the queryset isn't already ordered we need to add an  artificial ordering here to make sure that all formsets  constructed from this queryset have the same form order.
Removed queryset limiting here. As per discussion re: 13023  on django-dev, max_num should not prevent existing  related objects/inlines from being displayed.
Collect unique_checks and date_checks to run from all the forms.
Do each of the unique checks (unique and unique_together)  Get the data for the set of fields that must be unique among the forms.  Reduce Model instances to their primary key values  if we've already seen it then we have a uniqueness failure  poke error messages into the right places and mark  the form as invalid  remove the data from the cleaned_data dict since it was invalid  mark the data as seen  iterate over each of the date checks now  see if we have data for both fields  if it's a date lookup we need to get the data for all the fields  otherwise it's just the attribute on the date/datetime  object  if we've already seen it then we have a uniqueness failure  poke error messages into the right places and mark  the form as invalid  remove the data from the cleaned_data dict since it was invalid  mark the data as seen
If the pk is None, it means that the object can't be  deleted again. Possible reason for this is that the  object was already deleted from the DB. Refs 14877.
If someone has marked an add form for deletion, don't save the  object.
If we're adding the related instance, ignore its primary key  as it could be an auto-generated default which isn't actually  in the database.
Add the generated field to form._meta.fields if it's defined to make  sure validation isn't skipped on that field.
Remove the primary key from the form's data, we are only  creating new instances
Remove the foreign key from the form's data
Set the fk value here so that the form can do its validation.
Ensure the latest copy of the related instance is present on each  form (it may have been saved after the formset was originally  instantiated).  Use commit=False so we can assign the parent key afterwards, then  save the object.  form.save_m2m() can be called via the formset later on if commit=False
The foreign key field might not be on the form, so we poke at the  Model field to get the label, since we need that for error messages.
If we're adding a new object, ignore a parent's auto-generated key  as it will be regenerated on the save request.
avoid circular import  Try to discover what the ForeignKey from model to parent_model is
enforce a max_num=1 when the foreign key to the parent model is unique.
if there is no value act as we did before.  ensure the we compare the values as equal types.
Can't use iterator() when queryset uses prefetch_related()
This class is a subclass of ChoiceField for purity, but it doesn't  actually use any of ChoiceField's implementation.
Call Field instead of ChoiceField __init__() because we don't need  ChoiceField.__init__().
Need to force a new ModelChoiceIterator to be created, bug 11183
If self._choices is set, then somebody must have manually set  the property self.choices. In this case, just return self._choices.
Otherwise, execute the QuerySet in self.queryset to determine the  choices dynamically. Return a fresh ModelChoiceIterator that has not been  consumed. Note that we're instantiating a new ModelChoiceIterator *each*  time _get_choices() is called (and, thus, each time self.choices is  accessed) so that we can ensure the QuerySet has not been consumed. This  construct might look complicated but it allows for lazy evaluation of  the queryset.
Since this overrides the inherited ModelChoiceField.clean  we run custom validators here
deduplicate given values to avoid creating many querysets or  requiring the database backend deduplicate efficiently.  list of lists isn't hashable, for example
special field names
default minimum number of forms in a formset
default maximum number of forms in a formset, to prevent memory exhaustion
MIN_NUM_FORM_COUNT and MAX_NUM_FORM_COUNT are output with the rest of  the management form, but only for the convenience of client-side  code. The POST value of them returned from the client is not checked.
return absolute_max if it is lower than the actual total form  count in the data; this is DoS protection to prevent clients  from forcing the server to instantiate arbitrary numbers of  forms
Allow all existing related objects/inlines to be displayed,  but don't allow extra beyond max_num.
Use the length of the initial data if it's there, 0 otherwise.
DoS protection is included in total_form_count()
Don't render the HTML 'required' attribute as it may cause  incorrect validation for extra, optional, and deleted  forms in the formset.
Allow extra forms to be empty, unless they're part of  the minimum forms.
construct _deleted_form_indexes which is just a list of form indexes  that have had their deletion widget set to True  if this is an extra form and hasn't changed, don't consider it
Construct _ordering, which is a list of (form_index, order_field_value)  tuples. After constructing this list, we'll sort it by order_field_value  so we have a way to get to the form indexes in the order specified  by the form data.  if this is an extra form and hasn't changed, don't consider it  don't add data marked for deletion to self.ordered_data  After we're done populating self._ordering, sort it.  A sort function to order things numerically ascending, but  None should be sorted below anything else. Allowing None as  a comparison value makes it so we can leave ordering fields  blank.
Return a list of form.cleaned_data dicts in the order specified by  the form data.
We loop over every form.errors here rather than short circuiting on the  first failure to make sure validation gets triggered for every form.  This triggers a full clean.  This form is going to be deleted so any of its errors  should not cause the entire formset to be invalid.
Give self.clean() a chance to do cross-form validation.
Only pre-fill the ordering field for initial forms.
All the forms on a FormSet are the same, so you only need to  interrogate the first form for media.
XXX: there is no semantic division between forms here, there  probably should be. It might make sense to render each form as a  table row with each field as a td.
hard limit on forms instantiated, to prevent memory-exhaustion attacks  limit is simply max_num + DEFAULT_MAX_NUM (which is 2*DEFAULT_MAX_NUM  if max_num is None in the first place)
Collect fields from current class.
Walk through the MRO.  Collect fields from base class.
Field shadowing.
This is the main implementation of all the Form logic. Note that this  class is different than Form. See the comments by the Form class for more  information. Any improvements to the form API should be made to *this*  class, not to the Form class.
Translators: This is the default suffix added to form field labels
The base_fields class attribute is the *class-wide* definition of  fields. Because a particular *instance* of the class might want to  alter self.fields, we create self.fields here by copying base_fields.  Instances should always modify self.fields; they should not modify  self.base_fields.
Escape and cache in local variable.  Create a 'class="..."' attribute if the row should have any  CSS classes applied.
Chop off the trailing row_ender (e.g. '</td></tr>') and  insert the hidden fields.  This can happen in the as_p() case (and possibly others  that users write): if there are only top errors, we may  not be able to conscript the last row for our purposes,  so insert a new, empty row.
If there aren't any rows in the output, just append the  hidden fields.
Normalize to ValidationError and let its constructor  do the hard work of making sense of the input.
If the form is permitted to be empty, and none of the form data has  changed from the initial data, short circuit any validation.
Initial values are supposed to be clean  value_from_datadict() gets the data from the data dictionaries.  Each widget type knows how to retrieve its own data, because some  widgets split data over several HTML fields.
Always assume data has changed if validation fails.
To keep rendering order consistent, we can't just iterate over items().  We need to sort the keys, and iterate over the sorted list.
Get the media property of the superclass, if it exists
Get the media definition for this class
Only add the 'value' attribute if a value is non-empty.
An ID attribute was given. Add a numeric index as a suffix  so that the inputs don't all have the same ID attribute.
If the user contradicts themselves (uploads a new file AND  checks the "clear" checkbox), we return a unique marker  object that FileField will turn into a ValidationError.  False signals to clear any existing value, as opposed to just None
Use slightly better defaults than HTML's 20x2 box
Defined at module level so that CheckboxInput is picklable (17976)
check_test is a callable that takes a value and returns True  if the checkbox should be checked for that value.
Only add the 'value' attribute if a value is non-empty.
A missing value means False because HTML form submission does not  send results for unselected checkboxes.
Translate true and false strings to boolean values.
choices can be any iterable, but we may need to render this widget  multiple times. Thus, collapse it into a list so it can be consumed  more than once.
Only allow for a single selection.
Normalize to strings.
Override the default renderer if we were passed one.
Widgets using this RendererMixin are made of a collection of  subwidgets, each with their own <label>, and distinct ID.  The IDs are made distinct by y "_X" suffix, where X is the zero-based  index of the choice field. Thus, the label for the main widget should  reference the first subwidget, hence the "_0" suffix.
value is a list of values, each corresponding to a widget  in self.widgets.
See the comment for RadioSelect.id_for_label()
Optional list or tuple of years to use in the "year" select box.
Optional dict of months to use in the "month" select box.
Optional string, list, or tuple to use as empty_label.
The `list` reduce function returns an iterator as the fourth element  that is normally used for repopulating. Since we only inherit from  `list` for `isinstance` backward compatibility (Refs 17413) we  nullify this iterator as it would otherwise result in duplicate  entries. (Refs 23594)
Add an 'invalid' entry to default_error_message if you want a specific  field error message not raised by the field validators.
Tracks each time a Field instance is created. Used to retain order.
required -- Boolean that specifies whether the field is required.              True by default.  widget -- A Widget class, or instance of a Widget class, that should            be used for this Field when displaying it. Each Field has a            default Widget that it'll use if you don't specify this. In            most cases, the default widget is TextInput.  label -- A verbose name for this field, for use in displaying this           field in a form. By default, Django will use a "pretty"           version of the form field name, if the Field is part of a           Form.  initial -- A value to use in this Field's initial display. This value             is *not* used as a fallback if data isn't given.  help_text -- An optional string to use as "help text" for this Field.  error_messages -- An optional dictionary to override the default                    messages that the field will raise.  show_hidden_initial -- Boolean that specifies if it is needed to render a                         hidden widget with initial value after widget.  validators -- List of additional validators to use  localize -- Boolean that specifies if the field should be localized.  disabled -- Boolean that specifies whether the field is disabled, that              is its widget is shown in the form but not editable.  label_suffix -- Suffix to be added to the label. Overrides                  form's label_suffix.
Trigger the localization machinery if needed.
Let the widget know whether it should display as required.
Hook into self.widget_attrs() for any Field-specific HTML attributes.
Increase the creation counter, and save our local copy.
For purposes of seeing whether something has changed, None is  the same as an empty string, if the data or initial value we get  is None, replace it with ''.
The HTML attribute is maxlength, not max_length.
The HTML attribute is minlength, not min_length.
Localized number input is not well supported on most browsers
Strip trailing decimal and zeros.
Check for NaN (which is the only thing not equal to itself) and +/- infinity
Check for NaN, Inf and -Inf values. We can't compare directly for NaN,  since it is never equal to itself. However, NaN is the only value that  isn't equal to itself, so we can use this to identify NaN
Use exponential notation for small values since they might  be parsed as 0 otherwise. ref 20765
Try to coerce the value to unicode.  If unicode, try to strptime against each input format.
UploadedFile objects should have name and size attributes.
If the widget got contradictory inputs, we raise a validation error  False means the field value should be cleared; further validation is  not needed.  If the field is required, clearing is not possible (the widget  shouldn't return False data in that case anyway). False is not  in self.empty_value; if a False value makes it this far  it should be validated from here on out as None (so it will be  caught by the required check).
We need to get a file object for Pillow. We might have a path or we might  have to read the data into memory.
load() could spot a truncated JPEG, but it loads the entire  image in memory, which is a DoS vector. See 3848 and 18520.  verify() must be called immediately after the constructor.
Annotating so subclasses can reuse it for their own validation  Pillow doesn't detect the MIME type of all formats. In those  cases, content_type will be None.
Pillow doesn't recognize it as an image.
urlparse.urlsplit can raise a ValueError with some  misformatted URLs.
If no URL scheme given, assume http://
Assume that if no domain is provided, that the path segment  contains the domain.  Rebuild the url_fields list, since the domain segment may now  contain the path too.
Explicitly check for the string 'False', which is what a hidden field  will submit for False. Also check for '0', since this is what  RadioSelect will provide. Because bool("True") == bool('1') == True,  we don't need to handle that explicitly.
Sometimes data or initial may be a string equivalent of a boolean  so we should run it through to_python first to get a boolean value
Setting choices also sets the choices on the widget.  choices can be any iterable, but we call list() on it because  it will be consumed more than once.
This is an optgroup, so look inside the group for options
Validate that each value in the value list is in self.choices.
Set 'required' to False on the individual fields, because the  required validation will be handled by ComboField, not by those  individual fields.
Set 'required' to False on the individual fields, because the  required validation will be handled by MultiValueField, not  by those individual fields.
Raise a 'required' error if the MultiValueField is  required and any field is empty.
Otherwise, add an 'incomplete' error to the list of  collected errors and skip field cleaning, if a required  field is empty.
Collect all validation errors in a single list, which we'll  raise at the end of clean(), rather than raising a single  exception for the first error we encounter. Skip duplicates.
Raise a validation error if time or date is empty  (possible if SplitDateTimeField has required=False).
Prevent unnecessary reevaluation when accessing BoundField's attrs  from templates.
If this is an auto-generated default date, nix the  microseconds for standardized handling. See 22502.
Only add the suffix if the label does not end in punctuation.  Translators: If found as last label character, these punctuation  characters will prevent the default label_suffix to be appended to the label
HACK: datetime is an old-style class, create a new-style equivalent  so we can define additional attributes.
Obtain a timezone-aware datetime  Filters must never raise exceptions, and pytz' exceptions inherit  Exception directly, not a specific subclass. So catch everything.
Obtain a tzinfo instance
HACK: the convert_to_local_time flag will prevent        automatic conversion of the value to local time.
token.split_contents() isn't useful here because this tag doesn't accept variable as arguments
``language`` is either a language code string or a sequence  with the language code as its first item
Restore percent signs. Percent signs in template text are doubled  so they are not interpreted as string format flags.
Update() works like a push(), so corresponding context.pop() is at  the end of function
Either string is malformed, or it's a bug
token.split_contents() isn't useful here because this tag doesn't accept variable as arguments
token.split_contents() isn't useful here because this tag doesn't accept variable as arguments
token.split_contents() isn't useful here because this tag doesn't accept variable as arguments
token.split_contents() isn't useful here because tags using this method don't accept variable as arguments
The self-weakref trick is needed to avoid creating a reference  cycle.
A marker for caching
For convenience we create empty caches even if they are not used.  A note about caching: if use_caching is defined, then for each  distinct sender we cache the receivers that sender has in  'sender_receivers_cache'. The cache is cleaned when .connect() or  .disconnect() is called and populated on send().
If DEBUG is on, check that we got a good receiver
Check for **kwargs
Check for bound methods
Call each receiver with whatever arguments it can accept.  Return a list of tuple pairs [(receiver, response), ... ].
Note: caller is assumed to hold self.lock.
We could end up here with NO_RECEIVERS even if we do check this case in  .send() prior to calling _live_receivers() due to concurrent .send() call.
Note, we must cache the weakref versions.
Dereference the weak reference.
Mark that the self.receivers list has dead weakrefs. If so, we will  clean those up in connect, disconnect and _live_receivers while  holding self.lock. Note that doing the cleanup here isn't a good  idea, _remove_receiver() will be called as side effect of garbage  collection, and so the call can happen while we are already holding  self.lock.
Register an event to reset saved queries when a Django request is started.
Register an event to reset transaction state and close connections past  their lifetime.
Older code may be expecting FileField values to be simple strings.  By overriding the == operator, it can remain backwards compatibility.
open() doesn't alter the file's contents, but it does reset the pointer
Save the object because it has changed, unless save is False
Only close the file if it's already open, which we know by the  presence of self._file
FieldFile needs access to its associated model field and an instance  it's attached to in order to work properly, but the only necessary  data to be pickled is the file's name itself. Everything else will  be restored later, by FileDescriptor below.
The instance dict contains whatever was originally assigned  in __set__.
If this value is a string (instance.file = "path/to/file") or None  then we simply wrap it with the appropriate attribute class according  to the file field. [This is FieldFile for FileFields and  ImageFieldFile for ImageFields; it's also conceivable that user  subclasses might also want to subclass the attribute class]. This  object understands how to convert a path to a file, and also how to  handle None.
Other types of files may be assigned as well, but they need to have  the FieldFile interface added to them. Thus, we wrap any other type of  File inside a FieldFile (well, the field's attr_class, which is  usually FieldFile).
Finally, because of the (some would say boneheaded) way pickle works,  the underlying FieldFile might not actually itself have an associated  file. So we need to reset the details of the FieldFile in those cases.
Make sure that the instance is correct.
That was fun, wasn't it?
The class to wrap instance attributes in. Accessing the file object off  the instance will always return an instance of attr_class.
The descriptor to use for accessing the attribute off of the class.
Need to convert File objects provided via a form to unicode for database insertion
Commit the file to storage prior to saving the model
Important: None means "no change", other false value means "clear"  This subtle distinction (rather than a more explicit marker) is  needed because we need to consume values that are also sane for a  regular (non Model-) Form to find in its cleaned_data dictionary.  This value will be converted to unicode and stored in the  database, so leaving False as-is is not acceptable.
If a file has been provided previously, then the form doesn't require  that a new file is provided this time.  The code to mark the form field as not required is used by  form_for_instance, but can probably be removed once form_for_instance  is gone. ModelForm uses a different method to check for an existing file.
To prevent recalculating image dimensions when we are instantiating  an object from the database (bug 11084), only update dimensions if  the field had a value before this assignment.  Since the default  value for FileField subclasses is an instance of field.attr_class,  previous_file will only be None when we are called from  Model.__init__().  The ImageField.update_dimension_fields method  hooked up to the post_init signal handles the Model.__init__() cases.  Assignment happening outside of Model.__init__() will trigger the  update right here.
Clear the image dimensions cache
Attach update_dimension_fields so that dimension fields declared  after their corresponding image field don't stay cleared by  Model.__init__, see bug 11196.  Only run post-initialization dimension update on non-abstract models
Nothing to update if the field doesn't have dimension fields.
getattr will call the ImageFileDescriptor's __get__ method, which  coerces the assigned value into an instance of self.attr_class  (ImageFieldFile in this case).
Nothing to update if we have no file and not being forced to update.
When both dimension fields have values, we are most likely loading  data from the database or updating an image field that already had  an image stored.  In the first case, we don't want to update the  dimension fields because we are already getting their values from the  database.  In the second case, we do want to update the dimensions  fields and will skip this return because force will be True since we  were called from ImageFileDescriptor.__set__.
file should be an instance of ImageFieldFile or should be None.  No file, so clear dimensions fields.
Update the width and height fields.
-*- coding: utf-8 -*-
Avoid "TypeError: Item in ``from list'' not a string" -- unicode_literals  makes these strings unicode
The values to use for "blank" in SelectFields. Will be appended to the start  of most "choices" lists.
Designates whether empty strings fundamentally are allowed at the  database level.
These track each time a Field instance is created. Used to retain order.  The auto_creation_counter is used for fields that Django implicitly  creates, creation_counter is used for all user-specified fields.
Translators: The 'lookup_type' is one of 'date', 'year' or 'month'.  Eg: "Title must be unique for pub_date year"
Field flags
Generic field type description, usually overridden by subclasses
Adjust the appropriate creation counter, and save our local copy.
We cannot reliably check this for backends like Oracle which  consider NULL and '' to be equal (and thus set up  character-based fields a little differently).
Short-form way of fetching all the default parameters  Unroll anything iterable for choices into a concrete list  Do correct kind of comparison  Work out path - we shorten it for known Django core fields  Return basic info - other fields should override this.
Needed for @total_ordering
This is needed because bisect does not take a comparison function.
We don't have to deepcopy very much here, since most things are not  intended to be altered after initial creation.
We need to avoid hitting __reduce__, so define this  slightly weird copy construct.
Fields are sometimes used without attaching them to models (for  example in aggregation). In this case give back a plain field  instance. The code below will create a new empty instance of  class self.__class__, then update its dict with self.__dict__  values - so, this is very close to normal pickle.
Skip validation for non-editable fields.
This is an optgroup, so look inside the group for  options.
The default implementation of this method looks at the  backend-specific data_types dictionary, looking up the field by its  "internal type".  A Field class can implement the get_internal_type() method to specify  which *preexisting* Django Field class it's most similar to -- i.e.,  a custom field might be represented by a TEXT column type, which is  the same as the TextField Django field type, which means the custom  field's get_internal_type() returns 'TextField'.  But the limitation of the get_internal_type() / data_types approach  is that it cannot handle database column types that aren't already  mapped to one of the built-in Django field types. In this case, you  can implement db_type() instead of get_internal_type() to specify  exactly which wacky database column type you want to use.
Don't override classmethods with the descriptor. This means that  if you have a classmethod and a field with the same name, then  such fields can't be deferred (we don't have a check for this).
Fields with choices get special treatment.  Many of the subclass-specific formfield arguments (min_value,  max_value) don't apply for choice fields, so be sure to only pass  the values that TypedChoiceField will understand.
if value is 1 or 0 than it's equal to True or False, but we want  to return a true bool for semantic reasons.
Unlike most fields, BooleanField figures out include_blank from  self.null instead of self.blank.
Passing max_length to forms.CharField means that the value's length  will be validated twice. This is considered acceptable since we want  the value in the form field (to pass into widget for example).
auto_now, auto_now_add, and default are mutually exclusive  options. The use of more than one of these options together  will trigger an Error
Nothing to do, as dates don't have tz information
No explicit date / datetime value -- no checks necessary
Convert aware datetimes to the default time zone  before casting them to dates (17742).
Casts dates into the format expected by the backend
No explicit date / datetime value -- no checks necessary
For backwards compatibility, interpret naive datetimes in  local time. This won't work during DST change, but we can't  do much about it, so we let the exceptions percolate up the  call stack.
For backwards compatibility, interpret naive datetimes in local  time. This won't work during DST change, but we can't do much  about it, so we let the exceptions percolate up the call stack.
Casts datetimes into the format expected by the backend
Method moved to django.db.backends.utils.  It is preserved because it is used by the oracle backend  (django.db.backends.oracle.query), and also for  backwards-compatibility with any external code which may have used  this method.
Discard any fractional microseconds due to floating point arithmetic.
max_length=254 to be compliant with RFCs 3696 and 5321
We do not exclude max_length if it matches default as we want to change  the default in future.
As with CharField, this will cause email validation to be performed  twice.
These validators can't be added at field initialization time since  they're based on values retrieved from `connection`.
Set db_index=True unless it's been set manually.
Passing max_length to forms.CharField means that the value's length  will be validated twice. This is considered acceptable since we want  the value in the form field (to pass into widget for example).
No explicit time / datetime value -- no checks necessary
Not usually a good idea to pass in a datetime here (it loses  information), but this can be a side-effect of interacting with a  database backend (e.g. Oracle), so we'll be accommodating.
Casts times into the format expected by the backend
As with CharField, this will cause URL validation to be performed  twice.
If it's a string, it should be base64-encoded data
The exception can't be created at initialization time since the  related model might not be resolved yet; `rel.model` might still be  a string model reference.
FIXME: This will need to be revisited when we introduce support for  composite fields. In the meantime we take this practical approach to  solve a regression on 1.6 when the reverse manager in hidden  (related_name ends with a '+'). Refs 21410.  The check for len(...) == 1 is a special case that allows the query  to be join-less and smaller. Refs 21760.
Since we're going to assign directly in the cache,  we must manage the reverse relation cache manually.
The related instance is loaded from the database and then cached in  the attribute defined in self.cache_name. It can also be pre-cached  by the reverse accessor (ReverseOneToOneDescriptor).  Assuming the database enforces foreign keys, this won't fail.  If this is a one-to-one relation, set the reverse accessor  cache on the related object to the current instance to avoid  an extra SQL query if it's accessed later on.
An object must be an instance of the related class.
If we're setting the value of a OneToOneField to None, we need to clear  out the cache on any old related object. Otherwise, deleting the  previously-related object will also cause this object to be deleted,  which is wrong.  Look up the previously-related object, which may still be available  since we've not yet cleared out the related field.  Use the cache directly, instead of the accessor; if we haven't  populated the cache, then we don't care - we're only accessing  the object to invalidate the accessor cache, so there's no  need to populate the cache just to expire it again.
If we've got an old related object, we need to clear out its  cache. This cache also might not exist if the related object  hasn't been accessed yet.
Set the values of the related field.
Set the related instance cache used by __get__ to avoid an SQL query  when accessing the attribute we just set.
If this is a one-to-one relation, set the reverse accessor cache on  the related object to the current instance to avoid an extra SQL  query if it's accessed later on.
The exception isn't created at initialization time for the sake of  consistency with `ForwardManyToOneDescriptor`.
Since we're going to assign directly in the cache,  we must manage the reverse relation cache manually.
The related instance is loaded from the database and then cached in  the attribute defined in self.cache_name. It can also be pre-cached  by the forward accessor (ForwardManyToOneDescriptor).  Set the forward accessor cache on the related object to  the current instance to avoid an extra SQL query if it's  accessed later on.
Update the cached related instance (if any) & clear the cache.
An object must be an instance of the related class.
Set the value of the related field to the value of the related object's related field
Set the related instance cache used by __get__ to avoid an SQL query  when accessing the attribute we just set.
Set the forward accessor cache on the related object to the current  instance to avoid an extra SQL query if it's accessed later on.
We use **kwargs rather than a kwarg argument to enforce the  `manager='manager_name'` syntax.
Since we just bypassed this class' get_queryset(), we must manage  the reverse relation manually.
remove() and clear() are only provided if the ForeignKey can have a value of null.  Is obj actually part of this descriptor set?
`QuerySet.update()` is intrinsically atomic.
Force evaluation of `objs` in case it's a queryset whose value  could be affected by `manager.clear()`. Refs 19816.
through is provided so that you have easy access to the through  model (Book.authors.through) for inlines, etc. This is done as  a property to ensure that the fully resolved value is returned.
Even if this relation is not to pk, we require still pk value.  The wish is that the instance has been already saved to DB,  although having a pk value isn't a guarantee of that.
We use **kwargs rather than a kwarg argument to enforce the  `manager='manager_name'` syntax.
No need to add a subquery condition if removed_vals is a QuerySet without  filters.
For non-autocreated 'through' models, can't assume we are  dealing with PK values.
If this is a symmetrical m2m relation to self, add the mirror entry in the m2m table
Force evaluation of `objs` in case it's a queryset whose value  could be affected by `manager.clear()`. Refs 19816.
This check needs to be done here, since we can't later remove this  from the method lookup table, as we do with add and remove.
We only need to add() if created because if we got an object back  from get() then the relationship already exists.
We only need to add() if created because if we got an object back  from get() then the relationship already exists.
If there aren't any objects, there is nothing to do.
Don't send the signal when we are inserting the  duplicate data row for symmetrical reverse entries.
Add the ones that aren't there already
Don't send the signal when we are inserting the  duplicate data row for symmetrical reverse entries.
source_field_name: the PK colname in join table for the source object  target_field_name: the PK colname in join table for the target object  *objs - objects to remove
Check that all the objects are of the right type
Send a signal to the other end if need be.
Field flags
Reverse relations are always nullable (Django can't enforce that a  foreign key on the related model points to this model).
Some of the following cached_properties can't be initialized in  __init__ as the field doesn't have its model yet. Calling these methods  before field.contribute_to_class() has been called will result in  AttributeError
By default foreign object doesn't relate to any remote field (for  example custom multicolumn joins currently have no remote field).
This method encapsulates the logic that decides what name to give an  accessor descriptor that retrieves related many-to-one or  many-to-many objects. It uses the lower-cased object_name + "_set",  but this can be overridden with the "related_name" option.  Due to backwards compatibility ModelForms need to be able to provide  an alternate model. See BaseInlineFormSet.get_default_prefix().  If this is a symmetrical m2m relation on self, there is no reverse accessor.
A case like Restaurant.objects.filter(place=restaurant_instance),  where place is a OneToOneField and the primary key of Restaurant.
If we get here, we are dealing with single-column relations.  We need to run the related field's get_prep_value(). Consider case  ForeignKey to IntegerField given value 'abc'. The ForeignKey itself  doesn't have validation for non-integers, so we must run validation  using the target field.  Run the target field's get_prep_value. We can safely assume there is  only one as we don't get to the direct value branch otherwise.
For multicolumn lookups we need to build a multicolumn where clause.  This clause is either a SubqueryConstraint (for values that need to be compiled to  SQL) or a OR-combined list of (col1 = val1 AND col2 = val2 AND ...) clauses.
If we get here, we are dealing with single-column relations.  We need to run the related field's get_prep_value(). Consider case  ForeignKey to IntegerField given value 'abc'. The ForeignKey itself  doesn't have validation for non-integers, so we must run validation  using the target field.  Get the target field. We can safely assume there is only one  as we don't get to the direct value branch otherwise.
Check for recursive relations
Look for an "app.Model" relation
Field flags
Can't cache this property until all the models are loaded.
`f.remote_field.model` may be a string instead of a model. Skip if model name is  not resolved.
rel_opts.object_name == "Target"  If the field doesn't install a backward relation on the target model  (so `is_hidden` returns True), then there are no clashes to check  and we can skip these fields.
Check clashes between accessor or reverse query name of `field`  and any other field name -- i.e. accessor for Model.foreign is  model_set and it clashes with Target.model_set.
Check clashes between accessors/reverse query names of `field` and  any other field accessor -- i. e. Model.foreign accessor clashes with  Model.m2m accessor.
By default related field will not have a column as it relates to  columns from another table.
Work out string form of "to"
If this is a callable, do not invoke it here. Just pass  it in the defaults for when the form class will later be  instantiated.
Field flags
Work out string form of "to"  If swappable is True, then see if we're actually pointing to the target  of a swap.  If it's already a settings reference, error  Set it
Gotcha: in some cases (like fixture loading) a model can have  different values in parent_ptr_id and parent's id. So, use  instance.pk (that is, parent_ptr_id) when asked for instance.id.
Internal FK's - i.e., those with a related name ending with '+' -  and swapped models don't get a related descriptor.  While 'limit_choices_to' might be a callable, simply pass  it along for later - this is too early because it's still  model load time.
Field flags
For backwards compatibility purposes, we need to *try* and set  the to_field during FK construction. It won't be guaranteed to  be correct until contribute_to_class is called. Refs 12190.
Handle the simpler arguments  Rel needs more work.
Field flags
Override ForeignKey since check isn't applicable here.
Construct and return the new class.
Field flags
Class names must be ASCII in Python 2.x, so we forcibly coerce it  here to break early if there's a problem.
The relationship model is not installed.
Set some useful local variables
Check symmetrical attribute.
Count foreign keys in intermediate model
Count foreign keys in relationship model
Validate `through_fields`.  Validate that we're given an iterable of at least two items  and that none of them is "falsy".
Validate the given through fields -- they should be actual  fields on the through model, and also be foreign keys to the  expected models.
Handle the simpler arguments.  Rel needs more work.  If swappable is True, then see if we're actually pointing to the target  of a swap.  If it's already a settings reference, error.
If this is an m2m-intermediate to self,  the first foreign key you find will be  the source column. Keep searching for  the second foreign key.
To support multiple relations to self, it's useful to have a non-None  related name on symmetrical relations for internal reasons. The  concept doesn't make a lot of sense externally ("you want me to  specify *what* on my non-reversible relation?!"), so we set it up  automatically. The funky name reduces the chance of an accidental  clash.  If the backwards relation is disabled, replace the original  related_name with one generated from the m2m field name. Django  still uses backwards relations internally and we need to avoid  clashes between multiple m2m fields with related_name == '+'.
The intermediate m2m model is not auto created if:   1) There is a manually specified intermediate, or   2) The class owning the m2m field is abstract.   3) The class owning the m2m field has been swapped out.
Add the descriptor for the m2m relation.
Set up the accessor for the m2m table name for the relation.
Internal M2Ms (i.e., those with a related name ending with '+')  and swapped models don't get a related descriptor.
Set up the accessors for the column names on the m2m table.
If initial is passed in, it's a list of related objects, but the  MultipleChoiceField takes a list of IDs.
A ManyToManyField is not represented by a single column,  so return None.
base  datetime
Timezone conversions must happen to the input datetime *before*  applying a function. 2015-12-31 23:00:00 -02:00 is stored in the  database as 2016-01-01 01:00:00 +00:00. Any results should be  based on the input datetime not the stored datetime.
resolve_expression has already validated the output_field so this  assert should never be hit.
Passing dates to functions expecting datetimes is most likely a mistake.
Escape any params because trunc_sql will format the string.
DateTimeField is a subclass of DateField so this works for both.  If self.output_field was None, then accessing the field will trigger  the resolver to assign it to self.lhs.output_field.  Passing dates to functions expecting datetimes is most likely a  mistake.
self.output_field is definitely a DateField here.
Cast to date rather than truncate to date.
CAST would be valid too, but the :: shortcut syntax is more readable.
we can't mix TextField (NCLOB) and CharField (NVARCHAR), so convert  all fields to NCLOB when we expect NCLOB
Use CONCAT_WS with an empty separator so that NULLs are ignored.
null on either side results in null for expression, wrap with coalesce
wrap pairs of expressions in successive concat functions  exp = [a, b, c, d]  -> ConcatPair(a, ConcatPair(b, ConcatPair(c, d))))
Postgres' CURRENT_TIMESTAMP means "the time at the start of the  transaction". We use STATEMENT_TIMESTAMP to be cross-compatible with  other databases.
PathInfo is used when converting lookups (fk__somecol). The contents  describe the relation in Model terms (model Options and Fields for both  sides of the relation. The join_field is the field backing the relation.
Connection types
We must promote any new joins to left outer joins so that when Q is  used as an expression, rows aren't filtered due to joins.
self.field_name is the attname of the field, but only() takes the  actual name, so we need to translate it here.  Let's see if the field is part of the parent chain. If so we  might be able to reuse the already loaded value. Refs 18343.
To allow for inheritance, check parent class' class_lookups.
This class didn't have any class_lookups
If the field is a primary key, then doing a query against the field's  model is ok, too. Consider the case:  class Restaurant(models.Model):      place = OnetoOneField(Place, primary_key=True):  Restaurant.objects.filter(pk__in=Restaurant.objects.all()).  If we didn't have the primary key check, then pk__in (== place__in) would  give Place's opts as the target opts, but Restaurant isn't compatible  with that. This logic applies only to primary keys, as when doing __in=qs,  we are going to turn this into __in=qs.values('pk') later on.
Separator used to split filter strings apart.
This partial takes a single optional argument named "sender".  import models here to avoid a circular import  Skip lazy_model_operation to get a return value for disconnect()
Exceptions are special - they've got state that isn't  in self.__dict__. We assume it is all in self.args.
Also ensure initialization is only performed for subclasses of Model  (excluding Model class itself).
Create the class.
Look for an application configuration to attach the model to.
Non-abstract child classes inherit some attributes from their  non-abstract parent (unless an ABC comes before it in the  method resolution order).
If the model is a proxy, ensure that the base class  hasn't been swapped out.
Add all attributes to the class.
All the fields of any type declared on this model
Basic setup for proxy models.
Collect the parent links for multi-table inheritance.  Conceptually equivalent to `if base is Model`.  Skip concrete parent classes.  Locate OneToOneField instances.
Track fields inherited from base models.  Do the appropriate setup for any model parents.  Things without _meta aren't functional models, so they're  uninteresting parents.
Check for clashes between locally declared fields and those  on the base classes.
Concrete classes...
Only add the ptr field if it's not already present;  e.g. migrations will already have it specified
Add fields from abstract base class if it wasn't overridden.  Replace parent links defined on this base by the new  field. It will be appropriately resolved if required.
Pass any non-abstract parent classes onto child.
Inherit private fields (like GenericForeignKey) from the parent  class
Abstract base models can't be instantiated and don't appear in  the list of models for an app. We do the final setup for them a  little differently from normal models.
We should call the contribute_to_class method only if it's bound
Defer creating accessors on the foreign class until it has been  created and registered. If remote_field is None, we're ordering  with respect to a GenericForeignKey and don't know what the  foreign class is - we'll add those accessors later in  contribute_to_class().
Give the class a docstring -- its definition.
Step 1: Locate a manager that would have been promoted  to default manager with the legacy system.
Step 2: Since there are managers but none of them qualified as  default managers under the legacy system (meaning that there are  managers from concrete parents that would be promoted under the  new system), we need to create a new Manager instance for the  'objects' attribute as a deprecation shim.  If the "future" default manager was auto created there is no  point warning the user since it's basically the same manager.
If true, uniqueness validation checks will consider this a new, as-yet-unsaved object.  Necessary for correct validation of new instances of objects with explicit (non-auto) PKs.  This impacts validation only; it has no effect on the actual save.
Set up the storage for instance state
There is a rather weird disparity here; if kwargs, it's set, then args  overrides it. It should be one or the other; don't duplicate the work  The reason for the kwargs check is that standard iterator passes in by  args, and instantiation for iteration is 33% faster.  Daft, but matches old exception sans the err msg.
The ordering of the zip calls matter - zip throws StopIteration  when an iter throws it. So if the first iter throws it, the second  is *not* consumed. We rely on this, so don't change the order  without changing the logic.
Slower, kwargs-ready version.  Maintain compatibility with existing calls.
Virtual field  Assume object instance was passed in.  Object instance wasn't passed in -- must be an ID.  Object instance was passed in. Special case: You can  pass in "None" for related objects if it's allowed.  This is done with an exception rather than the  default argument on pop because we don't want  get_default() to be evaluated, and then not used.  Refs 12057.
If we are passed a related instance, set it using the  field.name instead of field.attname (e.g. "user" instead of  "user_id") so that the object gets properly cached (and type  checked) by the RelatedObjectDescriptor.
Any remaining kwargs must correspond to properties or  virtual fields.
Use provided fields, if not set then reload all non-deferred fields.
This field wasn't refreshed - skip ahead.
Throw away stale foreign key references.
Ensure that a model instance without a PK hasn't been assigned to  a ForeignKey or OneToOneField on this model. If the field is  nullable, allowing the save() would result in silent data loss.  If the related field isn't cached, then an instance hasn't  been assigned and there's no need to worry about this check.  A pk may have been assigned manually to a model instance not  saved to the database (or auto-generated in a case like  UUIDField), but we allow the save to proceed and rely on the  database to raise an IntegrityError if applicable. If  constraints aren't supported by the database, there's the  unavoidable risk of data corruption.  Remove the object from a related instance cache.
If update_fields is empty, skip the save. We do also check for  no-op saves later on for inheritance cases. This bailout is  still needed for skipping signal sending.
If saving to the same database, and this model is deferred, then  automatically do a "update_fields" save on the loaded fields.
Skip proxies, but keep the origin as the proxy model.  Store the database on which the object was saved  Once saved, this is no longer a to-be-added instance.
Signal that the save is complete
Make sure the link fields are synced between parent and self.  Set the parent's PK value to self.  Since we didn't have an instance of the parent handy set  attname directly, bypassing the descriptor. Invalidate  the related object cache, in case it's been accidentally  populated. A fresh instance will be re-built from the  database if necessary.
If possible, try an UPDATE. If that doesn't update anything, do an INSERT.  If this is a model with an order_with_respect_to  autopopulate the _order field
We can end up here when saving a model in inheritance chain where  update_fields doesn't target any field in current model. In that  case we just say the update succeeded. Another case ending up here  is a model with just PK - in that case check that the PK still  exists.
It may happen that the object is deleted from the DB right after  this check, causing the subsequent UPDATE to return zero matching  rows. The same result can occur in some rare cases when the  database returns zero despite the UPDATE being executed  successfully (a row is matched and updated). In order to  distinguish these two cases, the object's existence in the  database is again checked for if the UPDATE query returns 0.
If this is an excluded field, don't add this check.
These are checks for the unique_for_<date/year/month>.
no value, skip the lookup
no need to check for unique primary key when editing
some fields were skipped, no reason to do the check
Exclude the current object from the query if we are editing an  instance (as opposed to creating a new one)  Note that we need to use the pk as defined by model_class, not  self.pk. These can be different fields because model inheritance  allows single model to have effectively multiple primary keys.  Refs 17615.
there's a ticket to add a date lookup, we can remove this special  case if that makes it's way in
Exclude the current object from the query if we are editing an  instance (as opposed to creating a new one)
A unique field
unique_together
Form.clean() is run even if other validation fails, so do the  same with Model.clean() for consistency.
Run unique checks, but only for fields that passed validation.
Skip validation for empty fields with blank=True. The developer  is responsible for making sure they have a valid value.
If there are field name clashes, hide consequent column name  clashes.
Skip when the target model wasn't found.
Skip when the relationship model wasn't found.
fields is empty or consists of the invalid "id" field
Check that multi-inheritance doesn't cause field name shadowing.
Check that fields defined in the model don't clash with fields from  parents, including auto-generated fields like multi-table inheritance  child accessors.
Note that we may detect clash between user-defined non-unique  field "id" and automatically added unique field "id", both  defined at the same model. This special case is considered in  _check_id_field and here we ignore it.
Store a list of column names which have already been used by other fields.
Ensure the column name is not already in use.
In order to avoid hitting the relation tree prematurely, we use our  own fields_map instead of using get_field()
Skip '?' fields.
Convert "-field" to "field".
Skip ordering in the format field1__field2 (FIXME: checking  this format would be nice, but it's a little fiddly).
Skip ordering on pk. This is always a valid order_by field  but is an alias and therefore won't be found by opts.get_field.
Check for invalid or non-existent fields in ordering.
Any field name that is not present in field_names does not exist.  Also, ordering by m2m fields is not allowed.
Find the minimum max allowed length among all specified db_aliases.  skip databases where the model won't be created
Check if auto-generated name for the field is too long  for the database.
Check if auto-generated name for the M2M field is too long  for the database.
FIXME: It would be nice if there was an "update many" version of update  for situations like this.
Backwards compat - the model was cached directly in earlier versions.
Get the exception class from the class it is attached to:
The maximum number of items to display in a QuerySet.__repr__
Pull into this namespace for backwards compatibility.
Execute the query. This will also fill compiler.select, klass_info,  and annotations.
Add the known related objects to the model, if there are any  Avoid overwriting objects loaded e.g. by select_related
extra(select=...) cols are always at the start of the row.
extra(select=...) cols are always at the start of the row.
Reorder according to fields.
Address the circular dependency between `Queryset` and `Manager`.
Force the cache to be fully populated.
The default_alias property may raise a TypeError, so we use  a try/except construct rather than hasattr in order to remain  consistent between PY2 and PY3 (hasattr would swallow  the TypeError on PY2).
When you bulk insert you don't get the primary keys back (if it's an  autoincrement, except if can_return_ids_from_bulk_insert=True), so  you can't insert into the child tables which references this. There  are two workarounds:  1) This could be implemented if you didn't have an autoincrement pk  2) You could do it by doing O(n) normal inserts into the parent     tables to get the primary keys back and then doing a single bulk     insert into the childmost table.  We currently set the primary keys on the objects when using  PostgreSQL via the RETURNING ID clause. It should be possible for  Oracle as well, but the semantics for  extracting the primary keys is  trickier so it's not done yet.  Check that the parents share the same concrete model with the our  model to detect the inheritance pattern ConcreteGrandParent ->  MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy  would not identify that case as involving multiple tables.
The get() needs to be targeted at the write database in order  to avoid potential transaction consistency problems.
The delete is actually 2 queries - one to find related objects,  and one to delete. Make sure that the discovery of related  objects is performed on the same database as the deletion.
Disable non-supported fields.
Clear the result cache, in case this QuerySet gets reused.
This method can only be called once the result cache has been filled.
Shortcut - if there are no extra or annotations, then  the values() clause must be just field names.
The default_alias property may raise a TypeError, so we use  a try/except construct rather than hasattr in order to remain  consistent between PY2 and PY3 (hasattr would swallow  the TypeError on PY2).
Can only pass None to defer(), not only(), as the rest option.  That won't stop people trying to do this, so let's be explicit.
values() queryset can only be used as nested queries  if they are set up to select only a single field.
If the query is used as a subquery for a ForeignKey with non-pk  target field, make sure to select the target field in the subquery.
values() queryset can only be used as nested queries  if they are set up to select only a single field.
When used as part of a nested query, a queryset will never be an "always  empty" result.
If there is any hinting information, add it to what we already know.  If we have a new hint for an existing key, overwrite with the new value.
We trust that users of values() know what they are doing.
Cache some things for performance reasons outside the loop.
Find out which model's fields are not present in the query.  Associate fields to values
Done iterating the Query. If it has its own cursor, close it.
Adjust any column names which don't match field names  Ignore translations for non-existent column names
`prefetch_through` is the path we traverse to perform the prefetch.  `prefetch_to` is the path to the attribute that stores the result.
Top level, the list of objects to decorate is the result cache  from the primary QuerySet. It won't be for deeper levels.
Prepare main instances
Skip any prefetching, and any object preparation
Prepare objects:  Since prefetching can re-use instances, it is possible to have  the same instance multiple times in obj_list, so obj might  already be prepared.  Must be an immutable object from  values_list(flat=True), for example (TypeError) or  a QuerySet subclass that isn't returning Model  instances (AttributeError), either in Django or a 3rd  party. prefetch_related() doesn't make sense, so quit.
We assume that objects retrieved are homogeneous (which is the premise  of prefetch_related), so what applies to first object applies to all.
Last one, this *must* resolve to something that supports  prefetching, otherwise there is no point adding it and the  developer asking for it has made a mistake.
We need to ensure we don't keep adding lookups from the  same relationships to stop infinite recursion. So, if we  are already on an automatically added lookup, don't add  the new lookups from relationships we've seen already.
We replace the current list of parent objects with the list  of related objects, filtering out empty or missing values so  that we can continue with nullable or reverse relations.  We special-case `list` rather than something more generic  like `Iterable` because we don't want to accidentally match  user models that define __iter__.
For singly related objects, we have to avoid getting the attribute  from the object, as this will trigger the query. So we first try  on the class, in order to get the descriptor object.  singly related object, descriptor object has the  get_prefetch_queryset() method.  descriptor doesn't support prefetching, so we go ahead and get  the attribute on the instance rather than the class to  support many related managers
We have to handle the possibility that the QuerySet we just got back  contains some prefetch_related lookups. We don't want to trigger the  prefetch_related functionality by evaluating the query. Rather, we need  to merge in the prefetch_related lookups.  Copy the lookups in case it is a Prefetch object which could be reused  later (happens in nested prefetch_related).  Don't need to clone because the manager should have given us a fresh  instance, so we access an internal instead of using public interface  for performance reasons.
Make sure `to_attr` does not conflict with a field.  We assume that objects retrieved are homogeneous (which is the premise  of prefetch_related), so what applies to first object applies to all.
Whether or not we're prefetching the last part of the lookup.
We don't want the individual qs doing prefetch_related now,  since we have merged this into the current work.
Pre-compute needed attributes. The attributes are:   - model_cls: the possibly deferred model class to instantiate   - either:     - cols_start, cols_end: usually the columns in the row are       in the same order model_cls.__init__ expects them, so we       can instantiate by model_cls(*row[cols_start:cols_end])     - reorder_for_init: When select_related descends to a child       class, then we want to reuse the already selected parent       data. However, in this case the parent data isn't necessarily       in the same order that Model.__init__ expects it to be, so       we have to reorder the parent data. The reorder_for_init       attribute contains a function used to reorder the field data       in the order __init__ expects it.   - pk_idx: the index of the primary key field in the reordered     model data. Used to check if a related object exists at all.   - init_list: the field attnames fetched from the database. For     deferred models this isn't the same as all attnames of the     model's fields.   - related_populators: a list of RelatedPopulator instances if     select_related() descends to related models from this model.   - cache_name, reverse_cache_name: the names to use for setattr     when assigning the fetched object to the from_obj. If the     reverse_cache_name is set, then we also set the reverse link.
Arithmetic connectors  The following is a quoted % operator - it is quoted because it can be  used in strings that also have parameter substitution.
Bitwise operators - note that these are generated by .bitand()  and .bitor(), the '&' and '|' are reserved for boolean operator  usage.
everything must be resolvable to an expression
aggregate specific fields
custom logic
order of precedence
order of precedence
Use the first supplied value in this order: the parameter to this  method, a value supplied in __init__()'s **extra (the value in  `data`), or the value defined on the class.
check _output_field to avoid triggering an exception  cx_Oracle does not always convert None to the appropriate  NULL type (like in case expressions using numbers), so we  use a literal SQL NULL
The sub-expression `source` has already been resolved, as this is  just a reference to the name of `source`.
We're only interested in the fields of the result expressions.
This is not a complete expression and cannot be used in GROUP BY.
Connection types
If the effective connector is OR and this node contains an aggregate,  then we need to push the whole branch to HAVING clause.
Check if this node matches nothing or everything.  First check the amount of full nodes and empty nodes  to make this node empty/full.  Now, check if this node is full/empty using the  counts.
Some backends (Oracle at least) need parentheses  around the inner SQL in the negated case, even if the  inner SQL contains just a single expression.
For example another WhereNode
The contents are a black box - assume no aggregates are used.
Even if aggregates would be used in a subquery, the outer query isn't  interested about those.
QuerySet was sent  Do not override already existing values.  If there is no slicing in use, then we can safely drop all ordering
Valid query types (a set is used for speedy lookups). These are (currently)  considered SQL-specific; other storage systems may choose to use different  lookup types.
Size of each "chunk" for get_iterator calls.  Larger values are slightly faster at the expense of more storage space.
How many results to expect from a cursor.execute call
SQL join types.
The path travelled, this includes the path to the multijoin.
Join table  Note: table_alias is not necessarily known at instantiation time.  LOUTER or INNER  A list of 2-tuples to use in the ON clause of the JOIN.  Each 2-tuple will create one join condition in the ON clause.  Along which field (or ForeignObjectRel in the reverse join case)  Is this join nullabled?
Add a join condition for each pair of joining columns.
Add a single condition inside parentheses for whatever  get_extra_restriction() returns.
This might be a rel on the other end of an actual declared field.
Always execute a new query for a new iterator.  This could be optimized with a cache at the expense of RAM.  If the database can't use chunked reads we need to make sure we  evaluate the entire query up front.
Adapt parameters to the database, as much as possible considering  that the target type isn't known. See 17755.
alias_map is the most important data structure regarding joins.  It's used for recording which joins exist in the query and what  types they are. The key is the alias of the joined table (possibly  the table name) and the value is a Join-like object (see  sql.datastructures.Join for more information).  Sometimes the query contains references to aliases in outer queries (as  a result of split_exclude). Correct alias quoting needs to know these  aliases too.
SQL-related attributes  Select and related select clauses are expressions to use in the  SELECT clause of the query.  The select is used for cases where we want to set up the select  clause to contain other than default fields (values(), subqueries...)  Note that annotations go to annotations dictionary.
The group_by attribute can have one of the following forms:   - None: no group by at all in the query   - A list of expressions: group by (at least) those expressions.     String refs are also allowed for now.   - True: group by all select fields of the model  See compiler.get_group_by() for details.
Arbitrary limit for select_related to prevents infinite recursion.
Holds the selects defined by a call to values() or values_list()  excluding annotation_select and extra_select.
A tuple that is a set of model field names and either True, if these  are the fields to defer, or False if these are the only fields to  load.
_annotation_select_cache cannot be copied, as doing so breaks the  (necessary) state in which both annotations and  _annotation_select_cache point to the same underlying objects.  It will get re-populated in the cloned queryset the next time it's  used.
We must make sure the inner query has the referred columns in it.  If we are aggregating over an annotation, then Django uses Ref()  instances to note this. However, if we are annotating over a column  of a related model, then it might be that column isn't part of the  SELECT clause of the inner query, and we must manually make sure  the column is selected. An example case is:     .aggregate(Sum('author__awards'))  Resolving this expression results in a join to author, but there  is no guarantee the awards column of author is in the select clause  of the query. Thus we must manually add the column to the inner  query.  Its already a Ref to subquery (see resolve_ref() for  details)  Reference to column. Make sure the referenced column  is selected.  Some other expression not referencing database values  directly. Its subexpression might contain Cols.
Decide if we need to use a subquery.  Existing annotations would cause incorrect results as get_aggregation()  must produce just one result and thus must not use GROUP BY. But we  aren't smart enough to remove the existing annotations from the  query, so those would force us to use GROUP BY.  If the query has limit or distinct, then those operations must be  done in a subquery so that we are aggregating on the limit and/or  distinct results instead of applying the distinct and limit after the  aggregation.  Queries with distinct_fields need ordering and when a limit  is applied we must take the slice from the ordered query.  Otherwise no need for ordering.  If the inner query uses default select and it has some  aggregate annotations, then we must make sure the inner  query is grouped by the main model's primary key. However,  clearing the select clause can alter results if distinct is  used.
Remove any aggregates marked for reduction from the subquery  and move them to the outer AggregateQuery.  Make sure the annotation_select wont use cached results.  In case of Model.objects[0:3].count(), there would be no  field selected in the inner query, yet we must use a subquery.  So, make sure at least one field is selected.
Work out how to relabel the rhs aliases, if necessary.
Determine which existing joins can be reused. When combining the  query with AND we must recreate all joins for m2m filters. When  combining with OR we can reuse joins. The reason is that in AND  case a single row can't fulfill a condition like:      revrel__col=1 & revrel__col=2  But, there might be two different related rows matching this  condition. In OR case a single True is enough, so single row is  enough, too.  Note that we will be creating duplicate joins for non-m2m joins in  the AND case. The results will be correct but this creates too many  joins. This is something that could be fixed later on.  Base table must be present in the query - this is the same  table on both sides.  Now, add the joins from rhs query into the new query (skipping base  table).  If the left side of the join was already relabeled, use the  updated alias.  We can't reuse the same join again in the query. If we have two  distinct joins for the same connection in rhs query, then the  combined query must have two joins, too.  The alias was unused in the rhs query. Unref it so that it  will be unused in the new query, too. We have to add and  unref the alias so that join promotion has information of  the join type for the unused alias.
Now relabel a copy of the rhs where-clause and add it to the current  one.
Selection columns and extra extensions are those provided by 'rhs'.
It would be nice to be able to handle this, but the queries don't  really make sense (or return consistent value sets). Not worth  the extra complexity when you can write a real query instead.
Ordering uses the 'rhs' ordering, unless it has none, in which case  the current ordering is used.
Even if we're "just passing through" this model, we must add  both the current model's pk and the related reference field  (if it's not a reverse relation) to the things we select.
We need to load all fields for each model, except those that  appear in "seen" (for all models that appear in "seen"). The only  slight complexity here is handling fields that exist on parent  models.  If we haven't included a model in workset, we don't add the  corresponding must_include fields for that model, since an  empty set means "include all fields". That's why there's no  "else" branch here.
As we've passed through this model, but not explicitly  included any fields, we have to make sure it's mentioned  so that only the "must include" fields are pulled in.  Now ensure that every model in the inheritance chain is mentioned  in the parent list. Again, it must be mentioned to ensure that  only "must include" fields are pulled in.
Create a new alias for this table.  The first occurrence of a table uses the table name directly.
This is the base table (first FROM entry) - this table  isn't really joined at all in the query, so we should not  alter its join type.  Only the first alias (skipped above) should have None join_type
Join type of 'alias' changed, so re-examine all aliases that  refer to this one.
1. Update references in "select" (normal columns plus aliases),  "group by" and "where".
2. Rename the alias in the internal table/alias datastructures.
No clashes between self and outer query should be possible.
No reuse is possible, so we need a new alias.
Proxy model have elements in base chain  with no parents, assign the new options  object and skip to the next base in that  case
Default lookup if none given is exact.  Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all  uses of None as a query value.  Subqueries need to use a different set of aliases than the  outer query. Call bump_prefix to change aliases of the inner  query (the value).  For Oracle '' is equivalent to null. The check needs to be done  at this stage because join promotion can't be done at compiler  stage. Using DEFAULT_DB_ALIAS isn't nice, but it is the best we  can do here. Similar thing is done in is_nullable(), too.
QuerySets implement is_compatible_query_object_type() to  determine compatibility with the given field.
If there is just one part left, try first get_lookup() so  that if the lhs supports both transform and lookup for the  name, then lookup will be picked.  We didn't find a lookup. We are going to interpret  the name as transform, and do an Exact lookup against  it.
Work out the lookup type and remove it from the end of 'parts',  if necessary.
Prevent iterator from being consumed by check_related_objects()
split_exclude() needs to know which joins were generated for the  lookup parts
No support for transforms for relational fields
The condition added here will be SQL like this:  NOT (col IS NOT NULL), where the first NOT is added in  upper layers of code. The reason for addition is that if col  is null, then col != someval will result in SQL "unknown"  which isn't the same as in Python. The Python None handling  is wanted, and it can be gotten by  (col IS NULL OR col != someval)    <=>  NOT (col IS NOT NULL AND col = someval).
For join promotion this case is doing an AND for the added q_object  and existing conditions. So, any existing inner join forces the join  type to remain inner. Existing outer joins can however be demoted.  (Consider case where rel_a is LOUTER and rel_a__col=1 is added - if  rel_a doesn't produce any rows, then the whole condition must fail.  So, demotion is OK.
Fields that contain one-to-many relations with a generic  model (like a GenericForeignKey) cannot generate reverse  relations and therefore cannot be used for reverse querying.
We didn't find the current field, so move position back  one step.  Check if we need any joins for concrete inheritance cases (the  field lives in parent, but we are currently in one of its  children)
The field lives on a base class of the current model.  Skip the chain of proxy to the concrete proxied model
Local non-relational field.
First, generate the path for the names
Then, add the path to the query's joins. Note that we can't trim  joins at this stage - we will need the information about join type  of the trimmed joins.
Summarize currently means we are doing an aggregate() query  which is executed as a wrapped subquery if any of the  aggregate() elements reference an existing annotation. In  that case we need to return a Ref to the subquery's annotation.
Generate the inner query.  Try to have as simple as possible subquery -> trim leading joins from  the subquery.
Add extra check to make sure the selected field will not be null  since we are adding an IN <subquery> clause. This prevents the  database from tripping over IN (...,NULL,...) selects and returning  nothing  Need to add a restriction so that outer query's filters are in effect for  the subquery, too.  Note that the query.select[0].alias is different from alias  due to bump_prefix above.
Note that the end result will be:  (outercol NOT IN innerq AND outercol IS NOT NULL) OR outercol IS NULL.  This might look crazy but due to how IN works, this seems to be  correct. If the IS NOT NULL check is removed then outercol NOT  IN will return UNKNOWN. If the IS NULL check is removed, then if  outercol IS NULL we will not match the row.
Join promotion note - we must not remove any rows here, so  if there is no existing joins, use outer join.
For lookups spanning over relationships, show the error  from the model on which the lookup failed.
We need to pair any placeholder markers in the 'select'  dictionary with their parameters in 'select_params' so that  subsequent updates to the select dictionary also adjust the  parameters appropriately.  This is order preserving, since self.extra_select is an OrderedDict.
Fields on related models are stored in the literal double-underscore  format, so that we can use a set datastructure. We do the foo__bar  splitting and handling when computing the SQL column names (as part of  get_columns()).  Add to existing deferred names.  Remove names from the set of any existing "immediate load" names.
Remove any existing deferred names from the current set before  setting the new names.
Replace any existing "immediate load" field names.
We cache this because we call this function multiple times  (compiler.fill_related_selections, query.iterator)
Trim and operate only on tables that were generated for  the lookup part of the query. That is, avoid trimming  joins generated for F() expressions.  The path.join_field is a Rel, lets get the other side's field  Build the filter prefix.  Lets still see if we can trim the first join from the inner query  (that is, self). We can't do this for LEFT JOINs because we would  miss those rows that have nothing on the outer side.  TODO: It might be possible to trim more joins from the start of the  inner query if it happens to have a longer join chain containing the  values in select_fields. Lets punt this one for now.  The found starting point is likely a Join instead of a BaseTable reference.  But the first entry in the query's FROM clause must not be a JOIN.
We need to use DEFAULT_DB_ALIAS here, as QuerySet does not have  (nor should it have) knowledge of which connection is going to be  used. The proper fix would be to defer all decisions where  is_nullable() is needed to the compiler stage, but that is not easy  to do currently.
Maps of table alias to how many times it is seen as required for  inner and/or outer joins.
The effective_connector is used so that NOT (a AND b) is treated  similarly to (a OR b) for join promotion.  We must use outer joins in OR case when the join isn't contained  in all of the joins. Otherwise the INNER JOIN itself could remove  valid results. Consider the case where a model with rel_a and  rel_b relations is queried with rel_a__col=1 | rel_b__col=2. Now,  if rel_a join doesn't produce any results is null (for example  reverse foreign key or null value in direct foreign key), and  there is a matching row in rel_b with col=2, then an INNER join  to rel_a would remove a valid match from the query. So, we need  to promote any existing INNER to LOUTER (it is possible this  promotion in turn will be demoted later on).  If connector is AND and there is a filter that can match only  when there is a joinable row, then use INNER. For example, in  rel_a__col=1 & rel_b__col=2, if either of the rels produce NULL  as join output, then the col=1 or col=2 can't match (as  NULL=anything is always false).  For the OR case, if all children voted for a join to be inner,  then we can use INNER for the join. For example:      (rel_a__col__icontains=Alex | rel_a__col__icontains=Russell)  then if rel_a doesn't produce any rows, the whole condition  can't match. Hence we can safely use INNER join.  Finally, what happens in cases where we have:     (rel_a__col=1|rel_b__col=2) & rel_a__col__gte=0  Now, we first generate the OR clause, and promote joins for it  in the first if branch above. Both rel_a and rel_b are promoted  to LOUTER joins. After that we do the AND case. The OR case  voted no inner joins but the rel_a__col__gte=0 votes inner join  for rel_a. We demote it back to INNER join (in AND case a single  vote is enough). The demotion is OK, if rel_a doesn't produce  rows, then the rel_a__col__gte=0 clause can't be true, and thus  the whole clause must be false. So, it is safe to use INNER  join.  Note that in this example we could just as well have the __gte  clause and the OR clause swapped. Or we could replace the __gte  clause with an OR clause containing rel_a__col=1|rel_a__col=2,  and again we could safely demote to INNER.
The select, klass_info, and annotations are needed by QuerySet.iterator()  these are set as a side-effect of executing the query. Note that we calculate  separately a list of extra select columns needed for grammatical correctness  of the query, but these columns are not included in self.select.
The query.group_by is either None (no GROUP BY at all), True  (group by select fields), or a list of expressions to be added  to the group by.  If the group by is set to a list (by .values() call most likely),  then we need to add everything in it to the GROUP BY clause.  Backwards compatibility hack for setting query.group_by. Remove  when  we have public API way of forcing the GROUP BY clause.  Converts string references to expressions.  Note that even if the group_by is set, it is only the minimal  set to group by. So, we need to add cols in select, order_by, and  having into the select in any case.  We can skip References to select clause, as all expressions in  the select clause are already part of the group by.
If the DB can group by primary key, then group by the primary key of  query's main model. Note that for PostgreSQL the GROUP BY clause must  include the primary key of every table, but for MySQL it is enough to  have the main table's primary key.  The logic here is: if the main model's primary key is in the  query, then set new_expressions to that field. If that happens,  then also add having expressions to group by.  Is this a reference to query's base table primary key? If the  expression isn't a Col-like, then skip the expression.  MySQLism: Columns in HAVING clause must be added to the GROUP BY.  Filter out all expressions associated with a table's primary key  present in the grouped columns. This is done by identifying all  tables that have their primary key included in the grouped  columns and removing non-primary key columns referring to them.
self.query.select is a special case. These columns never go to  any model.
Reference to expression in SELECT clause
References to an expression which is masked out of the SELECT clause
This came in through an extra(order_by=...) addition. Pass it  on verbatim.
'col' is of the form 'field' or 'field1__field2' or  '-field1__field2__field', etc.
Don't add the same column twice, but the order direction is  not taken into account so we strip it. When this entire method  is refactored into expressions, then we can check each part as we  generate it.
This must come after 'select', 'ordering', and 'distinct' -- see  docstring of get_from_clause() for details.
If we've been asked for a NOWAIT query but the backend does  not support it, raise a DatabaseError otherwise we could get  an unexpected deadlock.
Finally do cleanup - get rid of the joins we created above.
If there is no slicing in use, then we can safely drop all ordering
The 'seen_models' is used to optimize checking the needed parent  alias for a given field. This also includes None -> start_alias to  be used by local fields.
A proxy model will have a different model and concrete_model. We  will assign None if the field belongs to this model.  Avoid loading data for already loaded parents.  We end up here in the case select_related() resolution  proceeds from parent model to child model. In that case the  parent model data is already present in the SELECT clause,  and we want to avoid reloading the same data again.
If we get to this point and the field is a relation to another model,  append the default ordering for that model unless the attribute name  of the field is specified.  Firstly, avoid infinite loops.
Extra tables can end up in self.tables, but not in the  alias_map if they aren't in a join. That's OK. We skip them.
Only add the alias if it's not already present (the table_alias()  call increments the refcount, so an alias refcount of one means  this is the only reference).
We've recursed far enough; bail out.
Setup for the case when only particular related fields should be  included in the related selection.
If a non-related field is used like a relation,  or if a single non-relational field is given.
This is always executed on a query clone, so we can modify self.query
Caller didn't specify a result_type, so just give them back the  cursor to process (and close).
done with the cursor
If we are using non-chunked reads, we return the same data  structure as normally, but ensure it is all read into memory  before going any further.
done with the cursor
A field value of None means the value is raw.
This is an expression, let's compile it.
Some fields (e.g. geo fields) need special munging before  they can be inserted.
Return the common case for the placeholder
The following hook is only used by Oracle Spatial, which sometimes  needs to yield 'NULL' and [] as its placeholder and params instead  of '%s' and [None]. The 'NULL' placeholder is produced earlier by  OracleOperations.get_geom_placeholder(). The following line removes  the corresponding None parameter. See ticket 10888.
Don't allow values containing Col expressions. They refer to  existing columns on a row, but in the case of insert the row  doesn't exist yet.
list of (sql, [params]) tuples for each object to be saved  Shape: [n_objs][n_fields][2]
tuple like ([sqls], [[params]s]) for each object to be saved  Shape: [n_objs][2][n_fields]
Extract separate lists for placeholders and params.  Each of these has shape [n_objs][n_fields]
Params for each field are still lists, and need to be flattened.
We don't need quote_name_unless_alias() here, since these are all  going to be column names (so we can avoid the extra overhead).
An empty object.
Currently the backends just accept values when generating bulk  queries and generate their own placeholders. Doing that isn't  necessary and it should be possible to use placeholders and  expressions in bulk inserts too.
Skip empty r_fmt to allow subclasses to customize behavior for  3rd party backends. Refs 19096.
Getting the placeholder for the field.
Ensure base table is in the query
Now we adjust the current query: reset the where clause and get rid  of all the tables we don't need (since they're in the sub-select).  Either we're using the idents in multiple update queries (so  don't want them to change), or the db backend doesn't support  selecting from the updating table (e.g. MySQL).  The fast path. Filters and updates in one query.
Empty SQL for the inner query is a marker that the inner query  isn't going to produce any results. This can happen when doing  LIMIT 0 queries (generated by qs[:0]) for example.
number of objects deleted
Make sure the inner query has at least one table in use.  The same for our new query.  There is only the base table in use in the query.  We can't do the delete using subquery.
Aggregates are not allowed in UPDATE queries, so ignore for_save
Normalize everything to tuples
If the value of option_together isn't valid, return it  verbatim; this will be picked up by the check framework later.
For any class that is a proxy (including automatically created  classes for deferred object loading), proxy_for_model tells us  which class this model is proxying. Note that proxy_for_model  can create a chain of proxy models. For non-proxy models, the  variable is always None.  For any non-abstract class, the concrete class is the model  in the end of the proxy_for_model chain. In particular, for  concrete models, the concrete_model is always the class itself.
List of all lookups defined in ForeignKey 'limit_choices_to' options  from *other* models. Needed for some admin checks. Internal use only.
A custom app registry to use, if you're making a separate model set.
Don't go through get_app_config to avoid triggering imports.
First, construct the default values for these options.
Store the original user-defined values for each option,  for use when serializing the model definition
Next, apply any overridden values from 'class Meta'.  Ignore any private attributes that Django doesn't care about.  NOTE: We can't modify a dictionary's contents while looping  over it, so we loop over the *original* dictionary instead.
verbose_name_plural is a special case because it uses a 's'  by default.
order_with_respect_and ordering are mutually exclusive.
Any leftover attributes must be invalid.
If the db_table wasn't provided, use the app_label + model_name.
The app registry will not be ready at this point, so we cannot  use get_field().
Promote the first parent link in lieu of adding yet another  field.  Look for a local field with the same name as the  first parent link. If a local field has already been  created, use it instead of promoting the parent
Insert the given field in the order in which it was created, using  the "creation_counter" attribute of the field.  Move many-to-many related fields from self.fields into  self.many_to_many.
If the field being added is a relation to another known field,  expire the cache on this field and the forward cache on the field  being referenced, because there will be new relationships in the  cache. Otherwise, expire the cache of references *to* this field.  The mechanism for getting at the related model is slightly odd -  ideally, we'd just ask for field.related_model. However, related_model  is a cached property, and all the models haven't been loaded yet, so  we need to make sure we don't cache a string reference.
setting not in the format app_label.model_name  raising ImproperlyConfigured here causes problems with  test cleanup code - instead it is raised in get_user_model  or as part of validation.
Used for deprecation of legacy manager inheritance,  remove afterwards. (RemovedInDjango20Warning)
Get the first parent's base_manager_name if there's one.
Deprecation shim for `use_for_related_fields`.
Get the first parent's default_manager_name if there's one.
For legacy reasons, the fields property should only contain forward  fields that are not private or with a m2m cardinality. Therefore we  pass these three filters as filters to the generator.  The third lambda is a longwinded way of checking f.related_model - we don't  use that property directly because related_model is a cached property,  and all the models may not have been loaded yet; we don't want to cache  the string reference to the related_model.
Due to the way Django's internals work, get_field() should also  be able to fetch a field by attname. In the case of a concrete  field with relation, includes the *_id name too
Due to the way Django's internals work, get_field() should also  be able to fetch a field by attname. In the case of a concrete  field with relation, includes the *_id name too
In order to avoid premature loading of the relation tree  (expensive) we prefer checking if the field is a forward field.
If the app registry is not ready, reverse fields are  unavailable, therefore we throw a FieldDoesNotExist exception.
Retrieve field instance by name from cached or just-computed  field map.
Tries to get a link field from the immediate parent  In case of a proxied model, the first link  of the chain to the ancestor is that parent  links
Abstract model's fields are copied to child models, hence we will  see the fields from the child models.
Set the relation_tree using the internal __dict__. In this way  we avoid calling the cached property. In attribute lookup,  __dict__ takes precedence over a data descriptor (such as  @cached_property). This means that the _meta._relation_tree is  only called if related_objects is not in __dict__.  It seems it is possible that self is not in all_models, so guard  against that with default for get().
This method is usually called by apps.cache_clear(), when the  registry is finalized, or when a new field is added.
We must keep track of which models we have already seen. Otherwise we  could include the same field multiple times from different models.
Creates a cache key composed of all arguments
In order to avoid list manipulation. Always return a shallow copy  of the results.
Recursively call _get_fields() on each parent, with the same  options provided in this call.  In diamond inheritance it is possible that we see the same  model from two different routes. In that case, avoid adding  fields from the same parent again.  Tree is computed once and cached until the app cache is expired.  It is composed of a list of fields pointing to the current model  from other models.  If hidden fields should be included or the relation is not  intentionally hidden, add to the fields dict.
Private fields are recopied to each child model, and they get a  different model as field.model in each child. Hence we have to  add the private fields separately from the topmost call. If we  did this recursively similar to local_fields, we would get field  instances with field.model != self.model.
In order to avoid list manipulation. Always  return a shallow copy of the results
Store result into cache for later access
Tracks each time a Manager instance is created. Used to retain order.
Set to True for the 'objects' managers that are automatically created.
We capture the arguments to make returning them trivial
using MyQuerySet.as_manager()
Make sure it's actually there and not an inner class
Refs http://bugs.python.org/issue1785.  Only copy missing methods.  Only copy public methods or methods with the attribute `queryset_only=False`.  Copy the method onto the manager.
We can't proxy this method through the `QuerySet` like we do for the  rest of the `QuerySet` methods. This is because `QuerySet.all()`  works by creating a "copy" of the current queryset and in making said  copy, all the cached `prefetch_related` lookups are lost. See the  implementation of `RelatedManager.get_queryset()` for a better  understanding of how this comes into play.
Warn the user as soon as possible if they are trying to apply  a bilateral transformation on a nested QuerySet: that won't work.  We need to import QuerySet here so as to avoid circular
Do not call get_db_prep_lookup here as the value will be  transformed before being used for lookup
Due to historical reasons there are a couple of different  ways to produce sql here. get_compiler is likely a Query  instance, _as_sql QuerySet and as_sql just something with  as_sql. Finally the value can of course be just plain  Python value.
For relational fields, use the output_field of the 'field' attribute.
rhs should be an iterable; use batch_process_rhs() to  prepare/transform those values.
This is a special case for databases which limit the number of  elements which can appear in an 'IN' clause.
Assume we are in startswith. We need to produce SQL like:      col LIKE %s, ['thevalue%']  For python values we can (and should) do that directly in Python,  but if the value is for example reference to other column, then  we need to add the % pattern match to the lookup by something like      col LIKE othercol || '%%'  So, for Python values we don't need any special pattern, but for  SQL reference values or SQL transformations we need the correct  pattern added.
rhs should be an iterable of 2 values, we use batch_process_rhs  to prepare/transform those values
We will need to skip the extract part and instead go  directly with the originating field, that is self.lhs.lhs.
We will need to skip the extract part and instead go  directly with the originating field, that is self.lhs.lhs.  Check that rhs_params[0] exists (IndexError),  it isn't None (TypeError), and is a number (ValueError)  Can't determine the bounds before executing the query, so skip  optimizations by falling back to a standard exact comparison.
The candidate relations are the ones that come from N-1 and 1-1 relations.  N-N  (i.e., many-to-many) relations aren't candidates for deletion.
Initially, {model: {instances}}, later values become lists.
fast_deletes is a list of queryset-likes that can be deleted without  fetching the objects into memory.
Nullable relationships can be ignored -- they are nulled out before  deleting, and therefore do not affect the order in which objects have  to be deleted.
The use of from_field comes from the need to avoid cascade back to  parent when parent delete is cascading to child.  Foreign keys pointing to this model, both from m2m and other  models.  It's something like generic foreign key.
Recursively collect concrete model's parent models, but not their  related objects. These will be found by meta.get_fields()  FIXME: This seems to be buggy and execute a query for each  parent object fetch. We have the parent data in the obj,  but we don't have a nice way to turn that data into parent  object instance.
It's something like generic foreign key.
sort instance collections
if possible, bring the models in an order suitable for databases that  don't support transactions or cannot defer constraint checks until the  end of a transaction.  number of objects deleted for each model label
send pre_delete signals
fast deletes
update fields
reverse instance collections
delete instances
update collected instances
Ignore any related fields.
Ignore any non-concrete fields
Do a two-pass search for indexes: on first pass check which indexes  are multicolumn, on second pass check which single-column indexes  are present.  It's possible to have the unique and PK constraints in separate indexes.
Now add in the indexes  Convert the sorted sets to lists
MySQL stores positive fields as UNSIGNED ints.
http://dev.mysql.com/doc/mysql/en/date-and-time-functions.html  DAYOFWEEK() returns an integer, 1-7, Sunday=1.  Note: WEEKDAY() returns 0-6, Monday=0.
RemovedInDjango20Warning
With MySQLdb, cursor objects have an (undocumented) "_last_executed"  attribute where the exact query sent to the database is saved.  See MySQLdb/cursors.py in the source distribution.
2**64 - 1, as recommended by the MySQL documentation
NB: The generated SQL below is specific to MySQL  'TRUNCATE x;', 'TRUNCATE y;', 'TRUNCATE z;'... style SQL statements  to clear all tables of all data
MySQLism: zero in AUTO_INCREMENT field does not work. Refs 17653.
MySQL doesn't support tz-aware datetimes
MySQL doesn't support tz-aware times
Inner import to allow module to fail to load gracefully
Simulate the effect of a one-off default.  field.default may be unhashable, so a set isn't used for "in" check.
Temporary setting db_index to False (in memory) to disable  index creation for FKs (index automatically created by MySQL)
We want version (1, 2, 1, 'final', 2) or later. We can't just use  lexicographic ordering in this check because then (1, 2, 1, 'gamma')  inadvertently passes the version test.
Remove this function and rely on the default adapter in Django 2.0.  This doesn't account for the database connection's timezone,  which isn't known. (That's why this adapter is deprecated.)
MySQLdb-1.2.1 returns TIME columns as timedelta -- they are more like  timedelta in terms of actual behavior as they are signed and include days --  and Django expects time, so we still need to override that. We also need to  add special handling for SafeText and SafeBytes as MySQLdb's type  checking is too tight to catch those (see Django ticket 6052).
This should match the numerical portion of the version numbers (we can treat  versions like 5.0.24 and 5.0.24a as the same). Based on the list of version  at http://dev.mysql.com/doc/refman/4.1/en/news.html and  http://dev.mysql.com/doc/refman/5.0/en/news.html .
args is None means no string interpolation
Map some error codes to IntegrityError, since they seem to be  misclassified and Django would prefer the more logical place.
Map some error codes to IntegrityError, since they seem to be  misclassified and Django would prefer the more logical place.
Ticket 17671 - Close instead of passing thru to avoid backend  specific behavior.
This dictionary maps Field objects to their associated MySQL column  types, as strings. Column-type strings can contain format strings; they'll  be interpolated against the values of Field.__dict__ before being output.  If a column type is set to None, it won't be included in the output.
The patterns below are used to generate SQL pattern lookup clauses when  the right-hand side of the lookup isn't a raw string (it might be an expression  or the result of a bilateral transformation).  In those cases, special characters for LIKE operators (e.g. \, *, _) should be  escaped on database side.  Note: we use str.format() here for readability as '%' is used as a wildcard for  the LIKE operator.
We need the number of potentially affected rows after an  "UPDATE", not the number of changed rows.
SQL_AUTO_IS_NULL controls whether an AUTO_INCREMENT column on  a recently inserted row will return when the field is tested  for NULL. Disabling this brings this aspect of MySQL in line  with SQL standards.
Override needs_rollback in case constraint_checks_disabled is  nested inside transaction.atomic.
See https://github.com/farcepest/MySQLdb1/issues/24 for the reason  about requiring MySQLdb 1.2.5
MySQL accepts full time zones names (eg. Africa/Nairobi) but rejects  abbreviations (eg. EAT). When pytz isn't installed and the current  time zone is LocalTimezone (the only sensible value in this  context), the current time zone name will be an abbreviation. As a  consequence, MySQL cannot perform time zone conversions reliably.
Test if the time zone definitions are installed.
This reg-exp is intentionally fairly flexible here.  Needs to be able to handle stuff like:    PostgreSQL ..    EnterpriseDB .    PostgreSQL . beta    PostgreSQL .beta
Maps type codes to Django Field types.
This query retrieves each index on the given table, including the  first associated field name  row[1] (idx.indkey) is stored in the DB as an array. It comes out as  a string of space-separated integers. This designates the field  indexes (1-based) of the fields that have indexes on the table.  Here, we skip any indexes across multiple fields.  It's possible to have the unique and PK constraints in separate indexes.
PostgreSQL will resolve a union as type 'text' if input types are  'unknown'.  http://www.postgresql.org/docs/9.4/static/typeconv-union-case.html  These fields cannot be implicitly cast back in the default  PostgreSQL configuration so we need to explicitly cast them.  We must also remove components of the type within brackets:  varchar(255) -> varchar.
http://www.postgresql.org/docs/current/static/functions-datetime.htmlFUNCTIONS-DATETIME-EXTRACT  For consistency across backends, we return Sunday=1, Saturday=7.
http://www.postgresql.org/docs/current/static/functions-datetime.htmlFUNCTIONS-DATETIME-TRUNC
http://www.postgresql.org/docs/current/static/functions-datetime.htmlFUNCTIONS-DATETIME-TRUNC
Cast text lookups to text to allow things like filter(x__contains=4)
Use UPPER(x) for case-insensitive lookups; it's faster.
Use pg_get_serial_sequence to get the underlying sequence name  from the table name and column name (available since PostgreSQL 8)
Perform a single SQL 'TRUNCATE x, y, z...;' statement.  It allows  us to truncate tables referenced by a foreign key in any other  table.
'ALTER SEQUENCE sequence_name RESTART WITH 1;'... style SQL statements  to reset sequence indices  This will be the case if it's an m2m using an autogenerated  intermediate table (see BaseDatabaseIntrospection.sequence_list)
http://initd.org/psycopg/docs/cursor.htmlcursor.query  The query attribute is a Psycopg extension to the DB API 2.0.
Fields with database column types of `varchar` and `text` need  a second index that specifies their operator class, which is  needed when performing correct LIKE queries outside the  C locale. See 12234.  The same doesn't apply to array fields such as varchar[size]  and text[size], so skip them.
Added an index? Create any PostgreSQL-specific indexes.
Removed an index? Drop any PostgreSQL-specific indexes.
CREATE DATABASE ... WITH TEMPLATE ... requires closing connections  to the template database.
Register support for inet[] manually so we don't have to handle the Inet()  object on load all the time.
This dictionary maps Field objects to their associated PostgreSQL column  types, as strings. Column-type strings can contain format strings; they'll  be interpolated against the values of Field.__dict__ before being output.  If a column type is set to None, it won't be included in the output.
The patterns below are used to generate SQL pattern lookup clauses when  the right-hand side of the lookup isn't a raw string (it might be an expression  or the result of a bilateral transformation).  In those cases, special characters for LIKE operators (e.g. \, *, _) should be  escaped on database side.  Note: we use str.format() here for readability as '%' is used as a wildcard for  the LIKE operator.
None may be used to connect to the default 'postgres' db
self.isolation_level must be set:  - after connecting to the database in order to obtain the database's    default when no value is explicitly specified in options.  - before calling _set_autocommit() because if autocommit is on, that    will set connection.isolation_level to ISOLATION_LEVEL_AUTOCOMMIT.  Set the isolation level to the value from OPTIONS.
Commit after setting the time zone (see 17062)
Use a psycopg cursor directly, bypassing Django's utilities.
Create temporary .pgpass file.  If the current locale can't encode the data, we let  the user input the password manually.
Maps type objects to Django Field types.
If it's a NUMBER with scale == 0, consider it an IntegerField
If we're the first column, make the record
If we're the first column, make the record  Record the details
Oracle uses NUMBER(11) and NUMBER(19) for integer fields.
TO_CHAR(field, 'D') returns an integer from 1-7, where 1=Sunday.
http://docs.oracle.com/cd/B19306_01/server.102/b14200/functions050.htm
http://docs.oracle.com/cd/B19306_01/server.102/b14200/functions230.htmi1002084
Oracle crashes with "ORA-03113: end-of-file on communication channel"  if the time zone name is passed in parameter. Use interpolation instead.  https://groups.google.com/forum/!msg/django-developers/zwQju7hbG78/9l934yelwfsJ  This regexp matches all time zone names from the zoneinfo database.
Convert from UTC to local time, returning TIMESTAMP WITH TIME ZONE.  Extracting from a TIMESTAMP WITH TIME ZONE ignore the time zone.  Convert to a DATETIME, which is called DATE by Oracle. There's no  built-in function to do that; the easiest is to go through a string.
Re-convert to a TIMESTAMP because EXTRACT only handles the date part  on DATE values, even though they actually store the time part.
http://docs.oracle.com/cd/B19306_01/server.102/b14200/functions230.htmi1002084
Oracle stores empty strings as null. We need to undo this in  order to adhere to the Django convention of using the empty  string instead of null, but only if the field accepts the  empty string.
http://cx-oracle.sourceforge.net/html/cursor.htmlCursor.statement  The DB API definition does not define this attribute.
Unlike Psycopg's `query` and MySQLdb`'s `_last_executed`, CxOracle's  `statement` doesn't contain the query parameters. refs 20010.
SQL92 requires delimited (quoted) names to be case-sensitive.  When  not quoted, Oracle has case-insensitive behavior for identifiers, but  always defaults to uppercase.  We simplify things by making Oracle identifiers always uppercase.  Oracle puts the query text into a (query % args) construct, so % signs  in names need to be escaped. The '%%' will be collapsed back to '%' at  that stage so we aren't really making the name longer here.
Return a list of 'TRUNCATE x;', 'TRUNCATE y;',  'TRUNCATE z;'... style SQL statements  Oracle does support TRUNCATE, but it seems to get us into  FK referential trouble, whereas DELETE FROM table works.  Since we've just deleted all the rows, running our sequence  ALTER code will reset the sequence to 0.
Only one AutoField is allowed per model, so don't  continue to loop
cx_Oracle doesn't support tz-aware datetimes
Oracle doesn't support tz-aware times
If we're changing type to an unsupported type we need a  SQLite-ish workaround
Make a new field that's like the new one but with a temporary  column name.  Add it  Explicit data type conversion  https://docs.oracle.com/cd/B19306_01/server.102/b14200/sql_elements002.htmsthref340  TimeField are stored as TIMESTAMP with a 1900-01-01 date part.  Transfer values across  Drop the old field  Rename and possibly make the new field NOT NULL
if we want to keep the db, then no need to do any of the below,  just return and skip it all.  Ran into a database error that isn't about leftover objects in the tablespace
If we want to keep the db, then we want to also keep the user.
There are objects in the test tablespace which prevent dropping it  The easy fix is to drop the test user -- but are we allowed to do so?
Ignore "tablespace already exists" error when keepdb is on.
Ignore "user already exists" error when keepdb is on  Most test-suites can be run without the create-view privilege. But some need it.
Statement can fail when acceptable_ora_err is not None
Cygwin requires some special voodoo to set the environment variables  properly so that Oracle will see them.
Oracle takes client-side character set encoding from the environment.  This prevents unicode from getting mangled by getting encoded into the  potentially non-unicode database character set.
If connection.operators is looked up before a connection has been  created, transparently initialize connection.operators to avert an  AttributeError.  Creating a cursor will initialize the operators.
This dictionary maps Field objects to their associated Oracle column  types, as strings. Column-type strings can contain format strings; they'll  be interpolated against the values of Field.__dict__ before being output.  If a column type is set to None, it won't be included in the output.  Any format strings starting with "qn_" are quoted before being used in the  output (the "qn_" prefix is stripped before the lookup is performed.
The patterns below are used to generate SQL pattern lookup clauses when  the right-hand side of the lookup isn't a raw string (it might be an expression  or the result of a bilateral transformation).  In those cases, special characters for LIKE operators (e.g. \, *, _) should be  escaped on database side.  Note: we use str.format() here for readability as '%' is used as a wildcard for  the LIKE operator.
Set the territory first. The territory overrides NLS_DATE_FORMAT  and NLS_TIMESTAMP_FORMAT to the territory default. When all of  these are set in single statement it isn't clear what is supposed  to happen.  Set Oracle date to ANSI date format.  This only needs to execute  once when we create a new connection. We also set the Territory  to 'AMERICA' which forces Sunday to evaluate to a '1' in  TO_CHAR().  Ticket 14149: Check whether our LIKE implementation will  work for this connection or we need to fall back on LIKEC.  This check is performed only once per DatabaseWrapper  instance per thread, since subsequent connections will use  the same settings.
Django docs specify cx_Oracle version 4.3.1 or higher, but  stmtcachesize is available only in 4.3.2 and up.  Ensure all changes are preserved even when AUTOCOMMIT is False.
cx_Oracle 5.0.4 raises a cx_Oracle.DatabaseError exception  with the following attributes and values:   code = 2091   message = 'ORA-02091: transaction rolled back             'ORA-02291: integrity constraint (TEST_DJANGOTEST.SYS                _C00102056) violated - parent key not found'  We convert that particular case to our IntegrityError exception
Oracle doesn't support releasing savepoints. But we fake them when query  logging is enabled to keep query counts consistent with other backends.
With raw SQL queries, datetimes can reach this function  without being converted by DateTimeField.get_db_prep_value.
Oracle doesn't recognize True and False correctly in Python 3.  The conversion done below works both in 2 and 3.  To transmit to the database, we need Unicode if supported  To get size right, we must consider bytes.  We could optimize by only converting up to 4000 bytes here  If parameter has `input_size` attribute, use that.  Mark any string param greater than 4000 characters as a CLOB.
Necessary to retrieve decimal values without rounding error.  Default arraysize of 1 is highly sub-optimal.
Try dict handling; if that fails, treat as sequence  It's not a list of dicts; it's a list of sequences
Try dict handling; if that fails, treat as sequence
cx_Oracle wants no trailing ';' for SQL statements.  For PL/SQL, it  it does want a trailing ';' but not a trailing '/'.  However, these  characters must be included in the original query in case the query  is being passed to SQL*Plus.  Handle params as dict  Handle params as sequence
cx_Oracle <= 4.4.0 wrongly raises a DatabaseError for ORA-01400.
No params given, nothing to do  uniform treatment for sequences and iterables
we build a list of formatted params; as we're going to traverse it  more than once, we can't make it lazy by using a generator  cx_Oracle <= 4.4.0 wrongly raises a DatabaseError for ORA-01400.
already closed
Cast numeric values as the appropriate Python type based upon the  cursor description, and convert strings to unicode.  NUMBER column: decimal-precision floating point  This will normally be an integer from a sequence,  but it could be a decimal value.  FLOAT column: binary-precision floating point.  This comes from FloatField columns.  NUMBER(p,s) column: decimal-precision fixed point.  This comes from IntField and DecimalField columns.  No type information. This normally comes from a  mathematical expression in the SELECT list. Guess int  or Decimal based on whether it has a decimal point.
The `do_offset` flag indicates whether we need to construct  the SQL needed to use limit/offset with Oracle.  Wrap the base query in an outer SELECT * with boundaries on  the "_RN" column.  This is the canonical way to emulate LIMIT  and OFFSET on Oracle.
select for update with limit can be achieved on Oracle, but not with the current backend.
Check whether cx_Oracle was compiled with the WITH_UNICODE option if cx_Oracle is pre-5.1. This will  also be True for cx_Oracle 5.1 and in Python 3.0. See 19606
Structure returned by DatabaseIntrospection.get_table_list()
Structure returned by the DB-API cursor.description interface (PEP 249)
If this is an m2m using an intermediate table,  we don't need to reset the sequence.
Integer field safe ranges by `internal_type` as documented  in docs/ref/models/fields.txt.
RemovedInDjango20Warning
Convert params to contain Unicode values.
Same as prep_for_like_query(), but called for "iexact" matches, which  need not necessarily be implemented using "LIKE" in the backend.
Filters out m2m objects from reverse relations.  Returns (old_relation, new_relation) tuples.
Overrideable SQL templates
Log the command we're running, then run it
Get the column's type and use that as the basis of the SQL  Check for fields that aren't actually columns (e.g. M2M)  Work out nullability  If we were told to include a default value, do so  Some databases can't take defaults as a parameter (oracle)  If this is the case, the individual schema backend should  implement prepare_default  Oracle treats the empty string ('') as null, so coerce the null  option whenever '' is a possible value.  Primary key/unique outputs  Optionally add the tablespace if it's an implicitly indexed column  Return the sql
If it's a callable, call it  Run it through the field's get_db_prep_save method so we can send it  to the database.
Create column SQL, add FK deferreds if needed  SQL  Check constraints can go on the column SQL here  Autoincrement SQL (for backends with inline variant)  FK  Add the SQL to our big list  Autoincrement SQL (for backends with post table definition variant)
Add any unique_togethers (always deferred, as some fields might be  created afterwards, like geometry fields with some backends)  Make the table  Prevent using [] as params, in the case a literal '%' is used in the definition
Add any field index and index_together's (deferred as SQLite3 _remake_table needs it)
Make M2M tables
Handle auto-created intermediary models
Delete the table
Deleted uniques  Created uniques
Deleted indexes  Created indexes
Special-case implicit M2M tables  Get the column's definition  It might not actually have a column behind it  Check constraints can go on the column SQL here  Build the SQL and run it  Drop the default if we need to  (Django usually does not use in-database defaults)  Add an index, if required  Add any FK constraints later  Reset connection if required
Special-case implicit M2M tables  It might not actually have a column behind it  Drop any FK constraints, MySQL requires explicit deletion  Delete the column  Reset connection if required
Ensure this field is even column-based  Both sides have through models; this is a no-op.
Drop any FK constraints, we'll remake them later  Has unique been removed?  Find the unique constraint for this field  Drop incoming FK constraints if we're a primary key and things are going  to change.  '_meta.related_field' also contains M2M reverse fields, these  will be filtered out  Removed an index? (no strict check, as multiple indexes are possible)  Find the index for this field  Change check constraints?  Have they renamed the column?  Next, start accumulating actions to do  Type change?  When changing a column NULL constraint to NOT NULL with a given  default value, we need to perform 4 steps:   1. Add a default for new incoming writes   2. Update existing NULL rows with new default   3. Replace NULL constraint with NOT NULL   4. Drop the default again.  Default change?  Some databases can't take defaults as a parameter (oracle)  If this is the case, the individual schema backend should  implement prepare_default  Nullability change?  The field is nullable in the database anyway, leave it alone  Only if we have a default and there is a change from NULL to NOT NULL  If we don't have to do a 4-way default alteration we can  directly run a (NOT) NULL alteration  Combine actions together if we can (e.g. postgres)  Apply those actions  Update existing rows with default value  Since we didn't run a NOT NULL change before we need to do it  now  Added a unique?  Added an index?  Type alteration on primary key? Then we need to alter the column  referring to us.  Changed to become primary key?  Note that we don't detect unsetting of a PK, as we assume another field  will always come along and replace it.  First, drop the old PK  Make the new one  Update all referencing columns  Handle our type alters on the other end of rels from the PK stuff above  Does it have a foreign key?  Rebuild FKs that pointed to us if we previously had to drop them  Does it have check constraints we need to add?  Drop the default if we need to  (Django usually does not use in-database defaults)  Reset connection if required
Rename the through table  Repoint the FK to the other side  We need the field that points to the target model, so we can tell alter_field to change it -  this is m2m_reverse_field_name() (as opposed to m2m_field_name, which points to our model)  for self-referential models we need to alter field from the other end too
If there is just one column in the index, use a default algorithm from Django  Else generate the name for the index using a different algorithm  If the index name is too long, truncate it  It shouldn't start with an underscore (Oracle hates this)  If it's STILL too long, just hash it down  It can't start with a number on Oracle, so prepend D if we need to
The prefix to put on the default database name when creating  the test database.
Don't import django.core.management if it isn't needed.
We could skip this call if keepdb is True, but we instead  give it the keepdb param. This is to handle the case  where the test DB doesn't exist, in which case we need to  create it, then just not destroy it. If we instead skip  this, we will get an exception.
We report migrate messages at one level lower than that requested.  This ensures we don't get flooded with messages during testing  (unless you really ask to be flooded).
We then serialize the current state of the database into a string  and store it on the connection. This slightly horrific process is so people  who are testing on databases without transactions or who are using  a TransactionTestCase still get a clean database on every test run.
Ensure a connection for the side effect of initializing the test database.
Build list of all apps to serialize
Make a function to iteratively return every object  Serialize to a string
Create the test database and connect to it.  if we want to keep the db, then no need to do any of the below,  just return and skip it all.
We could skip this call if keepdb is True, but we instead  give it the keepdb param. See create_test_db for details.
When this function is called, the test database has been created  already and its name has been copied to settings_dict['NAME'] so  we don't need to call _get_test_db_name.
if we want to preserve the database  skip the actual destroying piece.
Restore the original database name
Remove the test database to clean up after  ourselves. Connect to the previous database (not the test database)  to do so, because it's not allowed to delete a database while being  connected to it.  Wait to avoid "database is being accessed by other users" errors.
Mapping of Field objects to their column types.  Mapping of Field objects to their SQL suffix such as AUTOINCREMENT.  Mapping of Field objects to their SQL for CHECK constraints.
Connection related attributes.  The underlying database connection.  `settings_dict` should be a dictionary containing keys such as  NAME, USER, etc. It's called `settings_dict` instead of `settings`  to disambiguate it from Django settings modules.  Query logging in debug mode or when explicitly enabled.
Transaction related attributes.  Tracks if the connection is in autocommit mode. Per PEP 249, by  default, it isn't.  Tracks if the connection is in a transaction managed by 'atomic'.  Increment to generate unique savepoint ids.  List of savepoints created by 'atomic'.  Tracks if the outermost 'atomic' block should commit on exit,  ie. if autocommit was active on entry.  Tracks if the transaction should be rolled back to the next  available savepoint because of an exception in an inner block.
Connection termination related attributes.
Thread-safety related attributes.
A list of no-argument functions to run when the transaction commits.  Each entry is an (sids, func) tuple, where sids is a set of the  active savepoint IDs when this function was registered.
Should we run the on-commit hooks the next time set_autocommit(True)  is called?
Only this branch requires pytz.
Check for invalid configurations.  In case the previous connection was closed while in an atomic block  Reset parameters defining when to close the connection  Establish the connection
A successful commit means that the database connection works.
A successful rollback means that the database connection works.
Don't call validate_no_atomic_block() to avoid making it difficult  to get rid of a connection in an invalid state. The next connect()  will reset the transaction state anyway.
Savepoints cannot be created outside a transaction
Remove any callbacks registered while this savepoint was active.
If the application didn't restore the original autocommit setting,  don't take chances, drop the connection.
If an exception other than DataError or IntegrityError occurred  since the last commit / rollback, check if the connection works.
Transaction in progress; save for execution on commit.
No transaction in progress and in autocommit mode; execute  immediately.
This should be a string representing the name of the executable  (e.g., "psql"). Subclasses must override this.
connection is an instance of BaseDatabaseWrapper.
Does the backend distinguish between '' and None?
Does the backend allow inserting duplicate NULL rows in a nullable  unique field? All core backends implement this correctly, but other  databases such as SQL Server do not.
Does the backend allow inserting duplicate rows when a unique_together  constraint exists and some fields are nullable but not all of them?
If True, don't use integer foreign keys referring to, e.g., positive  integer primary keys.
Does the default test database allow multiple connections?  Usually an indication that the test database is in-memory
Can an object be saved without an explicit primary key?
Can a fixture contain forward references? i.e., are  FK constraints checked at the end of transaction, or  at the end of each save operation?
Does the backend truncate names properly when they are too long?
Is there a REAL datatype in addition to floats/doubles?
Is there a true datatype for uuid?
Is there a true datatype for timedeltas?
Does the database driver supports same type temporal data subtraction  by returning the type used to store duration field?
Does the database driver support timedeltas as arguments?  This is only relevant when there is a native duration field.  Specifically, there is a bug with cx_Oracle:  https://bitbucket.org/anthony_tuininga/cx_oracle/issue/7/
Do time/datetime fields have microsecond precision?
Does the __regex lookup support backreferencing and grouping?
Can date/datetime lookups be performed using a string?
Can datetimes with timezones be used?
Does the database have a copy of the zoneinfo database?
When performing a GROUP BY, is an ORDER BY NULL required  to remove any ordering?
Does the backend order NULL values as largest or smallest?
Is there a 1000 item limit on query parameters?
Can an object have an autoincrement primary key of 0? MySQL says No.
Do we need to NULL a ForeignKey out, or can the constraint check be  deferred
date_interval_sql can properly handle mixed Date/DateTime fields and timedeltas
Does the backend support tablespaces? Default to False because it isn't  in the SQL standard.
Does the backend reset sequences between tests?
Can the backend determine reliably the length of a CharField?
Can the backend determine reliably if a field is nullable?  Note that this is separate from interprets_empty_strings_as_nulls,  although the latter feature, when true, interferes with correct  setting (and introspection) of CharFields' nullability.  This is True for all core backends.
Can the backend introspect the default value of a column?
Confirm support for introspected foreign keys  Every database can do this reliably, except MySQL,  which can't do it for MyISAM tables
Can the backend introspect an AutoField, instead of an IntegerField?
Can the backend introspect a BigIntegerField, instead of an IntegerField?
Can the backend introspect an BinaryField, instead of an TextField?
Can the backend introspect an DecimalField, instead of an FloatField?
Can the backend introspect an IPAddressField, instead of an CharField?
Can the backend introspect a PositiveIntegerField, instead of an IntegerField?
Can the backend introspect a SmallIntegerField, instead of an IntegerField?
Can the backend introspect a TimeField, instead of a DateTimeField?
Support for the DISTINCT ON clause
Does the backend decide to commit before SAVEPOINT statements  when autocommit is disabled? http://bugs.python.org/issue8145msg109965
Does the backend prevent running SQL queries in broken transactions?
Can we roll back DDL in a transaction?
Can we issue more than one ALTER COLUMN clause in an ALTER TABLE?
Does it support foreign keys?
Does it support CHECK constraints?
Does the backend support 'pyformat' style ("... %(name)s ...", {'name': value})  parameter passing? Note this can be provided by the backend even if not  supported by the Python driver
Does the backend require literal defaults, rather than parameterized ones?
Does the backend require a connection reset after each material schema change?
What kind of error does the backend throw when accessing closed cursor?
Does 'a' LIKE 'A' match?
Does the backend require the sqlparse library for splitting multi-line  statements before executing them?
Suffix for backends that don't support "SELECT xxx;" queries.
If NULL is implied on columns without needing to be explicitly specified
Does the backend support "select for update" queries with limit (and offset)?
Does the backend ignore null expressions in GREATEST and LEAST queries unless  every expression is null?
Can the backend clone databases for parallel test execution?  Defaults to False to allow third-party backends to opt-in.
This light wrapper "fakes" a dictionary interface, because some SQLite data  types include variables in them -- e.g. "varchar(30)" -- and can't be matched  as a simple dictionary lookup.  Maps SQL types to Django Field types. Some of the SQL types have multiple  entries here because SQLite allows for anything and doesn't normalize the  field type; it uses whatever was given.
TODO: remove when SQLite < 3.7.15 is sufficiently old.  3.7.13 ships in Debian stable as of 2014-03-21.
Dictionary of relations to return
Schema for this table  It might be a view, then no results will be returned
Walk through and look for references to other tables. SQLite doesn't  really have enforced references, but since it echoes out the SQL used  to create the table we can look for REFERENCES statements used there.
Find name of the target FK field
Schema for this table
Walk through and look for references to other tables. SQLite doesn't  really have enforced references, but since it echoes out the SQL used  to create the table we can look for REFERENCES statements used there.
This will append (column_name, referenced_table_name, referenced_column_name) to key_columns
seq, name, unique  Skip indexes across multiple fields
Don't use PRAGMA because that causes issues with some transactions
cid, name, type, notnull, default_value, pk
Get the index info  Sqlite3 3.8.9+ has 5 columns, however older versions only give 3  columns. Discard last 2 columns if there.  Get the index info for that index  Get the PK  SQLite doesn't actually give a name to the PK constraint,  so we invent one. This is fine, as the SQLite backend never  deletes PK constraints by name, as you can't delete constraints  in SQLite; we remake the table with a new PK instead.
Not every subexpression has an output_field which is fine  to ignore.
sqlite doesn't support extract, so we fake it with the user-defined  function django_date_extract that's registered in connect(). Note that  single quotes are used because this is a string (and could otherwise  cause a collision with a field name).
sqlite doesn't support DATE_TRUNC, so we fake it with a user-defined  function django_date_trunc that's registered in connect(). Note that  single quotes are used because this is a string (and could otherwise  cause a collision with a field name).
Same comment as in date_extract_sql.
Same comment as in date_trunc_sql.
sqlite doesn't support extract, so we fake it with the user-defined  function django_time_extract that's registered in connect(). Note that  single quotes are used because this is a string (and could otherwise  cause a collision with a field name).
This function is limited both by SQLITE_LIMIT_VARIABLE_NUMBER (the  number of parameters, default = 999) and SQLITE_MAX_COLUMN (the  number of return values, default = 2000). Since Python's sqlite3  module doesn't expose the get_limit() C API, assume the default  limits are in effect and split the work in batches if needed.
Bypass Django's wrappers and use the underlying sqlite3 connection  to avoid logging this query - it would trigger infinite recursion.  Native sqlite3 cursors cannot be used as context managers.
Python substitutes parameters in Modules/_sqlite/cursor.c with:  pysqlite_statement_bind_parameters(self->statement, parameters, allow_8bit_chars);  Unfortunately there is no way to reach self->statement from Python,  so we quote and substitute parameters manually.  For consistency with SQLiteCursorWrapper.execute(), just return sql  when there are no parameters. See 13648 and 17158.
NB: The generated SQL below is specific to SQLite  Note: The DELETE FROM... SQL generated below works for SQLite databases  because constraints don't exist  Note: No requirement for reset of auto-incremented indices (cf. other  sql_flush() implementations). Just return SQL at this point
SQLite doesn't support tz-aware datetimes
SQLite doesn't support tz-aware datetimes
SQLite doesn't have a power function, so we fake it with a  user-defined function django_power that's registered in connect().
SQLite doesn't enforce any integer constraints
Some SQLite schema alterations need foreign key constraints to be  disabled. This is the default in SQLite but can be changed with a  build flag and might change in future, so can't be relied upon.  We enforce it here for the duration of the transaction.
Restore initial FK setting - PRAGMA values can't be parametrized
The backend "mostly works" without this function and there are use  cases for compiling Python without the sqlite3 libraries (e.g.  security hardening).  Manual emulation of SQLite parameter quoting  Bytes are only allowed for BLOB fields, encoded as string  literals containing hexadecimal data and preceded by a single "X"  character:  value = b'\x01\x02' => value_hex = b'0102' => return X'0102'  Use 'ascii' encoding for b'01' => '01', no need to use force_text here.
Self-referential fields must be recreated rather than copied from  the old model to ensure their remote_field.field_name doesn't refer  to an altered field.  Work out the new fields dict / mapping  Since mapping might mix column names and default values,  its values must be already quoted.  This maps field names (not columns) for things like unique_together  If any of the new or altered fields is introducing a new PK,  remove the old one  Add in any created fields  Choose a default and insert it into the copy map  Add in any altered fields  Remove any deleted fields  Remove any implicit M2M tables  Work inside a new app registry
Provide isolated instances of the fields to the new model body so  that the existing model's internals aren't interfered with when  the dummy model is constructed.
Work out the new value of unique_together, taking renames into  account
Work out the new value for index_together, taking renames into  account
Construct a new model for the new state
We need to modify model._meta.db_table, but everything explodes  if the change isn't reversed before the end of this method. This  context manager helps us avoid that situation.
Rename the old table to make way for the new
Create a new table with the updated schema. We remove things  from the deferred SQL that match our table name, too
Copy data from the old table into the new table
Delete the old table
Run deferred SQL on correct table  Fix any PK-removed field
Delete the table (and only that)
Special-case implicit M2M tables
M2M fields are a special case  For implicit M2M tables, delete the auto-created table  For explicit "through" M2M fields, do nothing  For everything else, remake.  It might not actually have a column behind it
Alter by remaking table
The field name didn't change, but some options did; we have to propagate this altering.  We need the field that points to the target model, so we can tell alter_field to change it -  this is m2m_reverse_field_name() (as opposed to m2m_field_name, which points to our model)
Make a new through table  Copy the data across  Delete the old through table
Erase the old test database
Forking automatically makes a copy of an in-memory database.  Erase the old test database
Remove the SQLite database file
Remove this function and rely on the default adapter in Django 2.0.  This doesn't account for the database connection's timezone,  which isn't known. (That's why this adapter is deprecated.)
SQLite doesn't actually support most of these types, but it "does the right  thing" given more verbose field definitions, so leave them as is so that  schema inspection is more useful.  SQLite requires LIKE statements to include an ESCAPE clause if the value  being escaped has a percent or underscore in it.  See http://www.sqlite.org/lang_expr.html for an explanation.
The patterns below are used to generate SQL pattern lookup clauses when  the right-hand side of the lookup isn't a raw string (it might be an expression  or the result of a bilateral transformation).  In those cases, special characters for LIKE operators (e.g. \, *, _) should be  escaped on database side.  Note: we use str.format() here for readability as '%' is used as a wildcard for  the LIKE operator.
Always allow the underlying SQLite connection to be shareable  between multiple threads. The safe-guarding will be handled at a  higher level by the `BaseDatabaseWrapper.allow_thread_sharing`  property. This is necessary as the shareability is disabled by  default in pysqlite and it cannot be changed once a connection is  opened.
If database is in memory, closing the connection destroys the  database. To prevent accidental data loss, ignore close requests on  an in-memory db.
When 'isolation_level' is not None, sqlite3 commits before each  savepoint; it's a bug. When it is None, savepoints don't make sense  because autocommit is enabled. The only exception is inside 'atomic'  blocks. To work around that bug, on SQLite, 'atomic' starts a  transaction explicitly rather than simply disable autocommit.
sqlite3's internal default is ''. It's different from None.  See Modules/_sqlite/connection.c.  'isolation_level' is a misleading API.  SQLite always runs at the SERIALIZABLE isolation level.
typecast_timestamp returns a date or a datetime without timezone.  It will be formatted as "%Y-%m-%d" or "%Y-%m-%d %H:%M:%S[.%f]"
SQLite cannot handle us only partially reading from a cursor's result set  and then writing the same rows to the database in another cursor. This  setting ensures we always read result sets fully into memory all in one  go.
Override the base class implementations with null  implementations. Anything that tries to actually  do something raises complain; anything that tries  to rollback or undo something raises ignore.
Ticket 17671 - Close instead of passing thru to avoid backend  specific behavior. Catch errors liberally because errors in cleanup  code aren't useful.
"2005-07-29 15:48:00.590358-05"  "2005-07-29 09:56:00-05"  Extract timezone information, if it exists. Currently we just throw  it away, but in the future we may make use of it.
Sanity-check that there are no duplicated field names, bases, or  manager names
Check we didn't inherit from the model  Check we have no FKs/M2Ms with it  Now go over all the strings and compare them
Don't allow optimizations of FKs through models they reference  Check that it doesn't point to the model  Check that it's not through the model
Get all of the related objects we need to repoint  Rename the model  Repoint the FKs and M2Ms pointing to us  The model being renamed does not participate in this relation  directly. Rather, a superclass does.  Use the new related key for self referential related objects.  Repoint M2Ms with through pointing to us
Move the main table  Alter the fields pointing to us  Rename M2M fields whose name is based on this model's name.  Skip self-referential fields as these are renamed above.  Rename the M2M table that's based on this model's name.  Rename the column in the M2M table that's based on this  model's name.
Skip `ModelOperation.reduce` as we want to run `references_model`  against self.new_name.
Rename M2M fields whose name is based on this model's db_table
Remove a field if we need to  Add a field if we need to (altering the column is untouched as  it's likely a rename)
Model options we want to compare and preserve in an AlterModelOptions op
If this migration can be run in reverse.  Some operations are impossible to reverse, like deleting data.
Can this migration be represented as SQL? (things like RunPython cannot)
Should this operation be forced as atomic even on backends with no  DDL transaction support (i.e., does it have no DDL, like RunPython)
Should this operation be considered safe to elide and optimize across?
We capture the arguments to make returning them trivial
We calculate state separately in here since our state functions aren't useful
We calculate state separately in here since our state functions aren't useful  to_state now has the states of all the database_operations applied  which is the from_state for the backwards migration of the last  operation.
Forwards code  Reverse code
RunPython objects have no state effect. To add some, combine this  with SeparateDatabaseAndState.
We now execute the Python code in a context that contains a 'models'  object, representing the versioned models as an app registry.  We could try to override the global cache, but then people will still  use direct imports, so we go with a documentation approach instead.
If preserve default is off, don't use the default for future state
Rename the field  Fix index/unique_together to refer to the new field
Skip `FieldOperation.reduce` as we want to run `references_field`  against self.new_name.
No support on Python 2 if enum34 isn't installed.
See if this operation is in django.db.migrations. If it is,  We can just use the fact we already have that imported,  otherwise, we need to add an import for the operation class.
Only iterate over remaining arguments
Deconstruct operations
Format dependencies and write out swappable dependencies right  No need to output bytestrings for dependencies
Format imports nicely, swapping imports of functions from migration files  for comments
django.db.migrations is always used, but models import may not be.  If models import exists, merge it with migrations import.
Sort imports by the package / module to be imported (the part after  "from" in "from ... import ..." or after "import" in "import ...").
If there's a replaces, make a string for it  Hinting that goes into comment
See if we can import the migrations module directly
Alright, see if it's a direct submodule of the app
In case of using MIGRATION_MODULES setting and the custom package  doesn't exist, create one, starting from an existing package
-*- coding: utf-8 -*-  Generated by Django %(version)s on %(timestamp)s
Get the migrations module directory  I hate doing this, but I don't want to squash other import errors.  Might be better to try a directory check directly.  PY3 will happily import empty dirs as namespaces.  Module is not a package (e.g. migrations.py).  Force a reload if it's already loaded (tests need this)  Scan for .py files  Load them
Do the search
Special-case __first__, which means "the first migration" for  migrated apps, and is ignored for unmigrated apps. It allows  makemigrations to declare dependencies on apps before they even have  migrations.  Ignore __first__ references to the same app (22325)  This app isn't migrated, but something depends on it.  The models will get auto-added into the state, though  so we're fine.
Ignore __first__ references to the same app (22325).
Skip internal dependencies
Load disk data  Load database data  To start, populate the migration graph with nodes for ALL migrations  and their dependencies. Also make note of replacing migrations at this step.  Internal (aka same-app) dependencies.  Replacing migrations.  Add external dependencies now that the internal ones have been resolved.  Carry out replacements where possible.  Get applied status of each of this migration's replacement targets.  Ensure the replacing migration is only marked as applied if all of  its replacement targets are.  A replacing migration can be used if either all or none of its  replacement targets have been applied.  This replacing migration cannot be used because it is partially applied.  Remove it from the graph and remap dependencies to it (25945).  Ensure the graph is consistent.  Check if the missing node could have been replaced by any squash  migration but wasn't because the squash migration was partially  applied before. In that case raise a more understandable exception  (23556).  Get reverse replacements.  Try to reraise exception with more detail.
If the migration is unknown, skip it.
If the table's there, that's fine - we've never changed its schema  in the codebase.  Make the table
No support on Python 2 if enum34 isn't installed.
Prepend the `b` prefix since we're importing unicode_literals
Further error checking  Python 3 is a lot easier, and only uses this branch if it's not local.
Python 2/fallback version  Make sure it's actually there and not an unbound method  Needed on Python 2 only
Serialize functools.partial() arguments  Add any imports needed by arguments
When len(strings)==0, the empty iterable should be serialized as  "()", not "(,)" because (,) is invalid Python syntax.
Nested operation, trailing comma is handled in upper OperationWriter._write()
Don't use the literal "{%s}" as it doesn't support empty set
Strip the `u` prefix since we're importing unicode_literals
When len(value)==0, the empty tuple should be serialized as "()",  not "(,)" because (,) is invalid Python syntax.
The unwrapped value is returned as the first item of the arguments  tuple.
Anything that knows how to deconstruct itself.
Unfortunately some of these are order-dependent.
Use manual caching, @cached_property effectively doubles the  recursion depth for each recursion.  Use self.key instead of self to speed up the frequent hashing  when constructing an OrderedSet.
Use manual caching, @cached_property effectively doubles the  recursion depth for each recursion.  Use self.key instead of self to speed up the frequent hashing  when constructing an OrderedSet.
If the key already exists, then it must be a dummy node.  Promote DummyNode to Node.
Cast list of replaced keys to set to speed up lookup later.  We don't want to create dependencies between the replaced  node and the replacement node as this would lead to  self-referencing on the replacement node at a later iteration.  Again, to avoid self-referencing.
We're only interested in the latest replaced node, so filter out  replaced nodes that are parents of other replaced nodes.  NOTE: There is no need to remap parent dependencies as we can  assume the replaced nodes already have the correct ancestry.
Use parent.key instead of parent to speed up the frequent hashing in ensure_not_cyclic  fallback to iterative dfs
Use child.key instead of child to speed up the frequent hashing in ensure_not_cyclic  fallback to iterative dfs
reverse sorting is needed because prepending using deque.extendleft  also effectively reverses values
Algo from GvR:  http://neopythonic.blogspot.co.uk/2009/01/detecting-cycles-in-directed-graph.html
If it was specified on the command line, definitely true  Otherwise, we look to see if it has a migrations module  without any Python files in it, apart from __init__.py.  Apps from the new app template will have these; the python  file check will ensure we skip South ones.
It's an application with migrations disabled.
None means quit
None means quit
None means quit
Six does not correctly abstract over the fact that  py3 input returns a unicode string, while py2 raw_input  returns a bytestring.
We can't ask the user, so act like the user aborted.
We can't ask the user, so set as not provided.
We can't ask the user, so act like the user aborted.
remove current from todo's nodes & dependencies
Internal tracking variable for test assertions about  of loops
Compare it to each operation after it  Optimize! Add result, then remaining others, then return  We can't optimize across `other`.
Operations to apply during this migration, in order.
Other migrations that should be run before this migration.  Should be a list of (app, migration_name).
Other migrations that should be run after this one (i.e. have  this migration added to their dependencies). Useful to make third-party  apps' migrations run after your AUTH_USER replacement, for example.
Migration names in this app that this migration replaces. If this is  non-empty, this migration will only be applied if all these migrations  are not applied.
Is this an initial migration? Initial migrations are skipped on  --fake-initial if the table or fields already exist. If None, check if  the migration has any dependencies to determine if there are dependencies  to tell if db introspection needs to be done. If True, always perform  introspection. If False, never perform introspection.
Whether to wrap the whole migration in a transaction. Only has an effect  on database backends which support transactional DDL.
Copy dependencies & other attrs as we might mutate them at runtime
If this operation cannot be represented as SQL, place a comment  there instead  Save the state before the operation has run  Run the operation  Force a transaction on a non-transactional-DDL backend or an  atomic operation inside a non-atomic migration.  Normal behaviour
Construct all the intermediate states we need for a reverse migration  Phase 1  If it's irreversible, error out  Preserve new state from previous run to not tamper the same state  over all operations
Phase 2  We're forcing a transaction on a non-transactional-DDL backend  Normal behaviour
If the target is (app_label, None), that means unmigrate everything  If the migration is already applied, do backwards mode,  otherwise do forwards mode.  Don't migrate backwards all the way to the target node (that  may roll back dependencies in other apps that don't need to  be rolled back); instead roll back through target's immediate  child(ren) in the same app, and no further.
Create the forwards plan Django would follow on an empty database
Nothing to do for an empty plan, except for building the post  migrate project state
This should only happen if there's a mixed plan
No need to check for `elif all_backwards` here, as that condition  would always evaluate to true.
We remove every migration that we applied from these sets so  that we can bail out once the last migration has been applied  and don't always run until the very end of the migration  process.
Only mutate the state if the migration is actually applied  to make sure the resulting state doesn't include changes  from unrelated migrations.
Holds all migration states prior to the migrations being unapplied  We remove every migration that we applied from this set so  that we can bail out once the last migration has been applied  and don't always run until the very end of the migration  process.
The state before this migration  The old state keeps as-is, we continue with the new state
Only mutate the state if the migration is actually applied  to make sure the resulting state doesn't include changes  from unrelated migrations.
Generate the post migration state by starting from the state before  the last migration is unapplied and mutating it to include all the  remaining applied migrations.
Test to see if this is an already-applied initial migration
Alright, do it normally  For replacement migrations, record individual statuses
Report progress
For replacement migrations, record individual statuses  Report progress
Bail if the migration isn't the first one in its app
Bail if it's NOT an initial migration
Make sure all create model and add field operations are done  We have to fetch the model to test with from the  main app cache, as it's not a direct dependency.  We have to fetch the model to test with from the  main app cache, as it's not a direct dependency.
Handle implicit many-to-many tables created by AddField.
If we get this far and we found at least one CreateModel or AddField migration,  the migration is considered implicitly applied.
Reverse accessors of foreign keys to proxy models are attached to their  concrete proxied model.
Apps to include from main registry, usually unmigrated ones
Need to do this explicitly since unregister_model() doesn't clear  the cache automatically (24513)
Get all relations to and from the old model before reloading,  as _meta.apps may change
Get all outgoing references from the model to be rendered  Directly related models are the models pointed to by ForeignKeys,  OneToOneFields, and ManyToManyFields.
For all direct related models recursively get all related models.
Include the model itself
Unregister all related models
Gather all models states of those models that will be rerendered.  This includes:  1. All related models of unmigrated apps
2. All related models of migrated apps
Render all models
Not used, but required by AppConfig.__init__
App-label and app-name are not the same thing, so technically passing  in the label here is wrong. In practice, migrations don't care about  the app name, but we need something unique, and the label works fine.
Any apps in self.real_apps should have all their models included  in the render. We don't use the original model instances as there  are some variables that refer to the Apps object.  FKs/M2Ms from real apps are also not included as they just  mess things up with partial states (due to lack of dependencies)  Populate the app registry with a stub for each application.
There shouldn't be any operations pending at this point.
Avoid clearing each model's cache for each change. Instead, clear  all caches when we're finished updating the model instances.
We keep trying to render the models in a loop, ignoring invalid  base errors, until the size of the unrendered models doesn't  decrease by at least one, meaning there's a base dependency loop/  missing base.  Prevent that all model caches are expired for each render.
No need to actually clone them, they'll never change
Sanity-check that fields is NOT a dict. It must be ordered.  Sanity-check that fields are NOT already bound to a model.  Sanity-check that relation fields are NOT referring to a model class.
Deconstruct the fields  Extract the options  Ignore some special options  Force-convert all options to text_type (23226)  If we're ignoring relationships, remove all field-listing model  options (that option basically just means "make a stub model")  Private fields are ignored, so remove options that refer to them.
We can't rely on __mro__ directly because we only want to flatten  abstract models and not the whole tree. However by recursing on  __bases__ we may end up with duplicates and ordering issues, we  therefore discard any duplicates and reorder the bases according  to their index in the MRO.
Make our record  Ensure at least one base inherits from models.Model
Make sure the default manager is always first since ordering chooses  the default manager.
If the default manager doesn't have `use_in_migrations = True`,  shim a default manager so another manager isn't promoted in its  place.
Construct the new ModelState
Sort all managers by their creation counter
First, make a Meta object  Then, work out our bases  Turn fields into a dict for the body, add other bits
Restore managers
Then, make a Model object (apps.register_model is called in __new__)
If this is a type that implements 'deconstruct' as an instance method,  avoid treating this as being deconstructible itself - see 22951
we have a field which also returns a name
The first phase is generating all the operations for each app  and gathering them into a big per-app list.  We'll then go through that list later and order it and split  into migrations to resolve dependencies caused by M2Ms and FKs.
Prepare some old/new state and model lists, separating  proxy models and ignoring unmigrated apps.
Renames have to come first
Prepare lists of fields and generate through model map
Generate non-rename model operations
Generate field operations
On every iteration, we step through all the apps and see if there  is a completed set of operations.  If we find that a subset of the operations are complete we can  try to chop it off from the rest and continue, but we only  do this if we've already been through the list once before  without any chopping and nothing has changed.  We need to temporarily resolve the swappable dependency to prevent  circular references. While keeping the dependency checks on the  resolved model we still add the swappable dependencies.  See 23322  External app dependency. See if it's not yet  satisfied.  If we can't find the other app, we add a first/last dependency,  but only if we've already been through once and checked everything  If the app already exists, we add a dependency on the last migration,  as we don't know which migration contains the target field.  If it's not yet migrated or has no migrations, we use __first__  Make a migration! Well, only if there's stuff to put in it
construct a dependency graph for intra-app dependencies
we use a stable sort for deterministic tests & general behavior
Add in internal dependencies among the migrations
De-dupe dependencies
Optimize migrations
Created model  Created field  Removed field  Removed model  Field being altered  order_with_respect_to being unset for a field  Field is removed and part of an index/unique_together  Unknown dependency. Raise an error.
Dependencies are (app_label, model_name, field_name, create/delete as True/False)
Gather related fields  through will be none on M2Ms on swapped-out models;  we can treat lack of through as auto_created=True, though.  Are there unique/index_together to defer?  Depend on the deletion of any possible proxy version of us  Depend on all bases  Depend on the other end of the primary key if it's a relation  Generate creation operation
Don't add operations which modify the database for unmanaged models
Generate operations for each related field  Depend on our own model being created  Make operation  Generate other opns
Depend on the deletion of any possible non-proxy version of us  Depend on all bases  Generate creation operation  Depend on the deletion of any possible non-proxy version of us
Skip here, no need to handle fields for unmanaged models
Gather related fields  through will be none on M2Ms on swapped-out models;  we can treat lack of through as auto_created=True, though.  Generate option removal first  Then remove each related field  Finally, remove the model.  This depends on both the removal/alteration of all incoming fields  and the removal of all its own related fields, and if it's  a through model the field that references it.
We're referenced in another field's through=  Finally, make the operation, deduping any dependencies
Scan to see if this is actually a rename!
Fields that are foreignkeys/m2ms depend on stuff  You can't just add NOT NULL fields with no default or fields  which don't allow empty strings as default.
We might need to depend on the removal of an  order_with_respect_to or index/unique_together operation;  this is safely ignored if there isn't one
Did the field change?  Implement any model renames on relations; these are handled by RenameModel  so we need to exclude them from the comparison  Either both fields are m2m or neither is  We cannot alter between m2m and concrete fields
Account for FKs to swappable models
We run the old version through the field renames to account for those
unmanaged converted to managed
managed converted to unmanaged
Make sure it comes second if we're adding  (removal dependency is part of RemoveField)  Actually generate the operation
Find the app label's current leaf node  Do they want an initial migration for this app?  They don't.  Work out the next number in the sequence  Name each migration  Now fix dependencies
Gather other app dependencies in a first pass  Keep resolving till there's no change  Remove all migrations that aren't needed
Reset state when entering an outermost atomic block.  Some database adapters (namely sqlite3) don't handle  transactions and savepoints properly when autocommit is off.  Turning autocommit back on isn't an option; it would trigger  a premature commit. Give up if that happens.  Pretend we're already in an atomic block to bypass the code  that disables autocommit to enter a transaction, and make a  note to deal with this case in __exit__.
We're already in a transaction; create a savepoint, unless we  were told not to or we're already waiting for a rollback. The  second condition avoids creating useless savepoints and prevents  overwriting needs_rollback until the rollback is performed.
Prematurely unset this flag to allow using commit or rollback.
The database will perform a rollback by itself.  Wait until we exit the outermost block.
Release savepoint if there is one  The savepoint won't be reused. Release it to  minimize overhead for the database server.  If rolling back to a savepoint fails, mark for  rollback at a higher level and avoid shadowing  the original exception.
Commit transaction  An error during rollback means that something  went wrong with the connection. Drop it.
This flag will be set to True again if there isn't a savepoint  allowing to perform the rollback at this level.  Roll back to savepoint if there is one, mark for rollback  otherwise.  The savepoint won't be reused. Release it to  minimize overhead for the database server.  If rolling back to a savepoint fails, mark for  rollback at a higher level and avoid shadowing  the original exception.  Roll back transaction  An error during rollback means that something  went wrong with the connection. Drop it.
Outermost block exit when autocommit was enabled.  Outermost block exit when autocommit was disabled.
Bare decorator: @atomic -- although the first argument is called  `using`, it's actually the function being decorated.  Decorator: @atomic(...) or context manager: with atomic(...): ...
Only set the 'errors_occurred' flag for errors that may make  the connection unusable.
Note that we are intentionally not using @wraps here for performance  reasons. Refs 21109.
This backend was renamed in Django 1.9.
The database backend wasn't found. Display a helpful error message  listing all possible (built-in) database backends.  If there's some other error, this must be an error in Django
If the router doesn't have a method, skip to the next one.
If the router doesn't have a method, skip to the next one.
If the router doesn't have a method, skip to the next one.
These values, if given to validate(), will trigger the self.required check.
Compile the regex if it was not passed pre-compiled.
IP patterns
Host patterns  Max length for domain name labels is 63 characters per RFC 1034 sec. 3.1
Check first if the scheme is valid
Then check full URL  Trivial case failed. Try for possible IDN domain
Now verify IPv6 in the netloc part
The maximum length of a full host name is 253 characters per RFC 1034  section 3.1. It's defined to be 255 bytes or less, but this includes  one byte for the length of the name and one byte for the trailing dot  that's used to indicate absolute names in DNS.
max length for domain name labels is 63 characters per RFC 1034
literal form, ipv4 or ipv6 address (SMTP 4.1.3)
Try for possible IDN domain-part
digit_tuple doesn't include any leading zeros.  We have leading zeros up to or past the decimal point. Count  everything past the decimal point as a digit. We do not count  0 before the decimal point as a digit since that would mean  we would not allow max_digits = decimal_places.
Get the proper name for the file, as it will actually be saved.
If the filename already exists, add an underscore and a random 7  character alphanumeric string (before the file extension, if one  exists) to the filename until the generated filename doesn't exist.  Truncate original name if required, so the new filename does not  exceed the max_length.  file_ext includes the dot.  Truncate file_root if max_length exceeded.  Entire file_root was truncated in attempt to find an available filename.
`filename` may include a path as returned by FileField.upload_to.
At the end of the deprecation:  raise NotImplementedError('subclasses of Storage must provide a get_accessed_time() method')
At the end of the deprecation:  raise NotImplementedError('subclasses of Storage must provide a get_created_time() method')
At the end of the deprecation:  raise NotImplementedError('subclasses of Storage must provide a get_modified_time() method')
This function is only needed to help with the deprecations above and can  be removed in Django 2.0, RemovedInDjango20Warning.
Create any intermediate directories that do not exist.  Note that there is a race between os.path.exists and os.makedirs:  if os.makedirs fails with EEXIST, the directory was created  concurrently, and we can continue normally. Refs 16082.  os.makedirs applies the global umask, so we reset it,  for consistency with file_permissions_mode behavior.
This file has a file path that we can move.
This is a normal uploadedfile that we can stream.  This fun binary flag incantation makes os.open throw an  OSError if the file already exists before we open it.  The current umask value is masked out by os.open!
Ooops, the file exists. We need a new file name.
OK, the file save worked. Break out of the loop.
Store filenames with forward slashes, even on Windows.
If the file exists, delete it from the filesystem.  If os.remove() fails with ENOENT, the file may have been removed  concurrently, and it's safe to continue normally.
Safe to use .replace() because UTC doesn't have DST
Macintosh, Unix.
All other platforms: check for same pathname.
There's no reason to move if we don't have to.
If the destination file exists and allow_overwrite is False then raise an IOError
This will happen with os.rename if moving to another filesystem  or when moving opened files on certain operating systems
first open the old file, so that it won't go away  now open the new file, not forgetting allow_overwrite
Certain operating systems (Cygwin and Windows)  fail when deleting opened files, ignore it.  (For the  systems where this happens, temporary files will be auto-deleted  on close anyway.)
Sanitize the file name so that it can't be dangerous.  Just use the basename of the file -- anything else is dangerous.
File names longer than 255 characters can cause problems on older OSes.
Means the file was moved or deleted before the tempfile  could unlink it.  Still sets self.file.close_called and  calls self.file.file.close() before the exception
Since it's in memory, we'll never have multiple chunks.
Most of the time Pillow only needs a small chunk to parse the image  and get the dimensions, but with some TIFF files Pillow needs to  parse the whole file.  ignore zlib complaining on truncated stream, just feed more  data to parser (ticket 19457).  Ignore PIL failing on a too short buffer when reads return  less bytes than expected. Skip and feed more data to the  parser (ticket 24544).
--- Adapted from the pyserial project ---  detect size of ULONG_PTR
--- Union inside Structure by stackoverflow:3480240 ---
--- Define function prototypes for extra safety ---
File locking is not supported.
Dummy functions that don't do anything.  File is not locked
File is unlocked
Iterate over this file-like object by newlines  Line split after a \r newline; yield buffer_.  Continue with line.  Line either split without a newline (line  continues after buffer_) or with \r\n  newline (line == b'\n').  buffer_ handled, clear it.
If this is the end of a \n or \r\n line, yield.
Check the content-length header to see if we should  If the post is too large, we cannot use the Memory handler.
Because close can be called during shutdown  we need to cache os.unlink and access it  as self.unlink only
Flag for if it's been compressed or not
Avoid zlib dependency unless compress is being used
TimestampSigner.unsign always returns unicode but base64 and zlib  compression operate on bytes.  It's compressed; uncompress it first
Use of native strings in all versions of Python
Convert the signature from bytes to str only on Python 3
Check timestamp is not older than max_age
-*- coding: utf-8 -*-
... perform checks and collect `errors` ...  or
By default, 'database'-tagged checks are not run as they do more  than mere static code analysis.
Check resolver recursively
This is not a url() instance
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Levels
We need to hardcode ModelBase and Field cases because its __str__  method doesn't return "applabel.modellabel" and cannot be changed.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Short circuit if there aren't any errors.
The or clauses are redundant but work around a bug (25945) in  functools.partial in Python 3 <= 3.5.1 and Python 2 <= 2.7.11.
The receiver is either a function or an instance of class  defining a `__call__` method.
Maps common uses of lazy operations to corresponding error functions  defined above. If a key maps to None, no error will be produced.  default_error() will be used for usages that don't appear in this dict.
-*- coding: utf-8 -*-
no invalid tags
We may have previously seen a "all-models" request for  this app (no model qualifier was given). In this case  there is no need adding specific models to the list.
This is just an app - no model qualifier
Check that the serialization format exists; this is a shortcut to  avoid collating all the objects and _then_ failing.
If dumpdata is outputting to stdout, there is no way to display progress
Legacy behavior, tablename specified as argument
"key" is a reserved word in MySQL, so use "cache_key" instead.
Note that we're assuming OSError means that the client program  isn't installed. There's a possibility OSError would be raised  for some other reason, in which case this error message would be  inaccurate. Still, this message catches the common case.
Close the DB connection -- unless we're still in a transaction. This  is required as a workaround for an  edge case in MySQL: if the same  connection is used to create tables, load data, and query, the query  can return incorrect results. See Django 7572, MySQL 37735.
Keep a count of the installed objects and fixtures
Forcing binary mode may be revisited after dropping Python 2 support (see 22399)
Django's test suite repeatedly tries to load initial_data fixtures  from apps that don't have any fixtures. Because disabling constraint  checks can be expensive on some database (especially MSSQL), bail  out early if no fixtures are found.
Since we disabled constraint checks, we must manually check for  any invalid keys that might have been added
If we found even one object in a fixture, we need to reset the  database sequences.
Warn if the fixture we loaded contains 0 objects.
Save the fixture_dir and fixture_name for future error messages.
Check kept for backwards-compatibility; it isn't clear why  duplicates are only allowed in different directories.
Validation is called explicitly each time the server is reloaded.
We rely on the environment because it's currently the only  way to reach WSGIRequestHandler. This seems an acceptable  compromise considering `runserver` runs indefinitely.
If an exception was silenced in ManagementUtility.execute in order  to be raised in the child process, raise it now.
'shutdown_message' is a stealth option.
Need to check migrations here, so can't use the  requires_migrations_check attribute.
Use helpful error messages instead of ugly tracebacks.  Need to use an OS exit because sys.exit doesn't work in a thread
Kept for backward compatibility
'table_name_filter' is a stealth option
Add primary_key and unique, if necessary.
Calling `get_field_type` to get the field type string and any  additional parameters and notes.
Don't output 'id = meta.AutoField(primary_key=True)', because  that's assumed if it doesn't exist.
Custom fields will have a dotted path
Only add the comment if the double underscore was in the original name
This is a hook for data_types_reverse to return a tuple of  (field_type, field_params_dict).
Add max_length for all CharFields.
we do not want to include the u"" or u'' prefix  so we build the string rather than interpolate the tuple
Inspired by Postfix's "postconf -n".
Because settings are imported lazily, we need to explicitly load them.
Load the current graph state, check the app and migration they asked for exists
Work out the list of predecessor migrations
Tell them what we're doing and optionally ask if we should proceed
Load the operations from all those migrations and concat together,  along with collecting external dependencies and detecting  double-squashing  We need to take all dependencies from the first migration in the list  as it may be 0002 depending on 0001
Work out the value of replaces (any squashed ones we're re-squashing)  need to feed their replaces into ours
Make a new migration with those operations
Write out the new migration file
Remove '.py' suffix  Preserve '.\' prefix on Windows to respect gettext behavior
This check is needed for the case of a symlinked file and its  source being processed inside a single group (locale dir);  removing either of those two removes both.
Ensure last line has its EOL
Strip the header
Need to ensure that the i18n framework is enabled
Avoid messing with mutable class variables
Allow to run makemessages inside an app dir
Build locale list
Account for excluded locales
Build po files for each selected locale
Gettext tools will output system-encoded bytestrings instead of UTF-8,  when looking up the version. It's especially a problem on Windows.
Print warnings
-*- coding: utf-8 -*-
sqlmigrate doesn't support coloring its output but we need to force  no_color=True so that the BEGIN/COMMIT statements added by  output_transaction don't get colored either.
Get the database we're operating from
Load up an executor to get all the migration data
Resolve command-line arguments into a migration
Show begin/end around output only for atomic migrations
Make a plan that represents just the requested migrations and show SQL  for it
Create a test database.
Import the fixture data into the test database.
Run the development server. Turn off auto-reloading because it causes  a strange error -- it causes this handle() method to be called  multiple times.
The following are stealth options used by Django's internals.
Import the 'management' module within each installed app, to register  dispatcher events.
Empty sql_list may signify an empty database and post_migrate would then crash  Emit the post migrate signal. This allows individual applications to  respond as if the database had been migrated from scratch.
Check that the app_name cannot be imported.
no IPython, raise ImportError
Set up a dictionary to serve as the environment for the shell, so  that tab completion works on objects that are imported at runtime.
We don't have to wrap the following import in a 'try', because  we already know 'readline' was imported successfully.  Enable tab completion on systems using libedit (e.g. Mac OSX).  These lines are copied from Lib/site.py on Python 3.4.
We want to honor both $PYTHONSTARTUP and .pythonrc.py, so follow system  conventions and get $PYTHONSTARTUP first then .pythonrc.py.
Execute the command and exit.
Known side effect: updating file access/modified time to current time if  it is writable.
Walk entire tree, looking for locale directories
Gather existing directories.
Build locale list
Account for excluded locales
Check writability on first location
-*- coding: utf-8 -*-
Get the database we're operating from
Load migrations from disk/DB  If we were passed a list of apps, validate it  Otherwise, show all apps in alphabetic order  For each app, print its migrations in order from oldest (roots) to  newest (leaves).  Give it a nice title if it's a squashed one  Mark it as applied/unapplied  If we didn't print anything, then a small message
Load migrations from disk/DB
Generate the plan
Output
Check that the project_name cannot be imported.
Create a random SECRET_KEY to put it in the main settings.
-*- coding: utf-8 -*-
Import the 'management' module within each installed app, to register  dispatcher events.
Get the database we're operating from
Hook for backends needing any database preparation  Work out which apps have migrations and which do not
Raise an error if any migrations are applied before their dependencies.
Before anything else, see if there's conflicting apps and drop out  hard if there are any
If they supplied command line arguments, work out what they mean.
Print some useful info
Run the syncdb phase.
Migrate!  If there's changes that aren't in migrations yet, tell them how to fix it.
Re-render models of real apps to include relationships now that  we've got a final state. This wouldn't be necessary if real apps  models were rendered with relationships in the first place.
Send the post_migrate signal, so individual apps can do whatever they need  to do at this point.
Get a list of already installed *models* so that references work right.
Build the manifest of apps and models that are to be synchronized
Note that if a model is unmanaged we short-circuit and never try to install it
Create the tables for each model
Make sure the app they asked for exists
Load the current graph state. Pass in None for the connection so  the loader doesn't try to resolve replaced migrations from DB.
Raise an error if any migrations are applied before their dependencies.
Before anything else, see if there's conflicting apps and drop out  hard if there are any and they don't want to merge
If app_labels is specified, filter out conflicting migrations for unspecified apps
If they want to merge and there's nothing to merge, then politely exit
If they want to merge and there is something to merge, then  divert into the merge code
Set up autodetector
If they want to make an empty migration, make one for each app  Make a fake changes() result we can pass to arrange_for_graph
Detect changes
No changes? Tell them.
Describe the migration  Display a relative path if it's below the current working  directory, or an absolute path otherwise.  Write the migrations file to the disk.  We just do this once per app  Alternatively, makemigrations --dry-run --verbosity 3  will output the migrations to stdout rather than saving  the file to the disk.
Grab out the migrations in question, and work out their  common ancestor.
Now work out the operations along each divergent branch  In future, this could use some of the Optimizer code  (can_optimize_through) to automatically see if they're  mergeable. For now, we always just prompt the user.  If they still want to merge it, then write out an empty  file depending on the migrations needing merging.
Write the merge migrations file to the disk
Alternatively, makemigrations --merge --dry-run --verbosity 3  will output the merge migrations to stdout rather than saving  the file to the disk.
Command object passed in.
Load the command object by name.
If the command is already loaded, use it directly.
Simulate argument parsing to get the option defaults (see 10080 for details).  Use the `dest` option name from the parser option  Move positional args out of options to mimic legacy optparse
Output an extra note if settings are not properly configured
Get commands outside of try block to prevent swallowing exceptions  If `subcommand` is missing due to misconfigured settings, the  following line will retrigger an ImproperlyConfigured exception  (get_commands() swallows the original one) so the user is  informed about it.  If the command is already loaded, use it directly.
Don't complete if user hasn't sourced bash_completion file.
subcommand  subcommand options  special case: the 'help' subcommand has no options  special case: add the names of installed apps to options  Get the last part of the dotted path as the app name.  Fail silently if DJANGO_SETTINGS_MODULE isn't set. The  user will find out once they execute the command.  filter out previously specified options from available options
filter options by current input  append '=' to options which require args  Exit code of the bash completion function is never passed back to  the user, so it's safe to always exit with 0.  For more details see 25420.
Preprocess options to extract --settings and --pythonpath.  These options could affect the commands that are available, so they  must be processed early.
A handful of built-in management commands work without settings.  Load the default settings -- where INSTALLED_APPS is empty.
Start the auto-reloading dev server even if the code is broken.  The hardcoded condition is a code smell but we can't rely on a  flag on the command class because we haven't located it yet.  The exception will be raised later in the child process  started by the autoreloader. Pretend it didn't happen by  loading an empty list of applications.
In all other cases, django.setup() is required to succeed.
Special-cases: We want 'django-admin --version' and  'django-admin --help' to work, for backwards compatibility.
Can't import settings during this command, because they haven't  necessarily been created.  The supported URL schemes  Can't perform any active locale changes during this command, because  setting might not be available at all.  Rewrite the following suffixes when determining the target filename.  Allow shipping invalid .py files without byte-compilation.
if some directory is given, make sure it's nicely expanded
Setup a stub settings environment for template rendering
Ignore some files as they cause various breakages.
Only render the Python files, as we don't want to  accidentally render Django templates files
downloads the file and returns the path
If it's not a valid directory name.  Provide a smart error message, depending on the error.
Trying to get better name from response headers
Falling back to content type guessing
Move the temporary file to a filename that has better  chances of being recognized by the archive utils
Giving up
On Jython there is no os.access()
Catch missing argument for a better error message
Metadata about this command.
Configuration shortcuts that alter various logic.
Move positional args out of options to mimic legacy optparse
SystemCheckError takes care of its own formatting.
Only mess with locales if we can assume we have a working  settings file, because django.utils.translation requires settings  (The final saying about whether the i18n machinery is active will be  found in the value of the USE_I18N setting)  Deactivate translations, because django-admin creates database  content like permissions, and those shouldn't contain any  translations.
No databases are configured (or the dummy one)
isatty is not always implemented, 6223.
The nocolor palette has all available roles.  Use that palette as the basis for populating  the palette as defined in the environment.
For backwards compatibility,  set style for ERROR_OUTPUT == ERROR
check if there are funny path extensions for executables, e.g. Windows  don't use extensions if the command ends with one of them  check if we find the command on PATH
Emit the pre_migrate signal for every application.
Emit the post_migrate signal for every application.
Try to get the CACHES entry for the given backend name first  Trying to import the given backend, in case it's a dotted path
Some caches -- python-memcached in particular -- need to do a cleanup at the  end of a request cycle. If not implemented in a particular backend  cache.close is a no-op
ENOENT can happen if the cache file is removed (by another  process) after the os.path.exists check.
Delete a random selection of entries
The exception type to catch from the underlying library for a key  that was not found. This is a ValueError for python-memcache,  pylibmc.NotFound for pylibmc, and cmemcache will return None without  raising an exception.
Using 0 in memcache sets a non-expiring timeout.
Other cache backends treat 0 as set-and-expire. To achieve this  in memcache backends, a negative timeout must be passed.
See http://code.google.com/p/memcached/wiki/NewProgrammingExpiration  "Expiration times can be set from 0, meaning "never expire", to  30 days. Any time higher than 30 days is interpreted as a Unix  timestamp date. If you want to expire an object on January 1st of  next year, this is how you do that."  This means that we have to switch to absolute timestamps.
Python 2 memcache requires the key to be a byte string.
make sure the key doesn't keep its old value in case of failure to set (memcached's 1MB limit)
memcached doesn't support a negative delta
python-memcache responds to incr on non-existent keys by  raising a ValueError, pylibmc by raising a pylibmc.NotFound  and Cmemcache returns None. In all cases,  we should raise a ValueError though.
memcached doesn't support a negative delta
python-memcache responds to incr on non-existent keys by  raising a ValueError, pylibmc by raising a pylibmc.NotFound  and Cmemcache returns None. In all cases,  we should raise a ValueError though.
Global in-memory store of cache data. Keyed by name, to provide  multiple named local memory caches.
The DB column is expecting a string, so make sure the value is a  string, not bytes. Refs 19274.  Note: typecasting for datetimes is needed by some 3rd party  database backends. All core backends work without typecasting,  so be careful about changes here - test suite will NOT pick  regressions.
To be threadsafe, updates/inserts are allowed to fail silently
Stub class to ensure not passing in a `timeout` argument results in  the default timeout
Memcached does not accept keys longer than this.
ticket 21147 - avoid time.time() related precision issues
Fetch the value again to avoid a race condition if another caller  added a value between the first get() and the add() above.
This is a separate method, rather than just a copy of has_key(),  so that it always has the same functionality as has_key(), even  if a subclass overrides it.
AttributeError if object_list has no count() method.  TypeError if object_list.count() requires arguments  (i.e. is of type list).
The object_list is converted to a list so that if it was a QuerySet  it won't be a database hit per __getitem__.
Special case, return zero if no items.
Special case for the last page because there can be orphans.
PY2 can't pickle naive exception: http://bugs.python.org/issue1692335.
PY2 has a `message` property which is always there so we can't  duck-type on it. It was introduced in Python 2.5 and already  deprecated in Python 2.6.
Normalize plain strings to instances of ValidationError.
Trigger an AttributeError if this ValidationError  doesn't have an error_dict.
encode() and decode() expect the charset to be a native string.
since size is not None here, len(self.buffer) < size
Sometimes PATH_INFO exists, but is empty (e.g. accessing  the SCRIPT_NAME URL without a trailing slash). We really need to  operate as if they'd requested '/'. Not amazingly nice to force  the path like this, but should be harmless.
be careful to only replace the first slash in the path because of  http://test/something and http://test//something being different as  stated in http://www.ietf.org/rfc/rfc2396.txt
The WSGI spec says 'QUERY_STRING' may be absent.
If Apache's mod_rewrite had a whack at the URL, Apache set either  SCRIPT_URL or REDIRECT_URL to the full resource URL before applying any  rewrites. Unfortunately not every Web server (lighttpd!) passes this  information through all the time, so FORCE_SCRIPT_NAME, above, is still  needed.
mod_wsgi squashes multiple successive slashes in PATH_INFO,  do the same with script_url before manipulating paths (17133).
Under Python 3, non-ASCII values in the WSGI environ are arbitrarily  decoded with ISO-8859-1. This is wrong for Django websites where UTF-8  is the default. Re-encode to recover the original bytestring.
We only assign to this when initialization is complete as it is used  as a flag for initialization being complete.
Unfortunately, inspect.getargspec result is not trustable enough  depending on the callback wrapping in decorators (frequent for handlers).  Falling back on try/except:
Setup default url resolver for this thread
Apply response middleware, regardless of the response  Complain if the response middleware returned None (a common error).
If the exception handler returns a TemplateResponse that has not  been rendered, force it to be rendered.
Apply view middleware
Complain if the view returned None (a common error).
If the response supports deferred rendering, apply template  response middleware and then render the response  Complain if the template response middleware returned None (a common error).
If Http500 handler is not installed, re-raise last exception  Return an HttpResponse that displays a friendly error message.
Apply request middleware
Imported for backwards compatibility and for the sake  of a cleaner namespace. These symbols used to be in  django/core/mail.py before the introduction of email  backends and the subsequent reorganization (See 10355)
Nothing to do if the connection is already open.
If local_hostname is not specified, socket.getfqdn() gets used.  For performance, we use the cached FQDN for local_hostname.
TLS/SSL are mutually exclusive, so only attempt TLS over  non-secure connections.
This happens when calling quit() on a TLS connection  sometimes, or when the connection was already disconnected  by the server.
We failed silently on open().  Trying to send would be pointless.
Make sure self.file_path is a string.  Make sure that self.file_path is an directory if it exists.  Try to create it, if it not exists.  Make sure that self.file_path is writable.  Finally, call super().  Since we're using the console-based backend as a base,  force the stream to be None, so we don't default to stdout
Cache the hostname, but do it lazily: socket.getfqdn() can take a couple of  seconds, which slows down the restart of the server.
Don't BASE64-encode UTF-8 messages so that we avoid unwanted attention from  some spam filters.
Default MIME type to use on attachments (if it is not explicitly given  and cannot be guessed).
stdlib uses socket.getfqdn() here instead
Header names that contain structured address data (RFC 5322)
Try to get the simplest encoding - ascii if possible so that  to@example.com doesn't become =?utf-8?q?to?=@example.com. This  makes unit testing a bit easier and more readable.
On Python 2, use the stdlib since `email.headerregistry` doesn't exist.
On Python 3, an `email.headerregistry.Address` object is used since  email.utils.formataddr() naively encodes the name as ascii (see 25986).
message/rfc822 attachments must be ASCII
Unfortunately, Python < 3.5 doesn't support setting a Charset instance  as MIMEText init parameter (http://bugs.python.org/issue16324).  We do it manually and trigger re-encoding of the payload.  Quoted-Printable encoding has the side effect of shortening long  lines, if any (22561).
the default value of '_charset' is 'us-ascii' on Python 2
Email header names are case-insensitive (RFC 2045), so we have to  accommodate that when doing comparisons.  Use cached DNS_NAME for performance
Don't bother creating the network connection if there's nobody to  send to.
If mimetype suggests the file is text but it's actually  binary, read() will raise a UnicodeDecodeError on Python 3.
If the previous read in text mode failed, try binary mode.
Bug 18967: per RFC2046 s5.2.1, message/rfc822 attachments  must not be base64 encoded.  convert content into an email.Message first  For compatibility with existing code, parse the message  into an email.Message object if it is not one already.
Encode non-text attachments with base64.
Inheriting from object required on Python 2.  Ignore broken pipe errors, otherwise pass on
Short-circuit parent method to not call socket.getfqdn
0x16 = Handshake, 0x03 = SSL 3.0 or TLS 1.x
Strip all headers with underscores in the name before constructing  the WSGI environ. This prevents header-spoofing based on ambiguity  between underscores and dashes both normalized to underscores in WSGI  env vars. Nginx and Apache 2.4+ both do this as well.
Under Python 3, non-ASCII values in the WSGI environ are arbitrarily  decoded with ISO-8859-1. We replicate this behavior here.  Refs comment in `get_bytes_from_wsgi()`.
ThreadingMixIn.daemon_threads indicates how threads will behave on an  abrupt shutdown; like quitting the server by the user or restarting  by the auto-reloader. True means the server will not wait for thread  termination before it quits. This will make auto-reloader faster  and will prevent the need to kill the server manually if a thread  isn't terminating correctly.
Use the C (faster) implementation if possible
A nasty special case: base YAML doesn't support serialization of time  types (as opposed to dates or datetimes, which it does support). Since  we want to use the "safe" serializer for better interoperability, we  need to do something with those pesky times. Converting 'em to strings  isn't perfect, but it's better than a "!!python/time" type which would  halt deserialization under any other language.
Grand-parent super
Map to deserializer error
Get a "string version" of the object's data.
If related object has a natural key, use it  Iterable natural keys are rolled out as subelements
If the objects in the m2m have a natural key, use it  Iterable natural keys are rolled out as subelements
Look up the model using the model loading mechanism. If this fails,  bail.
Start building a data dictionary from the object.
Also start building a dict of m2m data (this is saved as  {m2m_accessor_attribute : [list_of_related_objects]})
Deserialize each field.  If the field is missing the name attribute, bail (are you  sensing a pattern here?)
Get the field from the Model. This will raise a  FieldDoesNotExist if, well, the field doesn't exist, which will  be propagated correctly unless ignorenonexistent=True is used.
As is usually the case, relation fields get the special treatment.
Return a DeserializedObject so that the m2m data has a place to live.
Check if there is a child node named 'None', returning None if so.  If there are 'natural' subelements, it must be a natural key  If this is a natural foreign key to an object that  has a FK/O2O as the foreign key, use the FK value  Otherwise, treat like a normal PK
If there are 'natural' subelements, it must be a natural key
Otherwise, treat like a normal PK value.
inspired by http://mail.python.org/pipermail/xml-sig/2005-March/011022.html
expat 1.2
Built-in serializers
Process the list of models, and get the list of dependencies
Add any explicitly defined dependencies
Now add a dependency for any FK relation with a model that  defines a natural key  Also add a dependency for any simple M2M relation with a model  that defines a natural key.  M2M relations with explicit through  models don't count as dependencies.
Now sort the models to ensure that dependencies are met. This  is done by repeatedly iterating over the input list of models.  If all the dependencies of a given model are in the final list,  that model is promoted to the end of the final list. This process  continues until the input list is empty, or we do a full iteration  over the input models without promoting a model to the final list.  If we do a full iteration without a promotion, that means there are  circular dependencies in the list.
If all of the models in the dependency list are either already  on the final model list, or not on the original serialization list,  then we've found another model with all it's dependencies satisfied.
Protected types (i.e., primitives like None, numbers, dates,  and Decimals) are passed through as is. All other values are  converted to string first.
Look up the model and starting build a dict of data for it.
Handle each field
skip fields no longer on model
Handle M2M relations
Handle FK fields  If this is a natural foreign key to an object that  has a FK/O2O as the foreign key, use the FK value
Handle all other fields
Avoid shadowing the standard library json module
Use JS strings to represent Python Decimal instances (ticket 16850)
Prevent trailing spaces
self._current has the field data
Grand-parent super
Map to deserializer error
See "Date Time String Format" in the ECMA-262 specification.
Older, deprecated class name (for backwards compatibility purposes).
Indicates if the implemented serializer is only available for  internal Django use.
Use the concrete parent class' _meta instead of the object's _meta  This is to avoid local_fields problems for proxy models. Refs 17717.
Call save on the Model baseclass directly. This bypasses any  model-defined save. The save is also forced to be raw.  raw=True is passed to any pre/post_save signals.
prevent a second (possibly accidental) call to save() from saving  the m2m data twice.
Since Engine is imported in django.template and since  DjangoTemplates is a wrapper around this Engine class,  local imports are required to avoid import loops.  Unwrap the Engine instance inside DjangoTemplates
RemovedInDjango20Warning: Use old api for non-recursive  loaders.
template needs to be compiled
Django < 1.8 accepted a Context in `context` even though that's  unintended. Preserve this ability but don't rewrap `context`.
If we get here, none of the templates could be loaded
Include a reference to the real function (used to check original  arguments by the template parser, and to bear the 'is_safe' attribute  when multiple decorators are applied).
Values for testing floatformat input against infinity and NaN representations,  which differ across platforms and Python versions.  Some (i.e. old Windows  ones) are not recognized by Decimal but we want to return them unchanged vs.  returning an empty string as we do for completely invalid input.  Note these  need to be built up from values that are not inf/nan, since inf/nan values do  not reload properly from .pyc files on Windows prior to some level of Python 2.5  (see Python Issue757815 and Issue1080440).
Set the precision high enough to avoid an exception, see 15789.
Avoid conversion to scientific notation by accessing `sign`, `digits`  and `exponent` from `Decimal.as_tuple()` directly.
Find the maximum width of the line count, for use with zero padding  string format command
Ignore mark_for_escaping deprecation -- this will use  conditional_escape() in Django 2.0.
Unpack list of wrong size (no "maybe" value provided).
Since this package contains a "django" module, this is required on Python 2.
Since this package contains a "django" module, this is required on Python 2.
No templatetags package defined. This is safe to ignore.
Since this package contains a "django" module, this is required on Python 2.
Immutable return value because it's cached and shared by callers.
The joined path was located outside of this template_dir  (it might be inside another one, so this isn't fatal).
Since this package contains a "django" module, this is required on Python 2.
Since this package contains a "django" module, this is required on Python 2.
Null denotation - called in prefix context
Left denotation - called in infix context
Templates shouldn't throw exceptions when rendering.  We are  most likely to get exceptions for things like {% if foo in bar  %} where 'bar' does not support 'in', so default to False
Operator precedence follows Python.  We defer variable evaluation to the lambda to ensure that terms are  lazily evaluated using Python's boolean parsing logic.
Assign 'id' to each:
IfParser uses Literal in create_var, but TemplateIfParser overrides  create_var so that a proper implementation that actually resolves  variables, filters etc. is used.
Turn 'is','not' and 'not','in' into single tokens.
Check that we have exhausted all the tokens
It's very probable that the token is missing because of  misconfiguration, so we raise a warning
First time the node is rendered in template
Apply filters.
Create a forloop value in the context.  We'll update counters on each  iteration just below.  Shortcuts for current loop iteration number.  Reverse counter iteration numbers.  Boolean values designating first and last times through loop.
To complete this deprecation, remove from here to the  try/except block as well as the try/except itself,  leaving `unpacked_vars = ...` and the "else" statements.  Check loop variable count before unpacking
The loop variables were pushed on to the context so pop them  off again. This is necessary because the tag lets the length  of loopvars differ to the length of each set of items and we  don't want to leave any vars from the previous loop on the  context.
Init state storage
Consider multiple parameters.  This automatically behaves  like an OR evaluation of the multiple variables.
The "{% ifchanged %}" syntax (without any variables) compares the rendered output.
render true block if not already rendered
The Context object behaves like a stack where each template tag can create a new scope.  Find the place where to store the state to detect changes.  Ifchanged is bound to the local for loop.  When there is a loop-in-loop, the state is bound to the inner loop,  so it resets when the outer loop continues.  Using ifchanged outside loops. Effectively this is a no-op because the state is associated with 'self'.
This method is called for each object in self.target. See regroup()  for the reason why we temporarily put the object in the context.
target variable wasn't found in context; fail silently.  List of dictionaries in the format:  {'grouper': 'key', 'list': [list of contents]}.
Try to look up the URL. If it fails, raise NoReverseMatch unless the  {% url ... as var %} construct is used, in which case return nothing.
var and name are legacy attributes, being left in case they are used  by third-party subclasses of this Node.
token.split_contents() isn't useful here because this tag doesn't accept variable as arguments
{% cycle foo %} case.
{% cycle ... as foo [silent] %} case.
token.split_contents() isn't useful here because this tag doesn't accept variable as arguments
{% if ... %}
{% elif ... %} (repeatable)
{% else %} (optional)
{% endif %}
token.split_contents() isn't useful here because this tag doesn't accept variable as arguments  from syntax is used; load individual tags from the library  one or more libraries are specified; load and add them to the parser
Random bit  Method bit  Count bit
RegroupNode will take each item in 'target', put it in the context under  'var_name', evaluate 'var_name'.'expression' in the current context, and  group by the resulting value. After all items are processed, it will  save the final result in the context under 'var_name', thus clearing the  temporary values. This hack is necessary because the template engine  doesn't provide a context-aware equivalent of Python's getattr.
token.split_contents() isn't useful here because this tag doesn't accept variable as arguments
Hard-coded processor for easier use of CSRF protection.
because dictionaries can be put in different order  we have to flatten them like in templates
if it's not comparable return false
Set to the original template -- as opposed to extended or included  templates -- during rendering, see bind_template.
placeholder for context processors output
empty dict for any new modifications  (so that context processors don't overwrite them)
Set context processors according to the template engine's settings.
Unset context processors.
This is for backwards-compatibility: RequestContexts created via  Context.new don't include values from context processors.
The following pattern is required to ensure values from  context override those from template context processors.
template syntax constants
what to report as the origin for templates that come from non-loader sources  (e.g. strings)
match a variable or block tag and capture the entire tag, including start/end  delimiters
If Template is instantiated directly rather than from an Engine and  exactly one Django template engine is configured, use that engine.  This is required to preserve backwards-compatibility for direct use  e.g. Template('...').render(Context({...}))
In some rare cases exc_value.args can be empty or an invalid  unicode string.
Handle translation-marked template pieces
The [2:-2] ranges below strip off *_TAG_START and *_TAG_END.  We could do len(BLOCK_TAG_START) to be more "correct", but we've  hard-coded the 2s here for performance. And it's not like  the TAG_START values are going to change anytime, anyway.
A matching token has been reached. Return control to  the caller. Put the token back on the token list so the  caller knows where it terminated.  Add the token to the command stack. This is used for error  messages if further parsing fails due to an unclosed block  tag.
Get the tag callback function from the ones registered with  the parser.  Compile the callback into a node object and add it to  the node list.  Compile success. Remove the token from the command stack.
Check that non-text nodes don't appear before an extends tag.  Set origin and token here since we can't modify the node __init__()  method.
Ignore mark_for_escaping deprecation as this will be  removed in Django 2.0.
First argument, filter input, is implied.  Check to see if a decorator is providing the real function.
Not enough OR Too many
First try to treat this variable as a number.  Note that this could cause an OverflowError here that we're not  catching. Since this should only happen at compile time, that's  probably OK.
So it's a float... is it an int? If the original value contained a  dot or an "e" then it was a float, not an int.
"2." is invalid
A ValueError means that the variable isn't a number.  The result of the lookup should be translated at rendering  time.  If it's wrapped with quotes (single or double), then  we're also dealing with a literal.  Otherwise we'll set self.lookups so that resolve() knows we're  dealing with a bonafide variable
We're dealing with a variable that needs to be resolved
We're dealing with a literal, so it's already been "resolved"
ValueError/IndexError are for numpy.array lookup on  numpy < 1.9 and 1.9+ respectively
Don't return class attributes if the class is the context:
Reraise an AttributeError raised by a @property
Set this to True for nodes that must be first in the template (although  they can be preceded by text nodes.
Set to True the first time a non-TextNode is inserted by  extend_nodelist().
Unicode conversion can fail sometimes for reasons out of our  control (e.g. exception rendering). In that case, we fail  quietly.
Regex for token keyword arguments
Wrapper for loading templates from eggs via pkg_resources.resource_string.
The joined path was located outside of this template_dir  (it might be inside another one, so this isn't fatal).
RemovedInDjango20Warning: Add template_dirs for compatibility  with old loaders
A cached previous failure:  If compiling the template we found raises TemplateDoesNotExist,  back off to returning the source and display name for the template  we were asked to load. This allows for correct identification (later)  of the actual template that does not exist.
RemovedInDjango20Warning: Allow loaders to be called like functions.
RemovedInDjango20Warning: Add template_dirs for compatibility with  old loaders
If compiling the template we found raises TemplateDoesNotExist,  back off to returning the source and display name for the  template we were asked to load. This allows for correct  identification of the actual template that does not exist.
This will raise an exception if 'BACKEND' doesn't exist or  isn't a string containing at least one dot.
If importing or initializing the backend raises an exception,  self._engines[alias] isn't set and this code may get executed  again, so we must preserve the original params. See 24265.
Immutable return value because it will be cached and shared by callers.
Dictionary of FIFO queues.
Create new block so we can store context without thread-safety issues.
RemovedInDjango20Warning: If any non-recursive loaders are installed  do a direct template lookup. If the same template name appears twice,  raise an exception to avoid system recursion.
parent is a django.template.Template
parent is a django.template.backends.django.Template
Add the block nodes from this node to the block context
If this block's parent doesn't have an extends node it is the root,  and its block nodes also need to be added to the block context.  The ExtendsNode has to be the first non-text node.
Call Template._render explicitly so the parser context stays  the same.
Does this quack like a Template?  If not, we'll try our cache, and get_template()
token.split_contents() isn't useful here because this tag doesn't accept variable as arguments  Keep track of the names of BlockNodes found in this template, so we can  check for duplication.
This check is kept for backwards-compatibility. See 3100.
relative_name is a variable or a literal that doesn't contain a  relative path.
@register.tag()
@register.tag
@register.tag('somename') or @register.tag(name='somename')
register.tag('somename', somefunc)
@register.filter()
@register.filter
@register.filter('somename') or @register.filter(name='somename')
register.filter('somename', somefunc)  set the flag on the filter for FilterExpression.resolve  set the flag on the innermost decorated function  for decorators that need it, e.g. stringfilter
@register.simple_tag(...)
@register.simple_tag
Copy across the CSRF token, if present, because inclusion tags are  often used for forms, and we need instructions for using CSRF  protection to be as simple as possible.
First we try to extract a potential kwarg from the bit  The kwarg was successfully extracted  An unexpected keyword argument was supplied  The keyword argument has already been supplied once  All good, record the keyword argument  If using the keyword syntax for a positional arg, then  consume it.  Record the positional argument  Consume from the list of expected positional arguments
Consider the last n params handled, where n is the  number of defaults.
Some positional arguments were not supplied
It would seem obvious to call these next two members 'template' and  'context', but those names are reserved as part of the test Client  API. To avoid the name collision, we use different names.
_request stores the current request object in subclasses that know  about requests, like TemplateResponse. It's defined in the base class  to minimize code duplication.  It's called self._request because self.request gets overwritten by  django.test.client.Client. Unlike template_name and context_data,  _request should not be considered part of the public API.
content argument doesn't make sense here because it will be replaced  with rendered template so we always pass empty string in order to  prevent errors and provide shorter signature.
_is_rendered tracks whether the template and context has been baked  into a final response.  Super __init__ doesn't know any better than to set self.content to  the empty string we just gave it, which wrongly sets _is_rendered  True, so we initialize it to False after the call to super __init__.
In order to be able to provide debugging info in the  case of misconfiguration, we use a sentinel value  instead of returning an empty dict.
Return a lazy reference that computes connection.queries on access,  to ensure it contains queries triggered after this function runs.
First, try to get the "index" sitemap URL.
Next, try for the "global" sitemap URL.
This limit is defined by Google. See the index documentation at  http://www.sitemaps.org/protocol.htmlindex.
If protocol is None, the URLs in the sitemap will use the protocol  with which the sitemap was requested.
Determine protocol
Determine domain
Make sure to return a clone; we don't want premature evaluation.
if lastmod is defined for all sites, set header so as  ConditionalGetMiddleware is able to send 304 NOT MODIFIED
Support network-path reference (see 16753) - RSS requires a protocol
if item_pubdate or item_updateddate is defined for the feed, set  header so as ConditionalGetMiddleware is able to send 304 NOT MODIFIED
Titles should be double escaped by default (see 6533)
Check co_argcount rather than try/excepting the function and  catching the TypeError, because something inside the function  may raise the TypeError. This technique is more accurate.
Instantiating the DataSource from the string.
Creating the dictionary.
Generating the field name for each field in the layer.
generate_model.py
Getting the DataSource
Getting the layer corresponding to the layer key and getting  a string listing of all OGR fields in the Layer.
Creating lists from the `null`, `blank`, and `decimal`  keyword arguments.
Gets the `null` and `blank` keywords for the given field name.
For those wishing to disable the imports.
The model field name.
Getting the keyword args string.
By default OFTReals are mapped to `FloatField`, however, they  may also be mapped to `DecimalField` if specified in the  `decimal` keyword.
TODO: Autodetection of multigeometry types (see 7218).
Setting up the SRID keyword string.  WGS84 is already the default.
If argument is not a `SpatialReference` instance, use it as parameter  to construct a `SpatialReference` instance.
Initializing the keyword arguments dictionary for both PostGIS  and SpatiaLite.
Backend-specific fields for the SpatialRefSys model.  Spatialite specific
Creating the spatial_ref_sys model.  Try getting via SRID only, because using all kwargs may  differ from exact wkt/proj in database.
TODO: Support 3D geometries.
LayerMapping exceptions.
Acceptable 'base' types for a multi-geometry type.
Acceptable Django field types and corresponding acceptable OGR  counterparts.
Getting the DataSource and the associated Layer.
Setting the mapping & model attributes.
Checking the layer -- initialization of the object will fail if  things don't check out before hand.
Getting the geometry column associated with the model (an  exception will be raised if there is no geometry column).
Checking the source spatial reference system, and getting  the coordinate transformation object (unless the `transform`  keyword is set to False)
Setting the encoding for OFTString fields, if specified.  Making sure the encoding exists, if not a LookupError  exception will be thrown.
Setting the transaction decorator with the function in the  transaction modes dictionary.
Checking routines used during initialization
The geometry field of the model is set here.  TODO: Support more than one geometry field / model.  However, this  depends on the GDAL Driver in use.
Getting lists of the field names and the field types available in  the OGR Layer.
Function for determining if the OGR mapping field is in the Layer.
No need to increment through each feature in the model, simply check  the Layer metadata against what was given in the mapping dictionary.  Ensuring that a corresponding field exists in the model  for the given field name in the mapping.
Getting the string name for the Django field class (e.g., 'PointField').
Getting the coordinate dimension of the geometry field.
Making sure that the OGR Layer's Geometry is compatible.
Setting the `geom_field` attribute w/the name of the model field  that is a Geometry.  Also setting the coordinate dimension  attribute.
Is every given related model mapping field in the Layer?
Is the model field type supported by LayerMapping?
Is the OGR field in the Layer?
Can the OGR field type be mapped to the Django field type?
Otherwise just pulling the SpatialReference from the layer
List of fields to determine uniqueness with
Only a single field passed in.
The keyword arguments for model construction.
Incrementing through each model field and OGR field in the  dictionary mapping.
Verify OGR geometry.
The related _model_, not a field was passed in -- indicating  another mapping for the related Model.
Otherwise, verify OGR Field type.
Setting the keyword arguments for the field name with the  value obtained above.
The encoding for OGR data sources may be specified here  (e.g., 'cp437' for Census Bureau boundary files).
Creating an instance of the Decimal value to use.
Getting the decimal value as a tuple.
Maximum amount of precision, or digits to the left of the decimal.
Getting the digits to the left of the decimal place for the  given decimal.
If we have more than the maximum digits allowed, then throw an  InvalidDecimal exception.
Attempt to convert any OFTReal and OFTString value to an OFTInteger.
Constructing and verifying the related model keyword arguments.
Attempting to retrieve and return the related model.
Downgrade a 3D geom to a 2D one, if necessary.
Constructing a multi-geometry type to contain the single geometry
Transforming the geometry with our Coordinate Transformation object,  but only if the class variable `transform` is set w/a CoordTransform  object.
Returning the WKT of the geometry.
Other model methods   Getting the target spatial reference system
Creating the CoordTransform object
Use `get_field()` on the model's options so that we  get the correct field instance if there's model inheritance.
Getting the default Feature ID range.
Setting the progress interval, if requested.
Getting the keyword arguments  Something borked the validation  Constructing the model using the keyword args  If we want unique models on a particular field, handle the  geometry appropriately.  Getting the keyword arguments and retrieving  the unique model.
Getting the geometry (in OGR form), creating  one from the kwargs WKT, adding in additional  geometries, and update the attribute with the  just-updated geometry WKT.
No unique model exists yet, create.
Attempting to save.
Bailing out if the `strict` keyword is set.
Printing progress information, if requested.
Only used for status output purposes -- incremental saving uses the  values returned here.
Incremental saving is requested at the given interval (step)
Constructing the slice to use for this step; the last slice is  special (e.g, [100:] instead of [90:100]).
Otherwise, just calling the previously defined _save() function.
Checking the parameters.
If no locations specified, then we try to build for  every model in installed applications.
Geo-enabled Sitemap classes.
Database will take care of transformation.
If the database offers no KML method, we use the `kml`  attribute of the lazy geometry instead.
Getting the render function and rendering to the correct.
Creating a template context that contains Django settings  values needed by admin map templates.
Update the template parameters with any attributes passed in.
Defaulting the WKT value to a blank string -- this  will be tested in the JavaScript and the appropriate  interface will be constructed.
If a string reaches here (via a validation error on another  field) then just reconstruct the Geometry.
Constructing the dictionary of the map options.
Constructing the JavaScript module name using the name of  the GeometryField (passed in via the `attrs` keyword).  Use the 'name' attr for the field name (rather than 'field')  note: we must switch out dashes for underscores since js  functions are created using the module variable
Transforming the geometry to the projection used on the  OpenLayers map.
Setting the parameter WKT with that of the transformed  geometry.
JavaScript construction utilities for the Bounds and Projection.
An array of the parameter name, the name of their OpenLayers  counterpart, and the type of variable they are.
Building the map options hash.
The default map settings that may be overloaded -- still subject  to API changes.
Setting the widget with the newly defined widget.
Creating the settings dictionary with any settings, if needed.
The flags for GeoIP memory caching.  Try MODE_MMAP_EXT, MODE_MMAP, MODE_FILE in that order.  Use the C extension with memory map.  Read from memory map. Pure Python.  Read database as standard file. Pure Python.  Load database into memory. Pure Python.
Paths to the city & country binary databases.
Initially, pointers to GeoIP file references are NULL.
Checking the given cache option.
Getting the GeoIP data path.
Constructing the GeoIP database filenames using the settings  dictionary. If the database files for the GeoLite country  and/or city datasets exist, then try to open them.
Otherwise, some detective work will be needed to figure out  whether the given database path is for the GeoIP country or city  databases.
GeoLite City database detected.
GeoIP Country database detected.
Cleanup any GeoIP file handles lying around.
Making sure a string was passed in for the query.
Extra checks for the existence of country and city databases.
Return the query string back to the caller. GeoIP2 only takes IP addresses.
Returning the country code and name
Coordinate retrieval routines
GeoIP Database Information Routines
NumPy supported?
Getting the OGR DataSource from the string parameter.
Returning the output of ogrinspect with the given arguments  and options.  Filter options to params accepted by `_ogrinspect`
Constructing the keyword arguments for `mapping`, and  calling it on the data source.  This extra legwork is so that the dictionary definition comes  out in the same order as the fields in the model definition.
Getting a more specific field type and any additional parameters  from the `get_geometry_type` routine for the spatial backend.
If a string reaches here (via a validation error on another  field) then just reconstruct the Geometry.
Check that srid of value and map match
Use the official spherical mercator projection SRID when GDAL is  available; otherwise, fallback to 900913.
Pop out attributes from the database field, or use sensible  defaults (e.g., allow None).
Try to set the srid
Ensuring that the geometry is of the correct type (indicated  using the OGC string label).
Transforming the geometry if the SRID was set.
Only do a geographic comparison if both values are available  If the initial value was not added by the browser, the geometry  provided may be slightly different, the first time it is saved.  The comparison is done with a very low tolerance.  Check for change of state of existence
Getting the envelope of the input polygon (used for automatically  determining the zoom level).
Translating the coordinates into a JavaScript array of  Google `GLatLng` objects.
Stroke settings.
Fill settings.
If a GEOS geometry isn't passed in, try to construct one.  Generating the lat/lng coordinate pairs.
Getting the envelope for automatic zoom determination.
XOR with hash of GIcon type so that hash('varname') won't  equal hash(GIcon('varname')).
If a GEOS geometry isn't passed in, try to construct one.  Getting the envelope for automatic zoom determination.  TODO: Add support for more GMarkerOptions
The default Google Maps URL (for the API javascript)  TODO: Internationalize for Japan, UK, etc.
The Google Maps API Key defined in the settings will be used  if not passed in as a parameter.  The use of an API key is  _required_.
Getting the Google Maps API version, defaults to using the latest ("2.x"),  this is not necessarily the most stable.
Can specify the API URL in the `api_url` keyword.
Setting the DOM id of the map, the load function, the JavaScript  template, and the KML URLs array.
Does the user want any GMarker, GPolygon, and/or GPolyline overlays?
If GMarker, GPolygons, and/or GPolylines are used the zoom will be  automatically calculated via the Google Maps API.  If both a zoom  level and a center coordinate are provided with polygons/polylines,  no automatic determination will occur.
Defaults for the zoom level and center coordinates if the zoom  is not automatically calculated.
The `google-multi.js` template is used instead of `google-single.js`  by default.
This is the template used to generate the GMap load JavaScript for  each map in the set.
Running GoogleMap.__init__(), and resetting the template  value with default obtained above.
If a tuple/list passed in as first element of args, then assume
Generating DOM ids for each of the maps in the set.
Backup copies the GoogleMap DOM id and template attributes.  They are overridden on each GoogleMap instance in the set so  that only the loading JavaScript (and not the header variables)  is used with the generated DOM ids.  Restoring the backup values.
Overloaded to use the `load` function defined in the  `google-multi.js`, which calls the load routines for  each one of the individual maps in the set.
Constants used for degree to radian conversion, and vice-versa.
Google's tilesize is 256x256, square tiles are assumed.
The number of zoom levels
Multiplying `z` by 2 for the next iteration.
Setting up, unpacking the longitude, latitude values and getting the  number of pixels for the given zoom level.
Calculating the pixel x coordinate by multiplying the longitude value  with the number of degrees/pixel at the given zoom level.
Creating the factor, and ensuring that 1 or -1 is not passed in as the  base to the logarithm.  Here's why:   if fac = -1, we'll get log(0) which is undefined;   if fac =  1, our logarithm base will be divided by 0, also undefined.
Calculating the pixel y coordinate.
Returning the pixel x, y to the caller of the function.
Getting the number of pixels for the given zoom level.
Calculating the longitude value, using the degrees per pixel.
Calculating the latitude value.
Returning the longitude, latitude coordinate pair.
The given lonlat is the center of the tile.
Getting the pixel coordinates corresponding to the  the longitude/latitude.
Getting the lower-left and upper-right lat/lon coordinates  for the bounding box of the tile.
Constructing the Polygon, representing the tile and returning.
Checking the input type.
Getting the envelope for the geometry, and its associated width, height  and centroid.
Getting the tile at the zoom level.
When we span more than one tile, this is an approximately good  zoom level.
Otherwise, we've zoomed in to the max.
Getting the lower-left, upper-left, and upper-right  coordinates from the extent.  Calculating the width and height.
TODO: In 1.4.6 this changed from `int dma_code;` to  `union {int metro_code; int dma_code;};`.  Change  to a `ctypes.Union` in to accommodate in future when  pre-1.4.6 versions are no longer distributed.
GeoIP_lib_version appeared in version 1.4.7.
For freeing memory allocated within a record
For retrieving records by name or address.  Checking the pointer to the C structure, if valid pull out elements  into a dictionary.
Now converting the strings to unicode using the proper encoding.
Free the memory allocated for the struct & return.
For opening & closing GeoIP database files.
This is so the string pointer can be freed within Python.
String output routines.
Creating the settings dictionary with any settings, if needed.
The shared library for the GeoIP C API.  May be downloaded   from http://www.maxmind.com/download/geoip/api/c/  TODO: Is this really the library name for Windows?
Getting the path to the GeoIP library.
Getting the C `free` for the platform.
Regular expressions for recognizing the GeoIP free database editions.
The flags for GeoIP memory caching.  GEOIP_STANDARD - read database from filesystem, uses least memory.  GEOIP_MEMORY_CACHE - load database into memory, faster performance         but uses more memory  GEOIP_CHECK_CACHE - check for updated database.  If database has been         updated, reload filehandle and/or memory cache.  This option         is not thread safe.  GEOIP_INDEX_CACHE - just cache the most frequently accessed index         portion of the database, resulting in faster lookups than         GEOIP_STANDARD, but less memory usage than GEOIP_MEMORY_CACHE -         useful for larger databases such as GeoIP Organization and         GeoIP City.  Note, for GeoIP Country, Region and Netspeed         databases, GEOIP_INDEX_CACHE is equivalent to GEOIP_MEMORY_CACHE  GEOIP_MMAP_CACHE - load database into mmap shared memory ( not available        on Windows).
Paths to the city & country binary databases.
Initially, pointers to GeoIP file references are NULL.
Checking the given cache option.
Getting the GeoIP data path.
Constructing the GeoIP database filenames using the settings  dictionary.  If the database files for the GeoLite country  and/or city datasets exist, then try and open them.
Otherwise, some detective work will be needed to figure  out whether the given database path is for the GeoIP country  or city databases.  GeoLite City database detected.  GeoIP Country database detected.
Cleaning any GeoIP file handles lying around.
Making sure a string was passed in for the query.
Extra checks for the existence of country and city databases.
Return the query string back to the caller. GeoIP only takes bytestrings.
If an IP address was passed in
If a FQDN was passed in.
Returning the country code and name
Coordinate retrieval routines
GeoIP Database Information Routines
Methods for compatibility w/the GeoIP-Python API.
Automatic SRID conversion so objects are comparable
Getting the area units of the geographic field.  TODO: Do we want to support raw number areas for geodetic fields?
No version parameter
Set parameters as geography if base field is geography
Geometry fields with geodetic (lon/lat) coordinates need special distance functions
Replace boolean param by the real spheroid of the base field
Geometry fields with geodetic (lon/lat) coordinates need length_spheroid
Reverse origin and size param ordering
Make srid the resulting srid of the transformation
Some backends do not set the srid on the returning geometry
Always provide the z parameter for ST_Translate (Spatialite >= 3.1)
Performing setup here rather than in `_spatial_attribute` so that  we can get the units for `AreaField`.
Geography fields support area calculation, returns square meters.
Getting the area units of the geographic field.
TODO: Do we want to support raw number areas for geodetic fields?
Does the spatial backend support this?
Initializing the procedure arguments.
Is there a geographic field in the model to perform this  operation on?
If the `geo_field_type` keyword was used, then enforce that  type limitation.
Setting the procedure args.
Default settings.
Performing setup for the spatial column, unless told not to.
The attribute to attach to the model.
Special handling for any argument that is a geometry.  Using the field's get_placeholder() routine to get any needed  transformation SQL.
Replacing the procedure format with that of any needed  transformation SQL.
Getting the format for the stored procedure.
If the result of this function needs to be converted.
Finally, setting the extra selection attribute with  the format string expanded with the stored procedure  arguments.
Setting up the distance procedure arguments.
If geodetic defaulting distance attribute to meters (Oracle and  PostGIS spherical distances return meters).  Otherwise, use the  units of the geometry field.
Shortcut booleans for what distance function we're using and  whether the geometry field is 3D.
The field's _get_db_prep_lookup() is used to get any  extra distance parameters.  Here we set up the  parameters that will be passed in to field's function.
Getting the spatial backend operations.
If the spheroid calculation is desired, either by the `spheroid`  keyword or when calculating the length of geodetic field, make  sure the 'spheroid' distance setting string is passed in so we  get the correct spatial stored procedure.
The `geom_args` flag is set to true if a geometry parameter was  passed in.
Getting whether this field is in units of degrees since the field may have  been transformed via the `transform` GeoQuerySet method.
Setting the `geom_args` flag to false because we want to handle  transformation SQL here, rather than the way done by default  (which will transform to the original SRID of the field rather   than to what was transformed to).  If the geom parameter srid is None, it is assumed the coordinates  are in the transformed units.  A placeholder is used for the  geometry parameter.  `GeomFromText` constructor is also needed  to wrap geom placeholder for SpatiaLite.  We need to transform the geom to the srid specified in `transform()`,  so wrapping the geometry placeholder in transformation SQL.  SpatiaLite also needs geometry placeholder wrapped in `GeomFromText`  constructor.
`transform()` was not used on this GeoQuerySet.
Spherical distance calculation is needed (because the geographic  field is geodetic). However, the PostGIS ST_distance_sphere/spheroid()  procedures may only do queries from point columns to point geometries  some error checking is required.  The `function` procedure argument needs to be set differently for  geodetic distance calculations.  Call to distance_spheroid() requires spheroid param as well.
There's no `length_sphere`, and `length_spheroid` also  works on 3D geometries.
Use 3D variants of perimeter and length routines on supported backends.
Setting up the settings for `_spatial_attribute`.  The geometry is passed in as a parameter because we handled  transformation conditions in this routine.
Is this operation going to be on a related geographic field?  If so, it'll have to be added to the select related information  (e.g., if 'location__point' was given as the field name, then  chop the non-relational field and add select_related('location')).  Note: the operation really is defined as "must add select related!"  Call pre_sql_setup() so that compiler.select gets populated.
This geographic field is inherited from another model, so we have to  use the db table for the _parent_ model instead.
Incrementing until the first geographic field is found.
Otherwise, check by the given field name -- which may be  a lookup to a _related_ geographic field.
If the database returns a Decimal, convert it to a float as expected  by the Python geometric objects.  If the units are known, convert value into area measure.
Hacky marker for get_db_converters()
this will be called again in parent, but it's needed now - before  we get the spatial_aggregate_name
Local cache of the spatial_ref_sys table, which holds SRID data for each  spatial database alias. This cache exists so that the database isn't queried  for SRID info each time a distance query is constructed.
The SpatialRefSys model for the spatial backend.
No `spatial_ref_sys` table in spatial backend (e.g., MySQL).
Initialize SRID dictionary for database if it doesn't exist.
Use `SpatialRefSys` model to query for spatial reference info.
This allows operations to be done on fields in the SELECT,  overriding their values -- used by the Oracle and MySQL  spatial backends to get database values as WKT, and by the  `transform` method.
Geodetic units.
Setting the index flag with the value of the `spatial_index` keyword.
Setting the SRID and getting the units.  Unit information must be  easily available in the field instance for distance queries.
Setting the verbose_name keyword argument with the positional  first parameter, so this works like normal fields.
Always include SRID for less fragility; include spatial index if it's  not the default value.
The following functions are used to get the units, their name, and  the spheroid corresponding to the SRID of the BaseSpatialField.  Get attributes from `get_srid_info`.
Some backends like MySQL cannot determine units name. In that case,  test if srid is 4326 (WGS84), even if this is over-simplification.
For IsValid lookups, boolean values are allowed.
When the input is not a geometry or raster, attempt to construct one  from the given string input.  Check if input is a candidate for conversion to raster or geometry.  With GDAL installed, try to convert the input to raster.
Assigning the SRID value.
The OpenGIS Geometry name.
Setting the dimension of the geometry field.
Is this a geography rather than a geometry column?
Oracle-specific private attributes for creating the entry in  `USER_SDO_GEOM_METADATA`
Include kwargs if they're not the default values.
Routines overloaded from Field
Setup for lazy-instantiated Geometry object.
Populating the parameters list, and wrapping the Geometry  with the Adapter of the spatial backend.  Getting the distance parameter in the units of the field.
The OpenGIS Geometry Type Fields
Make sure raster fields are used only on backends with raster support.
Prepare raster for writing to database.
Importing GDALRaster raises an exception on systems without gdal.  Setup for lazy-instantiated Raster object. For large querysets, the  instantiation of all GDALRasters can potentially be expensive. This  delays the instantiation of the objects to the moment of evaluation  of the raster attribute.
This manager should be used for queries on related fields  so that geometry columns on Oracle and MySQL are selected  properly.
No need to bother users with the use_for_related_fields  deprecation for this manager which is itself deprecated.
This takes into account the situation where the lookup is a  lookup to a related geographic field, e.g., 'address__point'.
Reversing so list operates like a queue of related lookups,  and popping the top lookup.
If the field list is still around, then it means that the  lookup was for a geometry field across a relationship --  thus we keep on getting the related model options and the  model field associated with the next field in the list  until there's no more left.
Finally, make sure we got a Geographic field and return.
PostGIS band indices are 1-based, so the band index needs to be  increased to be consistent with the GDALRaster band indices.
get_db_prep_lookup is called by process_rhs from super class  First param is assumed to be the geometric object
If rhs is some QuerySet, don't touch it
Make sure the F Expression destination field exists, and  set an `srid` attribute with the same as that of the  destination.
Check if a band index was passed in the query argument.
Unlike BuiltinLookup, the GIS get_rhs_op() implementation should return  an object (SpatialOperator) with an as_sql() method to allow for more  complex computations (where the lhs part can be mixed in).
Alias of same_as
Check the pattern argument
Check if the second parameter is a band index.
Getting the distance parameter in the units of the field.
Accessed on a class, not an instance
Getting the value of the field.
Otherwise, a geometry or raster object is built using the field's  contents, and the model's corresponding attribute is set.
The geographic type of the field.
For raster fields, assure input is None or a string, dict, or  raster instance.
The geometry type must match that of the field -- unless the  general GeometryField is used.  Assigning the field SRID if the geometry has no SRID.
Set geometries with None, WKT, HEX, or WKB
Setting the objects dictionary with the value, and returning.
Updating the data_types_reverse dictionary with the appropriate  type for Geometry fields.
In order to get the specific geometry type of the field,  we introspect on the table definition using `DESCRIBE`.  Increment over description info until we get to the geometry  column.  Using OGRGeomType to convert from OGC name to Django field.  MySQL does not support 3D or SRIDs, so the field params  are empty.
Supported with MyISAM, or InnoDB on MySQL 5.7.5+
Geometry fields are stored as BLOB/TEXT and can't have defaults.
MySQL doesn't support spatial indexes on NULL columns
Lookup to convert pixel type values from GDAL to PostGIS
Lookup to convert pixel type values from PostGIS to GDAL
Struct pack structure for raster header, the raster header has the  following structure:  Endianness, PostGIS raster version, number of bands, scale, origin,  skew, srid, width, and height.  Scale, origin, and skew have x and y values. PostGIS currently uses  a fixed endianness (1) and there is only one version (0).
Lookup values to convert GDAL pixel types to struct characters. This is  used to pack and unpack the pixel values of PostGIS raster bands.
Size of the packed value in bytes for different numerical types.  This is needed to cut chunks of band data out of PostGIS raster strings  when decomposing them into GDALRasters.  See https://docs.python.org/3/library/struct.htmlformat-characters
Getting the WKB (in string form, to allow easy pickling of  the adaptor) and the SRID from the geometry or raster.
Does the given protocol conform to what Psycopg2 expects?
Psycopg will figure out whether to use E'\\000' or '\000'.
For rasters, add explicit type cast to WKB string.
Reverse dictionary for PostGIS geometry types not populated until  introspection is actually performed.
The value for the geography type is actually a tuple  to pass in the `geography=True` keyword to the field  definition.
The OID integers associated with the geometry type may  be different across versions; hence, this is why we have  to query the PostgreSQL pg_type table corresponding to the  PostGIS custom data types.
If the PostGIS types reverse dictionary is not populated, do so  now.  In order to prevent unnecessary requests upon connection  initialization, the `data_types_reverse` dictionary is not updated  with the PostGIS custom types until introspection is actually  performed -- in other words, when this function is called.
First seeing if this geometry column is in the `geometry_columns`
OGRGeomType does not require GDAL and makes it easy to convert  from OGC geom type name to Django field.
Getting any GeometryField keyword arguments that are not the default.
Identifier to mark raster lookups as bilateral.
Only a subset of the operators and functions are available for the  geography type.  Only a subset of the operators and functions are available for the  raster type. Lookups that don't suport raster will be converted to  polygons. If the raster argument is set to BILATERAL, then the  operator cannot handle mixed geom-raster lookups.
Get rhs value.
Check which input is a raster.
Look for band indices and inject them if provided.
Convert rasters to polygons if necessary.  Operators without raster support.  Operators with raster support but don't support mixed (rast-geom)  lookups.
Using distance_spheroid requires the spheroid of the field as  a parameter.
Trying to get the PostGIS version because the function  signatures will depend on the version used.  The cost  here is a database query to determine the version, which  can be mitigated by setting `POSTGIS_VERSION` with a 3-tuple  comprising user-supplied values for the major, minor, and  subminor revision of PostGIS.  Run a basic query to check the status of the connection so we're  sure we only raise the error below if the problem comes from  PostGIS and not from PostgreSQL itself (see 24862).
Type-based geometries.  TODO: Support 'M' extension.
Getting the distance parameter
Shorthand boolean flags.
Assuming the distance is in the units of the field.
handle_spheroid *might* be dropped in Django 2.0 as PostGISDistanceOperator  also handles it (25524).  using distance_spheroid requires the spheroid of the field as  a parameter.
Get the srid for this object
Adding Transform() to the SQL placeholder if the value srid  is not equal to the field srid.
If this is an F expression, then we don't really want  a placeholder and instead substitute in the column  of the expression.
Close out the connection.  See 9437.
Getting the PostGIS version
Routines for getting the OGC-compliant models.
Methods to convert between PostGIS rasters and dicts that are  readable by GDALRaster.
Spatial indexes created the same way for both Geometry and  Geography columns.  For raster fields, wrap index creation SQL statement with ST_ConvexHull.  Indexes on raster columns are based on the convex hull of the raster.  Use either "nd" ops  which are fast on multidimensional cases  or just plain gist index for the 2d case.
Create geometry columns
Create geometry columns
The positional arguments here extract the hex-encoded srid from the  header of the PostGIS raster string. This can be understood through  the POSTGIS_HEADER_STRUCTURE constant definition in the const module.
Split raster header from data
Parse band data  Get pixel type for this band
Subtract nodata byte from band nodata value if it exists
Convert datatype from PostGIS to GDAL & get pack type and size
Parse band nodata value. The nodata value is part of the  PGRaster string even if the nodata flag is True, so it always  has to be chunked off the data string.
Chunk and unpack band data (pack size times nr of pixels)
If the nodata flag is True, set the nodata value.
Append band data to band list
Store pixeltype of this band in pixeltypes array
Check that all bands have the same pixeltype.  This is required by GDAL. PostGIS rasters could have different pixeltypes  for bands of the same raster.
Return if the raster is null
Prepare the raster header data as a tuple. The first two numbers are  the endianness and the PostGIS Raster Version, both are fixed by  PostGIS at the moment.
Hexlify raster header
The PostGIS raster band header has exactly two elements, a 8BUI byte  and the nodata value.  The 8BUI stores both the PostGIS pixel data type and a nodata flag.  It is composed as the datatype integer plus 64 as a flag for existing  nodata values:  8BUI_VALUE = PG_PIXEL_TYPE (0-11) + FLAG (0 or 64)  For example, if the byte value is 71, then the datatype is  71-64 = 7 (32BSI) and the nodata value is True.
Get band pixel type in PostGIS notation
Set the nodata flag
Pack band header
Hexlify band data
Add packed header and band data to result
Cast raster to string before passing it to the DB
Check that postgis extension is installed.
Optional geometry representing the bounds of this coordinate  system.  By default, all are NULL in the table.
Fix single polygon orientation as described in __init__()
Fix polygon orientations in geometry collections as described in  __init__()
A modified shoelace algorithm to determine polygon orientation.  See https://en.wikipedia.org/wiki/Shoelace_formula
Associating any OBJECTVAR instances with GeometryField.  Of course,  this won't work right on Oracle objects that aren't MDSYS.SDO_GEOMETRY,  but it is the only object type supported within Django anyways.
Querying USER_SDO_GEOM_METADATA to get the SRID and dimension information.
TODO: Research way to find a more specific geometry field type for  the column's contents.
Getting the field parameters.  Length of object array ( SDO_DIM_ARRAY ) is number of dimensions.
We want to get SDO Geometries as WKT because it is much easier to  instantiate GEOS proxies from WKT than SDO_GEOMETRY(...) strings.  However, this adversely affects performance (i.e., Java is called  to convert to WKT on every query).  If someone wishes to write a  SDO_GEOMETRY(...) parser in Python, let me know =)
Generally, Oracle returns a polygon for the extent -- however,  it can return a single point if there's only one Point in the  table.  Construct the 4-tuple from the coordinates in the polygon.
dwithin lookups on Oracle require a special string parameter  that starts with "distance=".
No geometry value used for F expression, substitute in  the column name instead.
Routines for getting the OGC-compliant models.
Oracle doesn't allow object names > 30 characters. Use this scheme  instead of self._create_index_name() for backwards compatibility.
For pulling out the spheroid from the spatial reference string. This  regular expression is used only if the user does not have GDAL installed.  TODO: Flattening not used in all ellipsoids, could also be a minor axis,  or 'b' parameter.
For pulling out the units on platforms w/o GDAL installed.  TODO: Figure out how to pull out angular units of projected coordinate system and  fix for LOCAL_CS types.  GDAL should be highly recommended for performing  distance queries.
TODO: Is caching really necessary here?  Is complexity worth it?  Returning a clone of the cached SpatialReference object.  Attempting to cache a SpatialReference object.
Trying to get from WKT first.
`string` parameter used to place in format acceptable by PostGIS
Quick booleans for the type of this spatial backend, and  an attribute for the spatial database version tuple (if applicable)
How the geometry column should be selected.
Does the spatial database have a geometry or geography type?
Aggregates
Mapping between Django function names and backend names, when names do not  match; used in spatial_function_name().
Blacklist/set of known unsupported functions of the backend
Serialization
Constructors
Default conversion functions for aggregates; will be overridden if implemented  for the spatial backend.
For quoting column values, rather than columns.
Routines for getting the OGC-compliant models.
Does the database contain a SpatialRefSys model to store SRID information?
Does the backend support the django.contrib.gis.utils.add_srs_entry() utility?  Does the backend introspect GeometryField to its subtypes?
Does the backend support storing 3D geometries?  Reference implementation of 3D functions is:  http://postgis.net/docs/PostGIS_Special_Functions_Index.htmlPostGIS_3D_Functions  Does the database support SRID transform operations?  Do geometric relationship operations operate on real shapes (or only on bounding boxes)?  Can geometry fields be null?  Can the the function be applied on geodetic coordinate systems?  Is the database able to count vertices on polygons (with `num_points`)?
The following properties indicate if the database backend support  certain lookups (dwithin, left and right, relate, ...)
Does the database have raster support?
Does the database support a unique index on geometry fields?
For each of those methods, the class will have a property named  `has_<name>_method` (defined in __init__) which accesses connection.ops  to determine GIS method availability.
Specifies whether the Collect and Extent aggregates are supported by the database
Add dynamically properties for each GQS method, e.g. has_force_rhr_method, etc.
Querying the `geometry_columns` table to get additional metadata.
OGRGeomType does not require GDAL and makes it easy to convert  from OGC geom type name to Django field.  Spatialite versions >= 4 use the new SFSQL 1.2 offsets  1000 (Z), 2000 (M), and 3000 (ZM) to indicate the presence of  higher dimensional coordinates (M not yet supported by Django).
Getting any GeometryField keyword arguments that are not the default.
Returns true if B's bounding box completely contains A's bounding box.  Returns true if A's bounding box completely contains B's bounding box.  Returns true if A's bounding box overlaps B's bounding box.  These are implemented here as synonyms for Equals
No geometry value used for F expression, substitute in  the column name instead.
Adding Transform() to the SQL placeholder.
Routines for getting the OGC-compliant models.
Geometry columns are created by the `AddGeometryColumn` function
Create geometry columns
Drop spatial metadata (dropping the table does not automatically remove them)  Make sure all geom stuff is gone
Populate self.geometry_sql
NOTE: If the field is a geometry field, the table is just recreated,  the parent's remove_field can't be used cause it will skip the  recreation if the field does not have a database type. Geometry fields  do not have a db type cause they are added and removed via stored  procedures.
Remove geometry-ness from temp table  Alter table  Repoint any straggler names  Re-add geometry-ness and rename spatial index tables
Before we get too far, make sure pysqlite 2.5+ is installed.
Trying to find the location of the SpatiaLite library.  Here we are figuring out the path to the SpatiaLite library  (`libspatialite`). If it's not in the system library path (e.g., it  cannot be found by `ctypes.util.find_library`), then it may be set  manually in the settings via the `SPATIALITE_LIBRARY_PATH` setting.
Enabling extension loading on the SQLite connection.  Loading the SpatiaLite library extension on the connection, and returning  the created cursor.
Check if spatial metadata have been initialized in the database
SpatiaLite can only count vertices in LineStrings
SpatiaLite 4.1+ support initializing all metadata in one transaction  which can result in a significant performance improvement when  creating the database.
Unit aliases for `UNIT` terms encountered in Spatial Reference WKT.
Getting the square units values and the alias dictionary.
Shortcuts
calculate new length and dimensions
Special methods for arithmetic operations
self must be shorter
self must be shorter
Public list interface Methods    Non-mutating
Mutating
Private routines
CAREFUL: index.step and step are not the same!  step will never be None
extended slice, only allow assigning slice of same size
we're not changing the length of the sequence
extended slice, only allow assigning slice of same size
Keeping a reference to the original geometry object to prevent it  from being garbage collected which could then crash the prepared one  See 21662
If given a file name, get a real handle.
If we get WKB need to wrap in memoryview(), so run through regexes.
Checking the arguments  If only one geometry provided or a list of geometries is provided   in the first argument.
Ensuring that only the permitted geometries are allowed in this collection  this is moved to list mixin super class
Creating the geometry pointer array.
Methods for compatibility with ListMixin   Creating the geometry pointer array.  this is a little sloppy, but makes life easier  allow GEOSGeometry types (python wrappers) or pointer types
Checking the index and returning the corresponding GEOS geometry.
MultiPoint, MultiLineString, and MultiPolygon class definitions.
Setting the allowed types here since GeometryCollection is defined before  its subclasses.
Getting the ext_ring and init_holes parameters from the argument list
If initialized as Polygon(shell, (LinearRing, LinearRing)) [for backward-compatibility]
These routines are needed for list-like operation w/ListMixin   Instantiate LinearRing objects if necessary, but don't clone them yet  _construct_ring will throw a TypeError if a parameter isn't a valid ring  If we cloned the pointers here, we wouldn't be able to clean up  in case of error.
Getting the current pointer, replacing with the newly constructed  geometry, and destroying the old geometry.
Getting the interior ring, have to subtract 1 from the index.
Polygon Properties   Getting the number of rings
Properties for the exterior ring/shell.
Here a tuple or list was passed in under the `x` parameter.
Here X, Y, and (optionally) Z were passed in individually, as parameters.
Initializing using the address returned from the GEOS   createPoint factory.
can this happen?
Tuple setting and retrieval routines.
The tuple and coords properties
Getting the `free` routine used to free the memory allocated for  string pointers returned by GEOS.
Checking the status code  Double passed in by reference, return its value.
A c_size_t object is passed in by reference for the second  argument on these routines, and its needed to determine the  correct size.  Freeing the memory allocated within GEOS
Getting the string value at the pointer address.  Freeing the memory allocated within GEOS
Prepared geometry constructor and destructors.
Prepared geometry binary predicate support.
Initializing the context handler for this thread with  the notice and error handler.
Defining a thread-local object and creating an instance  to hold a reference to GEOSContextHandle for this thread.
GEOS thread-safe function signatures end with '_r', and  take an additional context handle parameter.  Create a reference here to thread_context so it's not  garbage-collected before an attempt to call this object.
Otherwise, use usual function.
If a context handle does not exist for this thread, initialize one.  Call the threaded GEOS routine with pointer of the context handle  as the first argument.
argtypes property
restype property
errcheck property
Binary & unary predicate factories
Unary Predicates
Binary Predicates
Topology Routines
GEOSRelate returns a string, not a geometry.
Linear referencing routines
Area, distance, and length prototypes.
This is the return type used by binary output (WKB, HEX) routines.
We create a simple subclass of c_char_p here because when the response  type is set to c_char_p, you get a _Python_ string and there's no way  to access the string's address inside the error checking function.  In other words, you can't free the memory allocated inside GEOS.  Previously,  the return type would just be omitted and the integer address would be  used -- but this allows us to be specific in the function definition and  keeps the reference so it may be free'd.
ctypes factory classes
HEX & WKB output
Deprecated creation routines from WKB, HEX, WKT
Deprecated output routines
The GEOS geometry type, typeid, num_coordinates and number of geometries
Geometry creation factories
Polygon and collection creation routines are special and will not  have their argument types defined.
Ring routines
Collection Routines
Cloning
Destruction routine.
SRID routines
Error-checking routines specific to coordinate sequences.
Object in by reference, return its value.
Coordinate sequence prototype factory classes.
Get routines have double parameter passed-in by reference.
Get/Set ordinate routines have an extra uint parameter.
Coordinate Sequence constructors & cloning.
Getting, setting ordinate
For getting, x, y, z
For setting, x, y, z
These routines return size & dimensions.
The WKB/WKT Reader/Writer structures and pointers
WKTReader routines
WKTWriter routines
WKBReader routines
Although the function definitions take `const unsigned char *`  as their parameter, we use c_char_p here so the function may  take Python strings directly as parameters.  Inside Python there  is not a difference between signed and unsigned characters, so  it is not a problem.
WKBWriter routines
WKB Writing prototypes.
WKBWriter property getter/setter prototypes.
Base I/O Class   Getting the pointer with the constructor.  Loading the real destructor function at this point as doing it in  __del__ is too late (import error).
Cleaning up with the appropriate destructor.
Non-public WKB/WKT reader classes for internal use because  their `read` methods return _pointers_ instead of GEOSGeometry  objects.
WKB/WKT Writer Classes
Property for getting/setting the byteorder.
Property for getting/setting the output dimension.
Property for getting/setting the include srid flag.
`ThreadLocalIO` object holds instances of the WKT and WKB reader/writer  objects that are local to the thread.  The `GEOSGeometry` internals  access these instances by calling the module-level functions, defined  below.
These module-level routines return the I/O object that is local to the  thread. If the I/O object does not exist yet it will be initialized.
Initially the pointer is NULL.
Default allowed pointer type.
Pointer access property.  Raise an exception if the pointer isn't valid don't  want to be passing NULL pointers to routines --  that's very bad.
Only allow the pointer to be set with pointers of the  compatible type or None (NULL).
Property for controlling access to the GEOS object pointers.  Using  this raises an exception when the pointer is NULL, thus preventing  the C library from attempting to access an invalid memory location.
If only one argument provided, set the coords array appropriately
If SRID was passed in with the keyword arguments
Getting the number of coords and the number of dimensions -- which   must stay the same, e.g., no LineString((1, 2), (1, 2, 3)).  Incrementing through each of the coordinates and verifying
Creating a coordinate sequence object because it is easier to  set the points using GEOSCoordSeq.__setitem__().
Calling the base geometry initialization with the returned pointer   from the function.
create a new coordinate sequence and populate accordingly
can this happen?
Sequence Properties
LinearRings are LineStrings used within Polygons.
Handling WKT input.
Handling HEXEWKB input.
Handling GeoJSON input.
When the input is a pointer to a geometry (GEOM_PTR).
When the input is a buffer (WKB).
Invalid geometry type.
Setting the pointer object with a valid pointer.
Post-initialization setup.
Setting the SRID, if given.
Setting the class type (e.g., Point, Polygon, etc.)  Lazy-loaded variable to avoid import conflicts with GEOSGeometry.
Setting the coordinate sequence for the geometry (will be None on  geometries that do not have coordinate sequences)
Pickling support  The pickled state is simply a tuple of the WKB (in string form)  and the SRID.
Instantiating from the tuple state that was pickled.
Geometry set-like operations   Thanks to Sean Gillies for inspiration:   http://lists.gispython.org/pipermail/community/2007-July/001034.html  g = g1 | g2
g = g1 & g2
g = g1 - g2
g = g1 ^ g2
Coordinate Sequence Routines
Geometry Info
Binary predicates.
SRID Routines
A possible faster, all-python, implementation:   str(self.wkb).encode('hex')
GDAL-specific output routines
short-circuit where source & dest SRIDs match
We don't care about SRID because CoordTransform presupposes  source SRS.
Creating an OGR Geometry, which is then transformed.  Getting a new GEOS pointer  User wants a cloned transformed geometry returned.  Reassigning pointer, and performing post-initialization setup  again due to the reassignment.
Topology Routines
Other Routines
Custom library path set?
Setting the appropriate names for the GEOS-C library.  Windows NT libraries  *NIX libraries
Using the ctypes `find_library` utility to find the path to the GEOS  shared library.  This is better than manually specifying each library name  and extension (e.g., libgeos_c.[so|so.1|dylib].).
No GEOS library could be found.  Getting the GEOS C library.  The C interface (CDLL) is used for  both *NIX and Windows.  See the GEOS C API source code for more details on the library function calls:   http://geos.refractions.net/ro/doxygen_docs/html/geos__c_8h-source.html  Here we set up the prototypes for the initGEOS_r and finishGEOS_r  routines.  These functions aren't actually called until they are  attached to a GEOS context handle -- this actually occurs in  geos/prototypes/threadsafe.py.
The notice and error handler C function callback definitions.  Supposed to mimic the GEOS message handler (C below):   typedef void (*GEOSMessageHandler)(const char *fmt, ...);
Opaque GEOS geometry structures, used for GEOM_PTR and CS_PTR
Pointers to opaque GEOS geometry structures.
Used specifically by the GEOSGeom_createPolygon and GEOSGeom_createCollection   GEOS routines
Returns the string version of the GEOS library. Have to set the restype  explicitly to c_char_p to ensure compatibility across 32 and 64-bit platforms.
Regular expression should be able to parse version strings such as  '3.0.0rc4-CAPI-1.3.3', '3.0.0-CAPI-1.4.1', '3.4.0dev-CAPI-1.8.0' or '3.4.0dev-CAPI-1.8.0 r0'
Checking the input value  Checking the dims of the input  Setting the X, Y, Z
Internal Routines
Ordinate getting and setting routines
Dimensions
Other Methods
Getting the substitution string depending on whether the coordinates have   a Z dimension.
Public classes for (WKB|WKT)Reader, which return GEOSGeometry
Getting the Geometry object.  Special case if a tuple/list was passed in.  The tuple may be  a point or a box  Box: ( (X0, Y0), (X1, Y1) )  Point: (X, Y)  Box: (X0, Y0, X1, Y1)  If a GeoRSS box was given via tuple.  Getting the lower-case geometry type.  For formatting consistent w/the GeoRSS simple standard:  http://georss.org/1.0simple  Only support the exterior ring.
SyndicationFeed subclasses
Regular expression for recognizing HEXEWKB and WKT.  A prophylactic measure  to prevent potentially malicious input from reaching the underlying C  library.  Not a substitute for good Web security programming practices.
Find the first declared geometry field
If needed, transform the geometry in the srid of the global geojson srid
Prepare array with arguments for capi function
Add additional argument to force computation if there is no  existing PAM file to take the values from.
Computation of statistics fails for empty bands.
Get value and nodata exists flag  If the pixeltype is an integer, convert to int
Create ctypes type array generator
Set read mode  Prepare empty ctypes array
Set write mode
Instantiate ctypes array holding the input data
Access band
Return data as numpy array if possible, otherwise as list  reshape() needs a reshape parameter with the height first.
See http://www.gdal.org/gdal_8h.htmla22e22ce0a55036a96f652765793fb7a4
A list of gdal datatypes that are integers.
Lookup values to convert GDAL pixel type indices into ctypes objects.  The GDAL band-io works with ctypes arrays to hold data to be written  or to hold the space for data to be read into. The lookup below helps  selecting the right ctypes object for a given gdal pixel type.
List of resampling algorithms that can be used to warp a GDALRaster.
Preprocess json inputs. This converts json strings to dictionaries,  which are parsed below the same way as direct dictionary inputs.
If input is a valid file path, try setting file as source.  GDALOpen will auto-detect the data source type.  A new raster needs to be created in write mode
Create driver (in memory by default)
For out of memory drivers, check filename argument
Check if width and height where specified
Check if srid was specified
Create GDAL Raster
Set band data if provided
Set SRID
Set additional properties if provided
Instantiate the object using an existing pointer to a gdal raster.
Raise an Exception if the value is being changed in read mode.
Create empty ctypes double array for data
Create ctypes double array with input and write data
Calculate boundary values based on scale and size  Calculate min and max values
Get the parameters defining the geotransform, srid, and size of the raster
Get the driver, name, and datatype of the target raster
Set the number of bands
Create target raster
Copy nodata values to warped raster
Select resampling algorithm
Reproject image
Make sure all data is written to file
Convert the resampling algorithm name into an algorithm id
Instantiate target spatial reference system
Create warped virtual dataset in the target reference system
Construct the target warp dictionary from the virtual raster
Set the driver and filepath if provided
Warp the raster into new srid
Encoding to ASCII if unicode passed in.  If SRID is a string, e.g., '4326', then make acceptable  as user input.
EPSG integer code was input.
Input is already an SRS pointer.
Creating a new SRS pointer, using the string buffer.
If the pointer is NULL, throw an exception.
Importing from either the user input string or an integer SRID.
Name & SRID properties
Unit Properties
Import Routines
Export Properties
Attempting to import objects that depend on the GDAL library.  The  HAS_GDAL flag will be set to True if the library is present on  the system.
Case-insensitive aliases for some GDAL/OGR Drivers.  For a complete list of original driver names see  http://www.gdal.org/ogr_formats.html (vector)  http://www.gdal.org/formats_list.html (raster)  vector  raster
If a string name of the driver was passed in
Checking the alias dictionary (case-insensitive) to see if an  alias exists for the given driver.
Attempting to get the GDAL/OGR driver by the string name.
Making sure we get a valid pointer to the OGR Driver
Only register all if the driver count is 0 (or else all drivers  will be registered over and over again)
Setting the feature pointer and index.
Getting the pointer for this field.
Setting the class depending upon the OGR Field Type (OFT)
OFTReal with no precision should be an OFTInteger.
Field Methods
Field Properties
Default is to get the field as a string.
The Field sub-classes for each OGR Field type.
If this is really from an OFTReal field with no precision,  read as a double and cast as Python int (to prevent overflow).
String & Binary fields, just subclasses
OFTDate, OFTTime, OFTDateTime fields.
TODO: Adapt timezone information.   See http://lists.osgeo.org/pipermail/gdal-dev/2006-February/007990.html   The `tz` variable has values of: 0=unknown, 1=localtime (ambiguous),   100=GMT, 104=GMT+1, 80=GMT-5, etc.
List fields are also just subclasses
Class mapping dictionary for OFT Types and reverse mapping.  New 64-bit integer types in GDAL 2
Helper routines for retrieving pointers and/or values from  arguments passed in by reference.
For routines that return a string.
Error-code return specified.  Getting the string value  Correctly freeing the allocated memory behind GDAL pointer  with the VSIFree routine.
Envelope checking
Geometry error-checking routines   OGR_G_Clone may return an integer, even though the  restype is set to c_void_p
Spatial Reference error-checking routines
Creation & destruction.
Getting the semi_major, semi_minor, and flattening functions.
WKT, PROJ, EPSG, XML importation routines.
Morphing to/from ESRI WKT.
Identifying the EPSG
Getting the angular_units, linear_units functions
For exporting to WKT, PROJ.4, "Pretty" WKT, and XML.
Memory leak fixed in GDAL 1.5; still exists in 1.4.
String attribute retrival routines.
SRS Properties
Coordinate transformation
Driver Routines
DataSource
Layer Routines
Feature Definition Routines
Feature Routines
Field Routines
Prepare partial functions that use cpl error codes
Raster Driver Routines
Raster Data Source Routines
Raster Band Routines
Reprojection routine
Setting the argument types
When a geometry pointer is directly returned.
Error code returned, geometry is returned by-reference.
Use subclass of c_char_p so the error checking routine  can free the memory at the pointer's address.
Error code is returned
Dynamically defining our error-checking function with the  given offset.
`errcheck` keyword may be set to False for routines that  return void, rather than a status code.
Generation routines specific to this module
GeoJSON routines.
GetX, GetY, GetZ all return doubles.
Geometry creation routines.
Geometry modification routines.
Destroys a geometry
Geometry spatial-reference related routines.
Geometry properties
Topology routines.
Transformation routines.
For retrieving the envelope of the geometry.
GDAL & SRS Exceptions
Legacy name
OGR Error Codes
CPL Error Codes  http://www.gdal.org/cpl__error_8h.html
Dictionary of acceptable OGRwkbGeometryType s and their string names.  Reverse type dictionary, keyed by lower-case of the name.
Setting the OGR geometry type number.
The OGR definition of an Envelope is a C structure containing four doubles.   See the 'ogr_core.h' source file for more information:    http://www.gdal.org/ogr/ogr__core_8h-source.html
OGREnvelope (a ctypes Structure) was passed in.
A tuple was passed in.
Individual parameters passed in.   Thanks to ww for the help
Checking the x,y coordinates
We provide a number of different signatures for this method,  and the logic here is all about converting them into a  4-tuple single parameter which does the actual work of  expanding the envelope.  A tuple was passed in.  An x and an y parameter were passed in  Individual parameters passed in.
TODO: Fix significant figures.
Custom library path set?
Windows NT shared libraries
*NIX library names.
Using the ctypes `find_library` utility  to find the  path to the GDAL library from the list of library names.
This loads the GDAL/OGR C library
On Windows, the GDAL binaries have some OSR routines exported with  STDCALL, while others are not.  Thus, the library will also need to  be loaded up as WinDLL for said OSR functions that require the  different calling convention.
Returns GDAL library version information with the given key.
Set library error handling so as errors are logged
For more information, see the OGR C API source code:   http://www.gdal.org/ogr/ogr__api_8h.html  The OGR_L_* routines are relevant here.
Does the Layer support random reading?
An integer index was given -- we cannot do a check based on the  number of features because the beginning and ending feature IDs  are not guaranteed to be 0 and len(layer)-1, respectively.
A slice was given
ResetReading() must be called before iteration is to begin.
If the Layer supports random reading, return.
Random access isn't supported, have to increment through  each feature until the given feature ID is encountered.  Should have returned a Feature, raise an OGRIndexError.
Layer properties
Map c_double onto params -- if a bad type is passed in it  will be caught here.
For more information, see the OGR C API source code:   http://www.gdal.org/ogr/ogr__api_8h.html  The OGR_G_* routines are relevant here.
If HEX, unpack input to a binary buffer.
Constructing the geometry,  If there's EWKT, set the SRS w/value of the SRID.  OGR_G_CreateFromWkt doesn't work with LINEARRING WKT.   See http://trac.osgeo.org/gdal/ticket/1992.  Seeing if the input is a valid short-hand string  (e.g., 'Point', 'POLYGON').  WKB was passed in  OGRGeomType was passed in, an empty geometry will be created.  OGR pointer (c_void_p) was the input.
Now checking the Geometry pointer before finishing initialization  by setting the pointer for the object.
Assigning the SpatialReference object to the geometry, if valid.
Setting the class depending upon the OGR Geometry Type
Pickle routines
Geometry set-like operations   g = g1 | g2
g = g1 & g2
g = g1 - g2
g = g1 ^ g2
Geometry Properties
TODO: Fix Envelope() for Point geometries.
The SRS property
Do not have to clone the `SpatialReference` object pointer because  when it is assigned to this `OGRGeometry` it's internal OGR  reference count is incremented, and will likewise be released  (decremented) when this geometry's destructor is called.
The SRID property
Output Methods
Creating the unsigned character buffer, and passing it in by reference.  Returning a buffer of the string at the pointer.
Geometry Methods
Closing the open rings.
Depending on the input type, use the appropriate OGR routine  to perform the transformation.
Returning the output of the given function with the other geometry's  pointer.
Geometry-generation Methods
The subclasses for OGR Geometry.
LinearRings are used in Polygons.
Polygon Properties
Summing up the number of points in each ring of the Polygon.
The centroid is a Point, create a geometry for this.
Geometry Collection base class.
Summing up the number of points in each geometry in this collection
Multiple Geometry types.
Class mapping dictionary (using the OGRwkbGeometryType as the key)
Initially the pointer is NULL.
Default allowed pointer type.
Pointer access property.  Raise an exception if the pointer isn't valid don't  want to be passing NULL pointers to routines --  that's very bad.
Only allow the pointer to be set with pointers of the  compatible type or None (NULL).
Feature Properties
Retrieving the geometry pointer for the feature.
Getting the geometry for the feature.
Getting the 'description' field for the feature.
We can also increment through all of the fields   attached to this feature.  Get the name of the field (e.g. 'description')
Get the type (integer) of the field, e.g. 0 => OFTInteger
For more information, see the OGR C API source code:   http://www.gdal.org/ogr/ogr__api_8h.html  The OGR_DS_* routines are relevant here.
The write flag.  See also http://trac.osgeo.org/gdal/wiki/rfc23_ogr_unicode
The data source driver is a void pointer.  OGROpen will auto-detect the data source type.  Making the error message more clear rather than something  like "Invalid pointer returned from OGROpen".
Raise an exception if the returned pointer is NULL
If the backend hasn't been used, no more retrieval is necessary.  If this storage class contained all the messages, no further  retrieval is necessary
Even if there are no more messages, continue iterating to ensure  storages which contained messages are flushed.
Using 0/1 here instead of False/True to produce more compact json
Compatibility with previously-encoded messages
uwsgi's default configuration enforces a maximum size of 4kb for all the  HTTP headers. In order to leave some room for other cookies and headers,  restrict the session cookie to 1/2 of 4kb. See 18781.
remove the sentinel value
If we get here (and the JSON decode works), everything is  good. In any other case, drop back and return None.
Mark the data as used (so it gets removed) since something was wrong  with the data.
Check that the message level is not less than the recording level.  Add the message.
A higher middleware layer may return a request which does not contain  messages storage, so make no assumption that it will be there.
Check that the user has delete permission for the actual model
Populate deletable_objects, a data structure of all related objects that  will also be deleted.
The user has already confirmed the deletion.  Do the deletion and return a None to display the change list view again.  Return None to display the change list page again.
Display the confirmation page
Translators: 'repr' means representation (https://docs.python.org/3/library/functions.htmlrepr)  change_message is either a string or a JSON structure
Wait for the next page to be loaded
IE7 occasionally returns an error "Internet Explorer cannot  display the webpage" and doesn't load the next page. We just  ignore it.
Prevent the `find_elements_by_css_selector` call from blocking  if the selector doesn't match any options as we expect it  to be the case.
ACTION_CHECKBOX_NAME is unused, but should stay since its import from here  has been referenced in documentation.
This dictionary will eventually contain the request's query string  parameters actually used by this filter.
The parameter that should be used in the query string for that filter.
This is to allow overriding the default filters for certain types  of fields with some custom filters. The first found in the list  is used in priority.
When time zone support is enabled, convert "now" to the user's time  zone so Django's definition of "Today" matches what the user expects.
This should be registered last, because it's a last resort. For example,  if a field is eligible to use the BooleanFieldListFilter, that'd be much  more appropriate, and the AllValuesFieldListFilter won't get used for it.  Obey parent ModelAdmin queryset when deciding which options to show
checkboxes should not have a label suffix as the checkbox appears  to the left of the label.
Make self.field look a little bit like a field. This means that  {{ field.name }} must be a useful class name to identify the field.  For convenience, store other field-related data here too.
Auto fields are editable (oddly), so need to check for auto or non-editable pk  Also search any parents for an auto field. (The pk info is propagated to child  models so that does not need to be checked in parents.)
-*- coding: utf-8 -*-
No database changes; removes auto_add and adds default/editable.
-*- coding: utf-8 -*-
Loop through all the fields (one per cell)
Delete checkbox
Backwards compatibility alias for django.templatetags.static.static().  Deprecation should start in Django 2.0.
If there are 10 or fewer pages, display links to every page.  Otherwise, do some fancy  Insert "smart" pagination links, so that there are always ON_ENDS  links at either end of the list of pages, and there are always  ON_EACH_SIDE links at either end of the "current page" link.
if the field is the action checkbox: no sorting and special class
Not sortable
OK, it is sortable if we got this far  Is it currently being sorted on?
We want clicking on this header to bring the ordering to the  front  o_list_remove - omit
If list_display_links not defined, add the link tag to the first field
Display link to the result's change_view if the url exists, else  display just the result's representation.  Convert the pk to something that can be used in Javascript.  Problem cases are long ints (23L) and non-ASCII strings.
By default the fields come from ModelAdmin.list_editable, but if we pull  the fields out of the form instead of list_editable custom admins  can provide fields on a per request basis
Wrapper class used to return items in a list_editable  changelist, annotated with the form object for error  reporting purposes. Needed to maintain backwards  compatibility with existing admin templates.
select appropriate start level
Text to put at the end of each page's <title>.
Text to put in each page's <h1>.
Text to put at the top of the admin index page.
URL for the "View site" link at the top of each admin page.
Ignore the registration if the model has been  swapped out.  If we got **options then dynamically construct a subclass of  admin_class with those **options.  For reasons I don't quite understand, without a __module__  the created class appears to "live" in the wrong place,  which causes issues later on.
Instantiate the admin class to save in the registry
Inner import to prevent django.contrib.admin (app) from  importing django.contrib.auth.models.User (unrelated model).
We add csrf_protect here so this function can be used as a utility  function for any view, without having to repeat 'csrf_protect'.
Since this module gets imported in the application's root package,  it cannot import models from other applications at the module level,  and django.contrib.contenttypes.views imports ContentType.
Admin-site-wide views.
Add in each model's views, and create a list of valid URLS for the  app_index
If there were ModelAdmins registered, we should have a list of app  labels for which we need to allow access to the app_index view,
Since the user isn't logged out at this point, the value of  has_permission must be overridden.
Already logged-in, redirect to admin index
Since this module gets imported in the application's root package,  it cannot import models from other applications at the module level,  and django.contrib.admin.forms eventually imports User.
Check whether user has any perm for this module.  If so, add the module to the model_list.
Sort the apps alphabetically.
Sort the models alphabetically within each app.
Sort the models alphabetically within each app.
This global object represents the default admin site, for the common case.  You can instantiate AdminSite in your own code to create a custom admin site.
Changelist settings
Get search parameters from the query string.
Remove all the parameters that are globally and systematically  ignored.
This is simply a custom list filter class.
This is a custom FieldListFilter class for a given field.
This is simply a field name, so use the default  FieldListFilter class that has been registered for  the type of the given field.
Check if we need to use distinct()
At this point, all the parameters used by the various ListFilters  have been removed from lookup_params, which now only contains other  parameters passed via the query string. We now loop through the  remaining parameters both to ensure that all the parameters are valid  fields and to determine if at least one of them needs distinct(). If  the lookup parameters aren't real fields, then bail out.
Get the number of objects, with admin filters applied.
Get the total number of objects, with no admin filters applied.
Get the list of objects to display on this page.
Admin actions are shown if there is at least one entry  or if entries are not counted because show_full_result_count is disabled
See whether field_name is a name of a non-field  that allows sorting.
Clear ordering and used params
reverse order if order_field has already "-" as prefix
Add the given query's ordering fields, if any.
Ensure that the primary key is systematically present in the list of  ordering fields so we can guarantee a deterministic order across all  database backends.  The two sets do not intersect, meaning the pk isn't present. So  we add it.
We must cope with more than one column having the same underlying sort  field, so we base things on column numbers.  for ordering specified on ModelAdmin or model Meta, we don't know  the right column numbers absolutely, because there might be more  than one column associated with that ordering, so we guess.
First, we collect all the declared list filters.
Then, we let every list filter modify the queryset to its liking.
Finally, we apply the remaining lookup parameters from the query  string (i.e. those that haven't already been processed by the  filters).
Allow certain types of errors to be re-raised as-is so that the  caller can treat them in a special way.
Every other error is caught with a naked except, because we don't  have any other way of validating lookup parameters. They might be  invalid if the keyword arguments are incorrect, or if the values  are not in the correct type, so we might get FieldError,  ValueError, ValidationError, or ?.
Set ordering.
Apply search results
Remove duplicates from results, if necessary
Note that we're calling MultiWidget, not SplitDateTimeWidget, because  we want to define widgets.
The related object is registered with the same AdminSite
TODO: "lookup_id_" is hard-coded here. This should instead use  the correct API to determine the ID dynamically.
The related object is registered with the same AdminSite
Backwards compatible check for whether a user can add related  objects.  XXX: The UX does not support multiple selected values.  XXX: The deletion UX can be confusing when dealing with cascading deletion.  so we can check if the related object is registered with this AdminSite
Since this module gets imported in the application's root package,  it cannot import models from other applications at the module level.
Merge FORMFIELD_FOR_DBFIELD_DEFAULTS with the formfield_overrides  rather than simply overwriting.
If the field specifies choices, we don't need to look for special  admin widgets - we just need to use a select widget of some kind.
ForeignKey or ManyToManyFields  Combine the field kwargs with any options for formfield_overrides.  Make sure the passed in **kwargs override anything in  formfield_overrides because **kwargs is more specific, and should  always win.
Get the correct formfield.
For non-raw_id fields, wrap the widget with a wrapper that adds  extra HTML -- the "add other" interface -- to the end of the  rendered output. formfield can be None if it came from a  OneToOneField with parent_link=True or a M2M intermediary.
If we've got overrides for the formfield defined, use 'em. **kwargs  passed to formfield_for_dbfield override the defaults.
For any other type of field, just call its formfield() method.
If the field is named as a radio_field, use a RadioSelect  Avoid stomping on custom widget/choices arguments.
If it uses an intermediary model that isn't auto created, don't show  a field in admin.
use the ContentType lookup if view_on_site is True
TODO: this should be handled by some parameter to the ChangeList.
Check FKey lookups that are allowed, so that popups produced by  ForeignKeyRawIdWidget, on the basis of ForeignKey.limit_choices_to,  are allowed to work.  As ``limit_choices_to`` can be a callable, invoke it here.
Lookups on non-existent fields are ok, since they're ignored  later.  It is allowed to filter on values that would be found from local  model anyways. For example, if you filter on employee__department__id,  then the id value would be found already from employee__department_id.
This is not a relational field, so further parts  must be transforms.
Either a local field filter, or no fields at all.
Always allow referencing the primary key since it's already possible  to get this information from the change view URL.
Allow reverse relationships to models defining m2m fields if they  target the specified field.
Make sure at least one of the models registered for this site  references this field through a FK or a M2M relationship.
Custom templates (designed to be over-ridden in subclasses)
Actions
For backwards compatibility (was the change url before 1.9)
Take the custom ModelForm's Meta.exclude into account only if the  ModelAdmin doesn't define its own.  if exclude is an empty list we pass None to be consistent with the  default on modelform_factory
Remove declared form fields which are in readonly_fields.
If self.actions is explicitly set to None that means that we don't  want *any* actions enabled on this page.
Gather actions from the admin site first
Then gather them from the model admin and all parent classes,  starting with self and working back up.  Avoid trying to iterate over None
get_action might have returned None, so filter any of those out.
Convert the actions into an OrderedDict keyed by name.
If the action is a callable, just use it.
Next, look for a method. Grab it off self.__class__ to get an unbound  method instead of a bound one; this ensures that the calling  conventions are the same for functions and methods.
Finally, look for a named method on the admin site
Use only the first item in list_display as link
Apply keyword searches.
attempt to get the level if passed a string
Add a link to the object's change form if the user can edit the obj.  Here, we distinguish between different save types by checking for  the presence of keys in request.POST.
Redirecting after "Save as new".
Retrieve the `object_id` from the resolved pattern arguments.
There can be multiple action forms on the page (at the top  and bottom of the change list, for example). Get the action  whose button was pushed.
Construct the action form.
Use the action whose button was pushed  If we didn't get an action from the chosen form that's invalid  POST data, so by deleting action it'll fail the validation check  below. So no need to do anything here
If the form's valid we can handle the action.
Get the list of selected PKs. If nothing's selected, we can't  perform an action on it, so bail. Except we want to perform  the action explicitly on all objects.  Reminder that something needs to be selected or nothing will happen
Perform the action only on the selected objects
Actions may return an HttpResponse-like object, which will be  used as the response from the POST. If not, we'll be a good  little HTTP citizen and redirect back to the changelist page.
We have to special-case M2Ms as a list of comma-separated PKs.
Hide the "Save" and "Save and continue" buttons if "Save as New" was  previously chosen to prevent the interface from getting confusing.  Use the change template instead of the add template.
Check actions to see if any are available on this changelist  Add the action checkboxes if there are any actions available.
Wacky lookup parameters were given, so redirect to the main  changelist page, without parameters, and pass an 'invalid=1'  parameter via the query string. If wacky parameters were given  and the 'invalid=1' parameter was already in the query string,  something is screwed up with the database, so display an error  page.
If the request was POSTed, this might be a bulk action or a bulk  edit. Try to look up an action or confirmation first, but if this  isn't an action the POST will fall through to the bulk edit check,  below.
Actions with no confirmation
Actions with confirmation
If we're allowing changelist editing, we need to construct a formset  for the changelist given all the fields to be edited. Then we'll  use the formset to validate/process POSTed data.
Handle POSTed bulk-edit data.
Handle GET -- construct a formset for display.
Build the list of media to be used by the formset.
Build the action form and populate it with available actions.
Populate deleted_objects, a data structure of all related objects that  will also be deleted.
First check if the user can see this history.
Then get the history for this object.
Take the custom ModelForm's Meta.exclude into account only if the  InlineModelAdmin doesn't define its own.  If exclude is an empty list we use None, since that's the actual  default.
Translators: Model verbose name and instance representation,  suitable to be an item in a list.
We're checking the rights to an auto-created intermediate model,  which doesn't have its own individual permissions. The user needs  to have the change permission for the related model in order to  be able to do anything with the intermediate model.
The model was auto-created as intermediary for a  ManyToMany-relationship, find the target model
We're checking the rights to an auto-created intermediate model,  which doesn't have its own individual permissions. The user needs  to have the change permission for the related model in order to  be able to do anything with the intermediate model.
Remove the last item of the lookup path if it is a query term  Now go through the fields (following all relations) and look for an m2m  This field is a relation, update opts to follow the relation  This field is a m2m relation so we know we need to call distinct
if key ends with __in, split parameter into separate values  if key ends with __isnull, special case '' and the string literals 'false' and '0'
Change url doesn't exist -- don't display link to edit
Display a link to the admin page.
Don't display link to edit, because it either has no  admin or is edited inline.
For non-field values, the value is either a method, property or  returned via a callable.
Generic foreign keys OR reverse relations
field is likely a ForeignObjectRel
NullBooleanField needs special-case null-handling, so it comes  before the general null test.
Field should point to another model
-*- coding: utf-8 -*-
contrib.contenttypes must be installed.  The auth context processor must be installed if using the default  authentication backend.  Skip this non-critical check:  1. if the user has a non-trivial TEMPLATES setting and Django     can't find a default template engine  2. if anything goes wrong while loading template engines, in     order to avoid raising an exception from a confusing location  Catching ImproperlyConfigured suffices for 1. but 2. requires  catching all exceptions.
Stuff can be put in fields that isn't actually a model field if  it's in readonly_fields, readonly_fields will handle the  validation of such things.
If we can't find a field on the model that matches, it could  be an extra field on the form.
Skip ordering in the format field1__field2 (FIXME: checking  this format would be nice, but it's a little fiddly).
getattr(model, item) could be an X_RelatedObjectsDescriptor
This is a deliberate repeat of E108; there's more than one path  required to test this condition.
If item is option 3, it should be a ListFilter...  ...  but not a FieldListFilter.
item is option 2
item is option 1
Validate the field string
If list_display[0] is in list_editable, check that  list_display_links is set. See 22792 and 26229 for use cases.
Do not perform more specific checks if the base checks result in an  error.
Skip if `fk_name` is invalid.
Exclude methods starting with these strings from documentation
Display an error message for people without docutils
Non-trivial TEMPLATES settings aren't supported (24125).
Non-trivial TEMPLATES settings aren't supported (24125).
Get the model class.
Gather fields/field descriptions.  ForeignKey is a special case since the field will actually be a  descriptor that returns the other object
Gather many-to-many fields.
Gather model methods.  If a method has no arguments, show it as a 'field', otherwise  as a 'method with arguments'.  Join arguments with ', ' and in case of default value,  join it with '='. Use repr() so that strings will be  correctly displayed.
Gather related objects
Non-trivial TEMPLATES settings aren't supported (24125).
This doesn't account for template loaders (24128).
handle named groups first
handle non-named groups
clean up any outstanding regex-y characters.
Convert tabs to spaces and split into lines
reST roles
Some backends (e.g. memcache) raise an exception on invalid  cache keys. If this happens, reset the session. See 17810.
Because a cache can fail silently (e.g. memcache), we don't know if  we are failing to create a new session because of a key collision or  because the cache is missing. So we try for a (large) number of times  and then raise an exception. That's the risk you shoulder if using  cache backing.
This doesn't handle non-default expiry dates, see 19201
BadSignature, ValueError, or unpickling exceptions. If any of  these happen, reset the session.
Avoids a circular import and allows importing SessionStore when  django.contrib.sessions is not in INSTALLED_APPS.
Save immediately to ensure we have a unique entry in the  database.
Key wasn't unique. Try again.
Some backends (e.g. memcache) raise an exception on invalid  cache keys. If this happens, reset the session. See 17810.
Duplicate DBStore.load, because we need to keep track  of the expiry date to set it properly in the cache.
Make sure the storage path is valid.
Make sure we're not vulnerable to directory traversal. Session keys  should always be md5s, so they should never contain directory  components.
Don't fail if there is no data in the session file.  We may have opened the empty placeholder file.
Remove expired sessions.
Get the session data now, before we start messing  with the file it is stored within.
Make sure the file exists.  If it does not already exist, an  empty placeholder file is created.
Write the session file without interfering with other threads  or processes.  By writing to an atomically generated temporary  file and then using the atomic os.rename() to make the complete  file visible, we avoid having to lock the session file, while  still maintaining its integrity.  Note: Locking the session file was explored, but rejected in part  because in order to be atomic and cross-platform, it required a  long-lived lock file for each session, doubling the number of  files in the session storage directory at any given time.  This  rename solution is cleaner and avoids any additional overhead  when reading the session data, which is the more common case  unless SESSION_SAVE_EVERY_REQUEST = True.  See ticket 8616.
This will atomically rename the file (os.rename) if the OS  supports it. Otherwise this will result in a shutil.copy2  and os.unlink (for example on Windows). See 9084.
When an expired session is loaded, its file is removed, and a  new file is immediately created. Prevent this by disabling  the create() method.
session_key should not be case sensitive because some backends can store it  on case insensitive file systems.
could produce ValueError if there is no ':'
ValueError, SuspiciousOperation, unpickling exceptions. If any of  these happen, just return an empty dictionary (an empty session).
To avoid unnecessary persistent storage accesses, we set up the  internals directly (loading data wastes time, since we are going to  set it to an empty dict anyway).
Make the difference between "expiry=None passed in kwargs" and  "expiry not passed in kwargs", in order to guarantee not to trigger  self.load() when expiry is provided.
Same comment as in get_expiry_age
Remove any custom expiration for this session.
-*- coding: utf-8 -*-
First check if we need to delete this cookie.  The session should be deleted only if the session is entirely empty  Save the session data and refresh the client cookie.  Skip session save for 500 responses, refs 3881.  The user is now logged out; redirecting to same  page will result in a redirect to the login page  if required.
-*- encoding: utf-8 -*-
Mark value safe so i18n does not break with <sup> or <sub> see 19988
A tuple of standard large number to their converters
Passed value wasn't a date object
Date arguments out of range
Translators: please keep a non-breaking space (U+00A0)  between count and time unit.
Translators: please keep a non-breaking space (U+00A0)  between count and time unit.
Translators: please keep a non-breaking space (U+00A0)  between count and time unit.
Translators: please keep a non-breaking space (U+00A0)  between count and time unit.
Translators: please keep a non-breaking space (U+00A0)  between count and time unit.
Translators: please keep a non-breaking space (U+00A0)  between count and time unit.
For performance, only add a from_db_value() method if the base field  implements it.
Remove the field name checks as they are not needed here.
Assume we're deserializing
Distinguish NULL and empty arrays
In.process_rhs() expects values to be hashable, so convert lists  to tuples.
Initializing base_field here ensures that its model matches the model for self.
Assume we're deserializing
Register hstore straight away as it cannot be done before the  extension is installed, a subsequent data migration would use the  same connection
Hstore is not available on the database.  If someone tries to create an hstore field it will error there.  This is necessary as someone may be using PSQL without extensions  installed but be using other features of contrib.postgres.  This is also needed in order to create the connection in order to  install the hstore extension.
See the comment for RadioSelect.id_for_label()
Cast everything to strings for ease.
For purposes of seeing whether something has changed, None is  the same as an empty dict, if the data or initial value we get  is None, replace it w/ {}.
On Combinable, these are not implemented to reduce confusion with Q. In  this case we are actually (ab)using them to do logical combination so  it's consistent with other usage in Django.
We can't simply concatenate messages since they might require  their associated parameters to be expressed correctly which  is not something `string_concat` does. For example, proxied  ungettext calls require a count parameter and are converted  to an empty string if they are missing it.
FileSystemStorage fallbacks to MEDIA_ROOT when location  is empty, so we restore the empty value.
Handle directory paths and fragments
Special casing for a @font-face hack, like url(myfont.eot?iefix")  http://www.fontspring.com/blog/the-new-bulletproof-font-face-syntax
Ignore absolute/protocol-relative, fragments and data-uri URLs.
Ignore absolute URLs that don't point to a static file (dynamic  CSS / JS?). Note that STATIC_URL cannot be empty.
Strip off the fragment so a path-like fragment won't interfere.
Otherwise the condition above would have returned prematurely.
We're using the posixpath module to mix paths and URLs conveniently.
Determine the hashed name of the target file with the storage backend.
Restore the fragment that was stripped off earlier.
Return the hashed version to the file
don't even dare to process the files if we're in dry run mode
where to store the new paths
build a list of adjustable files
then sort the files by the directory level
use the original, local file, not the copied-but-unprocessed  file, which might be somewhere far away, like S3
generate the hash with the original content, even for  adjustable files.
then get the original's file content..
..to apply each replacement pattern to the content  then save the processed result  or handle the case in which neither processing nor  a change to the original file happened
and then set the cache accordingly
Finally store the processed paths
store the hashed name if there was a miss, e.g.  when the files are still processed
Use the default backend
Only append if urlpatterns are empty
Prefix the relative path if the source storage contains it
Here we check if the storage backend has a post_process  method and pass it the list of modified files.  Add a blank line before the traceback, otherwise it's  too easy to miss the relevant part of the error message.
Delete broken symlinks
When was the target file modified last time?
The storage doesn't support get_modified_time() or failed
When was the source file modified last time?
The full path of the target file  Skip the file if the source file is younger  Avoid sub-second precision (see 14665, 19540)  Then delete the existing file if really needed
Skip this file if it was already copied earlier  Delete the target file if needed or break  The full path of the source file  Finally link the file
Skip this file if it was already copied earlier  Delete the target file if needed or break  The full path of the source file  Finally start copying
May be used to differentiate between handler types (e.g. in a  request_finished signal)
Backwards compatibility alias for django.templatetags.static.static().  Deprecation should start in Django 2.0.
Backwards compatibility alias for django.templatetags.static.do_static().  Deprecation should start in Django 2.0.
To keep track on which directories the finder has searched the static files.
List of locations with static files  Maps dir paths to an appropriate storage instance
The list of apps that are handled  Mapping of app names to storage instances
only try to find a file if the source dir actually exists
Make sure we have an storage instance here.
No match.
First attempt to look up the site by host with or without port.
Fallback to looking up site after stripping port from the host.
Imports are inside the function because its point is to avoid importing  the Site models when django.contrib.sites isn't installed.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
The default settings set SITE_ID = 1, and some tests in Django's test  suite rely on this value. However, if database sequences are reused  (e.g. in the test suite after flush/syncdb), it isn't guaranteed that  the next id will be 1, so we coerce it. See 15573 and 16353. This  can also crop up outside of tests - see 15346.
We set an explicit pk instead of relying on auto-incrementation,  so we need to reset the database sequence. See 17415.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Defined as class-level attributes to be subclassing-friendly.
No need to check for a redirect for non-404 responses.
No redirect was found. Return the response.
Cache shared by all the get_for_* methods to speed up  ContentType retrieval.
The ContentType entry was not found in the cache, therefore we  proceed to load or create it.  Start with get() and not get_or_create() in order to use  the db_for_read (see 20401).  Not found in the database; we proceed to create it. This time  use get_or_create to take care of any race conditions.
Final results  models that aren't already in the cache  These weren't in the cache, or the DB, create them.
This could raise a DoesNotExist; that's correct behavior and will  make sure that only correct ctypes get stored in the cache dict.
Note it's possible for ContentType objects to be stale; model_class() will return None.  Hence, there is no reliance on model._meta.app_label here, just using the model fields instead.
Look up the object, making sure it's got a get_absolute_url() function.
If the object actually defines a domain, we're done.
Otherwise, we need to introspect the object's relationships for a  relation to the Site object
First, look for an many-to-many relationship to Site.  Caveat: In the case of multiple related Sites, this just  selects the *first* one, which is arbitrary.
Next, look for a many-to-one relationship to Site.
Fall back to the current site (if possible).
Fall back to the current request's site.
If all that malarkey found an object domain, use it. Otherwise, fall back  to whatever get_absolute_url() returned.
There's no FK to exclude, so no exclusion checks are required.
Check that the ct_field and ct_fk_fields exist
There's one or more GenericForeignKeys; make sure that one of them  uses the right ct_field and ct_fk_field.
Take the custom ModelForm's Meta.exclude into account only if the  GenericInlineModelAdmin doesn't define its own.
if there is no field called `ct_field` let the exception propagate
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Gracefully fallback if a stale content type causes a  conflict as update_contenttypes will take care of asking the  user what should be done next.
Clear the cache as the `get_by_natual_key()` call will cache  the renamed ContentType instance by its old model name.
Determine whether or not the ContentType model is available.
There's no point in going forward if the initial contenttypes  migration is unapplied as the ContentType model will be  unavailable from this point.  The ContentType model is not available yet.
Always clear the global content types cache.
Get all the content types
Field flags
This should never happen. I love comments like this, don't you?
For efficiency, group the instances by content type and then do one  query per model  We need one instance for each group in order to get the right db:  We avoid looking for values if either ct_id or fkey value is None
For doing the join in Python, we have to match both the FK val and the  content type, so we use a callable that returns a (fk, class) pair.
Don't use getattr(instance, self.ct_field) here because that might  reload the same ContentType over and over (5570). Instead, get the  content type ID here, and later when the actual instance is needed,  use ContentType.objects.get_for_id(), which has a global cache.
Field flags
This construct is somewhat of an abuse of ForeignObject. This field  represents a relation from pk to object_id field. But, this relation  isn't direct, the join is generated reverse along foreign key. So,  the from_field is object_id field, to_field is pk because of the  reverse join.
With an inheritance chain ChildTag -> Tag and Tag defines the  GenericForeignKey, and a TaggedItem model has a GenericRelation to  ChildTag, then we need to generate a join from TaggedItem to Tag  (as Tag.object_id == TaggedItem.pk), and another join from Tag to  ChildTag (as that is where the relation is to). Do this by first  generating a join to the parent model, then generating joins to the  child models.  Collect joins needed for the parent -> child chain. This is easiest  to do if we collect joins for the child -> parent chain and then  reverse the direction (call to reverse() and use of  field.remote_field.get_path_info()).
Add get_RELATED_order() and set_RELATED_order() to the model this  field belongs to, if the model on the other end of this relation  is ordered with respect to its corresponding GenericForeignKey.
We use **kwargs rather than a kwarg argument to enforce the  `manager='manager_name'` syntax.
We (possibly) need to convert object IDs to the type of the  instances' PK in order to match up instances:
`QuerySet.delete()` creates its own atomic block which  contains the `pre_delete` and `post_delete` signal handlers.
Force evaluation of `objs` in case it's a queryset whose value  could be affected by `manager.clear()`. Refs 19816.
-*- coding: utf-8 -*-
Handle script prefix manually because we bypass reverse()
If registration is required for accessing this page, and the user isn't  logged in, redirect to the login page.
To avoid having to always use the "|safe" filter in flatpage templates,  mark the title and content as already safe (since they are raw HTML  content in the first place).
-*- coding: utf-8 -*-
If a prefix was specified, add a filter
If the provided user is not authenticated, or no user  was provided, filter the list to only public flatpages.
Must have at 3-6 bits in the tag
If there's an even number of bits, there's no prefix
The very last bit must be the context name
If there are 5 or 6 bits, there is a user defined
This override makes get_response optional during the  MIDDLEWARE_CLASSES deprecation.
Return the original response if any errors happened. Because this  is a middleware, we can't assume the errors will be caught elsewhere.
Parse the token
Check that the timestamp/uid has not been tampered with
Check the timestamp is within limit
timestamp is number of days since 2001-1-1.  Converted to  base 36, this gives us a 3 digit string until about 2121
Ensure results are consistent across DB backends
Used for mocking in tests
A few helper functions for common logic between User and AnonymousUser.
Active superusers have all permissions.
Otherwise we need to check the backends.
Active superusers have all permissions.
This value in the session is always serialized to a string, so we need  to convert it back to Python whenever we access it.
This backend doesn't accept these credentials as arguments. Try the next one.
This backend says to stop in our tracks - this user should not be allowed in at all.
Annotate the user object with the path of the backend.
The credentials supplied are invalid to all backends, fire signal
To avoid reusing another user's session, create a new, empty  session if the existing session corresponds to a different  authenticated user.
Dispatch the signal before the user is logged out so the receivers have a  chance to find out *who* logged out.
remember language choice saved to session
Verify the session
Stores the raw password if set_password() is called so that it can  be passed to password_changed() after the model is saved.
Password hash upgrades shouldn't be considered password changes.
Set a value that will never be a valid hash
Avoid shadowing the login() and logout() views below.
Ensure the user-originating redirection URL is safe.
Security check -- don't allow redirection to a different host.
Redirect to this page until the session has been cleared.
urlsafe_base64_decode() decodes to bytestring on Python 3
Updating the password logs out all other sessions for the user  except the current one.
Avoid a major performance hit resolving permission names which  triggers a content_type load:
See 20078: we don't want to allow any lookups involving passwords.
It's an error for a user to have add permission but NOT change  permission for users. If we allowed such users to add users, they  could create superusers, which would mean they would essentially have  the permission to change users. To avoid the problem entirely, we  disallow users from adding users if they don't have change  permission.  Raise Http404 in debug mode so that the user gets a helpful  error message.
We should allow further modification of the user just added i.e. the  'Save' button should behave like the 'Save and continue editing'  button except in two scenarios:  * The user has pressed the 'Save and add another' button  * We are adding a user in a popup
Always return initial because the widget doesn't  render an input field.
Regardless of what the user provides, return the initial value.  This is done here, rather than on the field, because the  field does not have access to the initial value
Set the label for the "username" field.
Email subject *must not* contain newlines
Don't validate passwords that don't match.
If not provided, create the user with an unusable password  Same as user_data but with foreign keys as fake model instances  instead of raw IDs.
Do quick and dirty validation if --noinput
Prompt for username/password, and any other required fields.  Enclose this whole thing in a try/except to catch  KeyboardInterrupt and exit gracefully.
Get a username
Wrap any foreign keys in fake model instances
Get a password  Don't validate passwords that don't match.
Don't validate blank passwords.
This will hold the permissions we're looking for as  (content_type, (codename, name))  The codenames and ctypes that should exist.  Force looking up the content types in the current database  before creating foreign keys to them.
Find all the Permissions that have a content_type for a model we're  looking for.  We don't need to check for codenames since we already have  a list of the ones we're going to create.
KeyError will be raised by os.getpwuid() (called by getuser())  if there is no corresponding entry in the /etc/passwd file  (a very restricted chroot environment, for example).
UnicodeDecodeError - preventive treatment for non-latin Windows.
This file is used in apps.py, it should not trigger models import.
If the User model has been swapped out, we can't make any assumptions  about the default user name.
Run the username validator
Don't return the default username if it is already taken.
Run the default password hasher once to reduce the timing  difference between an existing and a non-existing user (20760).
Create a User object if not already in the database?
Note that this could be accomplished in one try-except clause, but  instead we use get_or_create when creating unknown users since it has  built-in safeguards for multiple threads.
-*- coding: utf-8 -*-
Ensure the contenttypes migration is applied before sending  post_migrate signals (which create ContentTypes).
-*- coding: utf-8 -*-
No database changes; modifies validators and error_messages (13147).
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
db connection state is managed similarly to the wsgi handler  as mod_wsgi may call these functions outside of a request/response cycle
If the hasher didn't change (we don't protect against enumeration if it  does) and the password should get updated, try to close the timing gap  between the work factor of the current encoded password and the default  work factor.
Ancient versions of Django created plain MD5 passwords and accepted  MD5 passwords with an empty salt.  Ancient versions of Django accepted SHA1 passwords with an empty salt.
The runtime for Argon2 is too complicated to implement a sensible  hardening algorithm.
Argon2 < 1.3
Hash the password prior to using bcrypt to prevent password  truncation as described in 20138.  Use binascii.hexlify() because a hex encoded bytestring is  Unicode on Python 3.
work factor is logarithmic, adding one doubles the load.
we don't need to store the salt, but Django used to do this
If the login url is the same scheme and net location then just  use the path as the "next" url.
First check if the user has the permission (even anon users)  In case the 403 handler should be called raise the exception  As the last resort, show the login form
-*- coding: utf-8 -*-
Checks might be run against a set of app configs that don't  include the specified user model. In this case we simply don't  perform the checks defined below.
Check that REQUIRED_FIELDS is a list
Check that the USERNAME FIELD isn't included in REQUIRED_FIELDS.
Check that the username field is unique
Check builtin permission name length.  Check custom permission name length.  Check custom permissions codename clashing.
Name of request header to grab username from.  This will be the key as  used in the request.META dictionary, i.e. the normalization of headers to  all uppercase and the addition of "HTTP_" prefix apply.
AuthenticationMiddleware is required so that request.user exists.  If specified header doesn't exist then remove any existing  authenticated remote-user, or return (leaving request.user set to  AnonymousUser by the AuthenticationMiddleware).  If the user is already authenticated and that user is the user we are  getting passed in the headers, then the correct user is already  persisted in the session and we don't need to continue.  An authenticated user is associated with the request, but  it does not match the authorized user in the header.
We are seeing this user for the first time in this session, attempt  to authenticate the user.  User is valid.  Set request.user and persist user in the session  by logging the user in.
backend failed to load
To fix 'item in perms.someapp' and __getitem__ interaction we need to  define __iter__. See 18979 for details.
I am large, I contain multitudes.
The name refers to module.
callable returning a namespace hint
No namespace hint - use manually provided namespace
Make sure we can iterate through the patterns (without this, some  testcases will break).  Test if the LocaleRegexURLResolver is used within the include;  this should throw an error since this is not allowed!
For include(...) processing.
No-op if not in debug mode or an non-local prefix
Hardcode the class name as otherwise it yields 'Settings'.
update this dict from global settings (but only for ALL_CAPS settings)
store the settings module in case someone later cares
When we can, attempt to validate the timezone. If we can't find  this file, no check happens and it's harmless.  Move the time zone info into os.environ. See ticket 2315 for why  we don't do this unconditionally (breaks Windows).
SETTINGS_MODULE doesn't make much sense in the manually configured  (standalone) case.
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
'%d. %b %Y', '%d %b %Y',             '25. okt 2006', '25 okt 2006'  '%d. %b. %Y', '%d %b. %Y',           '25. okt. 2006', '25 okt. 2006'  '%d. %B %Y', '%d %B %Y',             '25. oktober 2006', '25 oktober 2006'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d. %B %Y', '%d. %b. %Y',   '25. October 2006', '25. Oct. 2006'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d. %b %y.', '%d. %B %y.',      '25. Oct 06.', '25. October 06.'  '%d. %b \'%y.', '%d. %B \'%y.',  '25. Oct '06.', '25. October '06.'  '%d. %b %Y.', '%d. %B %Y.',      '25. Oct 2006.', '25. October 2006.'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%b %d %Y', '%b %d, %Y',           'Oct 25 2006', 'Oct 25, 2006'  '%d %b %Y', '%d %b, %Y',           '25 Oct 2006', '25 Oct, 2006'  '%B %d %Y', '%B %d, %Y',           'October 25 2006', 'October 25, 2006'  '%d %B %Y', '%d %B, %Y',           '25 October 2006', '25 October, 2006'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
'%b %d %Y', '%b %d, %Y',             'Oct 25 2006', 'Oct 25, 2006'  '%d %b %Y', '%d %b, %Y',             '25 Oct 2006', '25 Oct, 2006'  '%B %d %Y', '%B %d, %Y',             'October 25 2006', 'October 25, 2006'  '%d %B %Y', '%d %B, %Y',             '25 October 2006', '25 October, 2006'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d. %b %y.', '%d. %B %y.',      '25. Oct 06.', '25. October 06.'  '%d. %b \'%y.', '%d. %B \'%y.',  '25. Oct '06.', '25. October '06.'  '%d. %b %Y.', '%d. %B %Y.',      '25. Oct 2006.', '25. October 2006.'
-*- encoding: utf-8 -*-
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d. %B %Y', '%d. %b. %Y',   '25. October 2006', '25. Oct. 2006'
these are the separators for non-monetary numbers. For monetary numbers,  the DECIMAL_SEPARATOR is a . (decimal point) and the THOUSAND_SEPARATOR is a  ' (single quote).  For details, please refer to http://www.bk.admin.ch/dokumentation/sprachen/04915/05016/index.html?lang=de  (in German) and the documentation
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  MONTH_DAY_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
'%d %b %Y', '%d %b, %Y', '%d %b. %Y',    '25 Oct 2006', '25 Oct, 2006', '25 Oct. 2006'  '%d %B %Y', '%d %B, %Y',                 '25 October 2006', '25 October, 2006'  '%d.%m.%Y', '%d.%m.%y',                  '25.10.2006', '25.10.06'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  MONTH_DAY_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
'%b %d %Y', '%b %d, %Y',             'Oct 25 2006', 'Oct 25, 2006'  '%d %b %Y', '%d %b, %Y',             '25 Oct 2006', '25 Oct, 2006'  '%B %d %Y', '%B %d, %Y',             'October 25 2006', 'October 25, 2006'  '%d %B %Y', '%d %B, %Y',             '25 October 2006', '25 October, 2006'
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d %B %Y', '%d %b %Y',  '25 octobre 2006', '25 oct. 2006'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  SHORT_DATETIME_FORMAT =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  YEAR_MONTH_FORMAT =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%b %d %Y', '%b %d, %Y',           'Oct 25 2006', 'Oct 25, 2006'  '%d %b %Y', '%d %b, %Y',           '25 Oct 2006', '25 Oct, 2006'  '%B %d %Y', '%B %d, %Y',           'October 25 2006', 'October 25, 2006'  '%d %B %Y', '%d %B, %Y',           '25 October 2006', '25 October, 2006'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d. %B %Y', '%d. %b. %Y',   '25. October 2006', '25. Oct. 2006'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d %b %Y', '%d %b %y',            '20 jan 2009', '20 jan 09'  '%d %B %Y', '%d %B %y',            '20 januari 2009', '20 januari 09'  Kept ISO formats as one is in first position
With time in %H:%M:%S :  '20-01-2009 15:23:35', '20-01-09 15:23:35', '2009-01-20 15:23:35'  '20/01/2009 15:23:35', '20/01/09 15:23:35', '2009/01/20 15:23:35'  '%d %b %Y %H:%M:%S', '%d %b %y %H:%M:%S',    '20 jan 2009 15:23:35', '20 jan 09 15:23:35'  '%d %B %Y %H:%M:%S', '%d %B %y %H:%M:%S',    '20 januari 2009 15:23:35', '20 januari 2009 15:23:35'  With time in %H:%M:%S.%f :  '20-01-2009 15:23:35.000200', '20-01-09 15:23:35.000200', '2009-01-20 15:23:35.000200'  '20/01/2009 15:23:35.000200', '20/01/09 15:23:35.000200', '2009/01/20 15:23:35.000200'  With time in %H.%M:%S :
'%d %b %Y', '%d %b %y',                      '20 jan 2009', '20 jan 09'  '%d %B %Y', '%d %B %y',                      '20 januari 2009', '20 januari 2009'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
'%d. %b %Y', '%d %b %Y',             '25. okt 2006', '25 okt 2006'  '%d. %b. %Y', '%d %b. %Y',           '25. okt. 2006', '25 okt. 2006'  '%d. %B %Y', '%d %B %Y',             '25. oktober 2006', '25 oktober 2006'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d. %B %Y', '%d. %b. %Y',   '25. October 2006', '25. Oct. 2006'  Kept ISO formats as one is in first position
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d %B %Y', '%d %b. %Y',   '25 Ekim 2006', '25 Eki. 2006'
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  YEAR_MONTH_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  DATETIME_FORMAT =  YEAR_MONTH_FORMAT =  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
'%d de %b de %Y', '%d de %b, %Y',    '25 de Out de 2006', '25 Out, 2006'  '%d de %B de %Y', '%d de %B, %Y',    '25 de Outubro de 2006', '25 de Outubro, 2006'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  Kept ISO formats as they are in first position
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d de %b de %Y', '%d de %b, %Y',    '25 de Out de 2006', '25 Out, 2006'  '%d de %B de %Y', '%d de %B, %Y',    '25 de Outubro de 2006', '25 de Outubro, 2006'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
'31/12/2009', '31/12/09'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  '31/12/2009', '31/12/09'
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior
'%d. %B %Y', '%d. %b. %Y',   '25. October 2006', '25. Oct. 2006'
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  SHORT_DATETIME_FORMAT =  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
-*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  '31/12/2009', '31/12/09'
NUMBER_GROUPING =  -*- encoding: utf-8 -*-  This file is distributed under the same license as the Django package.
The *_FORMAT strings use the Django date format syntax,  see http://docs.djangoproject.com/en/dev/ref/templates/builtins/date  FIRST_DAY_OF_WEEK =
The *_INPUT_FORMATS strings use the Python strftime format syntax,  see http://docs.python.org/library/datetime.htmlstrftime-strptime-behavior  DATE_INPUT_FORMATS =  TIME_INPUT_FORMATS =  DATETIME_INPUT_FORMATS =  NUMBER_GROUPING =
This is defined here as a do-nothing function because we can't import  django.utils.translation -- that module depends on the settings.
Whether the framework should propagate raw exceptions rather than catching  them. This is useful under some testing situations and should never be used  on a live site.
Whether to use the "Etag" header. This saves bandwidth but slows down performance.
People who get code error notifications.  In the format [('Full Name', 'email@example.com'), ('Full Name', 'anotheremail@example.com')]
List of IP addresses, as strings, that:    * See debug comments, when DEBUG is true    * Receive x-headers
Hosts/domain names that are valid for this site.  "*" matches anything, ".example.com" matches example.com and all subdomains
Local time zone for this installation. All choices can be found here:  https://en.wikipedia.org/wiki/List_of_tz_zones_by_name (although not all  systems may support all possibilities). When USE_TZ is True, this is  interpreted as the default user time zone.
If you set this to True, Django will use timezone-aware datetimes.
Language code for this installation. All choices can be found here:  http://www.i18nguy.com/unicode/language-identifiers.html
Languages we provide translations for, out of the box.
Languages using BiDi (right-to-left) layout
If you set this to False, Django will make some optimizations so as not  to load the internationalization machinery.
Settings for language cookie
If you set this to True, Django will format dates, numbers and calendars  according to user current locale.
Not-necessarily-technical managers of the site. They get broken link  notifications and other various emails.
Default content type and charset to use for all HttpResponse objects, if a  MIME type isn't manually specified. These are used to construct the  Content-Type header.
Encoding of files read from disk (template and initial SQL files).
Email address that error messages come from.
Database connection info. If left empty, will default to the dummy backend.
Classes used to implement DB routing behavior.
The email backend to use. For possible shortcuts see django.core.mail.  The default is to use the SMTP backend.  Third-party backends can be specified by providing a Python path  to a module that defines an EmailBackend class.
Host for sending email.
Port for sending email.
Optional SMTP authentication information for EMAIL_HOST.
List of strings representing installed apps.
Default email address to use for various automated correspondence from  the site managers.
Subject-line prefix for email messages send with django.core.mail.mail_admins  or ...mail_managers.  Make sure to include the trailing space.
Whether to append trailing slashes to URLs.
Whether to prepend the "www." subdomain to URLs that don't have it.
Override the server-derived value of SCRIPT_NAME
List of compiled regular expression objects representing User-Agent strings  that are not allowed to visit any page, systemwide. Use this for bad  robots/crawlers. Here are a few examples:      import re      DISALLOWED_USER_AGENTS = [          re.compile(r'^NaverBot.*'),          re.compile(r'^EmailSiphon.*'),          re.compile(r'^SiteSucker.*'),          re.compile(r'^sohu-search')      ]
List of compiled regular expression objects representing URLs that need not  be reported by BrokenLinkEmailsMiddleware. Here are a few examples:     import re     IGNORABLE_404_URLS = [         re.compile(r'^/apple-touch-icon.*\.png$'),         re.compile(r'^/favicon.ico$),         re.compile(r'^/robots.txt$),         re.compile(r'^/phpmyadmin/),         re.compile(r'\.(cgi|php|pl)$'),     ]
A secret key for this particular Django installation. Used in secret-key  hashing algorithms. Set this in your settings, or Django will complain  loudly.
Default file storage mechanism that holds media.
Absolute filesystem path to the directory that will hold user-uploaded files.  Example: "/var/www/example.com/media/"
URL that handles the media served from MEDIA_ROOT.  Examples: "http://example.com/media/", "http://media.example.com/"
Absolute path to the directory static files should be collected to.  Example: "/var/www/example.com/static/"
URL that handles the static files served from STATIC_ROOT.  Example: "http://example.com/static/", "http://static.example.com/"
List of upload handler classes to be applied in order.
Maximum number of GET/POST parameters that will be read before a  SuspiciousOperation (TooManyFieldsSent) is raised.
Directory in which upload streamed files will be temporarily saved. A value of  `None` will make Django use the operating system's default temporary directory  (i.e. "/tmp" on *nix systems).
The numeric mode to set newly-uploaded files to. The value should be a mode  you'd pass directly to os.chmod; see http://docs.python.org/lib/os-file-dir.html.
The numeric mode to assign to newly-created directories, when uploading files.  The value should be a mode as you'd pass to os.chmod;  see http://docs.python.org/lib/os-file-dir.html.
Python module path where user will place custom format definition.  The directory where this setting is pointing should contain subdirectories  named as the locales, containing a formats.py file  (i.e. "myproject.locale" for myproject/locale/en/formats.py etc. use)
Default formatting for date objects. See all available format strings here:  http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
Default formatting for datetime objects. See all available format strings here:  http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
Default formatting for time objects. See all available format strings here:  http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
Default formatting for date objects when only the year and month are relevant.  See all available format strings here:  http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
Default formatting for date objects when only the month and day are relevant.  See all available format strings here:  http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
Default short formatting for date objects. See all available format strings here:  http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
Default short formatting for datetime objects.  See all available format strings here:  http://docs.djangoproject.com/en/dev/ref/templates/builtins/date
Default formats to be used when parsing dates from input boxes, in order  See all available format string here:  http://docs.python.org/library/datetime.htmlstrftime-behavior  * Note that these format strings are different from the ones to display dates
Default formats to be used when parsing times from input boxes, in order  See all available format string here:  http://docs.python.org/library/datetime.htmlstrftime-behavior  * Note that these format strings are different from the ones to display dates
Default formats to be used when parsing dates and times from input boxes,  in order  See all available format string here:  http://docs.python.org/library/datetime.htmlstrftime-behavior  * Note that these format strings are different from the ones to display dates
First day of week, to be used on calendars  0 means Sunday, 1 means Monday...
Decimal separator symbol
Boolean that sets whether to add thousand separator when formatting numbers
Number of digits that will be together, when splitting them by  THOUSAND_SEPARATOR. 0 means no grouping, 3 means splitting by thousands...
Thousand separator symbol
The tablespaces to use for each model when not specified otherwise.
Default X-Frame-Options header value
The Python dotted path to the WSGI application that Django's internal server  (runserver) will use. If `None`, the return value of  'django.core.wsgi.get_wsgi_application' is used, thus preserving the same  behavior as previous versions of Django. Otherwise this should point to an  actual WSGI application object.
If your Django app is behind a proxy that sets a header to specify secure  connections, AND that proxy ensures that user-submitted headers with the  same name are ignored (so that people can't spoof it), set this value to  a tuple of (header_name, header_value). For any requests that come in with  that header/value, request.is_secure() will return True.  WARNING! Only set this if you fully understand what you're doing. Otherwise,  you may be opening yourself up to a security risk.
List of middleware to use. Order is important; in the request phase, these  middleware will be applied in the order given, and in the response  phase the middleware will be applied in reverse order.
Cache to store session data if using the cache session backend.  Cookie name. This can be whatever you want.  Age of cookie, in seconds (default: 2 weeks).  A string like ".example.com", or None for standard domain cookie.  Whether the session cookie should be secure (https:// only).  The path of the session cookie.  Whether to use the non-RFC standard httpOnly flag (IE, FF3+, others)  Whether to save the session data on every request.  Whether a user's session cookie expires when the Web browser is closed.  The module to store session data  Directory to store session files if using the file session module. If None,  the backend will use a sensible default.  class to serialize session data
The cache backends to use.
The number of days a password reset link is valid for
the first hasher in this list is the preferred algorithm.  any  password using different algorithms will be converted automatically  upon login
Dotted path to callable to be used as view when a request is  rejected by the CSRF middleware.
Settings for CSRF cookie.
Class to use as messages backend
The callable to use to configure logging
Custom logging configuration.
Default exception reporter filter class used in case none has been  specifically assigned to the HttpRequest instance.
The name of the class to use to run the test suite
Apps that don't need to be serialized at test database creation time  (only apps with migrations are to start with)
The list of directories to search for fixtures
A list of locations of additional static files
The default file storage backend used during the build process
List of finder classes that know how to find static files in  various locations.  'django.contrib.staticfiles.finders.DefaultStorageFinder',
Migration module overrides for apps, by app label.
List of all issues generated by system checks that should be silenced. Light  issues like warnings, infos or debugs will not generate a message. Silencing  serious issues like errors and criticals does not result in hiding the  message, but Django will not stop you from e.g. running server.
SECURITY MIDDLEWARE
If the default template doesn't exist, use the string template.
Raise if a developer-specified template doesn't exist.
Try to get an "interesting" exception message, if any (and not the ugly  Resolver404 dictionary)
Reraise if it's a missing custom template.
Reraise if it's a missing custom template.
Reraise if it's a missing custom template.
No exception content is passed to the template, to not disclose any sensitive information.
Reraise if it's a missing custom template.
We need this to behave just like the CsrfViewMiddleware, but not reject  requests or log warnings.
Forces process_response to send the cookie
We could just do view_func.csrf_exempt = True, but decorators  are nicer if they don't have side-effects, so we return a new  function.
We also add some asserts to give better error messages in case people are  using other ways to call cache_page that no longer work.
Compute values (if any) for the requested resource.
Set relevant headers on the response if they don't already exist.
Shortcut decorators for common cases based on ETag or Last-Modified only
When pagination is enabled and object_list is a queryset,  it's better to do a cheap query than to load the unpaginated  queryset in memory.
If template_name isn't specified, it's not a problem --  we just start with an empty list.
If the list is a queryset, we'll invent a template name based on the  app and model name. This name gets put at the end of the template  name list so that user-supplied names override the automatically-  generated ones.
Forcing possible reverse_lazy evaluation
If a model has been explicitly provided, use it
If this view is operating on a single object, use  the class of that object
Try to get a queryset and extract the model class  from that
PUT is a valid HTTP verb for creating (with a known URL) or editing an  object, note that browsers only support POST for now.
Add support for browsers which only accept GET and POST for now.
Skip self._make_date_lookup_arg, it's a no-op in this branch.
When pagination is enabled, it's better to do a cheap query  than to load the unpaginated queryset in memory.
We need this to be a queryset since parent classes introspect it  to find information about the model.
Use a custom queryset if provided
Filter down a queryset from self.queryset using the date from the  URL. This'll get passed as the queryset to DetailView.get_object,  which'll handle the 404
Bounds of the current interval
If allow_empty is True, the naive result will be valid
Otherwise, we'll need to go to the database to look for an object  whose date_field is at least (greater than/less than) the given  naive result  Construct a lookup and an ordering depending on whether we're doing  a previous date or a next date lookup.
Filter out objects in the future if appropriate.  Fortunately, to match the implementation of allow_future,  we need __lte, which doesn't conflict with __lt above.
Snag the first object from the queryset; if it doesn't exist that  means there's no next/previous link available.
Convert datetimes to dates in the current time zone.
Return the first day of the period.
Go through keyword arguments, and either save their values to our  instance, or raise an error.
take name and docstring from class
and possible attributes set by decorators  like csrf_exempt from dispatch
Try to dispatch to the right method; if a method doesn't exist,  defer to the error handler. Also defer to the error handler if the  request method isn't on the approved list.
Use a custom queryset if provided; this is required for subclasses  like DateDetailView
Next, try looking up by primary key.
Next, try looking up by slug.
If none of those are defined, it's an error.
Get the single item from the filtered queryset
If template_name isn't specified, it's not a problem --  we just start with an empty list.
If self.template_name_field is set, grab the value of the field  of that name from the object; this is the most specific template  name, if given.
The least-specific option is the default <app>/<model>_detail.html;  only use this if the object in question is a model.
If we still haven't managed to find any template names, we should  re-raise the ImproperlyConfigured to alert the user.
paths of requested packages
this should actually be a compiled function of a typical plural-form:  Plural-Forms: nplurals=3; plural=n%10==1 && n%100!=11 ? 0 :                n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2;
Translations catalog  Language formats for date, time, etc.
If packages are not provided, default to all installed packages, as  DjangoTranslation without localedirs harvests them all.
paths of requested packages
This should be a compiled function of a typical plural-form:  Plural-Forms: nplurals=3; plural=n%10==1 && n%100!=11 ? 0 :                n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2;
Translations catalog  Language formats for date, time, etc.
Strip empty path components.
Strip '.' and '..' in path.
Respect the If-Modified-Since header.
Minimal Django templates engine to render the error templates  regardless of the project's TEMPLATES setting.
If the key isn't regex-able, just return as-is.
For fixing 21345 and 23070
Instantiate the default filter for the first time and cache it.
Cleanse all parameters.
Cleanse only the specified parameters.
If value is lazy or a complex object of another kind, this check  might raise an exception. isinstance checks that lazy  MultiValueDicts will have a return value.
Cleanse MultiValueDicts (request.POST is the one we usually care about)
Loop through the frame's callers to see if the sensitive_variables  decorator was used.  The sensitive_variables decorator was used, so we take note  of the sensitive variables' names.
Cleanse all variables
Cleanse specified variables
Potentially cleanse the request and any MultiValueDicts if they  are one of the frame variables.
For good measure, obfuscate the decorated function's arguments in  the sensitive_variables decorator's frame, in case the variables  associated with those arguments were meant to be obfuscated from  the decorated function's frame.
Handle deprecated string exceptions
The force_escape filter assume unicode, make sure that works
Trim large blobs of data
Check whether exception info is available
If we just read the source from a file, or if the loader did not  apply tokenize.detect_encoding to decode the source into a Unicode  string, then we should do that ourselves.  File coding may be specified. Match pattern from PEP-263  (http://www.python.org/dev/peps/pep-0263/)
Get the exception and all its causes
No exceptions were supplied to ExceptionReporter
In case there's just one exception (always in Python 2,  sometimes in Python 3), take the traceback from self.tb (Python 2  doesn't have a __traceback__ attribute on Exception)
Support for __traceback_hide__ which is used by a few libraries  to hide internal frames.
If the traceback for current exception is consumed, try the  other exception.
installed_apps is set to None when creating the master registry  because it cannot be populated at that point. Other registries must  provide a list of installed apps and are populated immediately.
Mapping of app labels => model names => model classes. Every time a  model is imported, ModelBase.__new__ calls apps.register_model which  creates an entry in all_models. All imported models are registered,  regardless of whether they're defined in an installed application  and whether the registry has been populated. Since it isn't possible  to reimport a module safely (it could reexecute initialization code)  all_models is never overridden or reset.
Mapping of labels to AppConfig instances for installed apps.
Stack of app_configs. Used to store the current state in  set_available_apps and set_installed_apps.
Whether the registry is populated.
Lock for thread-safe population.
Maps ("app_label", "modelname") tuples to lists of functions to be  called when the corresponding model is ready. Used by this class's  `lazy_model_operation()` and `do_pending_operations()` methods.
Populate apps and models, unless it's the master registry.
populate() might be called by two threads in parallel on servers  that create threads before initializing the WSGI callable.
app_config should be pristine, otherwise the code below won't  guarantee that the order matches the order in INSTALLED_APPS.
Load app configs and app modules.
Check for duplicate app names.
Load models.
Since this method is called when models are imported, it cannot  perform imports because of the risk of import loops. It mustn't  call get_app_config().
Is this model swapped out for the model given by to_string?  Is this model swappable and the one given by to_string?
Call expire cache on each model. This will purge  the relation tree and the fields cache.  Circumvent self.get_models() to prevent that the cache is refilled.  This particularly prevents that an empty value is cached while cloning.
Base case: no arguments, just execute the function.  Recursive case: take the head of model_keys, wait for the  corresponding model class to be imported and registered, then apply  that argument to the supplied function. Pass the resulting partial  to lazy_model_operation() along with the remaining model args and  repeat until all models are loaded and all arguments are applied.
This will be executed after the class corresponding to next_model  has been imported and registered. The `func` attribute provides  duck-type compatibility with partials.
If the model has already been imported and registered, partially  apply it to the function now. If not, add it to the list of  pending operations for the model, where it will be executed with  the model class as its sole argument once the model is ready.
Full Python path to the application eg. 'django.contrib.admin'.
Root module for the application eg. <module 'django.contrib.admin'  from 'django/contrib/admin/__init__.pyc'>.
Last component of the Python path to the application eg. 'admin'.  This value must be unique across a Django project.
Human-readable name for the application eg. "Admin".
Filesystem path to the application directory eg.  u'/usr/lib/python2.7/dist-packages/django/contrib/admin'. Unicode on  Python 2 and a str on Python 3.
Module containing models eg. <module 'django.contrib.admin.models'  from 'django/contrib/admin/models.pyc'>. Set by import_models().  None if the application doesn't have a models module.
Mapping of lower case model names to model classes. Initially set to  None to prevent accidental access before import_models() runs.
See 21874 for extended discussion of the behavior of this method in  various cases.  Convert paths to list because Python 3's _NamespacePath does not  support indexing.  For unknown reasons, sometimes the list returned by __path__  contains duplicates that must be removed (25246).
If import_module succeeds, entry is a path to an app module,  which may specify an app config class with default_app_config.  Otherwise, entry is a path to an app config class or an error.
Track that importing as an app module failed. If importing as an  app config class fails too, we'll trigger the ImportError again.
Raise the original exception when entry cannot be a path to an  app config class.
If this works, the app module specifies an app config class.
Otherwise, it simply uses the default app config class.
If we're reaching this point, we must attempt to load the app config  class located at <mod_path>.<cls_name>  If importing as an app module failed, that error probably  contains the most informative traceback. Trigger it again.
Check for obvious errors. (This check prevents duck typing, but  it could be removed if it became a problem in practice.)
Obtain app name here rather than in AppClass.__init__ to keep  all error checking for entries in INSTALLED_APPS in one place.
Ensure app_name points to a valid module.
Entry is a path to an app config class.
Dictionary of models for this app, primarily maintained in the  'all_models' attribute of the Apps this AppConfig is attached to.  Injected as a parameter because it gets populated when models are  imported, which might happen before populate() imports models.
Allow only ASCII alphanumerics  Older Django versions set cookies to values of CSRF_SECRET_LENGTH  alphanumeric characters. For backwards compatibility, accept  such values as unsalted secrets.  It's easier to salt here and be consistent later, rather than add  different code paths in the checks, although that might be a tad more  efficient.
Assume both arguments are sanitized -- that is, strings of  length CSRF_TOKEN_LENGTH, all CSRF_ALLOWED_CHARS.
The _accept and _reject methods currently only exist for the sake of the  requires_csrf_token decorator.  Avoid checking the request twice by adding a custom attribute to  request.  This will be relevant when both decorator and middleware  are used.
Cookie token needed to be replaced;  the cookie needs to be reset.  Use same token next time.
Wait until request.META["CSRF_COOKIE"] has been manipulated before  bailing out, so that get_token still works
Assume that anything not defined as 'safe' by RFC7231 needs protection  Mechanism to turn off CSRF checks for test suite.  It comes after the creation of CSRF cookies, so that  everything else continues to work exactly the same  (e.g. cookies are sent, etc.), but before any  branches that call reject().
Suppose user visits http://example.com/  An active network attacker (man-in-the-middle, MITM) sends a  POST form that targets https://example.com/detonate-bomb/ and  submits it via JavaScript.  The attacker will need to provide a CSRF cookie and token, but  that's no problem for a MITM and the session-independent  secret we're using. So the MITM can circumvent the CSRF  protection. This is true for any HTTP connection, but anyone  using HTTPS expects better! For this reason, for  https://example.com/ we need additional protection that treats  http://example.com/ as completely untrusted. Under HTTPS,  Barth et al. found that the Referer header is missing for  same-domain requests in only about 0.2% of cases or less, so  we can use strict Referer checking.
Make sure we have a valid URL for Referer.
Ensure that our Referer is also secure.
If there isn't a CSRF_COOKIE_DOMAIN, assume we need an exact  match on host:port. If not, obey the cookie rules.  request.get_host() includes the port.
Here we generate a list of all acceptable HTTP referers,  including the current host since that has been validated  upstream.
No CSRF cookie. For POST requests, we insist on a CSRF cookie,  and in this way we can avoid all CSRF attacks, including login  CSRF.
Check non-cookie token for match.  Handle a broken connection before we've completed reading  the POST data. process_view shouldn't raise any  exceptions, so we'll ignore and serve the user a 403  (assuming they're still listening, which they probably  aren't because of the error).
Fall back to X-CSRFToken, to make things easier for AJAX,  and possible for PUT/DELETE.
Set the CSRF cookie even if it's already set, so we renew  the expiry timer.  Content varies with the CSRF cookie, so set the Vary header.
The request logger receives events for any problematic request  The security logger receives events for all SuspiciousOperations
Allow sys.exit() to actually exit. See tickets 1023 and 4701
Get the exception info now, in case another exception is thrown later.
We don't need to update the cache, just return.
Don't cache responses that set a user-specific (and maybe security  sensitive) cookie in response to a cookie-less request.
Try to get the timeout from the "max-age" section of the "Cache-  Control" header before reverting to using the default cache_timeout  length.  max-age was set to 0, don't bother caching.
try and get the cached GET response
if it wasn't found and we are looking for a HEAD, try looking just for that
hit, return cached response
Don't set it if it's already in the response
Don't set it if they used @xframe_options_exempt
This override makes get_response optional during the  MIDDLEWARE_CLASSES deprecation.
Insert language after the script prefix and before the  rest of the URL
Check for denied User-Agents
Check for a redirect based on settings.PREPEND_WWW
Check if a slash should be appended
Return a redirect if necessary
If the given URL is "Not Found", then check if we should redirect to  a path with a slash appended.
Different subdomains are treated as different domains.
The referer is empty.
APPEND_SLASH is enabled and the referer is equal to the current URL  without a trailing slash indicating an internal redirect.
A '?' in referer is identified as a search engine source.
The referer is equal to the current URL, ignoring the scheme (assumed  to be a poorly implemented bot).
It's not worth attempting to compress really short responses.
Avoid gzipping if we've already got a content-encoding.
Delete the `Content-Length` header for streaming content, because  we won't know the compressed size until we stream it.
Return the compressed content only if it's actually shorter.
-*- coding: utf-8 -*-
A utf-8 verbose name (Ångström's Articles) to test they are valid.
Test models with non-default primary keys / AutoFields 5218
Intentionally broken (invalid start byte in byte string).
Chained foreign keys with to_field produce incorrect query 18432
this line will raise an AttributeError without the accompanying fix
NOTE: Part of the regression test here is merely parsing the model  declaration. The verbose_name, in particular, did not always work.  An empty choice field should return None for the display name.
Empty strings should be returned as Unicode
TextFields can hold more than 4000 characters (this was broken in  Oracle).
TextFields can hold more than 4000 bytes also when they are  less than 4000 characters
Regression test for 659  Regression test for 8510
Regression test for 18969
Date filtering was failing with NULL date values in SQLite  (regression test for 3501, among other things).
Check that get_next_by_FIELD and get_previous_by_FIELD don't crash  when we have usecs values stored on the database  It crashed after the Field.get_db_prep_* refactor, because on most  backends DateTimeFields supports usecs, but DateTimeField.to_python  didn't recognize them. (Note that  Model._get_next_or_previous_by_FIELD coerces values to strings)
Check Department and Worker (non-default PK type)
Models with broken unicode methods should still have a printable repr
Saving an updating with timezone-aware datetime Python objects.  Regression test for 10443.  The idea is that all these creations and saving should work without  crashing. It's not rocket science.
this is the actual test for 18432
Queryset to match most recent revision:
Queryset to search for string in title:
Following queryset should return the most recent revision:
Extra select parameters should stay tied to their corresponding  select portions. Applies when portions are updated or otherwise  moved around.
Extra clauses after an empty values clause are still included
Extra columns are ignored if not mentioned in the values() clause
Extra columns after a non-empty values() clause are ignored
Extra columns can be partially returned
Also works if only extra columns are included
Values list works the same way  All columns are returned for an empty values_list()
Extra columns after an empty values_list() are still included
Extra columns ignored completely if not mentioned in values_list()
Extra columns after a non-empty values_list() clause are ignored completely
Only the extra columns specified in the values_list() are returned
...also works if only extra columns are included
... and values are returned in the order they are specified
Test Case 1: should appear in queryset.  Test Case 2: should appear in queryset.  Test Case 3: should not appear in queryset, bug case.  Test Case 4: should not appear in queryset.  Test Case 5: should not appear in queryset.  Test Case 6: should not appear in queryset, bug case.
Note: the extra ordering must appear in select clause, so we get two  non-distinct results here (this is on purpose, see 7070).
Also try to remove the compiled file; if it exists, it could  mess up later tests that depend upon the .py file not existing  Jython produces module$py.class files  CPython produces module.pyc files  Also remove a __pycache__ directory, if it exists
The base dir for Django's tests is one level up.  The base dir for Django is one level above the test dir. We don't use  `import django` to figure that out, so we don't pick up a Django  from site-packages or similar.
Define a temporary environment for the subprocess
Set the test environment  Use native strings for better compatibility
Move to the test directory and run  Move back to the old working directory
Backup original state
Original state hasn't changed
Variable was correctly set
Restore original state
Only 4 characters, all of which could be in an ipv6 address
Uses only characters that could be in an ipv6 address
Check a warning is emitted
It's possible one outputs on stderr and the other on stdout, hence the set
Default palette has color.
If the Exception is not CommandError it should always  raise the original exception.
If the Exception is CommandError and --traceback is not present  this command should raise a SystemExit and don't print any  traceback to the stderr.
If the Exception is CommandError and --traceback is present  this command should raise the original CommandError as if it  were not a CommandError.
Test connections have been closed
running again..
running again..
We're using a custom command so we need the alternate settings
-*- coding: utf-8 -*-
Regression for 13368. This is an example of a model  that imports a class that has an abstract base class.
-*- coding: utf-8 -*-
Accessing "name" doesn't trigger a new database query. Accessing  "value" or "text" should.
Regression test for 10695. Make sure different instances don't  inadvertently share data in the deferred descriptor objects.
Some further checks for select_related() and inherited model  behavior (regression for 10710).
Models instances with deferred fields should still return the same  content types as their non-deferred versions (bug 10738).
Regression for 10733 - only() can be used on a model with two  foreign keys.
Regression for 16409 - make sure defer() and only() work with annotate()
Test for 12163 - Pickling error saving session with unsaved model  instances.
Regression for 16409 - make sure defer() and only() work with annotate()
Regression for 15790 - only() broken for proxy models
also test things with .defer()
Refs 14694. Test reverse relations which are known unique (reverse  side has o2ofield or unique FK) - the o2o case  Make sure that `only()` doesn't break when we pass in a unique relation,  rather than a field on the relation.
Regression for 22050  Defer fields with only()
Test for 17485.
use a non-default name for the default manager
For testing when upper case letter in app name; regression for 4057
Models to regression test 7572, 20820
Subclass of a model with a ManyToManyField for test_ticket_20820
Models to regression test 22421
Models to regression test 11428
Check for forward references in FKs and M2Ms with natural keys
Person doesn't actually have a dependency on store, but we need to define  one to test the behavior of the dependency resolution algorithm.
ome models with pathological circular dependencies
Model for regression test of 11101
Fake the dependency for a circularity
-*- coding: utf-8 -*-  Unittests for fixtures.
Load a fixture that uses PK=1
Load a pretty-printed XML fixture with Nulls.
Load a pretty-printed XML fixture with Nulls.
Just for good measure, run the same query again.  Under the influence of ticket 7572, this will  give a different result to the previous call.
Output order isn't guaranteed, so check for parts
Get rid of artifacts like '000000002' to eliminate the differences  between different Python versions.
Order between M2MComplexA and M2MComplexB doesn't matter. The through  model has dependencies to them though, so it should come last.
The dependency sorting should not result in an error, and the  through model should have dependencies to the other models and as  such come last in the list.
relative_prefix is something like tests/fixtures_regress or  fixtures_regress depending on how runtests.py is invoked.  All path separators must be / in order to be a proper regression test on  Windows, so replace as appropriate.
Since this package contains a "jinja2" directory, this is required to  silence an ImportWarning warning on Python 2.
self will be overridden to be a TemplateReference, so the self  variable will not come through. Attempting to use one though should  not throw an error.
See ticket 23789.
Check that context processors run
Check that context overrides context processors
libraries are discovered from installed applications  libraries are discovered from django.templatetags  libraries passed in OPTIONS are registered  libraries passed in OPTIONS take precedence over discovered ones
There's no way to trigger a syntax error with the dummy backend.  The test still lives here to factor it between other backends.
Incorrect: APP_DIRS and loaders are mutually incompatible.
Reloads all deferred fields if any of the fields is deferred.
Using 'pk' with only() should result in 3 deferred fields, namely all  of them except the model's primary key see 15494  You can use 'pk' with reverse foreign key lookups.  The related_id is alawys set even if it's not fetched from the DB,  so pk and related_id are not deferred.
User values() won't defer anything (you get the full list of  dictionaries back), but it still works.
Using defer() and only() with get() is also valid.
select_related() overrides defer().
Saving models with deferred fields is possible (but inefficient,  since every field has to be retrieved first).
Regression for 10572 - A subclass with no extra fields can defer  fields from the base class  You can defer a field on a baseclass when the subclass has no fields
You can retrieve a single column on a base class with no fields  on an inherited model, its PK is also fetched, hence '3' deferred fields.
You can defer a field on a baseclass
You can defer a field on a subclass
You can retrieve a single field on a baseclass  when inherited model, its PK is also fetched, hence '4' deferred fields.
You can retrieve a single field on a subclass
Customized refresh_from_db() reloads all deferred fields on  access of any of them.
one successful savepoint  one failed savepoint  another successful savepoint
only hooks registered during successful savepoints execute
Each item is two tuples:      First tuple is Paginator parameters - object_list, per_page,          orphans, and allow_empty_first_page.      Second tuple is resulting Paginator attributes - count,          num_pages, and page_range.  Ten items, varying orphans, no empty first page.  Ten items, varying orphans, allow empty first page.  One item, varying orphans, no empty first page.  One item, varying orphans, allow empty first page.  Zero items, varying orphans, no empty first page.  Zero items, varying orphans, allow empty first page.  Number if items one less than per_page.  Number if items equal to per_page.  Number if items one more than per_page.  Number if items one more than per_page with one orphan.  Non-integer inputs
With no content and allow_empty_first_page=True, 1 is a valid page number
Paginator can be passed other objects with a count() method.
Paginator can be passed other objects that implement __len__.
Each item is three tuples:      First tuple is Paginator parameters - object_list, per_page,          orphans, and allow_empty_first_page.      Second tuple is the start and end indexes of the first page.      Third tuple is the start and end indexes of the last page.  Ten items, varying per_page, no orphans.  Ten items, varying per_page, with orphans.  One item, varying orphans, no empty first page.  One item, varying orphans, allow empty first page.  Zero items, varying orphans, allow empty first page.
When no items and no empty first page, we should get EmptyPage error.
Prepare a list of objects for pagination.
Make sure object_list queryset is not evaluated by an invalid __getitem__ call.  (this happens from the template engine when using eg: {% page_obj.has_previous %})
Make sure slicing the Page object with numbers and slice objects work.  After __getitem__ is called, object_list is a list
disconnect all signal handlers
adding a default part to our car - no signal listener installed
give the BMW and Toyota some doors as well
remove the engine from the self.vw and the airbag (which is not set  but is returned)
give the self.vw some optional parts (second relation to same model)
add airbag to all the cars (even though the self.vw already has one)
remove airbag from the self.vw (reverse relation with custom  related_name)
clear all parts of the self.vw
take all the doors off of cars
take all the airbags off of cars (clear reverse relation with custom  related_name)
alternative ways of setting relation:
direct assignment clears the set first, then adds
set by clearing.
set by only removing what's necessary.
Check that signals still work when model inheritance is involved
Install a listener on the two m2m relations.
Forward declared intermediate model
using custom id column to test ticket 11107
Membership object defined as a class
A set of models that use an non-abstract inherited model as the 'through' model.
Using to_field on the through model
normal intermediate model
intermediate model with custom id column
We are testing if wrong objects get deleted due to using wrong  field value in m2m queries. So, it is essential that the pk  numberings do not match.  Create one intentionally unused driver to mix up the autonumbering  And two intentionally unused cars.
Low level tests for _add_items and _remove_items. We test these methods  because .add/.remove aren't available for m2m fields with through, but  through is the only way to set to_field currently. We do want to make  sure these methods are ready if the ability to use .add or .remove with  to_field relations is added some day.  Yikes - barney is going to drive...
This field name is intentionally 2 characters long (16080).
Users
Books
Departments
Employees
Make sure the correct queryset is returned
Make sure the correct choice is selected
Make sure the correct queryset is returned  In case one week ago is in the same month.
Make sure the correct choice is selected
Make sure the correct queryset is returned  In case one week ago is in the same year.
Make sure the correct choice is selected
Make sure the correct queryset is returned
Make sure the correct choice is selected
Null/not null queries
Make sure the correct queryset is returned
Make sure the correct choice is selected
Make sure the correct queryset is returned
Make sure the correct choice is selected
Regression for 17830
Make sure the correct queryset is returned
Make sure the last choice is None and is selected
Make sure the correct choice is selected
Make sure that correct filters are returned with custom querysets
Should have 'All', 1999 and 2009 options i.e. the subset of years of  books written by alfred (which is the filtering criteria set by  BookAdminWithCustomQueryset.get_queryset())
Make sure that all users are present in the author's list filter
Make sure the correct queryset is returned
Make sure the last choice is None and is selected
Make sure the correct choice is selected  order of choices depends on User model, which has no order
Make sure that all users are present in the contrib's list filter
Make sure the correct queryset is returned
Make sure the last choice is None and is selected
Make sure the correct choice is selected
FK relationship -----
Make sure the correct queryset is returned
Make sure the last choice is None and is selected
Make sure the correct choice is selected
M2M relationship -----
Make sure the correct queryset is returned
Make sure the last choice is None and is selected
Make sure the correct choice is selected
With one book, the list filter should appear because there is also a  (None) option.  With no books remaining, no list filters should appear.
Make sure that only actual authors are present in author's list filter
Only actual departments should be present in employee__department's  list filter.
Make sure that only actual contributors are present in contrib's list filter
Make sure the correct queryset is returned
Make sure the correct choice is selected
Make sure the correct queryset is returned
Make sure the correct choice is selected
Make sure the correct queryset is returned
Make sure the correct choice is selected
Make sure the correct queryset is returned
Make sure that the first option is 'All' ---------------------------
Make sure the correct queryset is returned
Make sure the correct choice is selected
Look for books in the 1980s ----------------------------------------
Make sure the correct queryset is returned
Make sure the correct choice is selected
Look for books in the 1990s ----------------------------------------
Make sure the correct queryset is returned
Make sure the correct choice is selected
Look for books in the 2000s ----------------------------------------
Make sure the correct queryset is returned
Make sure the correct choice is selected
Combine multiple filters -------------------------------------------
Make sure the correct queryset is returned
Make sure the correct choices are selected
Make sure the correct queryset is returned
When it ends with '__in' -----------------------------------------
Make sure the correct queryset is returned
Make sure the correct choice is selected
When it ends with '__isnull' ---------------------------------------
Make sure the correct queryset is returned
Make sure the correct choice is selected
Make sure the correct queryset is returned
Make sure the correct queryset is returned
Add the URL of our custom 'add_view' view to the front of the URLs  list.  Remove the existing one(s) first
Should get the change_view for model instance with PK 'add', not show  the add_view
Should correctly get the change_view for the model instance with the  funny-looking PK (the one with a 'path/to/html/document.html' value)
Anne is friends with Bill and Chuck
David is friends with Anne and Chuck - add in reverse direction
Who is friends with Anne?  Who is friends with Bill?  Who is friends with Chuck?  Who is friends with David?
Bill is already friends with Anne - add Anne again, but in the  reverse direction
Who is friends with Anne?  Who is friends with Bill?
Remove Anne from Bill's friends
Who is friends with Anne?  Who is friends with Bill?
Clear Anne's group of friends
Who is friends with Anne?
Reverse relationships should also be gone  Who is friends with Chuck?
Who is friends with David?
David is idolized by Anne and Chuck - add in reverse direction
Who are Anne's idols?  Who is stalking Anne?
Ann idolizes David
David is idolized by Anne
Who are Anne's idols?  As the assertQuerysetEqual uses a set for comparison,  check we've only got David listed once
Ann idolizes herself
Who are Anne's idols?  Who is stalking Anne?
The "full_name" property hasn't provided a "set" method.
And cannot be used to initialize the class.
But "full_name_2" has, and it can be used to initialize the class.
default serializable on Python 3 only
To test pickling we need a class that isn't defined on module, but  is still available from app-cache. So, the Container class moves  SomeModel outside of module level
Ticket 17776
Exceptions are not equal to equivalent instances of themselves, so  can't just use assertEqual(original, unpickled)
Also, deferred dynamic model works
With related field (14515)
First pickling
Second pickling
Happening.when has a callable default of datetime.datetime.now.
Translators: This comment should be extracted
This comment should not be extracted
This file has a literal with plural forms. When processed first, makemessages  shouldn't create a .po file with duplicate `Plural-Forms` headers
This string is intentionally duplicated in test.html
-*- encoding: utf-8 -*-
Populate _format_cache with temporary values
It should be possible to compare *_lazy objects.
Using repr() to check translated text and type
On Python 2, (n)gettext_lazy should not transform a bytestring to unicode
Other versions of lazy functions always return unicode
Now with a long
Inexisting context...
Existing context...  Using a literal
Using a variable
Using a filter
Using 'as'
Inexisting context...
Existing context...  Using a literal
Using a variable
Using a filter
Using 'count'
Using 'with'
Using trimmed
Mis-uses
here we rely on .split() being called inside the _fetch()  in trans_real.translation()
make sure sideeffect_str actually added a new translation
This unusual grouping/force_grouping combination may be triggered by the intcomma filter (17414)
date filter
We shouldn't change the behavior of the floatformat filter re:  thousand separator and grouping when USE_L10N is False even  if the USE_THOUSAND_SEPARATOR, NUMBER_GROUPING and  THOUSAND_SEPARATOR settings are specified
Even a second time (after the format has been cached)...
Even a second time (after the format has been cached)...
Catalan locale
Russian locale (with E as month)
English locale
Checking for the localized "products_delivered" field
Non-strings are untouched
Russian locale has non-breaking space (\xa0) as thousand separator  Check that usual space is accepted too when sanitizing inputs
Suspicion that user entered dot as decimal separator (22171)
Importing some format modules so that we can compare the returned  modules with these expected modules
Should return the correct default module when no setting is set
When the setting is a string, should return the given module and  the default module
When setting is a list of strings, should return the given  modules and the default module
Good headers.
Bad headers; should always return [].
This test assumes there won't be a Django translation to a US  variation of the Spanish language, a safe assumption. When the  user sets it as the preferred language, the main 'es'  translation should be selected instead.
This tests the following scenario: there isn't a main language (zh)  translation of Django but there is a translation to variation (zh-hans)  the user sets zh-hans as the preferred language, it should be selected  by Django without falling back nor ignoring it.
This test assumes there won't be a Django translation to a US  variation of the Spanish language, a safe assumption. When the  user sets it as the preferred language, the main 'es'  translation should be selected instead.
This tests the following scenario: there isn't a main language (zh)  translation of Django but there is a translation to variation (zh-hans)  the user sets zh-hans as the preferred language, it should be selected  by Django without falling back nor ignoring it.
Strings won't get translated as they don't match after escaping %
Original translation.
Different translation.  Force refreshing translations.
Doesn't work because it's added later in the list.
Force refreshing translations.
Unless the original is removed from the list.
A language with no translation catalogs should fallback to the  untranslated string.
Regression test for 5241
Regression test for 21473
Specifying encoding is not supported (Django enforces UTF-8)
issue 19919
issue 11915
Make sure the cache is empty before we are doing our tests.
Make sure we will leave an empty cache for other testcases.
Namespaced URL
language from outside of the supported LANGUAGES list
We only want one redirect, bypassing CommonMiddleware
-*- encoding: utf-8 -*-
tests "%s"
tests "% o"
keep the diff friendly - remove 'POT-Creation-Date'
Sample project used by test_extraction.CustomLayoutExtractionTests
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
put file in read-only mode
`call_command` bypasses the parser; by calling  `execute_from_command_line` with the help subcommand we  ensure that there are no issues with the parser itself.
po file contains wrong po formatting.
po file contains invalid msgstr content (triggers non-ascii error content).  Make sure the output of msgfmt is unaffected by the current locale.  Various assertRaises on PY2 don't support unicode error messages.
-*- encoding: utf-8 -*-
makemessages scans the current working directory and writes in the  locale subdirectory. There aren't any options to control this. As a  consequence tests can't run in parallel. Since i18n tests run in less  than 4 seconds, serializing them with SerializeMixin is acceptable.
: .\path\to\file.html:123
: path/to/file.html:123
Comments in templates
should not be trimmed  should be trimmed  21406 -- Should adjust for eaten line numbers
Check that the temporary file was cleaned up
{% trans %}
{% blocktrans %}
{% trans %}
{% blocktrans %}
Test detection/end user reporting of old, incorrect templates  translator comments syntax  Now test .po file contents
"Normal" output:
Version number with only 2 parts (23788)
Bad version output
On Python < 3.2 os.symlink() exists only on Unix  On Python >= 3.2) os.symlink() exists always but then can  fail at runtime when user hasn't the needed permissions on  Windows versions that support symbolink links (>= 6/Vista).  See Python issue 9333 (http://bugs.python.org/issue9333).  Skip the test in that case
16903 -- Standard comment with source file relative path should be present
`call_command` bypasses the parser; by calling  `execute_from_command_line` with the help subcommand we  ensure that there are no issues with the parser itself.
A user-defined format
Translators: This comment should be extracted
This comment should not be extracted
Use iterator() with datetimes() to return a generator that lazily  requests each result one at a time, to save memory.
-*- coding: utf-8 -*-
Different coercion, same validation.
This can also cause weirdness: be careful (bool(-1) == True, remember)
Even more weirdness: if you have a valid choice but your coercion function  can't coerce, you'll still get a validation error. Don't do this!  Required fields require values
Non-required fields aren't required
If you want cleaning an empty value to return a different type, tell the field
has_changed should not trigger required validation
-*- coding: utf-8 -*-
hangs "forever" if catastrophic backtracking in ticket:11198 not fixed
a second test, to make sure the problem is really addressed, even on  domains that don't fail the domain label length check in the regex
Valid IDN
-*- coding: utf-8 -*-
Initial value may have mutated to a string due to show_hidden_initial (19537)  HiddenInput widget sends string values for boolean but doesn't clean them in value_from_datadict
The edge cases of the IPv6 validation code are not deeply tested  here, they are covered in the tests for django.utils.ipv6
Test the normalizing code
-*- coding: utf-8 -*-
As with any widget that implements get_value_from_datadict(), we must  accept the input from the "as_hidden" rendering as well.
Invalid dates shouldn't be allowed
label tag is correctly associated with month dropdown
With Field.show_hidden_initial=False
With Field.show_hidden_initial=True
Invalid dates shouldn't be allowed  'Geef een geldige datum op.' = 'Enter a valid date.'
label tag is correctly associated with first rendered dropdown
Test whitespace stripping behavior (5714)
Test null bytes (18982)
assertIsInstance or assertRaises cannot be used because UnicodeEncodeError  is a subclass of ValueError
Return an empty dictionary if max_length and min_length are both None.
Return a maxlength attribute equal to max_length.
Return a minlength attribute equal to min_length.
Return both maxlength and minlength when both max_length and  min_length are set.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Different coercion, same validation.
This can also cause weirdness: be careful (bool(-1) == True, remember)
Even more weirdness: if you have a valid choice but your coercion function  can't coerce, you'll still get a validation error. Don't do this!  Required fields require values
Non-required fields aren't required  If you want cleaning an empty value to return a different type, tell the field
has_changed should not trigger required validation
Test whitespace stripping behavior (5714)
-*- coding: utf-8 -*-
No file was uploaded and no initial data.
A file was uploaded and no initial data.
A file was not uploaded, but there is initial data
A file was uploaded and there is initial data (file identity is not dealt  with here)
-*- coding: utf-8 -*-
Leading whole zeros "collapse" to one digit.  But a leading 0 before the . doesn't count towards max_digits  Only leading whole zeros "collapse" to one digit.
Test whitespace stripping behavior (5714)
-*- coding: utf-8 -*-
Check for runaway regex security problem. This will take a long time  if the security fix isn't in place.
The internal value is preserved if using HiddenInput (7753).
Make sure we're compatible with MySQL, which uses 0 and 1 for its  boolean values (9609).
HiddenInput widget sends string values for boolean but doesn't clean them in value_from_datadict
-*- coding: utf-8 -*-
Rendering the default state with empty_label setted as string.
Even with an invalid date, the widget should reflect the entered value (17401).
Years before 1900 should work.
w2 ought to be independent of w1, since MultiWidget ought  to make a copy of its sub-widgets when it is copied.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
only one query is required to pull the model from DB
Check that the empty value is a QuerySet  While we're at it, test whether a QuerySet is returned if there *is* a value.
FileModel with unicode filename and data
Boundary conditions on a PositiveIntegerField
Formfield initial values   If the model has default values for some fields, they are used as the formfield  initial values.
In a ModelForm that is passed an instance, the initial values come from the  instance's values, not the model's defaults.
Issue 12337. save_instance should honor the passed-in exclude keyword.
Test that saving a form with a blank choice results in the expected  value being stored in the database.
nl/formats.py has customized TIME_INPUT_FORMATS:  ['%H:%M:%S', '%H.%M:%S', '%H.%M', '%H:%M']
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip
Parse a time in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
ISO formats are accepted, even if not specified in formats.py
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip
Parse a time in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a time in an unaccepted format; get an error
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a time in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
ISO formats are accepted, even if not specified in formats.py
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip
Parse a date in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip
Parse a date in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
ISO formats are accepted, even if not specified in formats.py
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip
Parse a date in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip
Parse a date in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid, but non-default format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
Parse a date in an unaccepted format; get an error
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to the same format
Parse a date in a valid format, get a parsed result
Check that the parsed result does a round trip to default format
-*- coding: utf-8 -*-
FormSet allows us to use multiple instance of the same form on 1 page. For now,  the best way to create a FormSet is by using the formset_factory function.
Let's define a FormSet that takes a list of favorite drinks, but raises an  error if there are any duplicates. Used in ``test_clean_hook``,  ``test_regression_6926`` & ``test_regression_12878``.
Used in ``test_formset_splitdatetimefield``.
If a FormSet was not passed any data, its is_valid and has_changed  methods should return False.
FormSet instances can also have an error attribute if validation failed for  any of the forms.
FormSet instances has_changed method will be True if any data is  passed to his forms, even if the formset didn't validate
invalid formset test
valid formset test
Let's simulate what would happen if we submitted this form.
But the second form was blank! Shouldn't we get some errors? No. If we display  a form as blank, it's ok for it to be submitted as blank. If we fill out even  one of the fields of a blank form though, it will be validated. We may want to  required that at least x number of forms are completed, but we'll show how to  handle that later.
If we delete data that was pre-filled, we should get an error. Simply removing  data from form fields isn't the proper way to delete it. We'll see how to  handle that case later.
Displaying more than 1 blank form   We can also display more than 1 empty form at a time. To do so, pass a  extra argument to formset_factory.
We can also display more than 1 empty form passing min_num argument  to formset_factory. It will (essentially) increment the extra argument
Min_num forms are required; extra forms can be empty.
We can also display more than 1 empty form passing min_num argument
The extra argument also works when the formset is pre-filled with initial  data.
If we remove the deletion flag now we will have our validation back.
If max_num is 0 then no form is rendered at all.
One form from initial and extra=3 with max_num=2 should result in the one  initial form and one extra.
Regression tests for 16455 -- formset instances are iterable
confirm iterated formset yields formset.forms
confirm indexing of formset
Formsets can override the default iteration order
confirm that __iter__ modifies rendering order  compare forms from "reverse" formset with forms from original formset
Regression tests for 16479 -- formsets form use ErrorList instead of supplied error_class
reduce the default limit of 1000 temporarily for testing  someone fiddles with the mgmt form data...
But we still only instantiate 3 forms  and the formset isn't valid
reduce the default limit of 1000 temporarily for testing  for this form, we want a limit of 4
Four forms are instantiated and no exception is raised
Regression test for 11160  If non_form_errors() is called without calling is_valid() first,  it should ensure that full_clean() is called.
Regression test for 11418
Empty forms should be unbound
The empty forms should be equal.
-*- coding: utf-8 -*-
Tests to prevent against recurrences of earlier bugs.
Translations are done at rendering time, so multi-lingual apps can define forms)
There was some problems with form translations in 5216
Unicode decoding problems...
Translated error messages used to be buggy.
Deep copying translated text shouldn't raise an error)
Testing choice validation with UTF-8 bytestrings as input (these are the  Russian abbreviations "мес." and "шт.".
There once was a problem with Form fields called "data". Let's make sure that  doesn't come back.
A form with *only* hidden fields that has errors is going to be very unusual.
The forms layer doesn't escape input values directly because error messages  might be presented in non-HTML contexts. Instead, the message is just marked  for escaping by the template engine. So we'll need to construct a little  silly template to trigger the escaping.
-*- coding: utf-8 -*-
This form should print errors the default way.
This one should wrap error groups in the customized way.
Create choices for the model choice field tests below.
ModelChoiceField
ModelMultipleChoiceField
-*- coding: utf-8 -*-
RadioSelect uses a RadioFieldRenderer to render the individual radio inputs.  You can manipulate that object directly to customize the way the RadioSelect  is rendered.
A RadioFieldRenderer object also allows index access to individual RadioChoiceInput
These individual widgets can accept extra attributes if manually rendered.
Should be "\nTst\n" after 19251 is fixed
-*- coding: utf-8 -*-
Pass a dictionary to a Form's __init__().
Unicode values are handled properly.
cleaned_data will always *only* contain a key for fields defined in the  Form, even if you pass extra data when you define the Form. In this  example, we pass a bunch of extra fields to the form constructor,  but cleaned_data contains only the form's fields.
cleaned_data will include a key and value for *all* fields defined in the Form,  even if the Form's data didn't include a value for fields that are not  required. In this example, the data dictionary doesn't include a value for the  "nick_name" field, but cleaned_data includes it. For CharFields, it's set to the  empty string.
For DateFields, it's set to None.
'True' or 'true' should be rendered without a value attribute
A value of 'False' or 'false' should be rendered unchecked
A value of '0' should be interpreted as a True value (16820)
Any Field can have a Widget class passed to its constructor:
as_textarea(), as_text() and as_hidden() are shortcuts for changing the output  widget type:
The 'widget' parameter to a Field can also be an instance:
Instance-level attrs are *not* carried over to as_textarea(), as_text() and  as_hidden():
For a form with a <select>, use ChoiceField:
A subtlety: If one of the choices' value is the empty string and the form is  unbound, then the <option> for the empty-string choice will get selected="selected".
You can specify widget attributes in the Widget constructor.
When passing a custom widget instance to ChoiceField, note that setting  'choices' on the widget is meaningless. The widget will use the choices  defined on the Field, not the ones defined on the Widget.
You can set a ChoiceField's choices after the fact.
Add widget=RadioSelect to use that widget with a ChoiceField.
You can iterate over any BoundField, not just those with widget=RadioSelect.
MultipleChoiceField is a special case, as its data is required to be a list:
Disabled fields are generally not transmitted by user agents.  The value from the form's initial data is used.
Values provided in the form's data are ignored.
Initial data remains present on invalid forms.
DateTimeField rendered as_hidden() is special too
MultipleChoiceField can also be used with the CheckboxSelectMultiple widget.
Regarding auto_id, CheckboxSelectMultiple is a special case. Each checkbox  gets a distinct ID, formed by appending an underscore plus the checkbox's  zero-based index.
Data for a MultipleChoiceField should be a list. QueryDict and  MultiValueDict conveniently work with this.
The MultipleHiddenInput widget renders multiple values as hidden fields.
When using CheckboxSelectMultiple, the framework expects a list of input and  returns a list of input.
Validation errors are HTML-escaped when output as HTML.
There are a couple of ways to do multiple-field validation. If you want the  validation message to be associated with a particular field, implement the  clean_XXX() method on the Form, where XXX is the field name. As in  Field.clean(), the clean_XXX() method should return the cleaned value. In the  clean_XXX() method, you have access to self.cleaned_data, which is a dictionary  of all the data that has been cleaned *so far*, in order by the fields,  including the current field (e.g., the field XXX if you're in clean_XXX()).
Another way of doing multiple-field validation is by implementing the  Form's clean() method. Usually ValidationError raised by that method  will not be associated with a particular field and will have a  special-case association with the field named '__all__'. It's  possible to associate the errors to particular field with the  Form.add_error() method or by passing a dictionary that maps each  field to one or more errors.  Note that in Form.clean(), you have access to self.cleaned_data, a  dictionary of all the fields/values that have *not* raised a  ValidationError. Also note Form.clean() is required to return a  dictionary of all clean data.
Test raising a ValidationError as NON_FIELD_ERRORS.
Test raising ValidationError that targets multiple fields.
Test Form.add_error()
Ensure that the newly added list of errors is an instance of ErrorList.
Trigger validation.
Check that update_error_dict didn't lose track of the ErrorDict type.
It's possible to construct a Form dynamically by adding to the self.fields  dictionary in __init__(). Don't forget to call Form.__init__() within the  subclass' __init__().
Instances of a dynamic Form do not persist fields from one Form instance to  the next.
Similarly, changes to field attributes do not persist from one Form instance  to the next.
Similarly, choices do not persist from one Form instance to the next.  Refs 15127.
HiddenInput widgets are displayed differently in the as_table(), as_ul())  and as_p() output of a Form -- their verbose names are not displayed, and a  separate row is not displayed. They're displayed in the last row of the  form, directly after that row's form element.
A corner case: It's possible for a form to have only HiddenInputs.
A Form's fields are displayed in the same order in which they were defined.
Some Field classes have an effect on the HTML attributes of their associated  Widget. If you set max_length in a CharField and its associated widget is  either a TextInput or PasswordInput, then the widget's rendered HTML will  include the "maxlength" attribute.
If you specify a custom "attrs" that includes the "maxlength" attribute,  the Field's max_length attribute will override whatever "maxlength" you specify  in "attrs".
You can specify the label for a field by using the 'label' argument to a Field  class. If you don't specify 'label', Django will use the field name with  underscores converted to spaces, and the initial letter capitalized.
Labels for as_* methods will only end in a colon if they don't end in other  punctuation already.
If a label is set to the empty string for a field, that field won't get a label.
If label is None, Django will auto-create the label from the field name. This  is default behavior.
You can specify the 'label_suffix' argument to a Form class to modify the  punctuation symbol used at the end of a label.  By default, the colon (:) is  used, and is only appended to the label if the label doesn't already end with a  punctuation symbol: ., !, ? or :.  If you specify a different suffix, it will  be appended regardless of the last character of the label.
You can specify initial data for a field by using the 'initial' argument to a  Field class. This initial data is displayed when a Form is rendered with *no*  data. It is not displayed when a Form is rendered with any data (including an  empty dictionary). Also, the initial value is *not* used if data for a  particular required field isn't provided.
An 'initial' value is *not* used as a fallback if data is not provided. In this  example, we don't provide a value for 'username', and the form raises a  validation error rather than using the initial value for 'username'.
The previous technique dealt with "hard-coded" initial data, but it's also  possible to specify initial data after you've already created the Form class  (i.e., at runtime). Use the 'initial' parameter to the Form constructor. This  should be a dictionary containing initial values for one or more fields in the  form, keyed by field name.
A dynamic 'initial' value is *not* used as a fallback if data is not provided.  In this example, we don't provide a value for 'username', and the form raises a  validation error rather than using the initial value for 'username'.
If a Form defines 'initial' *and* 'initial' is passed as a parameter to Form(),  then the latter will get precedence.
The previous technique dealt with raw values as initial data, but it's also  possible to specify callable data.
We need to define functions that get called later.)
A callable 'initial' value is *not* used as a fallback if data is not provided.  In this example, we don't provide a value for 'username', and the form raises a  validation error rather than using the initial value for 'username'.
If a Form defines 'initial' *and* 'initial' is passed as a parameter to Form(),  then the latter will get precedence.
Test that field raising ValidationError is always in changed_data
BoundField is also cached
Nix microseconds (since they should be ignored). 22502
You can specify descriptive text for a field by using the 'help_text' argument)
help_text is not displayed for hidden fields. It can be used for documentation  purposes, though.
You can subclass a Form to add fields. The resulting form subclass will have  all of the fields of the parent Form, plus whichever fields you define in the  subclass.
Yes, you can subclass multiple forms. The fields are added in the order in  which the parent classes are listed.
Sometimes it's necessary to have multiple forms display on the same HTML page,  or multiple copies of the same form. We can accomplish this with form prefixes.  Pass the keyword argument 'prefix' to the Form constructor to use this feature.  This value will be prepended to each HTML form field name. One way to think  about this is "namespaces for HTML forms". Notice that in the data argument,  each field's key has the prefix, in this case 'person1', prepended to the  actual field name.
Let's try submitting some bad data to make sure form.errors and field.errors  work as expected.
In this example, the data doesn't have a prefix, but the form requires it, so  the form doesn't "see" the fields.
With prefixes, a single data dictionary can hold data for multiple instances  of the same form.
By default, forms append a hyphen between the prefix and the field name, but a  form can alter that behavior by implementing the add_prefix() method. This  method takes a field name and returns the prefixed field, according to  self.prefix.
Prefix can be also specified at the class level.
NullBooleanField is a bit of a special case because its presentation (widget)  is different than its data. This is handled transparently, though.
FileFields are a special case because they take their data from the request.FILES,  not request.POST.
Case 3: POST with valid data (the success message).)
Sometimes (pretty much in formsets) we want to allow a form to pass validation  if it is completely empty. We can accomplish this by using the empty_permitted  argument to a form constructor.
First let's show what happens id empty_permitted=False (the default):
Now let's show what happens when empty_permitted=True and the form is empty.
But if we fill in data for one of the fields, the form is no longer empty and  the whole thing must pass validation.
If a field is not given in the data then None is returned for its data. Lets  make sure that when checking for empty_permitted that None is treated  accordingly.
However, we *really* need to be sure we are checking for None as any data in  initial that returns False on a boolean call needs to be treated literally.
An empty value for any field will raise a `required` error on a  required `MultiValueField`.
Empty values for fields will NOT raise a `required` error on an  optional `MultiValueField`
For a required `MultiValueField` with `require_all_fields=False`, a  `required` error will only be raised if all fields are empty. Fields  can individually be required or optional. An empty value for any  required field will raise an `incomplete` error.
For an optional `MultiValueField` with `require_all_fields=False`, we  don't get any `required` error but we still get `incomplete` errors.
Fake json.loads
without anything: just print the <label>
passing just one argument: overrides the field's label
the overridden label is escaped
Passing attrs to add extra attributes on the <label>
Return a different dict. We have not changed self.cleaned_data.
-*- coding: utf-8 -*-
Can take a string.  Can take a unicode string.  Can take a lazy string.  Can take a list.  Can take a dict.  Can take a mixture in a list.
Can take a non-string.
Escapes non-safe input but not input marked safe.
-*- coding: utf-8 -*-
A widget can exist without a media definition
A widget can define media if it needs to.  Any absolute path will be preserved; relative paths are combined  with the value of settings.MEDIA_URL
Media objects can be combined. Any given media resource will appear only  once. Duplicated media definitions are ignored.
Regression check for 12879: specifying the same CSS or JS file  multiple times in a single Media instance should result in that file  only being included once.
Widget media can be defined as a property
Media properties can reference the media of their parents
Media properties can reference the media of their parents,  even if the parent media was defined using a class
If a widget extends another but provides no media definition, it inherits the parent widget's media
If a widget extends another but defines media, it extends the parent widget's media by default
If a widget extends another but defines media, it extends the parents widget's media,  even if the parent defined media using a property.
A widget can disable media inheritance by specifying 'extend=False'
A widget can explicitly enable full media inheritance by specifying 'extend=True'
A widget can enable inheritance of one media type by specifying extend as a tuple
A widget can define CSS media for multiple output media types
MultiWidgets have a default media definition that gets all the  media from the component widgets
Forms can also define media, following the same rules as widgets.
Custom managers
Custom manager
No custom manager on this class to make sure the default case doesn't break.
Managers from base classes are inherited and, if no manager is specified  *and* the parent has a manager specified, the first one (in the MRO) will  become the default.
Should be the default manager, although the parent managers are  inherited.
RelatedManagers
Since Child6 inherits from Child4, the corresponding rows from f1 and  f2 also appear here. This is the expected result.
Accessing the manager on an abstract model should  raise an attribute error with an appropriate message.  This error message isn't ideal, but if the model is abstract and  a lot of the class instantiation logic isn't invoked; if the  manager is implied, then we don't get a hook to install the  error-raising manager.
Accessing the manager on an abstract model with an custom  manager should raise an attribute error with an appropriate  message.
Accessing the manager on an abstract model with an explicit  manager should raise an attribute error with an appropriate  message.
Accessing the manager on a swappable model should  raise an attribute error with a helpful message
Accessing the manager on a swappable model with an  explicit manager should raise an attribute error with a  helpful message
Accessing the manager on a swappable model with an  explicit manager should raise an attribute error with a  helpful message
Make sure related managers core filters don't include an  explicit `__exact` lookup that could be interpreted as a  reference to a foreign `exact` field. refs 23940.
Shouldn't issue any warnings, since GeoManager itself will be  deprecated at the same time as use_for_related_fields, there  is no point annoying users with this deprecation.
With the new base_manager_name API there shouldn't be any warnings.
With the new base_manager_name API there shouldn't be any warnings.
With the new base_manager_name API there shouldn't be any warnings.
Shouldn't complain since the inherited manager  is basically the same that would have been created.
Should create 'objects' (set as default) and warn that  it will no longer be the case in the future.
When there is a local manager we shouldn't get any warning  and 'objects' shouldn't be created.
When there is an inherited manager we shouldn't get any warning  and 'objects' shouldn't be created.
With `manager_inheritance_from_future = True` 'objects'  shouldn't be created.
This will return a different result when it's run within or outside  of a git clone: 1.4.devYYYYMMDDHHMMSS or 1.4.
The normal case  Same thing, via an update
Won't work because force_update and force_insert are mutually  exclusive
Try to update something that doesn't have a primary key in the first  place.
Won't work because we can't insert a pk of the same value.
Trying to update should still fail, even with manual primary keys, if  the data isn't in the database already.
Create a book through the publisher.  The publisher should have one book.
Try get_or_create again, this time nothing should be created.  And the publisher should still have one book.
Add an author to the book.  The book should have one author.
Try get_or_create again, this time nothing should be created.  And the book should still have one author.
Add a second author to the book.
The book should have two authors now.
Create an Author not tied to any books.
There should be three Authors in total. The book object should have two.
Try creating a book through an author.
Now Ed has two Books, Fred just one.
Use the publisher's primary key value instead of a model instance.
Try get_or_create again, this time nothing should be created.
The publisher should have three books.
If we execute the exact same statement, it won't create a Person.
This dollar is ok as it is escaped
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
test register as decorator
test register as function
check results
One warning for each database alias
The resolver is checked recursively (examining url()s in include()).
-*- coding: utf-8 -*-
Explicit `output_field`.  Implicit `output_field`.
Selected author ages are 57 and 46
Refs 20782
The value doesn't matter, we just need any negative  constraint on a related model that's a noop.
Try to generate query tree
Check internal state
If the backend needs to force an ordering we make sure it's  the only "ORDER BY" clause present in the query.
ensure the F() is promoted to the group by clause
test completely changing how the output is rendered
test changing the dict and delegating
test overriding all parts of the template
Create a plain expression
Instantiate the Form and FormSet to prove  you can create a form with no data
Now create a new User and UserSite instance
Now update the UserSite instance
Now add a new UserSite instance
Instantiate the Form and FormSet to prove  you can create a form with no data
Now create a new Restaurant and Manager instance
Now update the Manager instance
Now add a new Manager instance
Testing the inline model's relation
Testing the inline model's relation
Instantiate the Form and FormSet to prove  you can create a formset with an instance of None
Add a new host, modify previous host, and save-as-new
To save a formset as new, it needs a new hub instance
check if the returned error classes are correct  note: formset.errors returns a list as documented
reload database
pass standard data dict & see none updated
reload database
create data dict with all fields marked for deletion
reload database
Create formset with custom Delete function  create data dict with all fields marked for deletion
verify two were deleted
verify no "odd" PKs left
Even if the "us" object isn't in the DB any more, the form  validates.
max_age parameter can also accept a datetime.timedelta object
-*- coding: utf-8 -*-
Should not set mode to None if it is not present.  See 14681, stdlib gzip module crashes if mode is set to None
Set chunk size to create a boundary after \n:  b'one\n...         ^
Set chunk size to create a boundary between \r and \n:  b'one\r\n...         ^
Set chunk size to create a boundary after \r:  b'one\r...         ^
The following seek() call is required on Windows Python 2 when  switching from reading to writing.
file_move_safe should raise an IOError exception if destination file exists and allow_overwrite is False
should allow it and continue on if allow_overwrite is True
ticket 14453
Use HTTP responses different from the defaults
Models for 19425
Models for 18263
The heading for the m2m inline block uses the right text  The "add another" label is correct  The '+' is dropped from the autogenerated form prefix (Author_books+)
Imelda likes shoes, but can't carry her own bags.
Here colspan is "4": two fields (title1 and title2), one hidden field and the delete checkbox.
Identically named callable isn't present in the parent ModelAdmin,  rendering of the add view shouldn't explode  View should have the child inlines section
Add parent object view should have the child inlines section  The right callable should be used for the inline readonly_fields  column cells
ReadOnly fields
The maximum number of forms should respect 'get_max_num' on the  ModelAdmin  The total number of forms will remain the same in either case
User always has permissions to add and change Authors, and Holders,  the main (parent) models of the inlines. Permissions on the inlines  vary per test.
Get the ID of the automatically created intermediate model for the Author-Book m2m
No change permission on books, so no inline
No permissions on Inner2s, so no inline
No change permission on books, so no inline
No permissions on Inner2s, so no inline
No change permission on Books, so no inline
Add permission on inner2s, so we get the inline
No change permission on books, so no inline
We have change perm on books, so we can add/change/delete inlines
Add permission on inner2s, so we can add but not modify existing  3 extra forms only, not the existing instance form
Change permission on inner2s, so we can change existing but not add new  Just the one form for existing instances  max-num 0 means we can't add new ones
Add/change perm, so we can add new and change existing  One form for existing instance and three extra for new
Change/delete perm on inner2s, so we can change/delete existing  One form for existing instance only, no new
All perms on inner2s, so we can add/change/delete  One form for existing instance only, three for new
Check that there's only one inline to start with and that it has the  correct ID.
Add an inline
Check that the inline has been added, that it has the right id, and  that it contains the right fields.
Let's add another one to be sure
Enter some data and click 'Save'
Check that the objects have been created in the database
Add a few inlines
Click on a few delete buttons  Verify that they're gone and that the IDs have been re-sequenced
Add a few inlines
Collapsed inlines have SHOW/HIDE links.  One field is in a stacked inline, other in a tabular one.
admin for 18433
admin for 19425 and 18388
admin for 19524
admin and form for 18263
Test bug 12561 and 12778  only ModelAdmin media  ModelAdmin and Inline media  only Inline media
These entries are in the format: (path, url_name, app_name, namespace, view_name, func, args, kwargs)  Simple case
Unnamed args are dropped if you have *any* kwargs in a pattern
Unnamed views should have None as the url_name. Regression data for 21157.
If you have no kwargs, you get an args list.
Namespaces
Nested namespaces
Namespaces capturing variables
Tests for nested groups. Nested capturing groups will only work if you  *only* supply the correct outer group.
Tests for 13154
Security tests
Reversing None should raise an error, not return the last un-named view.
Parentheses are allowed and should not cause errors or be escaped
Regression for 20022, adjusted for 24013 because ~ is an unreserved  character. Tests whether % is escaped.
Regression for 17076  this url exists, but requires an argument  we can't use .assertRaises, since we want to inspect the  exception
Pick a resolver from a namespaced URLconf
this list matches the expected URL types and names returned when  you try to resolve a non-existent URL in the first level of included  URLs in named_urls.py (e.g., '/included/non-existent-url')  make sure we at least matched the root ('/') url resolver:
We don't really need a model; just something with a get_absolute_url
Assert that we can redirect using UTF-8 strings  Assert that no imports are attempted when dealing with a relative path  (previously, the below would resolve in a UnicodeEncodeError from __import__ )
modules that are not listed in urlpatterns should not be importable
Views added to urlpatterns using include() should be reversible.
Test legacy support for extracting "function, args, kwargs"
Test ResolverMatch capabilities.
... and for legacy purposes:
View is not a callable (explicit import; arbitrary Python object)
Regex contains an error (refs 6170)
passing a callable should return the callable
A missing view (identified by an AttributeError) should raise  ViewDoesNotExist, ...  ... but if the AttributeError is caused by something else don't  swallow it.
no app_name -> deprecated
app_name argument to include -> deprecated
3-tuple -> deprecated
Partials should be fine.
This is non-reversible, but we shouldn't blow up when parsing it.
Tests for 13154. Mixed syntax to test both ways of defining URLs.
Security tests
A URLs file that doesn't use the default  from django.conf.urls import *  import pattern.
I just raise an AttributeError to confuse the view loading mechanism
-*- encoding: utf-8 -*-
ASCII unicode or bytes values are converted to native strings.
Latin-1 unicode or bytes values are also converted to native strings.
Other unicode values are MIME-encoded (there's no way to pass them as bytes).
The response also converts unicode or bytes keys to strings, but requires  them to contain ASCII
Bug 20889: long lines trigger newlines to be added to headers  (which is not allowed due to bug 10188)  This one is triggering http://bugs.python.org/issue20747, that is Python  will itself insert a newline in the header
Bug 10188: Do not allow newlines in headers (CR or LF)
Bug 16494: HttpResponse should behave consistently with non-strings
test content via property
test iter content via property
test odd inputs  '\xde\x9e' == unichr(1950).encode('utf-8')
.content can safely be accessed multiple times.  __iter__ can safely be called multiple times (20187).  Accessing .content still works.
Accessing .content also works if the response was iterated first.
Additional content can be written to the response.
Regression test for 13222
with Content-Encoding header
Test that standard HttpResponse init args can be used  Test that url attribute is right
304 responses should not have content/content-type
Test that standard HttpResponse init args can be used
iterating over the response itself yields bytestring chunks.
and the response can only be iterated once.
even when a sequence that can be iterated many times, like a list,  is given as content.
iterating over Unicode strings still yields bytestring chunks.  '\xc3\xa9' == unichr(233).encode('utf-8')
streaming responses don't have a `content` attribute.
and you can't accidentally assign to a `content` attribute.
but they do have a `streaming_content` attribute.
that exists so we can check if a response is streaming, and wrap or  replace the content iterator.
coercing a streaming response to bytes doesn't return a complete HTTP  message like a regular response does. it only gives us the headers.
and this won't consume its content.
additional content cannot be written to the response.
and we can't tell the current position.
Disable the request_finished signal during this test  to avoid interfering with the database connection.
file isn't closed until we close the response.
when multiple file are assigned as content, make sure they are all  closed with the response.
file isn't closed until we close the response.
when multiple file are assigned as content, make sure they are all  closed with the response.
Here parse_cookie() differs from Python's cookie parsing in that it  treats all semicolons as delimiters, even within quotes.  Illegal cookies that have an '=' char in an unquoted value.  Cookies with ':' character in their name.  Cookies with '[' and ']'.
Cookies that RFC6265 allows.  parse_cookie() has historically kept only the last cookie with the  same name.
Chunks without an equals sign appear as unnamed values per  https://bugzilla.mozilla.org/show_bug.cgi?id=169091  Even a double quote may be an unamed value.  Spaces in names and values, and an equals sign in values.  More characters the spec forbids.  Unicode characters. The spec only allows ASCII.  Browsers don't send extra whitespace or semicolons in Cookie headers,  but parse_cookie() should parse whitespace the same way  document.cookie parses whitespace.
TODO: why can't I make this ..app2
If ticket 1578 ever slips back in, these models will not be able to be  created (the field names being lower-cased versions of their opposite  classes is important here).
Protect against repetition of 1839, 2415 and 2536.
Multiple paths to the same model (7110, 7125)
Test related objects visibility.
Create a few Reporters.  Create an Article.
Article objects have access to their related Reporter objects.  These are strings instead of unicode strings because that's what was used in  the creation of this reporter (and we haven't refreshed the data from the  database, which always returns unicode strings).
You can also instantiate an Article by passing the Reporter's ID  instead of a Reporter object.
Similarly, the reporter ID can be a string.
Create an Article via the Reporter object.
Create a new article, and add it to the article set.
Add the same article to a different article set - check that it moves.
Adding an object of the wrong type raises TypeError.
Assign the article to the reporter.
Set the article back again.
Funny case - because the ForeignKey cannot be null,  existing members of the set must remain.
Assign the article to the reporter directly using the descriptor.
Set the article back again using set() method.
Because the ForeignKey cannot be null, existing members of the set  must remain.  Reporter cannot be null - there should not be a clear or remove method
Reporter objects have access to their related Article objects.  Get articles by id  Query on an article property  The API automatically follows relationships as far as you need.  Use double underscores to separate relationships.  This works as many levels deep as you want. There's no limit.  Find all Articles for any Reporter whose first name is "John".  Check that implied __exact also works  Query twice over the related field.  The underlying query only makes one join when a related table is referenced twice.
The automatically joined table has a predictable name.  ... and should work fine with the unicode that comes out of forms.Form.cleaned_data  Find all Articles for a Reporter.  Use direct ID check, pk check, and object comparison  You can also use a queryset instead of a literal list of instances.  The queryset must be reduced to a list of values using values(),  then converted into a query
Reporters can be queried  Reporters can query in opposite direction of ForeignKey definition
Counting in the opposite direction works in conjunction with distinct()
Queries can go round in circles.
Check that implied __exact also works.
It's possible to use values() calls across many-to-one relations.  (Note, too, that we clear the ordering here so as not to drag the  'headline' field into the columns being used to determine uniqueness)
Check that Article.objects.select_related().dates() works properly when  there are multiple Articles with the same date but different foreign-key  objects (Reporters).
If you delete a reporter, his articles will be deleted.  You can delete using a JOIN in the query.
Create a new Article with get_or_create using an explicit value  for a ForeignKey.
You can specify filters containing the explicit FK value.
Create an Article by Paul for the same date.
Get should respect explicit foreign keys as well.
Regression for 12876 -- Model methods that include queries that  recursive don't cause recursion depth problems under deepcopy.
Same twice
Same as each other
Look up the object again so that we get a "fresh" object.
Accessing the related object again returns the exactly same object.
But if we kill the cache, we get a new object.
Assigning a new object results in that object getting cached immediately.
Assigning None succeeds if field is null=True.
bestchild should still be None after saving.
bestchild should still be None after fetching the object again.
Assigning None will not fail: Child.parent is null=False.
You also can't assign an object of the wrong type here
You can assign None to Child.parent during object creation.
But when trying to save a Child with parent=None, the database will  raise IntegrityError.
Creation using keyword argument should cache the related object.
Creation using keyword argument and unsaved related instance (8070).
Creation using attname keyword argument and an id will cause the  related object to be fetched.
Test of multiple ForeignKeys to the same model (bug 7125).
Regression for 12190 -- Should be able to instantiate a FK outside  of a model, and interrogate its related field.
Test that the <field>_set manager does not join on Null value fields (17541)  The object isn't saved an thus the relation field is null - we won't even  execute a query in this case.  Now the model is saved, so we will need to execute an query.
Only one school is available via all() due to the custom default manager.
Make sure the base manager is used so that an student can still access  its related school even if the default manager doesn't normally  allow it.
If the manager is marked "use_for_related_fields", it'll get used instead  of the "bare" queryset. Usually you'd define this as a property on the class,  but this approximates that in a way that's easier in tests.
The exception raised on attribute access when a related object  doesn't exist should be an instance of a subclass of `AttributeError`  refs 21563
The first time autodiscover is called, we should get our real error.
Calling autodiscover again should raise the very same error it did  the first time, not an AlreadyRegistered error.
We can't test the DEFAULT_TABLESPACE and DEFAULT_INDEX_TABLESPACE settings  because they're evaluated when the model class is defined. As a consequence,  @override_settings doesn't work, and the tests depend
The unmanaged models need to be removed after the test in order to  prevent bad interactions with the flush operation in other tests.
1 for the table  1 for the index on the primary key
1 for the table + 1 for the index on the primary key
No tablespace-related SQL
1 for the table  1 for the primary key + 1 for the index on code
1 for the table + 1 for the primary key + 1 for the index on code
1 for the index on reference
No tablespace-related SQL
The join table of the ManyToManyField goes to the model's tablespace,  and its indexes too, unless DEFAULT_INDEX_TABLESPACE is set.  1 for the table  1 for the primary key  1 for the table + 1 for the index on the primary key
The ManyToManyField declares no db_tablespace, its indexes go to  the model's tablespace, unless DEFAULT_INDEX_TABLESPACE is set.
The join table of the ManyToManyField goes to the model's tablespace,  and its indexes too, unless DEFAULT_INDEX_TABLESPACE is set.  1 for the table  1 for the primary key  1 for the table + 1 for the index on the primary key
The ManyToManyField declares db_tablespace, its indexes go there.
model_options is the name of the application for this test.
coding: utf-8
The order of these fields matter, do not change. Certain backends  rely on field ordering to perform database conversions, and this  model helps to test that.
annotate references a field in values()
filter refs the annotated value
can annotate an existing values with a new field
and we respect deferred columns!
None indicates not to create a column in the database.
-*- encoding: utf-8 -*-
primary_key must be True. Refs 12467.
Prevent Django from autocreating `id` AutoField, which would  result in an error, because a model must have exactly one  AutoField.
-*- encoding: utf-8 -*-
Prevent checks from being run on the 'other' database, which doesn't have  its check_field() method mocked in the test.
-*- encoding: utf-8 -*-
There would be a clash if Model.field installed an accessor.
Model names are resolved when a model is being created, so we cannot  test relative fields in isolation and we need to attach them to a  model.
Too much foreign keys to Person.
Implicit symmetrical=False.
Explicit symmetrical=True.
Explicit symmetrical=True.
Note that both fields are not unique.
A model that can be, but isn't swapped out. References to this  model should *not* raise any validation error.
Python 2 crashes on non-ASCII strings.
Python 2 crashes on non-ASCII strings.
New tests should not be included here, because this is a single,  self-contained sanity check, not a test of everything.
-*- encoding: utf-8 -*-
unique_together tests are very similar to index_together tests.
unique_together can be a simple tuple
A model with very long name which will be used to set relations to.
Main model for which checks will be performed.
Models used for setting `through` in M2M field.
First error because of M2M field set on the model with long name.  Some databases support names longer than the test name.
Second error because the FK specified in the `through` model  `m2msimple` has auto-generated name longer than allowed.  There will be no check errors in the other M2M because it  specifies db_column for the FK in `through` model even if the actual  name is longer than the limits of the database.
Error because of the field with long name added to the model  without specifying db_column
Here we have two clashed: id (automatic field) and clash, because  both parents define these fields.
This field doesn't result in a clash.
This field clashes with parent "f_id" field.
In lieu of any other connector, an existing OneToOneField will be  promoted to the primary key.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
This field is intentionally 2 characters long (16080).
fieldsets_add and fieldsets_change should return a special data structure that  is used in the templates. They should generate the "right thing" whether we  have specified a custom form, the fields argument, or nothing at all.  Here's the default case. There are no custom form_add/form_change methods,  no fields argument, and no fieldsets argument.
If we specify the fields argument, fieldsets_add and fieldsets_change should  just stick the fields into a formsets structure and return it.
Using `fields`.
Using `fieldsets`.
Using `exclude`.
You can also pass a tuple to `exclude`.
Using `fields` and `exclude`.
`name` shouldn't appear in base_fields because it's part of  readonly_fields.  But it should appear in get_fields()/fieldsets() so it can be  displayed as read-only.
A middleware base class that tracks which methods have been called
Middleware examples that do the right thing
Sample middlewares that raise exceptions
Sample middlewares that omit to return an HttpResonse
Test client intentionally re-raises any exceptions being raised  during request handling. Hence actual testing that exception was  properly handled is done by relying on got_request_exception  signal being sent.
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Repopulate the list of middlewares since it's already been populated  by setUp() before the MIDDLEWARE setting got overridden.
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Check that the right middleware methods have been invoked
Removing ROOT_URLCONF is safe, as override_settings will restore  the previously defined settings.
DATA fields
M2M fields
VIRTUAL fields
GFK fields
GR fields
DATA fields
M2M fields
VIRTUAL fields
GFK fields
GR fields
DATA fields
M2M Fields
VIRTUAL fields
GFK fields
GR fields
ForeignKey to BasePerson
ForeignKey to Person
ForeignKey to ProxyPerson
ManyToManyField to BasePerson
ManyToManyField to Person
ParentListTests models
Running unit test twice to ensure both non-cached and cached result  are immutable.
If apps registry is not ready, get_field() searches over only  forward fields.  'data_abstract' is a forward field, and therefore will be found  'data_abstract' is a reverse field, and will raise an exception
The apps.clear_cache is setUp() should have deleted all trees.  Exclude abstract models that are not included in the Apps registry  and have no cache.
On first access, relation tree should have populated cache.
AbstractPerson does not have any relations, so relation_tree  should just return an EMPTY_RELATION_TREE.
All the other models should already have their relation tree  in the internal __dict__ .
Testing non hidden related objects  Testing hidden related objects
!/usr/bin/env python
Make deprecation warnings errors to ensure no usage of deprecated features.  Make runtime warning errors to ensure no usage of error prone patterns.  Ignore known warnings in test dependencies.
Create a specific subdirectory for the duration of the test suite.  Set the TMPDIR environment variable in addition to tempfile.tempdir  so that children processes inherit it.
Removing the temporary TMPDIR. Ensure we pass in unicode so that it will  successfully remove temp trees containing non-ASCII filenames on Windows.  (We're assuming the temp dir name itself only contains ASCII characters.)
Need to add the associated contrib app to INSTALLED_APPS in some cases to  avoid "RuntimeError: Model class X doesn't declare an explicit app_label  and isn't in an application in INSTALLED_APPS."
GIS tests are in nested apps
Force declaring available_apps in TransactionTestCase for faster tests.
Redirect some settings for the duration of these tests.  This lets us skip creating migrations for the test models as many of  them depend on one of the following contrib applications.  Filter out non-error logging so we don't have to capture it in lots of  tests.
Load all the ALWAYS_INSTALLED_APPS.
Load all the test model apps.
Reduce given test labels to just the app module path
if the module (or an ancestor) was named on the command line, or  no modules were named (i.e., run all), import  this module and add it to INSTALLED_APPS.  exact match or ancestor match
Add contrib.gis to INSTALLED_APPS if needed (rather than requiring  @override_settings(INSTALLED_APPS=...) on all test cases.
Restore the old settings.
This doesn't work before django.setup() on some databases.
Run the test suite, including the extra validation tests.
Make sure the bisection point isn't in the test list  Also remove tests that need to be run in specific combinations
Make sure the constant member of the pair isn't in the test list  Also remove tests that need to be run in specific combinations
mock is a required dependency
Allow including a trailing slash on app_labels for tab completion convenience
This should never receive unrendered content.
process_template_response must not be called for HttpResponse
response must not be rendered yet.  process_response must not be called until after response is rendered,  otherwise some decorators like csrf_protect and gzip_page will not  work correctly. See 16004  Check that process_response saw the rendered content
-*- coding: utf-8 -*-
Test uncached access  Test cached access
Test uncached access  Test cached access
Test uncached access  Test cached access
Test uncached access
Test cached access: no changes
Test cached access: add a module
originally from https://bitbucket.org/ned/jslex
comments  punctuation  regex
Valid inputs  Invalid inputs
Valid inputs  Invalid inputs
Valid inputs  Invalid inputs
implement specific and obviously wrong escaping  in order to be able to tell for sure when it runs
2:30 happens twice, once before DST ends and once after
2:30 never happened due to DST
round trip to UTC then back to CET
-*- encoding: utf-8 -*-
str(s) raises a TypeError on python 3 if the result is not a text type.  python 2 fails when it tries converting from str to unicode (via ASCII).
Valid UTF-8 sequences are encoded.
Reserved chars remain unescaped.
Test idempotency.
Valid UTF-8 sequences are decoded.
Broken UTF-8 sequences remain escaped.
Test idempotency.
It's hard to test for constant time, just test the result.
http://tools.ietf.org/html/draft-josefsson-pbkdf2-test-vectors-06   this takes way too long :(  {      "args": {          "password": "password",          "salt": "salt",          "iterations": 16777216,          "dklen": 20,          "digest": hashlib.sha1,      },      "result": "eefe3d61cd4da4e4e9945b3d6ba2158c2634e984",  },
Check leading zeros are not stripped (17481)
The ::1.2.3.4 format used to be valid but was deprecated  in rfc4291 section 2.5.5.1
setting the timezone requires a database query on PostgreSQL.
See ticket 20212
This would fail with "TypeError: can't pickle instancemethod objects",  only on Python 2.X.
Try the variant protocol levels.
This would fail with "TypeError: expected string or Unicode object, NoneType found".
dt is ambiguous in Europe/Copenhagen. LocalTimezone guesses the  offset (and gets it wrong 50% of the time) while pytz refuses the  temptation to guess. In any case, this shouldn't crash.
Try all formatters that involve self.timezone.
astimezone() is safe here because the target timezone doesn't have DST
Regression test for 18951
3h30m to the west of UTC
Ticket 16924 -- We don't need timezone support to test this
An importable child
A child that exists, but will generate an import error if loaded
A child that doesn't exist
A child that doesn't exist, but is the name of a package on the path
A module which doesn't have a __path__ (so no submodules)
An importable child
A child that exists, but will generate an import error if loaded
A child that doesn't exist
An importable child
A child that exists, but will generate an import error if loaded
A child that doesn't exist
Test exceptions raised
Same as test_setattr but in reversed order
Refs 21840
__contains__ doesn't work when the haystack is a string and the needle a LazyObject
See ticket 16563
Copying a list works and returns the correct objects.
Copying a list doesn't force evaluation.
Copying a class works and returns the correct objects.
Copying a class doesn't force evaluation.
Deep copying a list works and returns the correct objects.
Deep copying doesn't force evaluation.
Deep copying a class works and returns the correct objects.
Deep copying doesn't force evaluation.
By inheriting from LazyObjectTestCase and redefining the lazy_wrap()  method which all testcases use, we get to make sure all behaviors  tested in the parent testcase also apply to SimpleLazyObject.
First, for an unevaluated SimpleLazyObject  __repr__ contains __repr__ of setup function and does not evaluate  the SimpleLazyObject
See ticket 19456
See ticket 18447
Test every pickle protocol available
Test every pickle protocol available  Test both if we accessed a field on the model and if we didn't.  Assert that there were no warnings.
-*- encoding: utf-8 -*-
docstring should be preserved
check that it is cached
check that it returns the right thing
check that state isn't shared between instances
check that it behaves like a property when there's no instance
check that overriding name works
NOTE: \xa0 avoids wrapping between value and unit
%y will error before this date
Refs 23664
AttributeError: ImmutableList object is immutable.
AttributeError: Object is immutable!
-*- coding: utf-8 -*-
Substitution patterns for testing the above items.  Check repeated values.  Verify it doesn't double replace &.
caused infinite loop on Pythons not patched with  http://bugs.python.org/issue20288
Some convoluted syntax for which parsing may differ between python versions
Test with more lengthy content (also catching performance regressions)
Strings that should come out untouched.  Strings that have spaces to strip.
Ensure that IDNs are properly quoted  Ensure that everything unsafe is quoted, !*'();:@&=+$,/?[]~ is considered safe as per RFC
defines __html__ on its own
overrides __unicode__ and is marked as html_safe
defines __html__ on its own
overrides __str__ and is marked as html_safe
-*- encoding: utf-8 -*-
2-tuples (the norm)
A dictionary  Need to allow all of these as dictionaries have to be treated as  unordered
A MultiValueDict  MultiValueDicts are similarly unordered
reciprocity works
bad input
more explicit output testing
Check binary URLs, regression tests for 26308
Valid basic auth credentials are allowed.  A path without host is allowed.  Basic auth without host is not allowed.
start with the same children of node1 then add an item  add() returns the added data  we added exactly one item, len() should reflect that
negated is False by default
-*- encoding: utf-8 -*-
-*- coding: utf-8 -*-
Ensure that we normalize our unicode data first
Ensure the final length is calculated correctly when there are  combining characters with no precomposed form, and that combining  characters are not split up.
Ensure the length of the end text is correctly calculated when it  contains combining characters with no precomposed form.
Make a best effort to shorten to the desired length, but requesting  a length shorter than the ellipsis shouldn't break  Ensure that lazy strings are handled correctly
Ensure that lazy strings are handled correctly
Test with new line inside tag
Test self-closing tags
Test html entities
given - expected - unicode?
Always start off in TEST_DIR.
Make sure that get_current() does not return a deleted Site object.
After updating a Site object (e.g. via the admin), we shouldn't return a  bogus value from the SITE_CACHE.
When all site objects are deleted the cache should also  be cleared and get_current() should raise a DoesNotExist.
Test that the correct Site object is returned
Test that an exception is raised if the sites framework is installed  but there is no matching Site
A RequestSite is returned if the sites framework is not installed
Host header without port
Host header with port - match, no fallback without port
Host header with port - no match, fallback without port
Host header with non-matching domain
Ensure domain for RequestSite always matches host header
Regression for 17320  Domain names are not allowed contain whitespace characters
Site exists in 'default' database so using='other' shouldn't clear.  using='default' should clear.
Test response msg for RequestSite.save NotImplementedError
Test response msg for RequestSite.delete NotImplementedError
Delete the site created as part of the default migration process.
does nothing
Temporarily pretending apps are not ready yet. This issue can happen  if the value of 'list_filter' refers to a 'through__field'.
The data is ignored, but let's check it doesn't crash the system  anyway.
Check some response details
Check some response details
Check some response details
Check some response details
Check that the response was a 302 (redirect)
Check if parameters are intact
Check that the response was a 301 (permanent redirect)
Check that the response was a 302 (non-permanent redirect)
Check that the response was a 302, and that  the attempt to get the redirection location returned 301 when retrieved
Check that the response was a 404, and that the content contains MAGIC
Check that the multi-value data has been rolled out ok
Check that the response was a 404
Check that the path in the response includes it (ignore that it's a 404)
Get the page without logging in. Should result in 302.
Log in
Request a page that requires a login
Get the page without logging in. Should result in 302.
Log in
Request a page that requires a login
Get the page without logging in. Should result in 302.
Log in
Request a page that requires a login
Get the page without logging in. Should result in 302.
Log in
Request a page that requires a login
Get the page without logging in. Should result in 302.
Log in
Request a page that requires a login
Get the page without logging in. Should result in 302.
Log in
Request a page that requires a login
Get the page without logging in. Should result in 302.
Log in
Request a page that requires a login
Log in
Request a page that requires a login
Log out
Request a page that requires a login
Log in
Request a page that requires a login
Log out
Request a page that requires a login
Get the page without logging in. Should result in 302.
Log in
Request a page that requires a login
Get the page without logging in. Should result in 302.
Log in
Log in with wrong permissions. Should result in 302.
Get the page without logging in. Should result in 403.
Log in
Log in with wrong permissions. Should result in 403.
Get the page without logging in. Should result in 302.
Log in
Log in with wrong permissions. Should result in 302.
Session value isn't set initially
Check that the session was modified
Try the same assertion, a different way
Check some response details
The normal client allows the post
The CSRF-enabled client rejects it
A mapping between names of HTTP/1.1 methods and their test views.
based on Python 3.3's gzip.compress
Do nothing for 200 responses.
Strip content for some status codes.
Issue 20472
Strip content for HEAD requests.
Don't bother validating the formset unless each form is valid
A minimal set of apps to avoid system checks running on all apps.
Passing options as arguments also works (thanks argparse)
Deactivate translation when set to true
Leaves locale from settings when set to false
raise an error if some --parameter is flowing from options to args
Use a simple string for forward declarations.
You can also explicitly specify the related app.
Create a Parent
Create some children
Set the best child  No assertion require here; if basic assignment and  deletion works, the test passes.
Writes in the outer block are rolled back too.
atomic block shouldn't rollback, but force it.
trigger a database error inside an inner atomic without savepoint  prevent atomic from rolling back since we're recovering manually
The tests access the database after exercising 'atomic', initiating  a transaction ; a rollback is required before restoring autocommit.
The third insert couldn't be roll back. Temporarily mark the  connection as not needing rollback to check it.  The second insert couldn't be roll back. Temporarily mark the  connection as not needing rollback to check it.
The first block has a savepoint and must roll back.
The third insert couldn't be roll back. Temporarily mark the  connection as not needing rollback to check it.  The second block has a savepoint and must roll back.
Make sure autocommit wasn't changed.
The transaction is marked as needing rollback.
Mark the transaction as no longer needing rollback.
The connection is closed and the transaction is marked as  needing rollback. This will raise an InterfaceError on databases  that refuse to create cursors on closed connections (PostgreSQL)  and a TransactionManagementError on other databases.  The connection is usable again .
We cannot synchronize the two threads with an event here  because the main thread locks. Sleep for a little while.  2) ... and this line deadlocks. (see below for 1)
This is the thread-local connection, not the main connection.
Double atomic to enter a transaction and create a savepoint.  1) This line locks... (see above for 2)
Must not raise an exception
Expect an error when rolling back a savepoint that doesn't exist.  Done outside of the transaction block to ensure proper recovery.
Start a plain transaction.
Swallow the intentional error raised in the sub-transaction.
Start a sub-transaction with a savepoint.
This is expected to fail because the savepoint no longer exists.
Ensure settings.py exists
Add permissions auth.add_customuser and auth.change_customuser
LogEntry.user column doesn't get altered to expect a UUID, so set an  integer manually to avoid causing an error.
-*- coding: utf-8 -*-
See ticket 10265
Uses a mocked version of PasswordResetTokenGenerator so we can change  the value of 'today'
This will put a 14-digit base36 timestamp into the token, which is too large.
Usernames to be passed in REMOTE_USER for the test_known_user test case.
Another request with same user should not create any new users.
Test that a different user passed in the headers causes the new user  to be logged in.
Set last_login to something so we can determine if it changes.
Known user authenticates  During the session, the REMOTE_USER header disappears. Should trigger logout.  verify the remoteuser middleware will not remove a user  authenticated via another backend
Known user authenticates  During the session, the REMOTE_USER changes to a different user.  Ensure that the current user is not the prior remote_user  In backends that create a new user, username is "newnewuser"  In backends that do not create new users, it is '' (anonymous user)
REMOTE_USER strings with email addresses for the custom backend to  clean.
Known user authenticates  Should stay logged in if the REMOTE_USER header disappears.
The custom_perms test messes with ContentTypes, which will  be cached; flush the cache to ensure there are no side effects  Refs 14975, 14925
reloading user to purge the _perm_cache
Re-set the password, because this tests overrides PASSWORD_HASHERS
The get_group_permissions test messes with ContentTypes, which will  be cached; flush the cache to ensure there are no side effects  Refs 14975, 14925
user_login_failed signal is sent.
Get a session for the test user
Prepare a request object
Remove NewModelBackend  Get the user from the request
Assert that the user retrieval is successful and the user is  anonymous as the backend is not longer available.
Prepare a request object
*After* rendering, we check whether the session was accessed
special urls for auth test cases
This line is only required to render the password reset with is_admin=True
password_reset_confirm invalid token
password_reset_confirm valid token
For testing that auth backends can be referenced using a convenience import
-*- coding: utf-8 -*-
The verification password is incorrect.
One (or both) passwords weren't given
The success case.
To verify that the login form rejects inactive users, use an authentication  backend that allows them.
The user is inactive.
The user is inactive.
The user is inactive, but our custom form policy allows them to log in.
If we want to disallow some logins according to custom logic,  we should raise a django.forms.ValidationError in the form.
The success case
The two new passwords do not match.
The two new passwords do not match.
The success case.
Regression test - check the order of fields:
Just check we can create it
Use the form to construct the POST data
The password field should be readonly, so anything  posted here should be ignored; the form will be  valid, and give back the 'initial' value for the  password field.
original hashed password contains $
When rendering the bound password field,  ReadOnlyPasswordHashWidget needs the initial  value to render correctly
This cleanup is necessary because contrib.sites cache  makes tests interfere with each other, see 11505
Since we're not providing a request object, we must provide a  domain_override to prevent the save operation from failing in the  potential case where contrib.sites is not installed. Refs 16412.
Since we're not providing a request object, we must provide a  domain_override to prevent the save operation from failing in the  potential case where contrib.sites is not installed. Refs 16412.
The form itself is valid, but no email is sent
Rendering the widget with value set to None  mustn't raise an exception.
-*- coding: utf-8 -*-
On some platforms (e.g. OpenBSD), crypt.crypt() always return None.
Blank passwords
Blank passwords
Blank passwords
Blank passwords
Alternate unsalted syntax  Blank passwords
Raw SHA1 isn't acceptable  Blank passwords
Blank passwords
Verify that password truncation no longer works  Blank passwords
Blank passwords
Generate a password with 4 rounds.
Check that no upgrade is triggered.
Revert to the old rounds count and ...
... check if the password would get updated to the new count.
Increasing rounds from 4 to 6 means an increase of 4 in workload,  therefore hardening should run 3 times to make the timing the  same (the original encode() call already ran once).
Get the original salt (includes the original workload factor)
Assert that the unusable passwords actually contain a random part.  This might fail one day due to a hash collision.
Generate a password with 1 iteration.
Check that no upgrade is triggered
Revert to the old iteration count and ...
... check if the password would get updated to the new iteration count.
Encode should get called once ...
... with the original salt and 5 iterations.
Check that no upgrade is triggered
Revert to the old iteration count and check if the password would get  updated to the new iteration count.
Correct password supplied, no hardening needed
Wrong password supplied, hardening needed
Python 3 adds quotes around module name
Blank passwords  Old hashes without version attribute
Generate hash with attr set to 1
Check that no upgrade is triggered.
Revert to the old rounds count and ...
... check if the password would get updated to the new count.
the is_active attr is provided by AbstractBaseUser
Maybe required?
Admin required fields
The extension user is a simple extension of the built-in user class,  adding a required date_of_birth field. This allows us to check for  any hard references to the name "User" in forms/handlers etc.
-*- coding: utf-8 -*-
optional multipart text/html email has been added.  Make sure original,  default functionality is 100% the same
Skip any 500 handler action (like sending more mail...)  This attack is based on the way browsers handle URLs. The colon  should be used to separate the port, but if the URL contains an @,  the colon is interpreted as part of a username for login purposes,  making 'evil.com' the request domain. Since HTTP_HOST is used to  produce a meaningful reset URL, we need to be certain that the  HTTP_HOST header isn't poisoned. This is done as a check when get_host()  is invoked, but we check here as a practical consequence.
Skip any 500 handler action (like sending more mail...)
Start by creating the email
redirect to a 'complete' page:
Let's munge the token in the path, but keep the same length,  in case the URLconf will reject a different length.
Ensure that we get a 200 response for a non-existent user, not a 404
Ensure that we get a 200 response for a base36 user id that overflows int
Same as test_confirm_invalid, but trying  to do a POST instead.
Check the password has not been changed
Check the password has been changed
Check we can't use the link again
16919 -- The ``password_reset_confirm`` view should pass the user  object to the ``SetPasswordForm``, even on GET requests.  For this test, we render ``{{ form.user }}`` in the template  ``registration/password_reset_confirm.html`` so that we can test this.
However, the view should NOT pass any user object on a form if the  password reset link was invalid.
Start by creating the email
redirect to a 'complete' page:  then submit a new password
instead of fixture
if the hash isn't updated, retrieving the redirection page will fail.
Those URLs should not pass the security check
These URLs *should* still pass the security check
15198  the custom authentication form used by this login asserts  that a request is passed to the form successfully.
Do a GET to establish a CSRF token  TestClient isn't used here as we're testing middleware, essentially.  get_token() triggers CSRF token inclusion in the response
Prepare the POST request
Use POST request to log in
Check the CSRF token switched
If no password change, session key should not be flushed.
Bug 14377
Bug 11223
Those URLs should not pass the security check
These URLs *should* still pass the security check
Create a new session with language
Redirect in test_user_change_password will fail if session auth hash  isn't updated after password change (21649)
Make me a superuser before logging in.
20078 - users shouldn't be allowed to guess password hashes via  repeated password__startswith queries.  A lookup that tries to filter on password isn't OK
Test the link inside password field help_text.
A LogEntry is created with pk=1 which breaks a FK constraint on MySQL
The LogEntry.user column isn't altered to a UUID type so it's set to  an integer manually in CustomUserAdmin to avoid an error. To avoid a  constraint error, delete the entry before constraints are checked  after the test.
getpass on Windows only supports prompt as bytestring (19807)
prompt should be encoded in Python 2. This line will raise an  Exception if prompt contains unencoded non-ASCII on Python 2.
'Julia' with accented 'u':
'Julia' with accented 'u':
We can use the management command to create a superuser
created password should be unusable
We can suppress output on the management command
We can use the management command to create a superuser  We skip validation because the temporary substitution of the  swappable User model messes with validation.
created password should be unusable
We can use the management command to create a superuser  We skip validation because the temporary substitution of the  swappable User model messes with validation.
Returns '1234567890' the first two times it is called, then  'password' subsequently.
The first two passwords do not match, but the second two do match and  are valid.
The first two passwords are empty strings, but the second two are  valid.
add/change/delete permission by default + custom permission
custom permission only since default permissions is empty
Unvailable contenttypes.ContentType  Unvailable auth.Permission
User not in database
Valid user with correct password
correct password, but user is inactive
Valid user with incorrect password
User not in database
Valid user with correct password'
Valid user with incorrect password
User not in database
Create a silo'd admin site for just the user/group admins.
Create test contenttypes for both databases
Now we create the test UserPermission
According to  http://tools.ietf.org/html/rfc3696section-3  the "@" symbol can be part of the local part of an email address
valid send_mail parameters  Test that one message has been sent.  Verify that test email contains the correct attributes:
Upgrade the password iterations
is_active is true by default  the is_active flag is saved
you can set the attribute - but it will not save  there should be no problem saving - but the attribute is not saved  the attribute is always true for newly retrieved instance
Works for modules and full permissions.
These are 'functional' level tests for common use cases.  Direct  testing of the implementation (SimpleLazyObject) is in the 'utils'  tests.  bug 12037 is tested by the {% url %} in the template:
See if this object can be used for queries where a Q() comparing  a user can be used with another Q() (in an AND or OR fashion).  This simulates what a template tag might do with the user from the  context. Note that we don't need to execute a query, just build it.  The failure case (bug 12049) on Python 2.4 with a LazyObject-wrapped  User is a fatal TypeError: "function() takes at least 2 arguments  (0 given)" deep inside deepcopy().  Python 2.5 and 2.6 succeeded, but logged internally caught exception  spew:     Exception RuntimeError: 'maximum recursion depth exceeded while     calling a Python object' in <type 'exceptions.AttributeError'>     ignored"
Tests for user equality.  This is hard because User defines  equality in a non-duck-typing way  See bug 12060
After password change, user should be anonymous  session should be flushed
-*- coding: utf-8 -*-
Check we can manually set an unusable password
Check username getter
Check authentication/permissions
Check API-based user creation with no password
Backwards-compatibility callables
Backwards-compatibility callables
Only a successful login will trigger the success signal.  verify the password is cleansed
Like this:
Ensure there were no more failures.
The log_out function will still trigger the signal for anonymous  users.
Table name is a list 15216
The following test fails on Oracle due to 17202 (can't correctly  inspect the length of character columns).
Regression test for 9991 - 'real' types in postgres
That's {field_name: (field_name_other_table, other_table)}
Removing a field shouldn't disturb get_relations (17785)
-*- coding:utf-8 -*-
logging config prior to using filter with mail_admins
If tests are invoke with "-Wall" (or any -W flag actually) then  warning logging gets disabled (see configure_logging in django/utils/log.py).  However, these tests expect warnings to be logged, so manually force warnings  to the logs. Use getattr() here because the logging capture state is  undocumented and (I assume) brittle.
Reset warnings state.
Ensure that AdminEmailHandler does not get filtered out  even with DEBUG=True.
Backup then override original filters
Restore original filters
Backup then override original filters  Restore original filters
Monkeypatches
Revert Monkeypatches
Text email
HTML email
validate is just an example command to trigger settings configuration
These queries combine results from the m2m and the m2o relationships.  They're three ways of saying the same thing.
An explicit link to the parent (we can control the attribute name).
The parent_link connector need not be the pk on the model.
Test parent_link connector can be discovered in abstract classes.
Check that abstract classes don't get m2m tables autocreated.
Check concrete -> abstract -> concrete inheritance
Check concrete + concrete -> concrete -> concrete
Create a child-parent-grandparent chain
Create a child-parent chain with an explicit parent link
Check that no extra parent objects have been created.
You can also update objects when using a raw save.
No extra parent objects after an update, either.
Note that the name has not changed  - name is an attribute of Place, not ItalianRestaurant
Regressions tests for 7105: dates() queries should be able to use  fields from the parent model as easily as the child.
Regression test for 7276: calling delete() on a model with  multi-table inheritance should delete the associated rows from any  ancestor tables, as well as any descendent objects.
This should delete both Restaurants, plus the related places, plus  the ItalianRestaurant.
Regression test for 7488. This looks a little crazy, but it's the  equivalent of what the admin interface has to do for the edit-inline  case.
Ordering should not include any database column more than once (this  is most likely to occur naturally with model inheritance, so we  check it here). Regression test for 9390. This necessarily pokes at  the SQL string for the query, since the duplicate problems are only  apparent at that late stage.
the child->parent link
All fields from an ABC, including those inherited non-abstractly  should be available on child classes (7588). Creating this instance  should work without error.
Check that many-to-many relations defined on an abstract base class  are correctly inherited (and created) on the child class.
Check that a subclass of a subclass of an abstract model doesn't get  its own accessor.
... but it does inherit the m2m from its parent
Regression test for 11369: verbose_name_plural should be inherited  from an ABC even when there are one or more intermediate  abstract models in the inheritance chain, for consistency with  verbose_name.
Regression test for 13987: Primary key is incorrectly determined  when more than one model has a concrete->abstract->concrete  inheritance hierarchy.
Test for 17502 - check that filtering through two levels of  inheritance chain doesn't generate extra joins.
It would be nice (but not too important) to skip the middle join in  this case. Skipping is possible as nothing from the middle model is  used in the qs and top contains direct pointer to the bottom model.
Regression test for 7246
-*- encoding: utf-8 -*-
.GET and .POST should be QueryDicts
and FILES should be MultiValueDict
With trailing slash  Without trailing slash
With trailing slash  Without trailing slash
Regression for 19468
There is a timing weakness in this test; The  expected result for max-age requires that there be  a very slight difference between the evaluated expiration  time, and the time evaluated in set_cookie(). If this  difference doesn't exist, the cookie time will be  1 second larger. To avoid the problem, put in a quick sleep,  which guarantees that there will be a time difference.
Slight time dependency; refs 23450
A compat cookie may be in use -- check that it has worked  both as an output string, and using the cookie attributes
Read all of a limited stream  Reading again returns nothing.
Read a number of characters greater than the stream has to offer  Reading again returns nothing.
Read sequentially from a stream  Reading again returns nothing.
Read lines from a stream  Read a full line, unconditionally  Read a number of characters less than a line  Read the rest of the partial line  Read a full line, with a character limit greater than the line length  Read the next line, deliberately terminated at the line end  Read the next line... just the line end  Read everything else.
Regression for 15018  If a stream contains a newline, but the provided length  is less than the number of provided characters, the newline  doesn't reset the available character count  Now expire the available characters  Reading again returns nothing.
Same test, but with read, not readline.
Because multipart is used for large amounts of data i.e. file uploads,  we don't want the data held in memory twice, and we don't want to  silence the error by setting body = '' either.
Ticket 9054  There are cases in which the multipart data is related instead of  being a binary upload, in which case it should still be accessible  via body.
According to:  http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.htmlsec14.13  Every request.POST with Content-Length >= 0 is a valid request,  this test ensures that we handle Content-Length == 0.
Same test without specifying content-type
Consume enough data to mess up the parsing:
Check if X_FORWARDED_HOST is provided.  X_FORWARDED_HOST is ignored.
Check if X_FORWARDED_HOST isn't provided.
Check if HTTP_HOST isn't provided.
Check if HTTP_HOST isn't provided, and we're on a nonstandard port
Poisoned host headers are rejected as suspicious
Check if X_FORWARDED_HOST is provided.  X_FORWARDED_HOST is obeyed.
Check if X_FORWARDED_HOST isn't provided.
Check if HTTP_HOST isn't provided.
Check if HTTP_HOST isn't provided, and we're on a nonstandard port
Poisoned host headers are rejected as suspicious
Shouldn't use the X-Forwarded-Port header
Should use the X-Forwarded-Port header
Invalid hostnames would normally raise a SuspiciousOperation,  but we have DEBUG=True, so this check is disabled.
//// is needed to create a request with a path beginning with //
//// is needed to create a request with a path beginning with //
//// is needed to create a request with a path beginning with //
//// is needed to create a request with a path beginning with //
Assign a unicode string as name to make sure the intermediary model is  correctly created. Refs 20207
Models to test correct related_name inheritance
Create a couple of Publications.
Create an Article.  You can't associate it with a Publication until it's been saved.  Save it!  Associate the Article with a Publication.  Create another Article, and set it to appear in both Publications.  Adding a second time is OK
Adding an object of the wrong type raises TypeError
Add a Publication directly via publications.add by using keyword arguments.
Adding via the 'other' end of an m2m
Adding via the other end using keywords
Article objects have access to their related Publication objects.  Publication objects have access to their related Article objects.
We can perform kwarg queries across m2m relationships
The count() function respects distinct() as well.
Excluding a related item works as you would expect, too (although the SQL  involved is a little complex).
Reverse m2m queries are supported (i.e., starting at the table that  doesn't have a ManyToManyField).
If we delete a Publication, its Articles won't be able to access it.  If we delete an Article, its Publications won't be able to access it.
Bulk delete some Publications - references to deleted publications should go
Bulk delete some articles - references to deleted objects should go  After the delete, the QuerySet cache needs to be cleared,  and the referenced objects should be gone
Removing publication from an article:  And from the other end
An alternate to calling clear() is to assign the empty set
Relation sets can be assigned using set().
An alternate to calling clear() is to set an empty set.
Relation sets can also be set using primary key values
Ensure that querysets used in m2m assignments are pre-evaluated  so their value isn't affected by the clearing operation in  ManyRelatedManager.set() (19816).
Ensure that querysets used in M2M assignments are pre-evaluated  so their value isn't affected by the clearing operation in  ManyRelatedManager.set() (19816).
Relation sets can be cleared:
And you can clear from the other end
Avoid validation
Disable reverse relation  Set unique to enable model cache.
Add virtual relation to the ArticleTranslation model.
Model for index_together being used only with single list
Indexing a TextField on Oracle or MySQL results in index creation error.
Ensure the index name is properly quoted
Test for using index_together with a single list (22172)
unique=True and db_index=True should only create the varchar-specific  index (19441).
-*- encoding: utf-8 -*-
Override the STATIC_ROOT for all tests from setUp to tearDown  rather than as a context manager  Same comment as in runtests.teardown.
Override settings
Restore original settings
Backup original environment variable
If contrib.staticfiles isn't configured properly, the exception  should bubble up to the main thread.
Restore original environment variable
skip it, as setUpClass doesn't call its parent either
Intentionally empty method so that the test is picked up by the  test runner and the overridden setUpClass() method is executed.
AppDirectoriesFinder searched locations  FileSystemFinder searched locations  DefaultStorageFinder searched locations
get modification and access times for no_label/static/file2.txt
prepare duplicate of file2.txt from a temporary app  this file will have modification time older than no_label/static/file2.txt  anyway it should be taken to STATIC_ROOT because the temporary app is before  'no_label' app in installed apps
run collectstatic again
If this string is in the collectstatic output, it means the warning we're  looking for was emitted.
Make sure the warning went away again.
Clear hashed files to avoid side effects among tests.
check if the cache is filled correctly as expected  clearing the cache to make sure we re-set it correctly in the url method
CSS files shouldn't be touched by JS patterns.
Confirm JS patterns have been applied to JS files.
collect the additional file
delete the original file form the app, collect with clear
Don't run collectstatic command in this test class.
Store a version of the original raster field for testing the exception  raised if GDAL isn't installed.
If psycopg is installed but not geos, the import path hits  django.contrib.gis.geometry.backend which will "helpfully" convert  an ImportError into an ImproperlyConfigured.  Here, we make sure we're only catching this specific case and not another  ImproperlyConfigured one.
Delete table after testing
Test GeometryColumns when available
Test spatial indices when available
Test spatial indices when available
Test GeometryColumns when available
Test spatial indices when available
Test spatial indices when available
Test GeometryColumns when available
Make sure the right tables exist  Unmigrate everything  Make sure it's all gone  Even geometry columns metadata  Not all GIS backends have geometry columns model  Revert the "unmigration"
Throwing a curveball w/`db_column` here.
TODO: Related tests for KML, GML, and distance lookups.
Reference data for what's in the fixtures.
All the transformations are to state plane coordinate systems using  US Survey Feet (thus a tolerance of 0 implies error w/in 1 survey foot).
Each city transformed to the SRID of their state plane coordinate system.
Doing this implicitly sets `select_related` select the location.  TODO: Fix why this breaks on Oracle.
Relations more than one level deep can be queried.
This combines the Extent and Union aggregates into one query
One for all locations, one that excludes New Mexico (Roswell).
The tolerance value is to four decimal places because of differences  between the Oracle and PostGIS spatial backends on the extent calculation.
This combines the Extent and Union aggregates into one query
These are the points that are components of the aggregate geographic  union that is returned.  Each point  corresponds to City PK.
The second union aggregate is for a union  query that includes limiting information in the WHERE clause (in other  words a `.filter()` precedes the call to `.aggregate(Union()`).
Ordering of points in the result of the union is not defined and  implementation-dependent (DB backend, GEOS version)
Regression test for 9752.
Constructing a dummy parcel border and getting the City instance for  assigning the FK.
First parcel has incorrect center point that is equal to the City;  it also has a second border that is different from the first as a  100ft buffer around the City.
Now creating a second Parcel where the borders are the same, just  in different coordinate systems.  The center points are also the  same (but in different coordinate systems), and this time they  actually correspond to the centroid of the border.
Should return the second Parcel, which has the center within the  border.
This time center2 is in a different coordinate system and needs  to be wrapped in transformation SQL.
Should return the first Parcel, which has the center point equal  to the point in the City ForeignKey.
This time the city column should be wrapped in transformation SQL.
Incrementing through each of the models, dictionaries, and tuples  returned by the different types of GeoQuerySets.  The values should be Geometry objects and not raw strings returned  by the spatial database.
The expected ID values -- notice the last two location IDs  are out of order.  Dallas and Houston have location IDs that differ  from their PKs -- this is done to ensure that the related location  ID column is selected instead of ID column for the city.
TODO: fix on Oracle -- qs2 returns an empty result for an unknown reason
TODO: fix on Oracle -- get the following error because the SQL is ordered  by a geometry object, which Oracle apparently doesn't like:   ORA-22901: cannot compare nested table or VARRAY or LOB attributes of an object type  The City, 'Fort Worth' uses the same location as Dallas.
Count annotation should be 2 for the Dallas location now.
Should only be one author (Trevor Paglen) returned by this query, and  the annotation should have 3 for the number of books, see 11087.  Also testing with a values(), see 11489.
TODO: fix on Oracle -- get the following error because the SQL is ordered  by a geometry object, which Oracle apparently doesn't like:   ORA-22901: cannot compare nested table or VARRAY or LOB attributes of an object type
TODO: The phantom model does appear on Oracle.  Should be `None`, and not a 'dummy' model.
Reference query:  SELECT AsText(ST_Collect("relatedapp_location"."point")) FROM "relatedapp_city" LEFT OUTER JOIN     "relatedapp_location" ON ("relatedapp_city"."location_id" = "relatedapp_location"."id")     WHERE "relatedapp_city"."state" = 'TX';
Even though Dallas and Ft. Worth share same point, Collect doesn't  consolidate -- that's why 4 points in MultiPoint.
This triggers TypeError when `get_default_columns` has no `local_only`  keyword.  The TypeError is swallowed if QuerySet is actually  evaluated as list generation swallows TypeError in CPython.
The coordinates of each city, with Z values corresponding to their  altitude in meters.
Reference mapping of city name to its altitude (Z value).
3D freeway data derived from the National Elevation Dataset:   http://seamless.usgs.gov/products/9arc.php
Bounding box polygon for inner-loop of Houston (in projected coordinate  system 32140), with elevation values from the National Elevation Dataset  (see above).
Interstate (2D / 3D and Geographic/Projected variants)
Creating a geographic and projected version of the  interstate in both 2D and 3D.
The VRT is 3D, but should still be able to map sans the Z.
The city shapefile is 2D, and won't be able to fill the coordinates  in the 3D model -- thus, a LayerMapError is raised.
3D model should take 3D data just fine.
Making sure LayerMapping.make_multi works right, by converting  a Point25D into a MultiPoint25D.
KML should be 3D.  `SELECT ST_AsKML(point, 6) FROM geo3d_city3d WHERE name = 'Houston';`
GeoJSON should be 3D  `SELECT ST_AsGeoJSON(point, 6) FROM geo3d_city3d WHERE name='Houston';`
PostGIS query that returned the reference EWKT for this test:   `SELECT ST_AsText(ST_Union(point)) FROM geo3d_city3d;`  Ordering of points in the resulting geometry may vary between implementations
`SELECT ST_Extent3D(point) FROM geo3d_city3d;`
Reference query for values below:   `SELECT ST_Perimeter3D(poly), ST_Perimeter2D(poly) FROM geo3d_polygon3d;`
ST_Length_Spheroid Z-aware, and thus does not need to use  a separate function internally.  `SELECT ST_Length_Spheroid(line, 'SPHEROID["GRS 1980",6378137,298.257222101]')     FROM geo3d_interstate[2d|3d];`
Making sure `ST_Length3D` is used on for a projected  and 3D model rather than `ST_Length`.  `SELECT ST_Length(line) FROM geo3d_interstateproj2d;`  `SELECT ST_Length3D(line) FROM geo3d_interstateproj3d;`
Mapping of City name to reference Z values.
KML should be 3D.  `SELECT ST_AsKML(point, 6) FROM geo3d_city3d WHERE name = 'Houston';`
GeoJSON should be 3D  `SELECT ST_AsGeoJSON(point, 6) FROM geo3d_city3d WHERE name='Houston';`
Reference query for values below:   `SELECT ST_Perimeter3D(poly), ST_Perimeter2D(poly) FROM geo3d_polygon3d;`
ST_Length_Spheroid Z-aware, and thus does not need to use  a separate function internally.  `SELECT ST_Length_Spheroid(line, 'SPHEROID["GRS 1980",6378137,298.257222101]')     FROM geo3d_interstate[2d|3d];`
Making sure `ST_Length3D` is used on for a projected  and 3D model rather than `ST_Length`.  `SELECT ST_Length(line) FROM geo3d_interstateproj2d;`  `SELECT ST_Length3D(line) FROM geo3d_interstateproj3d;`
Mapping of City name to reference Z values.
Path where reference test data is located.
Shapefile is default extension, unless specified otherwise.
Converting lists to tuples of certain keyword args  so coordinate test cases will match (JSON has no  concept of tuple).
Load up the test geometry data from fixture into global.
Assert correct values for file based raster  Create in-memory rasters and change gtvalues
Create uint8 raster with full pixel data range (0-255)
Get array from raster
Assert data is same as original input
Prepare tempfile
Create file-based raster from scratch
Reload newly created raster from file
Create in memory raster
Test altering the scale, width, and height of a raster
Test altering the name and datatype (to float)
Prepare tempfile and nodata value
Create in file based raster
Transform raster into srid 4326.
Reload data from disk
The reprojection of a raster that spans over a large area  skews the data matrix and might introduce nodata values.
Check that statistics are persisted into PAM file on band close
Close band and remove file if created
Open raster in read mode
Setting attributes in write mode raises exception in the _flush method
Create in-memory raster and get band
Set nodata value
Set data for entire dataset
Prepare data for setting values in subsequent tests
Set data from list
Set data from packed block
Set data from bytes
Set data from bytearray
Set data from memoryview
Set data from numpy array
Test json input data
Populate statistics cache  Change data  Statistics are properly updated  Change nodata_value  Statistics are properly updated
Variations for input (data, shape, expected result).
List of acceptable data sources.
Loading up the data source
Making sure the layer count is what's expected (only 1 layer in a SHP file)
Making sure GetName works
Making sure the driver name matches up
Making sure indexing works
Incrementing through each layer, this tests DataSource.__iter__  Making sure we get the number of features we expect
Making sure we get the number of fields we expect
Testing the layer's extent (an Envelope), and its properties  There's a known GDAL regression with retrieving the extent  of a VRT layer in versions 1.7.0-1.7.2:   http://trac.osgeo.org/gdal/ticket/3783
Now checking the field names.
Negative FIDs are not allowed.
Testing `Layer.get_fields` (which uses Layer.__iter__)
Testing `Layer.__getitem__`.  Maybe this should be in the test below, but we might as well test  the feature values here while in this loop.
Using the first data-source because the same slice  can be used for both the layer and the control values.
See ticket 9448.  This DataSource object is not accessible outside this  scope.  However, a reference should still be kept alive  on the `Layer` returned.
Making sure we can call OGR routines on the Layer returned.
Same issue for Feature/Field objects, see 18640
Incrementing through each layer  Incrementing through each feature in the layer  Making sure the number of fields, and the geometry type  are what's expected.
Making sure the fields match to an appropriate OFT type.  Making sure we get the proper OGR Field instance, using  a string value index for the feature.
Testing Feature.__iter__
Incrementing through each layer and feature.
Making sure we get the right Geometry name & type
Making sure the SpatialReference is as expected.  Depending on lib versions, WGS_84 might be WGS_1984
When not set, it should be None.
Must be set a/an OGRGeometry or 4-tuple.
Setting the spatial filter with a tuple/list with the extent of  a buffer centering around Pueblo.
Setting the spatial filter with an OGRGeometry for buffer centering  around Houston.
Clearing the spatial filter by setting it to None.  Now  should indicate that there are 3 features in the Layer.
Using *.dbf from Census 2010 TIGER Shapefile for Texas,  which has land area ('ALAND10') stored in a Real field  with no precision.  Reference value obtained using `ogrinfo`.
Some Spatial Reference examples  This is really ESRI format, not WKT -- but the import should work the same
Well-Known Names
OGRGeomType should initialize on all these inputs.
Should throw TypeError on this input
Equivalence can take strings, ints, and other OGRGeomTypes
Testing the Django field name equivalent property.
'Geometry' initialization implies an unknown geometry type.
First with ewkt output when no SRID in EWKT  No test consumption with an SRID specified.
In GDAL 1.8, the non-conformant GML tag  <gml:GeometryCollection> was  replaced with <gml:MultiGeometry>.
Constructing w/HEX
Constructing w/WKB.
Loading jsons to prevent decimal differences
Test input with some garbage content (but valid json) (15529)
Testing the x, y properties.
self.assertEqual(101, lr.geom_type.num)
Testing `from_bbox` class method
Testing area & centroid.
Testing equivalence
Both rings in this geometry are not closed.
Creating a geometry w/spatial reference
Ensuring that SRS is propagated to clones.
Ensuring all children geometries (polygons and their rings) all  return the assigned spatial reference as well.
Ensuring SRS propagate in topological ops.
Instantiating w/an integer SRID
Incrementing through the multipolygon after the spatial reference  has been re-assigned.  Changing each ring in the polygon
Using the `srid` property.
srs/srid may be assigned their own values, even when srs is None.
Using an srid, a SpatialReference object, and a CoordTransform object  or transformations.
Testing use of the `clone` keyword.
Making sure the coordinate dimension is still 2D.
Can't insert a Point into a MultiPolygon.
GeometryCollection.add may take an OGRGeometry (if another collection  of the same type all child geoms will be added individually) or WKT.
The xmin, ymin, xmax, ymax of the MultiPoint should be returned.  Testing on the 'real world' Polygon.
A bug in GDAL versions prior to 1.7 changes the coordinate  dimension of a geometry after it has been transformed.  This test ensures that the bug workarounds employed within  `OGRGeometry.transform` indeed work.
For both the 2D and 3D MultiLineString, ensure _both_ the dimension  of the collection and the component LineString have the expected  coordinate dimension after transform.
vector  raster
Same as `City` above, but for testing model inheritance.
Mapping dictionaries for the models above.  ForeignKey's use another mapping dictionary for the _related_ Model (State in this case).
-*- coding: utf-8 -*-
Dictionaries to hold what's expected in the county shapefile.
Model field that does not exist.
Shapefile field that does not exist.
Nonexistent geographic field type.
Incrementing through the bad mapping dictionaries and  ensuring that a LayerMapError is raised.
A LookupError should be thrown for bogus encodings.
Setting up for the LayerMapping.
There should be three cities in the shape file.
Opening up the shapefile, and verifying the values in each  of the features made it to the model.
Comparing the geometries.
When the `strict` keyword is set an error encountered will force  the importation to stop.
This LayerMapping should work b/c `strict` is not set.
Two interstate should have imported correctly.
Verifying the values in the layer w/the model.
Only the first two features of this shapefile are valid.
Everything but the first two decimal digits were truncated,  because the Interstate model's `length` field has decimal_places=2.
Should only be one record b/c of `unique` keyword.
Multiple records because `unique` was not set.
Telling LayerMapping that we want no transformations performed on the data.
Specifying the source spatial reference system via the `source_srs` keyword.
Unique may take tuple or string parameters.
Testing invalid params for the `unique` keyword.
No source reference system defined in the shapefile, should raise an error.
Passing in invalid ForeignKey mapping parameters -- must be a dictionary  mapping for the model the ForeignKey points to.
There exist no State models for the ForeignKey mapping to work -- should raise  a MissingForeignKey exception (this error would be ignored if the `strict`  keyword is not set).
Now creating the state models so the ForeignKey mapping may work.
If a mapping is specified as a collection, all OGR fields that  are not collections will be converted into them.  For example,  a Point column would be converted to MultiPoint. Other things being done  w/the keyword args:   `transform=False`: Specifies that no transform is to be done; this     has the effect of ignoring the spatial reference check (because the     county shapefile does not have implicit spatial reference info).   `unique='name'`: Creates models on the condition that they have     unique county names; geometries from each feature however will be     appended to the geometry collection of the unique model.  Thus,     all of the various islands in Honolulu county will be in in one     database record with a MULTIPOLYGON type.
A reference that doesn't use the unique keyword; a new database record will  created for each polygon.
The county helper is called to ensure integrity of County models.
Function for clearing out all the counties before testing.
Initializing the LayerMapping object to use in these tests.
Bad feature id ranges should raise a type error.
Features IDs 3 & 4 are for Galveston County, Texas -- only  one model is returned because the `unique` keyword was set.
Features IDs 5 and beyond for Honolulu County, Hawaii, and  FID 0 is for Pueblo County, Colorado.
Only Pueblo & Honolulu counties should be present because of  the `unique` keyword.  Have to set `order_by` on this QuerySet  or else MySQL will return a different ordering than the other dbs.
Testing the `step` keyword -- should get the same counties  regardless of we use a step that divides equally, that is odd,  or that is larger than the dataset.
Parent model has geometry field.
Grandparent has geometry field.
-*- coding: utf-8 -*-
Note: Requires both the GeoIP country and city datasets.  The GEOIP_DATA path should be the only setting set (the directory  should contain links or the actual database files 'GeoLite2-City.mmdb' and  'GeoLite2-City.mmdb'.
Only passing in the location of one database.
Improper parameters.
No city database available, these calls should fail.
Non-string query should raise TypeError
Country queries should still work.
City information dictionary.
Some databases have only unaccented countries
Same test with a 25D-type geometry field
Getting the database identifier used by OGR, if None returned  GDAL does not have the support compiled in.
Writing shapefiles via GDAL currently does not support writing OGRTime  fields, so we need to actually use a database
The ordering of model fields might vary depending on several factors (version of GDAL, etc.)
Some backends may have srid=-1
Map from the django backend into the OGR driver name and database identifier  http://www.gdal.org/ogr/ogr_formats.html  TODO: Support Oracle (OCI).
Ensure that GDAL library has driver support for the database.
SQLite/Spatialite in-memory databases
Build the params of the OGR database connection string
Don't add the parameter if it is not in django's settings
This is an inherited model from City
Ensuring that data was loaded from initial data fixtures.
Testing on a Point
Making sure TypeError is thrown when trying to set with an   incompatible type.
Now setting with a compatible GEOS Geometry, saving, and ensuring   the save took, notice no SRID is explicitly set.
Ensuring that the SRID is automatically set to that of the   field after assignment, but before saving.
Ensuring the point was saved correctly after saving
Setting the X and Y of the Point  Checking assignments pre & post-save.
Testing on a Polygon
Creating a State object using a built Polygon
Testing the `ogr` and `srs` lazy-geometry properties.
Changing the interior ring on the poly attribute.
San Antonio in 'WGS84' (SRID 4326)
Oracle doesn't have SRID 3084, using 41157.  San Antonio in 'Texas 4205, Southern Zone (1983, meters)' (SRID 41157)  Used the following Oracle SQL to get this value:   SELECT SDO_UTIL.TO_WKTGEOMETRY(     SDO_CS.TRANSFORM(SDO_GEOMETRY('POINT (-98.493183 29.424170)', 4326), 41157))   )   FROM DUAL;  San Antonio in 'NAD83(HARN) / Texas Centric Lambert Conformal' (SRID 3084)  Used ogr.py in gdal 1.4.1 for this transform
Constructing & querying with a point from a different SRID. Oracle  `SDO_OVERLAPBDYINTERSECT` operates differently from  `ST_Intersects`, so contains is used instead.
Creating San Antonio.  Remember the Alamo.
Now verifying that San Antonio was transformed correctly
If the GeometryField SRID is -1, then we shouldn't perform any  transformation if the SRID of the input geometry is different.
Creating a Pennsylvanian city.
All transformation SQL will need to be performed on the  _parent_ table.
Only PostGIS would support a 'select *' query because of its recognized  HEXEWKB format for geometry fields
Reload now dumped data
Getting Texas, yes we were a country -- once ;)
Seeing what cities are in Texas, should get Houston and Dallas,   and Oklahoma City because 'contained' only checks on the   _bounding box_ of the Geometries.
Pulling out some cities.
Testing `contains` on the states using the point for Lawrence.
OK City is contained w/in bounding box of Texas.
Getting the borders for Colorado & Kansas
These cities should be strictly to the right of the CO border.
These cities should be strictly to the right of the KS border.
Note: Wellington has an 'X' value of 174, so it will not be considered   to the left of CO.
Creating a state with a NULL boundary.
Querying for both NULL and Non-NULL values.
Puerto Rico should be NULL (it's a commonwealth unincorporated territory)
The valid states should be Colorado & Kansas
Saving another commonwealth w/a NULL geometry.
Assigning a geometry and saving -- then UPDATE back to NULL.
To make things more interesting, we will have our Texas reference point in  different SRIDs.
Not passing in a geometry as first param should  raise a type error when initializing the GeoQuerySet
Making sure the right exception is raised for the given  bad arguments.
Relate works differently for the different backends.  TODO: This is not quite the same as the PostGIS mask above
Testing contains relation mask.
Testing within relation mask.
Testing intersection relation mask.
XXX For some reason SpatiaLite does something screwy with the Texas geometry here.  Also,  XXX it doesn't like the null intersection.
Should be able to execute the queries; however, they won't be the same  as GEOS (because Oracle doesn't use GEOS internally like PostGIS or  SpatiaLite).
Ordering might differ in collections
Reference query:  `SELECT ST_extent(point) FROM geoapp_city WHERE (name='Houston' or name='Dallas');`    =>  BOX(-96.8016128540039 29.7633724212646,-95.3631439208984 32.7820587158203)
Reference query:  SELECT ST_GeoHash(point) FROM geoapp_city WHERE name='Houston';  SELECT ST_GeoHash(point, 5) FROM geoapp_city WHERE name='Houston';
Only PostGIS and SpatiaLite support GeoJSON.
Precision argument should only be an integer
Reference queries and values.  SELECT ST_AsGeoJson("geoapp_city"."point", 8, 0)  FROM "geoapp_city" WHERE "geoapp_city"."name" = 'Pueblo';
SELECT ST_AsGeoJson("geoapp_city"."point", 8, 2) FROM "geoapp_city"  WHERE "geoapp_city"."name" = 'Houston';  This time we want to include the CRS by using the `crs` keyword.
SELECT ST_AsGeoJson("geoapp_city"."point", 8, 1) FROM "geoapp_city"  WHERE "geoapp_city"."name" = 'Houston';  This time we include the bounding box by using the `bbox` keyword.
SELECT ST_AsGeoJson("geoapp_city"."point", 5, 3) FROM "geoapp_city"  WHERE "geoapp_city"."name" = 'Chicago';  Finally, we set every available keyword.
Should throw a TypeError when trying to obtain GML from a  non-geometry field.
No precision parameter for Oracle :-/
Should throw a TypeError when trying to obtain KML from a   non-geometry field.
Ensuring the KML is as expected.
MakeLine on an inappropriate field returns simply None  Reference query:  SELECT AsText(ST_MakeLine(geoapp_city.point)) FROM geoapp_city;  We check for equality with a tolerance of 10e-5 which is a lower bound  of the precisions of ref_line coordinates
Both 'countries' only have two geometries.
Oracle and PostGIS 2.0+ will return 1 for the number of  geometries on non-collections.
Oracle cannot count vertices in Point geometries.
Reference values.  SELECT SDO_UTIL.TO_WKTGEOMETRY(SDO_GEOM.SDO_POINTONSURFACE(GEOAPP_COUNTRY.MPOLY, 0.05))  FROM GEOAPP_COUNTRY;
Using GEOSGeometry to compute the reference point on surface values  -- since PostGIS also uses GEOS these should be the same.
XXX This seems to be a WKT-translation-related precision issue?
Let's try and break snap_to_grid() with bad combinations of arguments.
Boundary for San Marino, courtesy of Bjorn Sandvik of thematicmapping.org  from the world borders dataset he provides.
Because floating-point arithmetic isn't exact, we set a tolerance  to pass into GEOS `equals_exact`.
SELECT AsText(ST_SnapToGrid("geoapp_country"."mpoly", 0.1)) FROM "geoapp_country"  WHERE "geoapp_country"."name" = 'San Marino';
SELECT AsText(ST_SnapToGrid("geoapp_country"."mpoly", 0.05, 0.23)) FROM "geoapp_country"  WHERE "geoapp_country"."name" = 'San Marino';
SELECT AsText(ST_SnapToGrid("geoapp_country"."mpoly", 0.5, 0.17, 0.05, 0.23)) FROM "geoapp_country"  WHERE "geoapp_country"."name" = 'San Marino';
SELECT AsSVG(geoapp_city.point, 0, 8) FROM geoapp_city WHERE name = 'Pueblo';  Even though relative, only one point so it's practically the same except for  the 'c' letter prefix on the x,y values.
Pre-transformed points for Houston and Pueblo.
Asserting the result of the transform operation with the values in   the pre-transformed points.  Oracle does not have the 3084 SRID.
XXX The low precision is for SpatiaLite
Houston, Dallas -- Ordering may differ depending on backend or GEOS version.  Using `field_name` keyword argument in one query and specifying an  order in the other (which should not be used because this is  an aggregate method on a spatial column)
Uses `GEOSGeometry` in `item_geometry`  Uses a 2-tuple in `item_geometry`
Making sure the box got added to the second GeoRSS feed.
Incrementing through the feeds.  Ensuring the georss namespace was added to the <rss> element.
Ensuring the georss element was added to each item in the feed.
Making sure the box got added to the second GeoRSS feed.
Ensuring the georsss namespace was added to the <feed> element.
Ensuring the georss element was added to each entry in the feed.
Ensuring the geo namespace was added to the <feed> element.
Ensuring the geo:lat and geo:lon element was added to each item in the feed.
geometry_field is considered even if not in fields (26138).
Without coordinate transformation, the serialization should succeed:  Coordinate transformations need GDAL
-*- encoding: utf-8 -*-
contrived example, but need a geo lookup paired with an id__in lookup
.count() should not throw TypeError in __eq__
TODO: fix on Oracle -- get the following error because the SQL is ordered  by a geometry object, which Oracle apparently doesn't like:   ORA-22901: cannot compare nested table or VARRAY or LOB attributes of an object type
verify types -- shouldn't be 0/1  verify values
Only PostGIS and SpatiaLite support GeoJSON.
Precision argument should only be an integer
Reference queries and values.  SELECT ST_AsGeoJson("geoapp_city"."point", 8, 0)  FROM "geoapp_city" WHERE "geoapp_city"."name" = 'Pueblo';
SELECT ST_AsGeoJson("geoapp_city"."point", 8, 2) FROM "geoapp_city"  WHERE "geoapp_city"."name" = 'Houston';  This time we want to include the CRS by using the `crs` keyword.
SELECT ST_AsGeoJson("geoapp_city"."point", 8, 1) FROM "geoapp_city"  WHERE "geoapp_city"."name" = 'Houston';  This time we include the bounding box by using the `bbox` keyword.
SELECT ST_AsGeoJson("geoapp_city"."point", 5, 3) FROM "geoapp_city"  WHERE "geoapp_city"."name" = 'Chicago';  Finally, we set every available keyword.
Should throw a TypeError when trying to obtain GML from a  non-geometry field.
No precision parameter for Oracle :-/
Should throw a TypeError when trying to obtain KML from a  non-geometry field.
Ensuring the KML is as expected.
SELECT AsSVG(geoapp_city.point, 0, 8) FROM geoapp_city WHERE name = 'Pueblo';  Even though relative, only one point so it's practically the same except for  the 'c' letter prefix on the x,y values.
SpatiaLite and Oracle do something screwy with the Texas geometry.
SpatiaLite and Oracle do something screwy with the Texas geometry.
Reference query:  SELECT ST_GeoHash(point) FROM geoapp_city WHERE name='Houston';  SELECT ST_GeoHash(point, 5) FROM geoapp_city WHERE name='Houston';
When the intersection is empty, Spatialite and MySQL return None
When the intersection is empty, Oracle returns an empty string
Create projected country objects, for this test to work on all backends.  Test in projected coordinate system  Some backends (e.g. Oracle) cannot group by multipolygon values, so  defer such fields in the aggregation query.  If the result is a measure object, get value.
Both 'countries' only have two geometries.
Oracle and PostGIS return 1 for the number of geometries on  non-collections, whereas MySQL returns None.
Spatialite and MySQL can only count points on LineStrings
Oracle cannot count vertices in Point geometries.
Reference values.  SELECT SDO_UTIL.TO_WKTGEOMETRY(SDO_GEOM.SDO_POINTONSURFACE(GEOAPP_COUNTRY.MPOLY, 0.05))  FROM GEOAPP_COUNTRY;  Using GEOSGeometry to compute the reference point on surface values  -- since PostGIS also uses GEOS these should be the same.
Test float/Decimal values
Let's try and break snap_to_grid() with bad combinations of arguments.
Boundary for San Marino, courtesy of Bjorn Sandvik of thematicmapping.org  from the world borders dataset he provides.
Because floating-point arithmetic isn't exact, we set a tolerance  to pass into GEOS `equals_exact`.
SELECT AsText(ST_SnapToGrid("geoapp_country"."mpoly", 0.1)) FROM "geoapp_country"  WHERE "geoapp_country"."name" = 'San Marino';
SELECT AsText(ST_SnapToGrid("geoapp_country"."mpoly", 0.05, 0.23)) FROM "geoapp_country"  WHERE "geoapp_country"."name" = 'San Marino';
SELECT AsText(ST_SnapToGrid("geoapp_country"."mpoly", 0.5, 0.17, 0.05, 0.23)) FROM "geoapp_country"  WHERE "geoapp_country"."name" = 'San Marino';
Oracle does something screwy with the Texas geometry.
Pre-transformed points for Houston and Pueblo.
Asserting the result of the transform operation with the values in   the pre-transformed points.
The low precision is for SpatiaLite
Some combined function tests
For some reason SpatiaLite does something screwy with the Texas geometry here.  Also, it doesn't like the null intersection.
Should be able to execute the queries; however, they won't be the same  as GEOS (because Oracle doesn't use GEOS internally like PostGIS or  SpatiaLite).
Undefined ordering
This should attach a <georss:box> element for the extent of  of the cities in the database.  This tuple came from  calling `City.objects.aggregate(Extent())` -- we can't do that call  here because `Extent` is not implemented for MySQL/Oracle.
Returning a simple tuple for the geometry.
This time we'll use a 2-tuple of coordinates for the box.
The following feeds are invalid, and will raise exceptions.
The feed dictionary to use for URLs.
Ensuring the right sitemaps namespace is present.
Getting the relative URL since we don't have a real site.
Have to decompress KMZ before parsing.
Ensuring the correct number of placemarks are in the KML doc.
-*- coding: utf-8 -*-
Naive check to see if there is DNS available to use.  Used to conditionally skip fqdn geoip checks.  See 25407 for details.
Only passing in the location of one database.
Improper parameters.
No city database available, these calls should fail.
Non-string query should raise TypeError
Country queries should still work.
City information dictionary.
Some databases have only unaccented countries
-*- coding: utf-8 -*-
this would work:  self._list = self._mytype(items)  but then we wouldn't be testing length parameter
Also works with a custom IndexError
Creating a WKTReader instance
read() should return a GEOSGeometry
Should only accept six.string_types objects.
Creating a WKTWriter instance, testing its ptr property.
Creating a WKBReader instance
read() should return a GEOSGeometry on either a hex string or  a WKB buffer.
Representations of 'POINT (5 23)' in hex -- one normal and  the other with the byte order changed.
Ensuring bad byteorders are not accepted.  Equivalent of `wkb_w.byteorder = bad_byteorder`
Setting the byteorder to 0 (for Big Endian)
Back to Little Endian
Now, trying out the 3D and SRID flags.
Ensuring bad output dimensions are not accepted
Now setting the output dimensions to be 3
Telling the WKBWriter to include the srid in the representation.
Testing out GEOSBase class, which provides a `ptr` property  that abstracts out access to underlying C pointers.
This one only accepts pointers to floats
Default ptr_type is `c_void_p`.  Default ptr_type is C float pointer
These assignments are OK -- None is allowed because  it's equivalent to the NULL pointer.
Because pointers have been set to NULL, an exception should be  raised when we try to access it.  Raising an exception is  preferable to a segmentation fault that commonly occurs when  a C method is given a NULL memory reference.  Equivalent to `fg.ptr`
Anything that is either not None or the acceptable pointer type will  result in a TypeError when trying to assign it to the `ptr` property.  Thus, memory addresses (integers) and pointers of the incorrect type  (in `bad_ptrs`) will not be allowed.  Equivalent to `fg.ptr = bad_ptr`
For testing HEX(EWKB).  `SELECT ST_AsHEXEWKB(ST_GeomFromText('POINT(0 1)', 4326));`  `SELECT ST_AsHEXEWKB(ST_GeomFromEWKT('SRID=4326;POINT(0 1 2)'));`
OGC-compliant HEX will not have SRID value.
HEXEWKB should be appropriate for its dimension -- have to use an  a WKBWriter w/dimension set accordingly, else GEOS will insert  garbage into 3D coordinate if there is none.
Same for EWKB.
Redundant sanity check.
string-based
Bad WKB
Some other object  None
we need to do this so decimal places get normalized
we need to do this so decimal places get normalized
Loading jsons to prevent decimal differences
Other tests use `fromfile()` on string filenames so those  aren't tested here.
Error shouldn't be raise on equivalence testing with  an invalid type.
Creating the point from the WKT
Making sure that the point's X, Y components are what we expect
Testing the third dimension, and getting the tuple arguments
Centroid operation on point should be point itself
Now testing setting the x and y
Setting via the tuple/coords property
Point individual arguments
Creating a LinearRing from a tuple, list, and numpy array
Testing numerical precision
Creating the Polygon, testing its properties.
Area & Centroid
Testing the geometry equivalence  Should not be equal to previous geometry
Testing the exterior ring
Testing __getitem__ and __setitem__ on invalid indices
Testing __iter__
Testing polygon construction.
Polygon(shell, (hole1, ... holeN))
Polygon(shell_tuple, hole_tuple1, ... , hole_tupleN)
Constructing with tuples of LinearRings.
Accessing Polygon attributes in templates should work.
Getting a polygon with interior rings, and pulling out the interior rings
These deletes should be 'harmless' since they are done on child geometries
Deleting the polygon
Access to these rings is OK since they are clones.
Constructing the polygon and getting the coordinate sequence
Checks __getitem__ and __setitem__
Constructing the test value to set the coordinate sequence with
Making sure every set point matches what we expect
The buffer we expect
Can't use a floating-point for the number of quadsegs.
Constructing our buffer
Now assuring that each point in the buffer is almost equal  Asserting the X, Y of each point are almost equal (due to floating point imprecision)
Testing SRID keyword on Point
Testing SRID keyword on fromstr(), and on Polygon rings.
Testing SRID keyword on GeometryCollection
GEOS may get the SRID from HEXEWKB  'POINT(5 23)' at SRID=4326 in hex form -- obtained from PostGIS  using `SELECT GeomFromText('POINT (5 23)', 4326);`.
Testing that geometry SRID could be set to its own value
Test conversion from custom to a known srid
Testing the mutability of Polygons
Should only be able to use __setitem__ with LinearRing geometries.
Constructing the new shell by adding 500 to every point in the old shell.
Assigning polygon's exterior ring w/the new shell
Testing the mutability of Geometry Collections  Creating a random point.  Testing the assignment
MultiPolygons involve much more memory management because each  Polygon w/in the collection has its own rings.  Offsetting the each ring in the polygon by 500.
Testing the assignment
Testing a 3D Point
Testing a 3D LineString
Distance to self should be 0.
Distance should be 1
Distance should be ~ sqrt(2)
Distances are from the closest vertex in each geometry --   should be 3 (distance from (2, 2) to (5, 2)).
Points have 0 length.
Should be ~ sqrt(2)
Should be circumference of Polygon
Should be sum of each element's length in collection.
Testing len() and num_geom.
Testing __getitem__ (doesn't work on Point or Polygon)
Creating a GeometryCollection WKT string composed of other  collections and polygons.
Should construct ok from WKT
Should also construct ok from individual geometry arguments.
And, they should be equal.
Using a srid, a SpatialReference object, and a CoordTransform object  for transformations.
Testing use of the `clone` keyword.
transform() should no-op if source & dest SRIDs match,  regardless of whether GDAL is available.
The xmin, ymin, xmax, ymax of the MultiPoint should be returned.  Extent of points is just the point itself repeated.  Testing on the 'real world' Polygon.
Using both pickle and cPickle -- just 'cause.
Creating a list of test geometries for pickling,  and setting the SRID on some of them.
Creating a simple multipolygon and getting a prepared version.
A set of test points.  Results should be the same (but faster)
Original geometry deletion should not crash the prepared one (21662)
_set_single
_set_list
_set_single
_set_list
_set_list
Create model instance from JSON raster  Test raster metadata properties  Compare srs  Compare pixel values  If numpy, convert result to list  Loop through rows in band data and assert single  value is as expected.
Parse json raster  Update srid to another value  Save model and get it from db  Confirm raster has been transformed to the default srid  Confirm geotransform is in lat/lon
Create test raster and geom.
Loop through all the GIS lookups.  Construct lookup filter strings.  Set lookup values for distance lookups.  Set lookup values for the relate lookup.  The isvalid lookup doesn't make sense for rasters.  Set lookup values for all function based operators.  Override band lookup for these, as it's not supported.  Set lookup values for all other operators.
Create query filter combinations.
Apply this query filter.
Evaluate normal filter qs.
Evaluate on conditional Q expressions.
Create test raster and geom.
Filter raster with different lookup raster formats.
Filter in an unprojected coordinate system.
Filter with band index transform.
Filter raster by geom.
Filter geom by raster.
Filter through related model.
Filter through related model with band index transform
Filter through conditional statements.
Filter through different lookup.
Point is in the interior  Point is in the exterior  A point on the boundary is not contained properly  Raster is located left of the point
Shift raster upwards  The raster in the model is not strictly below  Shift raster further upwards  The raster in the model is strictly below
Move raster to overlap with the model point on the left side  Raster overlaps with point in model  Change left side of raster to be nodata values  Raster does not overlap anymore after polygonization  where the nodata zone is not included.
Test with invalid dict lookup parameter  Test with invalid string lookup parameter
From proj's "cs2cs -le" and Wikipedia (semi-minor only)
Some of the authority names are borked on Oracle, e.g., SRID=32140.   also, Oracle Spatial seems to add extraneous info to fields, hence the   the testing with the 'startswith' flag.
No proj.4 and different srtext on oracle backends :(
Can't get 'NAD83 / Texas South Central' from PROJ.4 string  on SpatiaLite
Testing the SpatialReference object directly.
Getting the ellipsoid and precision parameters.
Getting our spatial reference and its ellipsoid
`GeoQuerySet.distance` is not allowed geometry fields.
Only a subset of the geometry functions & operator are available  to PostGIS geography types.  For more information, visit:  http://postgis.refractions.net/documentation/manual-1.5/ch08.htmlPostGIS_GeographyFunctions  ST_Within not available.  `@` operator not available.
Regression test for 14060, `~=` was never really implemented for PostGIS.
There is a similar test in `layermap` that uses the same data set,  but the County model here is a bit different.
Getting the shapefile and mapping dictionary.
Reference county names, number of polygons, and state names.
SELECT ST_Area(poly) FROM geogapp_zipcode WHERE code='77002';  Round to the nearest thousand as possible values (depending on  the database and geolib) include 5439084, 5439100, 5439101.
SELECT ST_Area(poly) FROM geogapp_zipcode WHERE code='77002';  Round to the nearest thousand as possible values (depending on  the database and geolib) include 5439084, 5439100, 5439101.
Decorators to disable entire test functions for specific  spatial backends.
Shortcut booleans to omit only portions of tests.
MySQL spatial indices can't handle NULL geometries.
A point we are testing distances with -- using a WGS84  coordinate that'll be implicitly transformed to that to  the coordinate system of the field, EPSG:32140 (Texas South Central  w/units in meters)  Another one for Australia
Distances -- all should be equal (except for the  degree/meter pair in au_cities, that's somewhat  approximate).
Expected cities for Australia and Texas.
Performing distance queries on two projected coordinate systems one  with units in meters and the other in units of U.S. survey feet.
Now performing the `dwithin` queries on a geodetic coordinate system.
Creating the query set.  A ValueError should be raised on PostGIS when trying to pass  Distance objects into a DWithin query using a geodetic field.
The point for La Grange, TX  Reference distances in feet and in meters. Got these values from  using the provided raw SQL statements.   SELECT ST_Distance(point, ST_Transform(ST_GeomFromText('POINT(-96.876369 29.905320)', 4326), 32140))   FROM distapp_southtexascity;   SELECT ST_Distance(point, ST_Transform(ST_GeomFromText('POINT(-96.876369 29.905320)', 4326), 2278))   FROM distapp_southtexascityft;  Oracle 11 thinks this is not a projected coordinate system, so it's  not tested.
Testing using different variations of parameters and using models  with different projected coordinate systems.
Original query done on PostGIS, have to adjust AlmostEqual tolerance  for Oracle.
Ensuring expected distances are returned for each distance queryset.
Testing geodetic distance calculation with a non-point geometry  (a LineString of Wollongong and Shellharbour coords).
Reference query:   SELECT ST_distance_sphere(point, ST_GeomFromText('LINESTRING(150.9020 -34.4245,150.8700 -34.5789)', 4326))   FROM distapp_australiacity ORDER BY name;  Testing equivalence to within a meter.
Got the reference distances using the raw SQL statements:   SELECT ST_distance_spheroid(point, ST_GeomFromText('POINT(151.231341 -33.952685)', 4326),     'SPHEROID["WGS 84",6378137.0,298.257223563]') FROM distapp_australiacity WHERE (NOT (id = 11));   SELECT ST_distance_sphere(point, ST_GeomFromText('POINT(151.231341 -33.952685)', 4326))   FROM distapp_australiacity WHERE (NOT (id = 11));  st_distance_sphere  PROJ.4 versions 4.7+ have updated datums, and thus different  distance values.
Testing with spheroid distances first.  PostGIS uses sphere-only distances by default, testing these as well.
We'll be using a Polygon (created by buffering the centroid  of 77005 to 100m) -- which aren't allowed in geographic distance  queries normally, however our field has been transformed to  a non-geographic system.
Reference query:  SELECT ST_Distance(ST_Transform("distapp_censuszipcode"."poly", 32140),    ST_GeomFromText('<buffer_wkt>', 32140))  FROM "distapp_censuszipcode";
Having our buffer in the SRID of the transformation and of the field  -- should get the same results. The first buffer has no need for  transformation SQL because it is the same SRID as what was given  to `transform()`.  The second buffer will need to be transformed,  however.
Retrieving the cities within a 20km 'donut' w/a 7km radius 'hole'  (thus, Houston and Southside place will be excluded as tested in  the `test02_dwithin` above).
Oracle 11 incorrectly thinks it is not projected.
Doing a distance query using Polygons instead of a Point.  If we add a little more distance 77002 should be included.
Line is from Canberra to Sydney.  Query is for all other cities within  a 100km of that line (which should exclude only Hobart & Adelaide).
Too many params (4 in this case) should raise a ValueError.
Not enough params should raise a ValueError.
Getting all cities w/in 550 miles of Hobart.
Cities that are either really close or really far from Wollongong --  and using different units of distance.
Normal geodetic distance lookup (uses `distance_sphere` on PostGIS.
Geodetic distance lookup but telling GeoDjango to use `distance_spheroid`  instead (we should get the same results b/c accuracy variance won't matter  in this test case).
With a combined expression
With spheroid param
Reference queries:  SELECT ST_Area(poly) FROM distapp_southtexaszipcode;  Tolerance has to be lower for Oracle
Reference query (should use `length_spheroid`).  SELECT ST_length_spheroid(ST_GeomFromText('<wkt>', 4326) 'SPHEROID["WGS 84",6378137,298.257223563,    AUTHORITY["EPSG","7030"]]');
Does not support geodetic coordinate systems.
Now doing length on a projected coordinate system.
Reference query:  SELECT ST_Perimeter(distapp_southtexaszipcode.poly) FROM distapp_southtexaszipcode;
Running on points; should return 0.
Creating SouthTexasZipcode w/NULL value.  Performing distance/area queries against the NULL PolygonField,  and ensuring the result of the operations is None.
Reference queries:  SELECT ST_Area(poly) FROM distapp_southtexaszipcode;  Tolerance has to be lower for Oracle  MySQL is returning a raw float value
The point for La Grange, TX  Reference distances in feet and in meters. Got these values from  using the provided raw SQL statements.   SELECT ST_Distance(point, ST_Transform(ST_GeomFromText('POINT(-96.876369 29.905320)', 4326), 32140))   FROM distapp_southtexascity;   SELECT ST_Distance(point, ST_Transform(ST_GeomFromText('POINT(-96.876369 29.905320)', 4326), 2278))   FROM distapp_southtexascityft;  Oracle 11 thinks this is not a projected coordinate system, so it's  not tested.
Testing using different variations of parameters and using models  with different projected coordinate systems.
Original query done on PostGIS, have to adjust AlmostEqual tolerance  for Oracle.
Ensuring expected distances are returned for each distance queryset.
Testing geodetic distance calculation with a non-point geometry  (a LineString of Wollongong and Shellharbour coords).
Reference query:   SELECT ST_distance_sphere(point, ST_GeomFromText('LINESTRING(150.9020 -34.4245,150.8700 -34.5789)', 4326))   FROM distapp_australiacity ORDER BY name;  Testing equivalence to within a meter.
Got the reference distances using the raw SQL statements:   SELECT ST_distance_spheroid(point, ST_GeomFromText('POINT(151.231341 -33.952685)', 4326),     'SPHEROID["WGS 84",6378137.0,298.257223563]') FROM distapp_australiacity WHERE (NOT (id = 11));   SELECT ST_distance_sphere(point, ST_GeomFromText('POINT(151.231341 -33.952685)', 4326))   FROM distapp_australiacity WHERE (NOT (id = 11));  st_distance_sphere  PROJ.4 versions 4.7+ have updated datums, and thus different  distance values.
Testing with spheroid distances first.  PostGIS uses sphere-only distances by default, testing these as well.
We'll be using a Polygon (created by buffering the centroid  of 77005 to 100m) -- which aren't allowed in geographic distance  queries normally, however our field has been transformed to  a non-geographic system.
Reference query:  SELECT ST_Distance(ST_Transform("distapp_censuszipcode"."poly", 32140),    ST_GeomFromText('<buffer_wkt>', 32140))  FROM "distapp_censuszipcode";
Having our buffer in the SRID of the transformation and of the field  -- should get the same results. The first buffer has no need for  transformation SQL because it is the same SRID as what was given  to `transform()`.  The second buffer will need to be transformed,  however.
Reference query (should use `length_spheroid`).  SELECT ST_length_spheroid(ST_GeomFromText('<wkt>', 4326) 'SPHEROID["WGS 84",6378137,298.257223563,    AUTHORITY["EPSG","7030"]]');
TODO: test with spheroid argument (True and False)  Does not support geodetic coordinate systems.
Now doing length on a projected coordinate system.
Reference query:  SELECT ST_Perimeter(distapp_southtexaszipcode.poly) FROM distapp_southtexaszipcode;
Running on points; should return 0.
Currently only Oracle supports calculating the perimeter on geodetic  geometries (without being transformed).  But should work fine when transformed to projected coordinates
Creating SouthTexasZipcode w/NULL value.  Performing distance/area queries against the NULL PolygonField,  and ensuring the result of the operations is None.
Input that doesn't specify the SRID is assumed to be in the SRID  of the input field.  Making the field in a different SRID from that of the geometry, and  asserting it transforms.  The cleaned geometry should be transformed to 32140.
Form fields, by default, are required (`required=True`)
This will clean None as a geometry (See 10660).
By default, all geometry types are allowed.
a WKT for any other geom_type will be properly transformed by `to_python`  but rejected by `clean`
to_python returns the same GEOSGeometry for a WKT  but raises a ValidationError for any other string
map_srid in operlayers.html template must not be localized.
Force deserialize use due to a string value
Ensure that resulting geometry has srid set
mark the name to show that this was called
This returns a different result each time,  to make sure it only gets called once.
-*- coding: utf-8 -*-
Error message may or may not be the fully qualified path.
Tests for TZ-aware time methods need pytz.
Changing TIME_ZONE may issue a query to set the database's timezone,  hence TestCase.
Set up a second temporary directory which is ensured to have a mixed  case name.
Check for correct behavior under both USE_TZ=True and USE_TZ=False.  The tests are similar since they both set up a situation where the  system time zone, Django's TIME_ZONE, and UTC are distinct.
Django's TZ (and hence the system TZ) is set to Africa/Algiers which  is UTC+1 and has no DST change. We can set the Django TZ to something  else so that UTC, Django's TIME_ZONE, and the system timezone are all  different.
Use a fixed offset timezone so we don't need pytz.  At this point the system TZ is +1 and the Django TZ  is -5. The following will be aware in UTC.
dt should be aware, in UTC
Check that the three timezones are indeed distinct.
dt and now should be the same effective time.
Django's TZ (and hence the system TZ) is set to Africa/Algiers which  is UTC+1 and has no DST change. We can set the Django TZ to something  else so that UTC, Django's TIME_ZONE, and the system timezone are all  different.
Use a fixed offset timezone so we don't need pytz.  At this point the system TZ is +1 and the Django TZ  is -5.
dt should be naive, in system (+1) TZ
Check that the three timezones are indeed distinct.
dt and naive_now should be the same effective time.  If we convert dt to an aware object using the Algiers  timezone then it should be the same effective time to  now_in_algiers.
should encode special chars except ~!*()'  like encodeURIComponent() JavaScript function do
25905: remove leading slashes from file names to prevent unsafe url output
22717: missing ending slash in base_url should be auto-corrected
Create a storage backend associated with the mixed case name  directory.  Ask that storage backend to store a file with a mixed case filename.
Monkey-patch os.makedirs, to simulate a normal call, a raced call,  and an error.
Check that OSErrors aside from EEXIST are still raised.
Monkey-patch os.remove, to simulate a normal call, a raced call,  and an error.
Check that OSErrors aside from ENOENT are still raised.
An object without a file has limited functionality.
Saving a file enables full functionality.
File objects can be assigned to FileField attributes, but shouldn't  get committed until the model it's attached to is saved.
Save another file with the same name.
Deleting an object does not delete the file it uses.
Files can be read in a little at a time, if necessary.
Files can be written to.
Multiple files with the same name get _(7 random chars) appended to them.
Given the max_length is limited, when multiple files get uploaded  under the same name, then the filename get truncated in order to fit  in _(7 random chars). When most of the max_length is taken by  dirname + extension and there are not enough  characters in the  filename to truncate, an exception should be raised.
Testing truncation.
Testing exception is raised when filename is too short to truncate.
Default values allow an object to access a single file.
But it shouldn't be deleted, even if there are no more objects using  it.
upload_to can be empty, meaning it does not use subdirectory.
Verify the fix for 5655, making sure the directory is only  determined once.
CustomValidNameStorage.get_valid_name() appends '_valid' to the name
Push an object into the cache to make sure it pickles properly
Create sample file
Load it as python file object  Save it using storage and read its content
Test passing StringIO instance as content argument to save
Save it and read written file
Simulate call to f.save()
Repeat test with a callable.  Return a non-normalized path on purpose.
Simulate call to f.save()
Creating countries  Creating People
Creating Groups
Membership objects have access to their related Person if both  country_ids match between them
Membership objects returns DoesNotExist error when the there is no  Person with the same id and country_id
Creating a valid membership because it has the same country has the person
Creating an invalid membership because it has a different country has the person
Creating a to valid memberships
Creating an invalid membership
Creating a to valid memberships
Creating an invalid membership
Creating an invalid membership
We start out by making sure that the Group 'CIA' has no members.
We start out by making sure that Bob is in no groups.
Bob should be in the CIA and a Republican
We start out by making sure that the Group 'CIA' has no members.
Something adds jane to group CIA but Jane is in Soviet Union which isn't CIA's country
There should still be no members in CIA
We start out by making sure that Jane has no groups.
Something adds jane to group CIA but Jane is in Soviet Union which isn't CIA's country
Jane should still not be in any groups
Note that we use ids instead of instances. This is because instances on ForeignObject  properties will set all related field off of the given instance
Test model initialization with active_translation field.
ForeignObjects should not have any form fields, currently the user needs  to manually deal with the foreignobject relation.
A very crude test checking that the non-concrete fields do not get form fields.
order mismatches the Contact ForeignObject.
Anything with as_sql() method works in get_extra_restriction().
Table Column Fields
Table Column Fields
Relation Fields
Table Column Fields
Table Column Fields
Relation Fields
Table Column Fields
Relation Fields
To test __search lookup a fulltext index is needed. This  is only available when using MySQL 5.6, or when using MyISAM  tables. As 5.6 isn't common yet, lets use MyISAM table for  testing. The table is manually created by the test method.  RemovedInDjango20Warning
Create a few Authors.  Create a couple of Articles.  Create a few Tags.
We can use .exists() to check that there are some  There should be none now!
Integer value can be queried using string
A date lookup can be performed using a string search
Each QuerySet gets iterator(), which is a generator that "lazily"  returns results using database-level iteration.
iterator() can be used on any QuerySet.
count() returns the number of objects matching search criteria.
count() should respect sliced query sets.
Date and date/time lookups can also be done with strings.
in_bulk() takes a list of IDs and returns a dictionary mapping IDs to objects.
values() returns a list of dictionaries instead of object instances --  and you can specify which fields you want to retrieve.  You can use values() with iterator() for memory savings,  because iterator() uses database-level iteration.  The values() method works with "extra" fields specified in extra(select).  You can specify fields from forward and reverse relations, just like filter().  However, an exception FieldDoesNotExist will be thrown if you specify  a non-existent field name in values() (a field that is neither in the  model nor in extra(select)).  If you don't specify field names to values(), all are returned.
values_list() is similar to values(), except that the results are  returned as a list of tuples, rather than a list of dictionaries.  Within each tuple, the order of the elements is the same as the order  of fields in the values_list() call.
Every DateField and DateTimeField creates get_next_by_FOO() and  get_previous_by_FOO() methods. In the case of identical date values,  these methods will use the ID as a fallback check. This guarantees  that no records are skipped or duplicated.
Underscores, percent signs and backslashes have special meaning in the  underlying SQL code, but Django handles the quoting of them automatically.
exclude() is the opposite of filter() when doing lookups:
none() returns a QuerySet that behaves like any other QuerySet object
using __in with an empty list should return an empty query set
Programming errors are pointed out with nice error messages
An invalid nested lookup on a related field raises a useful error.
Create some articles with a bit more interesting headlines for testing field lookups:  zero-or-more  one-or-more  wildcard  leading anchor  trailing anchor  character sets
and more articles:
alternation
greedy matching
grouping and backreferences
Here we're using 'gt' as a code number for the year, e.g. 111=>2009.
Games in 2010
Games in 2011
Games played in 2010 and 2011
Players who played in 2009
Players who played in 2010
Players who played in 2011
To use fulltext indexes on MySQL either version 5.6 is needed, or one must use  MyISAM tables. Neither of these combinations is currently available on CI, so  lets manually create a MyISAM table for Article model.  NOTE: Needs to be created after the article has been saved.
Create a few Alarms
-*- coding: utf-8 -*-
We expect a UnicodeWarning here, because we used broken utf-8 on purpose
Put tests for CSRF_COOKIE_* settings here  token_view calls get_token() indirectly
This is important to make pages cacheable.  Pages which do call  get_token(), assuming they use the token, are not cacheable because  the token is specific to the user  non_token_view_using_request_processor does not call get_token(), but  does use the csrf request processor.  By using this, we are testing  that the view processor is properly lazy and doesn't call get_token()  until needed.
Empty  Non-ASCII  missing scheme  >>> urlparse('//example.com/')  ParseResult(scheme='', netloc='example.com', path='/', params='', query='', fragment='')  missing netloc  >>> urlparse('https://')  ParseResult(scheme='https', netloc='', path='', params='', query='', fragment='')
See ticket 15617
Doesn't insert a token or anything
Doesn't insert a token or anything
Doesn't insert a token or anything
token_view calls get_token() indirectly
token_view calls get_token() indirectly
These are tests for 16715. The basic scheme is always the same: 3 models with  2 relations. The first relation may be null, while the second is non-nullable.  In some cases, Django would pick the wrong join type for the second relation,  resulting in missing objects in the queryset.    Model A    | (Relation A/B : nullable)    Model B    | (Relation B/C : non-nullable)    Model C  Because of the possibility of NULL rows resulting from the LEFT OUTER JOIN  between Model A and Model B (i.e. instances of A without reference to B),  the second join must also be LEFT OUTER JOIN, so that we do not ignore  instances of A that do not reference B.  Relation A/B can either be an explicit foreign key or an implicit reverse  relation such as introduced by one-to-one relations (through multi-table  inheritance).
This test failed in 16715 because in some cases INNER JOIN was selected  for the second foreign key relation instead of LEFT OUTER JOIN.
This failed.
This failed.
Simple filter/exclude queries for good measure.
These all work because the second foreign key in the chain has null=True.
This test failed in 16715 because in some cases INNER JOIN was selected  for the second foreign key relation instead of LEFT OUTER JOIN.
This failed.
These all work because the second foreign key in the chain has null=True.
Some additional tests for 16715. The only difference is the depth of the  nesting as we now use 4 models instead of 3 (and thus 3 relations). This  checks if promotion of join types works for deeper nesting too.
-*- coding: utf-8 -*-
We cannot simply use assertRaises because a SkipTest exception will go unnoticed
Total hack, but it works, just want an attribute that's always true.
Using an unordered queryset with more than one ordered value  is an error.  No error for one value.
equal html contains each other
when a root element is used ...
equal html contains each other one time
HACK: This depends on internals of our TestCase subclasses  Detect fixture loading by counting SQL queries, should be zero
context manager form of assertRaisesMessage()
callable form
callable_obj was a documented kwarg in Django 1.8 and older.
override to avoid a second cls._rollback_atomics() which would fail.  Normal setUpClass() methods won't have exception handling so this  method wouldn't typically be run.
Simulate a broken setUpTestData() method.
setUpTestData() should call _rollback_atomics() so that the  transaction doesn't leak.
self.available_apps must be None to test the serialized_rollback  condition.
with a mocked call_command(), this doesn't have any effect.
Explicit dependencies
Implied dependencies
Explicit dependencies
Implicit dependencies
reordering aliases shouldn't matter
Transaction support should be properly initialized for the 'other' DB  And all the DBs should report that they support transactions
Using the real current name as old_name to not mess with the test suite.
Output format changed in Python 3.5+
All others can follow in unspecified order, including doctests
Import all the models from subpackages
Generic inline with unique_together
Generic inline with can_delete=False
-*- coding: utf-8 -*-
Set DEBUG to True to ensure {% include %} will raise exceptions.  That is how inlines are rendered and 9498 will bubble up if it is an issue.
inline data
inline data
Works with no queryset
A queryset can be used to alter display ordering
Works with a queryset that omits items
Regression test for 10522.
Regression test for 12340.
inline data
Create a formset with default arguments
Create a formset with custom keyword arguments
Test that get_fieldsets is called when figuring out form fields.  Refs 18681.
Ensure author is always accessible in clean method
Optional secondary author
this is purely for testing the data doesn't matter here :)
models for testing unique_together validation when a fk is involved and  using inlineformset_factory.
models for testing callable defaults (see bug 7975). If you define a model  with a callable default value, you cannot rely on the initial value in a  form.
models for testing a null=True fk to a parent
Models for testing custom ModelForm save methods in formsets and inline formsets
Models for testing UUID primary keys
One existing untouched and two new unvalid forms
Make sure this form doesn't pass validation.
Then make sure that it *does* pass validation and delete the object,  even though the data in new forms aren't actually valid.
Make sure this form doesn't pass validation.
Then make sure that it *does* pass validation and delete the object,  even though the data isn't actually valid.
Simulate deletion of an object that doesn't exist in the database
The formset is valid even though poem.pk + 1 doesn't exist,  because it's marked for deletion anyway
Make sure the save went through correctly
Only changed or new objects are returned from formset.save()
Test the behavior of min_num with model formsets. It should be  added to extra.
Test the behavior of min_num with existing objects.
change the name to "Vladimir Mayakovsky" just to be a jerk.
As you can see, 'Les Paradis Artificiels' is now a book belonging to  Charles Baudelaire.
The save_as_new parameter lets you re-associate the data to a new  instance.  This is used in the admin for save_as functionality.
Test inline formsets where the inline-edited object has a custom  primary key that is not the fk to the parent object.
change the name to "Brooklyn Bridge" just to be a jerk.
The Poet instance is saved after the formset instantiation. This  happens in admin's changeform_view() when adding a new object and  some inlines in the same request.
Now test the same thing without the validate_max flag to ensure  default behavior is unchanged
attempt to save the same revision against the same repo.
inlineformset_factory tests with fk having null=True. see 9462.  create some data that will exhibit the issue
a formset for a Model that has a custom primary key that still needs to be  added to the formset automatically
has_changed should compare model instance and primary key  see 18898
has_changed should work with queryset and list of pk's  see 18898
Only the price field is specified, this should skip any unique checks since  the unique_together is not fulfilled. This will fail with a KeyError if broken.
-*- encoding: utf-8 -*-
Underscores  Other chars
-*- encoding: utf-8 -*-
Lets limit the introspection to tables created for models of this  application  contrib.contenttypes is one of the apps always installed when running  the Django test suite, check that one of its tables hasn't been  inspected
Inspecting Oracle DB doesn't produce correct results (19884):  - it gets max_length wrong: it returns a number of bytes.  - it reports fields as blank=True when they aren't.
Lets limit the introspection to tables created for models of this  application  Recursive foreign keys should be set to 'self'  As InspectdbPeople model is defined after InspectdbMessage, it should be quoted
Lets limit the introspection to tables created for models of this  application
Python 3 allows non-ASCII identifiers
There should be one unique_together tuple.  Fields with db_column = field name.  Fields from columns whose names are Python keywords.  Fields whose names normalize to the same Python field name and hence  are given an integer suffix.
(qset, expected) tuples  Does combining querysets work?  Fetch the alphabetically first coworker for each worker
Combining queries with different distinct_fields is not allowed.
Test join unreffing
distinct + annotate not allowed
However this check is done only when the query executes, so you  can use distinct() to remove the fields before execution.  distinct + aggregate not allowed
-*- coding: utf-8 -*-  Unit and doctests for specific database backends.
Check that '%' chars are escaped for query execution.
If the backend is Oracle, test that we can call a standard  stored procedure through our cursor wrapper.
If the backend is Oracle, test that we can pass cursor variables  as query parameters.
If the backend is Oracle, test that we can save a text longer  than 4000 chars and read it properly
If the backend is Oracle, test that the client encoding is set  correctly.  This was broken under Cygwin prior to r14781.
an 'almost right' datetime should work with configured  NLS parameters as per 18465.  Test that the query succeeds without errors - pre 18465 this  wasn't the case.
Now assume the 'postgres' db isn't available  Check a RuntimeWarning has been emitted
Helper mocks
psycopg2 < 2.0.12 code path
Ensure the database default time zone is different than  the time zone in new_connection.settings_dict. We can  get the default time zone by reset & show.
Invalidate timezone name cache, because the setting_changed  handler cannot know about new_connection.
Fetch a new connection with the new_tz as default  time zone, run a query and rollback.
Now let's see if the rollback rolled back the SET TIME ZONE.
Open a database connection.
Check the level on the psycopg2 connection, not the Django wrapper.
Start a transaction so the isolation level isn't reported as 0.  Check the level on the psycopg2 connection, not the Django wrapper.
Regression for 17158  This shouldn't raise an exception
The implementation of last_executed_queries isn't optimal. It's  worth testing that parameters are quoted. See 14091.  Note that the single quote is repeated
If SQLITE_MAX_VARIABLE_NUMBER (default = 999) has been changed to be  greater than SQLITE_MAX_COLUMN (default = 2000), last_executed_query  can hit the SQLITE_MAX_COLUMN limit. See 26063.  This should not raise an exception.
Some convenience aliases
Create an object with a manually specified PK
Reset the sequences for the database
If we create a new object now, it should have a PK greater  than the PK we specified manually.
This test needs to run outside of a transaction, otherwise closing the  connection would implicitly rollback and cause problems during teardown.
Unfortunately with sqlite3 the in-memory test database cannot be closed,  and so it cannot be re-opened during testing.
'%s' escaping support for sqlite3 13648  response should be an non-zero integer
Test cursor.executemany 4896
Test executemany with params=[] does nothing 4765
Test executemany accepts iterators 10320
same test for DebugCursorWrapper
Support pyformat style passing of parameters 10070
Support pyformat style passing of parameters 10070
same test for DebugCursorWrapper
fetchone, fetchmany, fetchall return strings as unicode objects 6254
As password is probably wrong, a database exception is expected
Ticket 13630
Both InterfaceError and ProgrammingError seem to be used when  accessing closed cursor (psycopg2 has InterfaceError, rest seem  to use ProgrammingError).  cursor should be closed, so no queries should be possible.
There isn't a generic way to test that cursors are closed, but  psycopg2 offers us a way to check that by closed attribute.  So, run only on psycopg2 for that reason.
Open a connection to the database.  Emulate a connection close by the database.  Even then is_usable() should not raise an exception.  Clean up the mess created by connection._close(). Since the  connection is already closed, this crashes on some backends.
Initialize the connection and clear initialization statements.
We don't make these tests conditional because that means we would need to  check and differentiate between:  * MySQL+InnoDB, MySQL+MYISAM (something we currently can't do).  * if sqlite3 (if/once we get 14204 fixed) has referential integrity turned    on or not, something that would be controlled by runtime support and user    preference.  verify if its type is django.database.db.IntegrityError.
Create a Reporter.
Now that we know this backend supports integrity checks we make sure  constraints are also enforced for proxy models. Refs 17519
Create an Article.  Retrieve it from the DB  Now that we know this backend supports integrity checks we make sure  constraints are also enforced for proxy models. Refs 17519  Create another article  Retrieve the second article from the DB
Create an Article.  Retrieve it from the DB
Create an Article.  Retrieve it from the DB
Create an Article.  Retrieve it from the DB
Map connections by id because connections with identical aliases  have the same hash.
Passing django.db.connection between threads doesn't work while  connections[DEFAULT_DB_ALIAS] does.  Allow thread sharing so the connection can be closed by the  main thread.
Check that each created connection got different inner connection.  Finish by closing the connections opened by the other threads (the  connection opened in the main thread will automatically be closed on  teardown).
Map connections by id because connections with identical aliases  have the same hash.
Allow thread sharing so the connection can be closed by the  main thread.
Finish by closing the connections opened by the other threads (the  connection opened in the main thread will automatically be closed on  teardown).
Without touching allow_thread_sharing, which should be False by default.  Forbidden!
If explicitly setting allow_thread_sharing to False  Forbidden!
If explicitly setting allow_thread_sharing to True  All good
First, without explicitly enabling the connection for sharing.
The exception was raised
Then, with explicitly enabling the connection for sharing.
Enable thread sharing
No exception was raised
Get a copy of the default connection. (Can't use django.db.connection  because it'll modify the default connection itself.)
A test db name isn't set.
A regular test db name is set.
A test db name prefixed with TEST_DATABASE_PREFIX is set.
This book manager doesn't do anything interesting; it just  exists to strip out the 'extra_arg' argument to certain  calls. This argument is used to establish that the BookManager  is actually getting used when it should be.
Create a book on the default database using create()
Create a book on the default database using a save
Check that book exists on the default database, but not on other database
Create a book on the second database
Create a book on the default database using a save
Check that book exists on the default database, but not on other database
Create a book and author on the default database
Create a book and author on the other database
Save the author relations
Inspect the m2m tables directly.  There should be 1 entry in each database
Check that queries work across m2m joins
Reget the objects to clear caches
Retrieve related object by descriptor. Related objects should be database-bound
Create a book and author on the other database
Save the author relations
Add a second author
Remove the second author
Clear all authors
Create an author through the m2m interface
Create a book and author on the other database
Save the author relations
Create a second book on the other database
Add a books to the m2m
Remove a book from the m2m
Clear the books associated with mark
Create a book through the m2m interface
Create a book and author on the default database
Create a book and author on the other database
Set a foreign key set with an object from a different database
Add to an m2m with an object from a different database
Set a m2m with an object from a different database
Add to a reverse m2m with an object from a different database
Set a reverse m2m with an object from a different database
Create a book and author on the other database
Check the initial state
Delete the object on the other database
The person still exists ...  ... but the book has been deleted  ... and the relationship object has also been deleted.
Now try deletion in the reverse direction. Set up the relation again
Check the initial state
Delete the object on the other database
The person has been deleted ...  ... but the book still exists  ... and the relationship object has been deleted.
Create a book and author on the default database
Create a book and author on the other database
Save the author's favorite books
Check that queries work across foreign key joins
Reget the objects to clear caches
Retrieve related object by descriptor. Related objects should be database-bound
Save the author relations
Add a second book edited by chris
Remove the second editor
Clear all edited books
Create an author through the m2m interface
Create a book and author on the default database
Create a book and author on the other database
Set a foreign key with an object from a different database
Set a foreign key set with an object from a different database
Add to a foreign key set with an object from a different database
Check the initial state
Delete the person object, which will cascade onto the pet
Both the pet and the person have been deleted from the right database
Create a user and profile on the default database
Create a user and profile on the other database
Retrieve related objects; queries should be database constrained
Check that queries work across joins
Reget the objects to clear caches
Retrieve related object by descriptor. Related objects should be database-bound
Create a user and profile on the default database
Create a user and profile on the other database
Set a one-to-one relation with an object from a different database
BUT! if you assign a FK object when the base object hasn't  been saved yet, you implicitly assign the database for the  base object.
assigning a profile requires an explicit pk as the object isn't saved
initially, no db assigned
old object comes from 'other', so the new object is set to use 'other'...
... but it isn't saved yet
When saved (no using required), new objects goes to 'other'
This also works if you assign the O2O relation in the constructor
... but it isn't saved yet
When saved, the new profile goes to 'other'
Create a book and author on the default database
Create a book and author on the other database
Reget the objects to clear caches
Retrieve related object by descriptor. Related objects should be database-bound
Add a second review
Remove the second author
Clear all reviews
Create an author through the generic interface
Create a book and author on the default database
Create a book and author on the other database
Set a foreign key with an object from a different database
Add to a foreign key set with an object from a different database
BUT! if you assign a FK object when the base object hasn't  been saved yet, you implicitly assign the database for the  base object.  initially, no db assigned
Dive comes from 'other', so review3 is set to use 'other'...  ... but it isn't saved yet
When saved, John goes to 'other'
Check the initial state
Delete the Book object, which will cascade onto the pet
Both the pet and the person have been deleted from the right database
Create a book and author on the other database
Retrieve the Person using select_related()
The editor instance should have a db state
When you call __str__ on the query object, it doesn't know about using  so it falls back to the default. If the subquery explicitly uses a  different database, an error should be raised.
Evaluating the query shouldn't work, either
extra_arg is removed by the BookManager's implementation of  create(); but the BookManager's implementation won't get called  unless edited returns a Manager, not a queryset
Init with instances instead of strings
Make the 'other' database appear to be a replica of the 'default'
Add the auth router to the chain. TestRouter is a universal  synchronizer, so it should have no effect.
Now check what happens if the router order is reversed.
Create a book and author on the other database
An update query will be routed to the default database
By default, the get query will be directed to 'other'
But the same query issued explicitly at a database will work.
Check that the update worked.
An update query with an explicit using clause will be routed  to the requested database.
Related object queries stick to the same database  as the original object, regardless of the router
get_or_create is a special case. The get needs to be targeted at  the write database in order to avoid potential transaction  consistency problems
Check the head count of objects  If a database isn't specified, the read database is used
A delete query will also be routed to the default database
The default database has lost the book.
Set a foreign key set with an object from a different database
Create a book and author on the default database
Create a book and author on the other database
Set a foreign key with an object from a different database
Database assignments of original objects haven't changed...
... but they will when the affected object is saved.
...and the source database now has a copy of any object saved
This isn't a real primary/replica database, so restore the original from other
Set a foreign key set with an object from a different database
Assignment implies a save, so database assignments of original objects have changed...
...and the source database now has a copy of any object saved
This isn't a real primary/replica database, so restore the original from other
Add to a foreign key set with an object from a different database
Add implies a save, so database assignments of original objects have changed...
...and the source database now has a copy of any object saved
This isn't a real primary/replica database, so restore the original from other
If you assign a FK object when the base object hasn't  been saved yet, you implicitly assign the database for the  base object.  initially, no db assigned
old object comes from 'other', so the new object is set to use the  source of 'other'...
This also works if you assign the FK in the constructor
For the remainder of this test, create a copy of 'mark' in the  'default' database to prevent integrity errors on backends that  don't defer constraints checks until the end of the transaction
This moved 'mark' in the 'default' database, move it back in 'other'
If you create an object through a FK relation, it will be  written to the write database, even if the original object  was on the read database
Same goes for get_or_create, regardless of whether getting or creating
Create books and authors on the inverse to the usual database
Now save back onto the usual database.  This simulates primary/replica - the objects exist on both database,  but the _state.db is as it is for all other tests.
Check that we have 2 of both types of object on both databases
Set a m2m set with an object from a different database
Database assignments don't change
All m2m relations should be saved on the default database
Reset relations
Add to an m2m with an object from a different database
Database assignments don't change
All m2m relations should be saved on the default database
Reset relations
Set a reverse m2m with an object from a different database
Database assignments don't change
All m2m relations should be saved on the default database
Reset relations
Add to a reverse m2m with an object from a different database
Database assignments don't change
All m2m relations should be saved on the default database
If you create an object through a M2M relation, it will be  written to the write database, even if the original object  was on the read database
Same goes for get_or_create, regardless of whether getting or creating
Create a user and profile on the default database
Create a user and profile on the other database
Set a one-to-one relation with an object from a different database
Database assignments of original objects haven't changed...
... but they will when the affected object is saved.
Create a book and author on the default database
Create a book and author on the other database
Set a generic foreign key with an object from a different database
Database assignments of original objects haven't changed...
... but they will when the affected object is saved.
...and the source database now has a copy of any object saved
This isn't a real primary/replica database, so restore the original from other
Add to a generic foreign key set with an object from a different database
Database assignments of original objects haven't changed...
... but they will when the affected object is saved.
...and the source database now has a copy of any object saved
BUT! if you assign a FK object when the base object hasn't  been saved yet, you implicitly assign the database for the  base object.  initially, no db assigned
Dive comes from 'other', so review3 is set to use the source of 'other'...
If you create an object through a M2M relation, it will be  written to the write database, even if the original object  was on the read database
When you call __str__ on the query object, it doesn't know about using  so it falls back to the default. Don't let routing instructions  force the subquery to an incompatible database.
If you evaluate the query, it should work, running on 'other'
Create one user using default allocation policy
Create another user, explicitly specifying the database
The second user only exists on the other database
The second user only exists on the default database
That is... there is one user on each database
Check that dumping the default database doesn't try to include auth  because allow_migrate prohibits auth on default
Check that dumping the other database does include auth
Check that "Pro Django" exists on the default database, but not on other database
Check that "Dive into Python" exists on the default database, but not on other database
Check that "Definitive Guide" exists on the both databases
No objects will actually be loaded
Make some signal receivers  Make model and connect receivers  Save and test receivers got calls  Delete, and test  Save again to a different database  Delete, and test
Make a receiver  Connect it
Create the models that will be used for the tests
Create a copy of the models on the 'other' database to prevent  integrity errors on backends that don't defer constraints checks
Test addition
Test removal
Test addition in reverse
Test clearing
test add  test remove  test clear  test setattr  test M2M collection
test related FK collection
We use default here to ensure we can tell the difference  between a read request and a write request for Auth objects
A router that only expresses an opinion on writes
-*- coding: utf-8 -*-
we need to register a custom ModelAdmin (instead of just using  ModelAdmin) because the field creator tries to find the ModelAdmin  for the related model
should be ordered by name (as defined by the model)
should be ordered by name (as defined by the model)
should be ordered by rank (defined by the ModelAdmin)
Exclude one of the two Bands from the querysets
No Articles yet, so we should get a Http404 error.
get_object_or_404 can be passed a Model to query.
We can also use the Article manager through an Author object.
No articles containing "Camelot".  This should raise a Http404 error.
Custom managers can be used too.
QuerySets can be used too.
Using an empty QuerySet raises a Http404 error.
get_list_or_404 can be used to get lists of objects
Http404 is returned if the list is empty.
Custom managers can be used too.
QuerySets can be used too.
Given an argument klass that is not a Model, Manager, or Queryset  raises a helpful ValueError message
Works for lists too
Save up the number of connected signals so that we can check at the  end that all the signals we register get properly unregistered (9989)
Check that all our signals got disconnected properly.
Calling an internal method purely so that we can trigger a "raw" save.
8285: signals can be any callable
Assigning and removing to/from m2m shouldn't generate an m2m signal.
-*- coding: utf-8 -*-
The generic relations regression test needs two different model  classes with the same PK value, and there are some (external)  DB backends that don't work nicely when assigning integer to AutoField  column (MSSQL at least).
Models for ticket 21150
don't do anything with the queryset (qs) before including it as a  subquery
force the queryset (qs) for the subquery to be evaluated in its  current state
tests that this query does not raise a DatabaseError due to the full  subselect being (erroneously) added to the GROUP BY parameters  force execution of the query
Ordering requests are ignored
Implicit ordering is also ignored
Baseline results
Empty values query doesn't affect grouping or results
Aggregate overrides extra selected column
Annotations get combined with extra select clauses  Different DB backends return different types for the extra select computation
Order of the annotate/extra in the query doesn't matter  Different DB backends return different types for the extra select computation
Values queries can be combined with annotate and extra
The order of the (empty) values, annotate and extra clauses doesn't  matter
If the annotation precedes the values clause, it won't be included  unless it is explicitly named
If an annotation isn't included in the values, it can still be used  in a filter
The annotations are added to values output if values() precedes  annotate()
Check that all of the objects are getting counted (allow_nulls) and  that values respects the amount of objects
Check that consecutive calls to annotate accumulate in the query
Aggregates can be composed over annotations.  The return type is derived from the composed aggregate
Regression for 15624 - Missing SELECT columns when using values, annotate  and aggregate in a single query
Bad field requests in aggregates are caught and reported
Old-style count aggregations can be mixed with new-style
Non-ordinal, non-computed Aggregates over annotations correctly  inherit the annotation's internal type if the annotation is ordinal  or computed
Aliases are quoted to protected aliases that might be reserved names
Regression for 10064: select_related() plays nice with aggregates
Regression for 10010: exclude on an aggregate field is correctly  negated
Aggregates can be used with F() expressions  ... where the F() is pushed into the HAVING clause
... and where the F() references an aggregate
Tests on fields with non-default table and column names.
Aggregates mixed up ordering of columns for backend's convert_values  method. Refs 21126.
Regression for 10089: Check handling of empty result sets with  aggregates
Regression for 10113 - Fields mentioned in order_by() must be  included in the GROUP BY. This only becomes a problem when the  order_by introduces a new join.
Regression for 10127 - Empty select_related() works with annotate
Regression for 10132 - If the values() clause only mentioned extra  (select=) columns, those columns are used for grouping
Regression for 10182 - Queries with aggregate calls are correctly  realiased when used in a subquery
Regression for 15709 - Ensure each group_by field only exists once  per query  Check that there is just one GROUP BY clause (zero commas means at  most one clause)
Regression for 11256 - duplicating a default alias raises ValueError.
Regression for 11256 - providing an aggregate name  that conflicts with a field name on the model raises ValueError
Regression for 11256 - providing an aggregate name  that conflicts with an m2m name on the model raises ValueError
age is a field on Author, so it shouldn't be allowed as an aggregate.  But age isn't included in values(), so it is.
Same problem, but aggregating over m2m fields
Same problem, but colliding with an m2m field
Regression for 11256 - providing an aggregate name  that conflicts with a reverse-related name on the model raises ValueError
Regression for 10197 -- Queries with aggregates can be pickled.  First check that pickling is possible at all. No crash = success
Then check that the round trip works.
Regression for 10199 - Aggregate calls clone the original query so  the original query can still be used
Regression for 10248 - Annotations work with dates()
Regression for 10290 - extra selects with parameters can be used for  grouping.
Regression for 10425 - annotations don't get in the way of a count()  clause
Note: intentionally no order_by(), that case needs tests, too.
Regression for 10666 - inherited fields work with annotations and  aggregations
Regression for 10766 - Shouldn't be able to reference an aggregate  fields in an aggregate() call.
Regression for 11789
Books with less than 200 pages per author.
Test that when a field occurs on the LHS of a HAVING clause that it  appears correctly in the GROUP BY clause  Results should be the same, all Books have more pages than authors
The name of the explicitly provided annotation name in this case  poses no problem  Neither in this case  This case used to fail because the ORM couldn't resolve the  automatically generated annotation name `book__count`  Referencing the auto-generated name in an aggregate() also works.
Check that the query executes without problems.
There should only be one GROUP BY clause, for the `id` column.  `name` and `age` should not be grouped on.
Ensure that we get correct results.
Works with only() too.
Ensure that we get correct results.
And select_related()  In the case of `group_by_selected_pks` we also group by contact.id because of the select_related.
Ensure that we get correct results.
Assign a tag to model with same PK as the book above. If the JOIN  used in aggregation doesn't have content type as part of the  condition the annotation will also count the 'hi mom' tag for b.
Test that aggregates are spotted correctly from F objects.  Note that Adrian's age is 34 in the fixtures, and he has one book  so both conditions match one author.
A query with an existing annotation aggregation on a relation should  succeed.
There are three books with rating of 4.0 and two of the books have  the same price. Hence, the distinct removes one rating of 4.0  from the results.
Force re-evaluation
No promotion for existing joins  Also, the existing join is unpromoted when doing filtering for already  promoted join.  But, as the join is nullable first use by annotate will be LOUTER
First try using a "normal" field  Now try with a ForeignKey
Bug 21785
Test max_length
Test basic pointing  Test swap detection for swappable model  Test nonexistent (for now) model  Test on_delete  Test to_field preservation  Test related_name preservation
It doesn't matter that we swapped out user for permission;  there's no validation. We just want to check the setting stuff works.
Test normal  Test swappable  Test through  Test custom db_table  Test related_name
It doesn't matter that we swapped out user for permission;  there's no validation. We just want to check the setting stuff works.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Disable auto loading of this model as we load it on our own
Disable auto loading of this model as we load it on our own
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Reset applied-migrations state.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Test operation in non-atomic migration is not wrapped in transaction
Let's look at the plan first and make sure it's up to scratch  Were the tables there before?  Alright, let's try running it  Are the tables there now?  Rebuild the graph to reflect the new DB state  Alright, let's undo what we did  Are the tables gone?
Check our leaf node is the squashed one  Check the plan  Were the tables there before?  Alright, let's try running it  Are the tables there now?  Rebuild the graph to reflect the new DB state  Alright, let's undo what we did. Should also just use squashed.  Are the tables gone?
Make the initial plan, check it  Fake-apply all migrations  Rebuild the graph to reflect the new DB state  Now plan a second time and make sure it's empty  Erase all the fake records
Prepare for mixed plan  Rebuild the graph to reflect the new DB state
Generate mixed plan  Rebuild the graph to reflect the new DB state  Are the tables gone?
Were the tables there before?  Run it normally  Are the tables there now?  We shouldn't have faked that one  Rebuild the graph to reflect the new DB state  Fake-reverse that  Are the tables still there?  Make sure that was faked  Finally, migrate forwards; this should fake-apply our initial migration  Applying the migration should raise a database level error  because we haven't given the --fake-initial option  Reset the faked state  Allow faking of initial CreateModel operations  And migrate back to clean up the database
Migrate forwards  Make sure the soft-application detection works (23093)  Change table_names to not return auth_user during this as  it wouldn't be there in a normal run, and ensure migrations.Author  exists in the global app registry temporarily.  And migrate back to clean up the database
from 0001  from 0002
Create the tables for 0001 but make it look like the migration hasn't  been applied.  Table detection sees 0001 is applied but not 0002.
Create the tables for both migrations but make it look like neither  has been applied.  Table detection sees 0002 is applied.
Leave the tables for 0001 except the many-to-many table. That missing  table should cause detect_soft_applied() to return False.
Cleanup by removing the remaining tables.
Rebuild the graph to reflect the new DB state
Migrate forwards -- This led to a lookup LookupErrors because  lookuperror_b.B2 is already applied
Rebuild the graph to reflect the new DB state
Cleanup
Rebuild the graph to reflect the new DB state
Migrate backwards -- This led to a lookup LookupErrors because  lookuperror_b.B2 is not in the initial state (unrelated to app c)
Rebuild the graph to reflect the new DB state
Cleanup
Migrate forward.  Migrate backward.
Were the tables there before?  Rebuild the graph to reflect the new DB state
Apply initial migrations  Rebuild the graph to reflect the new DB state
Apply PK type alteration
Rebuild the graph to reflect the new DB state
We can't simply unapply the migrations here because there is no  implicit cast from VARCHAR to INT on the database level.
Place the database in a state where the replaced migrations are  partially applied: 0001 is applied, 0002 is not.  Use fake because we don't actually have the first migration  applied, so the second will fail. And there's no need to actually  create/modify tables here, we're just testing the  MigrationRecord, which works the same with or without fake.
Because we've now applied 0001 and 0002 both, their squashed  replacement should be marked as applied.
Record all replaced migrations as applied
Because 0001 and 0002 are both applied, even though this migrate run  didn't apply anything new, their squashed replacement should be  marked as applied.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Make sure no tables are created  Run migration  Make sure the right tables exist  Unmigrate everything  Make sure it's all gone
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Test with a compiled regex.
Test a string regex with flag
Test message and code
Test with a subclass.
It should NOT be unicode.  We don't test the output formatting - that's too fragile.  Just make sure it runs for now, and that things look alright.  In order to preserve compatibility with Python 3.2 unicode literals  prefix shouldn't be added to strings.
Silence warning on Python 2: Not importing directory  'tests/migrations/migrations_test_apps/without_init_file/migrations':  missing __init__.py
Yes, it doesn't make sense to use a class as a default for a  CharField. It does make sense for custom fields though, for example  an enumfield that takes the enum class as an argument.
The default manager is used in migrations
No explicit managers defined. Migrations will fall back to the default
food_mgr is used in migration but isn't the default mgr, hence add the  default
The ordering we really want is objects, mgr1, mgr2
First, test rendering individually
We shouldn't be able to render yet
Once the parent model is in the app registry, it should be fine
We shouldn't be able to render yet
Once the parent models are in the app registry, it should be fine
Make a ProjectState and render it
Now make an invalid ProjectState and make sure it fails
Make a ProjectState and render it
Check that the relations between the old models are correct
Check that all models have changed  Check that the relations between the old models still hold  Check that the relations between the new models correct
Tests that model from old_state still has the relation
Same test for deleted model
At this point the model would be rendered twice causing its related  M2M through objects to point to an old copy and thus breaking their  attribute lookup.
Tests that the old model's _meta is still consistent
Tests that the new model's _meta is still consistent
Test two things that should be equal
Make a very small change (max_len 99) and see if that affects it
Make a valid ProjectState and render it
now make an invalid one with a ForeignKey
And another with ManyToManyField.
And now with multiple models and multiple fields.
If we just stick it into an empty state it should fail
If we include the real app it should succeed
Make a valid ProjectState and render it
The default manager is used in migrations
M has a pointer O2O field p_ptr to P
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Test atomic operation in non-atomic migration is wrapped in transaction
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Test the state alteration  Test the database alteration  And test reversal
Test the state alteration does nothing  Test the database alteration
Create the operation
Test the state alteration does nothing  Test the database alteration
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Make a fake graph  Use project state to make a new migration change set  Run through arrange_for_graph  Make sure there's a new name, deps match, etc.
Use project state to make a new migration change set  Run through arrange_for_graph  Make sure there's the right set of migrations
Make a fake graph
Use project state to make a new migration change set
Run through arrange_for_graph
Make sure there's a new name, deps match, etc.
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
An unchanged partial reference.
A changed partial reference.  Can't use assertOperationFieldAttributes because we need the  deconstructed version, i.e., the exploded func/args/keywords rather  than the partial: we don't care if it's not the same instance of the  partial, only if it's the same source function, args, and keywords.
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?  Now that RenameModel handles related fields too, there should be  no AlterField for the related field.
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?  Right number/type of migrations for related field rename?  Alter is already taken care of.
Make state  Note that testapp (author) has no dependencies,  otherapp (book) depends on testapp (author),  thirdapp (edition) depends on otherapp (book)  Right number/type of migrations?  Right number/type of migrations?  Right number/type of migrations?
Make state  Note that testapp (author) has no dependencies,  otherapp (book) depends on testapp (authorproxy)  Right number/type of migrations?  Right number/type of migrations?  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?  Right number/type of migrations?  both split migrations should be `initial`
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Right number/type of migrations?
Explicitly testing for not specified, since this is the case after  a CreateModel operation w/o any definition on the original model  Explicitly testing for None, since this was the issue in 23452 after  a AlterFooTogether operation with e.g. () as value  Explicitly testing for the empty set, since we now always have sets.  During removal (('col1', 'col2'),) --> () this becomes set([])
Make state  Right number/type of migrations?
Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number of migrations?
Make state  Right number/type of migrations?
Right number/type of migrations?
Right number of migrations?  Right number of actions?  Right actions order?
Right number/type of migrations?
Right number/type of migrations?
First, we test adding a proxy model  Right number/type of migrations?
Now, we test turning a proxy model into a non-proxy model  It should delete the proxy then make the real one  Right number/type of migrations?
First, we test the default pk field name  The field name the FK on the book model points to
Now, we test the custom pk field name  The field name the FK on the book model points to
First, we test adding an unmanaged model  Right number/type of migrations?
Now, we test turning an unmanaged model into a managed model  Right number/type of migrations?
Now, we turn managed to unmanaged.  Right number/type of migrations?
First, we test the default pk field name  The field name the FK on the book model points to
Now, we test the custom pk field name  The field name the FK on the book model points to
Right number/type of migrations?
Right number/type of migrations?
Make state  Right number/type of migrations?
Right number of migrations?
When lists contain items that deconstruct to identical values, those lists  should be considered equal for the purpose of detecting state changes  (even if the original items are unequal).
Legitimate differences within the deconstructed lists should be reported  as a change
When tuples contain items that deconstruct to identical values, those tuples  should be considered equal for the purpose of detecting state changes  (even if the original items are unequal).
Legitimate differences within the deconstructed tuples should be reported  as a change
When dicts contain items whose values deconstruct to identical values,  those dicts should be considered equal for the purpose of detecting  state changes (even if the original values are unequal).
Legitimate differences within the deconstructed dicts should be reported  as a change
If the items within a deconstructed object's args/kwargs have the same  deconstructed values - whether or not the items themselves are different  instances - then the object as a whole is regarded as unchanged.
Differences that exist solely within the args list of a deconstructed object  should be reported as changes
Additional args should also be reported as a change
Differences that exist solely within the kwargs dict of a deconstructed object  should be reported as changes
Additional kwargs should also be reported as a change
IntegerField intentionally not instantiated.
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Right number/type of migrations?
Right number/type of migrations?
Right number/type of migrations?
Right number/type of migrations?
Remove both the through model and ManyToMany  Right number/type of migrations?
Remove both the through model and ManyToMany  Right number/type of migrations?
Right number/type of migrations?
Right number/type of migrations?
Right number/type of migrations?
Right number/type of migrations?
Right number/type of migrations?
Changing them back to empty should also make a change  Right number/type of migrations?
Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state
Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Load graph  Make state  Right number/type of migrations?
Load graph  Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state  Right number/type of migrations?
Make state
Right number/type of migrations?  Right number/type of migrations?
Make state
Right number/type of migrations?  Right number/type of migrations?
Make state
Right number/type of migrations?
Right number/type of migrations?
Right number/type of migrations?
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Delete the tables if they already exist  Start with ManyToMany tables  Then standard model tables
Make the "current" state
Test the state alteration  Test the database alteration  And test reversal  And deconstruction  And default manager not in set
Test the database alteration
Test the state alteration  Test the database alteration  Make sure the M2M field actually works  And test reversal
Test the state alteration  Test the database alteration  And test reversal
Test the state alteration  Test the database alteration  And test reversal  And deconstruction
Test the state alteration  Test the database alteration  And test reversal
Test the state alteration
Test the state alteration  Test the database alteration  And test reversal  And deconstruction
Test the state alteration  Test the database alteration  And test reversal
Test the state alteration  Test initial state and database  Migrate forwards  Test new state and database  RenameModel also repoints all incoming FKs and M2Ms  Migrate backwards  Test original state and database  And deconstruction
Test the state alteration  Remember, RenameModel also repoints all incoming FKs and M2Ms  Test the database alteration  And test reversal
Test the state alteration  RenameModel shouldn't repoint the superclass's relations, only local ones  Before running the migration we have a table for Shetland Pony, not Little Horse  and the foreign key on rider points to pony, not shetland pony  Now we have a little horse table, not shetland pony  but the Foreign keys still point at pony, not little horse
Test the state alteration  Test the database alteration  And test reversal  And deconstruction
If not properly quoted digits would be interpreted as an int.  Manual quoting is fragile and could trip on quotes. Refs xyz.
If not properly quoted digits would be interpreted as an int.  Manual quoting is fragile and could trip on quotes. Refs xyz.
If not properly quoted digits would be interpreted as an int.  Manual quoting is fragile and could trip on quotes. Refs xyz.
SQLite returns buffer/memoryview, cast to bytes for checking.
Test the state alteration  Test the database alteration  And deconstruction
Test the state alteration  Test the database alteration  Make sure the M2M field actually works  And test reversal
Ensure the new field actually works
And test reversal
Test the state alteration  Test the database alteration  And test reversal  And deconstruction
Test the state alteration  Test the database alteration  And test reversal  And deconstruction
Test the state alteration  Test the database alteration  And test reversal
Add the M2M field  Rename the Pony db_table which should also rename the m2m table.  And test reversal
Test the state alteration  Test the database alteration  And test reversal  And deconstruction
Test the state alteration  Test the database alteration  And test reversal
Test the state alteration
Test the database alteration  And test reversal
Test the state alteration  Make sure the unique_together has the renamed column too  Make sure the index_together has the renamed column too  Test the database alteration  Ensure the unique constraint has been ported over  Ensure the index constraint has been ported over  And test reversal  Ensure the index constraint has been reset  And deconstruction
Test the state alteration  Make sure we can insert duplicate rows  Test the database alteration  And test reversal  Test flat unique_together  And deconstruction
Test the state alteration  Make sure there's no matching index  Test the database alteration  And test reversal  And deconstruction
Test the state alteration (no DB alteration to test)  And deconstruction
Test the state alteration (no DB alteration to test)  And deconstruction
Test the state alteration  Make sure there's no matching index  Create some rows before alteration  Test the database alteration  Check for correct value in rows  And test reversal  And deconstruction
Test the state alteration
Test the state alteration
Test adding and then altering the FK in one go
Test the state alteration  Test the database alteration  And test reversal
Create the operation  Use a multi-line string with a comment to test splitting on SQLite and MySQL respectively
Run delete queries to test for parameter substitution failure  reported in 23426
Test the state alteration  Make sure there's no table  Test SQL collection  Test the database alteration  Make sure all the SQL was processed  And test reversal  And deconstruction  And elidable reduction
Create the operation  forwards  backwards
Make sure there's no table  Test the database alteration
Test parameter passing  Make sure all the SQL was processed
And test reversal
forwards  backwards
Create the operation
Test the state alteration does nothing  Test the database alteration  Now test reversal  Now test we can't use a string  And deconstruction
Also test reversal fails, with an operation identical to above but without reverse_code set
And deconstruction
And elidable reduction
If we're a fully-transactional database, both versions should rollback  Otherwise, the non-atomic operation should leave a row there  And deconstruction
Create the operation  Test the state alteration  Make sure there's no table  Test the database alteration  And test reversal  And deconstruction
Create the operation  We use IntegerField and not AutoField because  the model is going to be deleted immediately  and with an AutoField this fails on Oracle  Test the state alteration
Check that tables and models exist, or don't, as they should:
Test the database alteration  And test reversal
Test the state alteration (it should still be there!)  Test the database alteration  And test reversal
Test the database alteration  And test reversal
Test the state alteration  Test the database alteration  And test reversal
These should work  This should not work - FK should block it  This should not work - bases should block it
Note: The middle model is not actually a valid through model,  but that doesn't matter, as we never render it.
AddField
AlterField
RenameField
RemoveField
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
That should not affect records of another database
Load and test the plan  Now render it out!
Ensure we've included unmigrated apps in there too
Load and test the plan  Now render it out!
Load and test the plan
Loading with nothing applied should just give us the one node  However, fake-apply one migration and it should now use the old two
Empty database: use squashed migration
Starting at 1 or 2 should use the squashed migration too
However, starting at 3 to 5 cannot use the squashed migration
Starting at 5 to 7 we are passed the squashed migrations
Empty database: use squashed migration
Starting at 1 or 2 should use the squashed migration too
However, starting at 3 or 4 we'd need to use non-existing migrations
Starting at 5 to 7 we are passed the squashed migrations
Load with nothing applied: both migrations squashed.
Fake-apply a few from app1: unsquashes migration in app1.
Fake-apply one from app2: unsquashes migration in app2 too.
-*- coding: utf-8 -*-
Make sure no tables are created  Run the migrations to 0001 only  Make sure the right tables exist  Run migrations all the way  Make sure the right tables exist  Unmigrate everything  Make sure it's all gone
Make sure no tables are created  Run the migrations to 0001 only  Fake rollback  Make sure fake-initial detection does not run
Real rollback  Make sure it's all gone
Make sure no tables are created  Run the migrations to 0001 only  Make sure the right tables exist  Also check the "other" database
Fake a roll-back  Make sure the tables still exist  Try to run initial migration  Run initial migration with an explicit --fake-initial  Run migrations all the way  Make sure the right tables exist  Fake a roll-back  Make sure the tables still exist  Try to run initial migration  Run initial migration with an explicit --fake-initial  Fails because "migrations_tribble" does not exist but needs to in  order to make --fake-initial work.  Fake a apply  Unmigrate everything  Make sure it's all gone
Fake an apply  Unmigrate everything
Giving the explicit app_label tests for selective `show_list` in the command  Cleanup by unmigrating everything
Cleanup by unmigrating everything
Cannot generate the reverse SQL unless we've applied the migration.
Cleanup by unmigrating everything
unmigrated_app.SillyModel has a foreign key to 'migrations.Tribble',  but that model is only defined in a migration, so the global app  registry never sees it and the reference is left dangling. Remove it  to avoid problems in subsequent tests.
Rollback changes
Check for empty __init__.py file in migrations folder
Check for existing 0001_initial.py file in migration folder
Meta.verbose_name  Meta.verbose_name_plural
Python 3 importlib caches os.listdir() on some platforms like  Mac OS X (23850).
If a migration fails to serialize, it shouldn't generate an empty file. 21280
Check for existing 0001_initial.py file in migration folder
Remove all whitespace to check for empty dependencies and operations
Monkeypatch interactive questioner to auto reject
Monkeypatch interactive questioner to auto accept
Output the expected changes directly, without asking for defaults
Normal --dry-run output
Migrations file is actually created in the expected path.
Command output indicates the migration is created.
Monkeypatch interactive questioner to auto reject  This will fail if interactive is False by default
Monkeypatch interactive questioner to auto accept
Monkeypatch interactive questioner to auto accept
Check for existing migration file in migration folder
generate an initial migration
Python 3 importlib caches os.listdir() on some platforms like  Mac OS X (23850).
generate an empty migration
Monkeypatch interactive questioner to auto accept
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Build graph  Test root migration case  Test branch B only  Test whole graph  Test reverse to b:0002  Test roots and leaves
Build graph  Test branch C only  Test whole graph  Test reverse to b:0001  Test roots and leaves
Build graph  Test whole graph
Build graph
Build graph
Build graph  Add dependency with missing parent node (skipping validation).  Add missing parent node and ensure `validate_consistency()` no longer raises error.  Add dependency with missing child node (skipping validation).  Add missing child node and ensure `validate_consistency()` no longer raises error.  Rawly add dummy node.
Add some dummy nodes to be replaced.  Add some normal parent and child nodes to test dependency remapping.  Try replacing before replacement node exists.  Ensure `validate_consistency()` still raises an error at this stage.  Remove the dummy nodes.  Ensure graph is now consistent and dependencies have been remapped
Add some dummy nodes to be replaced.  Try removing replacement node before replacement node exists.  Add a child node to test dependency remapping.  Remove the replacement node.  Ensure graph is consistent and child dependency has been remapped  Ensure child dependency hasn't also gotten remapped to the other replaced node.
Unit tests for cache framework  Uses whatever cache backend is set in the test settings file.
functions/classes for complex data type tests
Unpicklable using the default pickling protocol on Python 2.
`base` is used to pull in the memcached config from the original settings,  `params` are test specific overrides and `_caches_settings_base` is the  base config for the tests.  This results in the following search order:  params -> _caches_setting_base -> base
Simple cache set/get works
A key can be added to a cache
Test for same cache key conflicts between shared backend
should not be set in the prefixed cache
Non-existent cache keys return as None/default  get with non-existent keys
Multiple cache keys can be returned using get_many
Cache keys can be deleted
The cache can be inspected for cache keys
The in operator can be used to inspect cache contents
Cache values can be incremented
Cache values can be decremented
Many different data types can be cached
Don't want fields with callable as default to be called on cache read  We only want the default expensive calculation run once
Don't want fields with callable as default to be called on cache write  cache set should not re-evaluate default functions
Don't want fields with callable as default to be called on cache read  We only want the default expensive calculation run on creation and set
Cache values can be set to expire
Unicode values can be cached  Test `set`
Test `add`
Test `set_many`
Binary strings should be cacheable
Test set
Test add
Test set_many
Multiple keys can be set using set_many
set_many takes a second ``timeout`` parameter
Multiple keys can be deleted using delete_many
The cache can be emptied using clear
Make sure a timeout given as a float doesn't crash anything.
Create initial cache key entries. This will overflow the cache,  causing a cull.  Count how many keys are left in the cache.
mimic custom ``make_key`` method being defined since the default will  never show the below warnings
memcached does not allow whitespace or control characters in keys  warnings.warn() crashes on Python 2 if message isn't  coercible to str.
memcached limits key length to 250  warnings.warn() crashes on Python 2 if message isn't  coercible to str.
set, using default version = 1
set, default version = 1, but manually override version = 2
v2 set, using default version = 2
v2 set, default version = 2, but manually override version = 1
add, default version = 1, but manually override version = 2
v2 add, using default version = 2
v2 add, default version = 2, but manually override version = 1
has_key
set, using default version = 1
set, default version = 1, but manually override version = 2
v2 set, using default version = 2
v2 set, default version = 2, but manually override version = 1
Two caches with different key functions aren't visible to each other
Shouldn't fail silently if trying to cache an unpicklable type.
Simulate cache.add() failing to add a value. In that case, the  default value should be returned.
Spaces are used in the table name to ensure quoting/escaping is working
The super calls needs to happen first for the settings override.
The super call needs to happen first because it uses the database.
Use another table name to avoid the 'table already exists' message.
cache table should not be created on 'default'  cache table should be created on 'other'  Queries:    1: check table doesn't already exist    2: create savepoint (if transactional DDL is supported)    3: create the table    4: create the index    5: release savepoint (if transactional DDL is supported)
LocMem requires a hack to make the other caches  share a data store with the 'normal' cache.
memcached backend isn't guaranteed to be available.  To check the memcached backend, the test settings file will  need to contain at least one cache backend setting that points at  your memcache server.
memcached does not allow whitespace or control characters in keys  memcached limits key length to 250
Explicitly display a skipped test if no configured cache uses MemcachedCache  Regression test for 19810
Regression test for 22845
Regression test for 22845
culling isn't implemented, memcached deals with it.
culling isn't implemented, memcached deals with it.
By default memcached allows objects up to 1MB. For the cache_db session  backend to always use the current session, memcached needs to delete  the old key if it fails to set.  pylibmc doesn't seem to have SERVER_MAX_VALUE_LENGTH as far as I can  tell from a quick check of its source code. This is falling back to  the default value exposed by python-memcached on my system.
small_value should be deleted, or set if configured to accept larger values
Caches location cannot be modified through override_settings / modify_settings,  hence settings are manipulated directly here and the setting_changed signal  is triggered manually.
Call parent first, as cache.clear() may recreate cache base directory
This fails if not using the highest pickling protocol on Python 2.
this key is both longer than 250 characters, and has spaces
The 5 minute (300 seconds) default expiration time for keys is  defined in the implementation of the initializer method of the  BaseCache type.
Initial vary, new headers, resulting vary.
Expect None if no headers have been set yet.  Set headers to an empty list.
Verify that a specified key_prefix is taken into account.
Expect None if no headers have been set yet.  Set headers to an empty list.  Verify that the querystring is taken into account.
Make sure that the Vary header is added to the key hash
Initial Cache-Control, kwargs to patch_cache_control, expected Cache-Control parts
Test whether private/public attributes are mutually exclusive
This is tightly coupled to the implementation,  but it's the most straightforward way to test the key.
Regression test for 17476
cache with non empty request.GET
first access, cache must return None  cache must return content  different QUERY_STRING, cache must be empty
i18n tests
Check that we can recover the cache  Check that we use etags  Check that we can disable etags  change the session language and set content  change again the language  retrieve the content from cache  change again the language  reset the language
This test passes on Python < 3.3 even without the corresponding code  in UpdateCacheMiddleware, because pickling a StreamingHttpResponse  fails (http://bugs.python.org/issue14288). LocMemCache silently  swallows the exception and doesn't store the response in cache.
If no arguments are passed in construction, it's being used as middleware.
Now test object attributes against values defined in setUp above
If arguments are being passed in construction, it's being used as a decorator.  First, test with "defaults":
Value of DEFAULT_CACHE_ALIAS from django.core.cache
Next, test with custom values:
Put the request through the request middleware
Now put the response through the response middleware
Repeating the request should result in a cache hit
The same request through a different middleware won't hit
The same request with a timeout _will_ hit
decorate the same view with different cache decorators
Request the view once
Request again -- hit the cache
Requesting the same view with the explicit cache should yield the same result
Requesting with a prefix will hit a different cache key
Hitting the same view again gives a cache hit
And going back to the implicit cache will hit the same cache
Requesting from an alternate cache won't hit cache
But a repeated hit will hit cache
And prefixing the alternate cache yields yet another cache entry
But if we wait a couple of seconds...
... the default cache will still hit
... the default cache with a prefix will still hit
... the explicit default cache will still hit
... the explicit default cache with a prefix will still hit
.. but a rapidly expiring cache won't hit
.. even if it has a prefix
Inserting a CSRF cookie in a cookie-less request prevented caching.
Initial vary, new headers, resulting vary.
Expect None if no headers have been set yet.  Set headers to an empty list.
Verify that a specified key_prefix is taken into account.
Expect None if no headers have been set yet.  Set headers to an empty list.  Verify that the querystring is taken into account.
1 query to fetch all children of 0 (1 and 2)  1 query to fetch all children of 1 and 2 (none)  Should not require additional queries to populate the nested graph.
One for Location, one for Guest, and no query for EventGuide
Regression test for 13071: NullBooleanField has special  handling.
NOTE: cannot use @property decorator, because of  AttributeError: 'property' object has no attribute 'short_description'
safestring should not be escaped
normal strings needs to be escaped
-*- coding: utf-8 -*-
Changed title for 1st article  Second article is deleted  A new article is added
Make sure custom action_flags works
If the log entry doesn't have a content type it should still be  possible to view the Recent Actions part (10275).
add
change
delete
order_with_respect_to points to a model with a OneToOneField primary key.
Hook to allow subclasses to run these tests with alternate models.
Answers will always be ordered in the order they were inserted.
We can retrieve the answers related to a particular object, in the  order they were created, once we have a particular object.
We can retrieve the ordering of the queryset from a particular item.
It doesn't matter which answer we use to check the order, it will  always be the same.
The ordering can be altered
Swap the last two items in the order list
By default, the ordering is different from the swapped version
Change the ordering to the swapped version -  this changes the ordering of the queryset.
Some JVM GCs will execute finalizers in a different thread, meaning  we need to wait for that to complete before we go on looking for the  effects of that.
Collecting weakreferences can take two collections on PyPy.
Note that dead weakref cleanup happens as side effect of using  the signal's receivers through the signals API. So, first do a  call to an API method to force cleanup.
Disconnect after reference check since it flushes the tested cache.
The first two models represent a very simple null FK ordering case.
These following 4 models represent a far more complex ordering case.
We can't compare results directly (since different databases sort NULLs to  different ends of the ordering), but we can check that all results are  returned.
We have to test this carefully. Some databases sort NULL values before  everything else, some sort them afterwards. So we extract the ordered list  and check the length. Before the fix, this list was too short (some values  were omitted).
You can't proxy a swapped model
Related field filter on proxy
Select related + filter on proxy
Proxy of proxy, select_related + filter
Select related + filter on a related proxy field
Select related + filter on a related proxy of proxy field
We need to set settings.DEBUG to True so we can capture the output SQL  to examine.
This is executed in autocommit mode so that code in  run_select_for_update can see this data.
We need another database connection in transaction to test that one  connection issuing a SELECT ... FOR UPDATE will block.
Start a blocking transaction. At some point,  end_blocking_transaction() should be called.
Roll back the blocking transaction.
Examine the SQL that was executed to determine whether it  contains the 'SELECT..FOR UPDATE' stanza.
We need to enter transaction management again, as this is done on  per-thread basis
This method is run in a separate thread. It uses its own  database connection. Close it without waiting for the GC.
First, let's start the transaction in our thread.
Now, try it again using the ORM's select_for_update  facility. Do this in a separate thread.
The thread should immediately block, but we'll sleep  for a bit to make sure.
Check the person hasn't been updated. Since this isn't  using FOR UPDATE, it won't block.
When we end our blocking transaction, our thread should  be able to continue.
Check the thread has finished. Assuming it has, we should  find that it has updated the person's name.
We must commit the transaction to ensure that MySQL gets a fresh read,  since by default it runs in REPEATABLE READ mode
This method is run in a separate thread. It uses its own  database connection. Close it without waiting for the GC.
Deliberately put the image field *after* the width/height fields to  trigger the bug in 10404 with width/height not getting assigned.
don't allow this field to be used in form (real use-case might be  that you know the markup will always be X, but it is among an app  that allows the user to say it could be something else)  regressed at r10062
Model for 13776
Model for 639
Support code for the tests; this keeps track of how many times save()  gets called on each instance.
Models for 24706
A model with ForeignKey(blank=False, null=True)
form is valid because required=False for field 'character'
Make sure the exception contains some reference to the  field responsible for the problem.
Should have the same result as before,  but 'fields' attribute specified differently
Should have the same result as before,  but 'fields' attribute specified differently
This Price instance generated by this form is not valid because the quantity  field is required, but the form is valid because the field is excluded from  the form. This is for backwards compatibility.
The form should not validate fields that it doesn't contain even if they are  specified using 'fields', not 'exclude'.
The form should still have an instance of a model that is not complete and  not saved into a DB yet.
First class with a Meta class wins...
Can't create new form
Even if you provide a model instance
'title' has unique_for_date='posted'  'slug' has unique_for_year='posted'  'subtitle' has unique_for_month='posted'
Ensure all many-to-many categories appear in model_to_dict  Ensure many-to-many relation appears as a list
model_to_dict should not hit the database if it can reuse  the data populated by prefetch_related.
Ensure all many-to-many categories appear in model_to_dict  Ensure many-to-many relation appears as a list
Set up a callable initial value
If you call save() with commit=False, then it will return an object that  hasn't yet been saved to the database. In this case, it's up to you to call  save() on the resulting model instance.
If you call save() with invalid data, you'll get a ValueError.
You can restrict a form to a subset of the complete list of fields  by providing a 'fields' argument. If you try to save a  model created with such a form, you need to ensure that the fields  that are _not_ on the form have default values, or are allowed to have  a value of None. If a field isn't specified on a form, the object created  from the form can't provide a value for that field!
You can create a form over a subset of the available fields  by specifying a 'fields' argument to form_for_instance.
Create a new article, with categories, via the form.
Now, submit form data with no categories. This deletes the existing categories.
Create a new article, with no categories, via the form.
Create a new article, with categories, via the form, but use commit=False.  The m2m data won't be saved until save_m2m() is invoked on the form.
Manually save the instance
The instance doesn't have m2m data yet
Save the m2m data on the form
Here, we define a custom ModelForm. Because it happens to have the same fields as  the Category model, we can just call the form's save() to apply its changes to an  existing Category instance.
ModelChoiceField
Invalid types that require TypeError to be caught (22808).
Add a Category object *after* the ModelChoiceField has already been  instantiated. This proves clean() checks the database during clean() rather  than caching it at time of instantiation.
Delete a Category object *after* the ModelChoiceField has already been  instantiated. This proves clean() checks the database during clean() rather  than caching it at time of instantiation.
len can be called on choices
queryset can be changed after the field is created.
check that we can safely iterate choices repeatedly
check that we can override the label_from_instance method to print custom labels (4620)
To allow the widget to change the queryset of field1.widget.choices correctly,  without affecting other forms, the following must hold:
Invalid types that require TypeError to be caught (22808).
Add a Category object *after* the ModelMultipleChoiceField has already been  instantiated. This proves clean() checks the database during clean() rather  than caching it at time of instantiation.  Note, we are using an id of 1006 here since tests that run before  this may create categories with primary keys up to 6. Use  a number that will not conflict.
Delete a Category object *after* the ModelMultipleChoiceField has already been  instantiated. This proves clean() checks the database during clean() rather  than caching it at time of instantiation.
queryset can be changed after the field is created.
BetterWriter model is a subclass of Writer with an additional `score` field
WriterProfile has a OneToOneField to Writer
author object returned from form still retains original publication object  that's why we need to retrieve it from database again
Test conditions when files is either not given or empty.
Upload a file and ensure it all works as expected.
If the previous file has been deleted, the file name can be reused
Check if the max_length attribute has been inherited from the model.
Edit an instance that already has the file defined in the model. This will not  save the file again, but leave it exactly as it is.
Delete the current file since this is not done by Django.
Override the file by uploading a new one.
Delete the current file since this is not done by Django.
Test the non-required FileField
Instance can be edited w/out re-uploading the file and existing file should be preserved.
Delete the current file since this is not done by Django.
It's enough that the form saves without error -- the custom save routine will  generate an AssertionError if it is called more than once during save.
Grab an image for testing.
Fake a POST QueryDict and FILES MultiValueDict.
Check the savecount stored on the object (see the model).
Delete the "uploaded" file to avoid clogging /tmp.
Delete the current file since this is not done by Django, but don't save  because the dimension fields are not null=True.
Delete the current file since this is not done by Django, but don't save  because the dimension fields are not null=True.  Override the file by uploading a new one.
Delete the current file since this is not done by Django, but don't save  because the dimension fields are not null=True.
Delete the current file since this is not done by Django, but don't save  because the dimension fields are not null=True.
Test the non-required ImageField  Note: In Oracle, we expect a null ImageField to return '' instead of  None.
Editing the instance without re-uploading the image should not affect  the image or its width/height properties.
Delete the current file since this is not done by Django.
Test callable upload_to behavior that's dependent on the value of another field in the model
'created', non-editable, is excluded by default
Choices on CharField and IntegerField
form.instance.left will be None if the instance was not constructed  by form.full_clean().
Without a widget should not set the widget to textarea
With a widget should not set the widget to textarea
A bad callback provided by user still gives an error
This line turns on the ValidationError; it avoids the model erroring  when its own __init__() is called when creating form.instance.
This line turns on the ValidationError; it avoids the model erroring  when its own __init__() is called when creating form.instance.
Use a fast hasher to speed up tests.
compose(f, g)(*args, **kwargs) == f(g(*args, **kwargs))
django.views.decorators.http
django.views.decorators.vary
django.views.decorators.cache
django.contrib.auth.decorators  Apply user_passes_test twice to check 9474
django.contrib.admin.views.decorators
django.utils.functional
For testing method_decorator, a decorator that assumes a single argument.  We will get type arguments if there is a mismatch in the number of arguments.
For testing method_decorator, two decorators that add an attribute to the function
Sanity check myattr_dec and myattr2_dec
Decorate using method_decorator() on the method.
Decorate using method_decorator() on both the class and the method.  The decorators applied to the methods are applied before the ones  applied to the class.
Decorate using an iterable of decorators.
The rest of the exception message differs between Python 2 and 3.
Test for argumented decorator
The order should be consistent with the usual order in which  decorators are applied, e.g.     @add_exclamation_mark     @add_question_mark     def func():         ...
Since the real purpose of the exempt decorator is to suppress  the middleware's functionality, let's make sure it actually works...
No related name is needed here, since symmetrical relations are not  explicitly reversible.
Regression for 11956 -- a many to many to the base class
A related_name is required on one of the ManyToManyField entries here because  they are both addressable as reverse relations from Tag.
Two models both inheriting from a base model with a self-referential m2m field
Many-to-Many relation between models, where one of the PK's isn't an Autofield
Regression for 11226 -- A model with the same name that another one to  which it has a m2m relation. This shouldn't cause a name clash between  the automatically created m2m intermediary table FK field names when  running migrate
Regression for 24505 -- Two ManyToManyFields with the same "to" model  and related_name set to '+'.
Get same manager twice in a row:
Get same manager for different instances
Regression for 19236 - an abstract class with a 'split' method  causes a TypeError in add_lazy_relation
Regression for 24505 - Multiple ManyToManyFields to same "to"  model with related_name set to '+'.
Test that the deferred class does not remember that gender was  set, instead the instance should remember this.
The loaded salary of 3000 gets saved, the name of 'Clerk' isn't  overwritten.
Save is skipped.  Signals were skipped, too...
A little sanity check that we actually did updates...
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Getting a single item should work too:
This final query should only have seven tables (port, device and building  twice each, plus connection once). Thus, 6 joins plus the FROM table.
Still works if we're dealing with an inherited class
Still works if we defer an attribute on the inherited class
Also works if you use only, rather than defer
The select_related join wasn't promoted as there was already an  existing (even if trimmed) inner join to state.
The select_related join was promoted as there is already an  existing join.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
We can also do the above query using UTF-8 strings.
Test that the searches do not match the subnet mask (/32 in this case)
Starting from comment, make sure that a .select_related(...) with a specified  set of fields will properly LEFT JOIN multiple levels of NULLs (and the things  that come after the NULLs, or else data that should exist won't). Regression  test for 7369.
Regression test for 7530, 7716.
Each of these individually should return the item.
Logically, qs1 and qs2, and qs3 and qs4 should be the same.
Regression test for 15823.
models for test_q_object_or:
For testing 13085 fix, we also use Note model defined above
search with a non-matching note and a matching org name  search again, with the same query parameters, in reverse order
Fails with another, ORM-level error
__nonzero__() returns False -- This actually doesn't currently fail.  This test validates that
Saving model with GenericForeignKey to model instance with an  empty CharField PK
Create a couple of extra HasLinkThing so that the autopk value  isn't the same for Link and HasLinkThing.
A crude check that content_type_id is taken in account in the  join/subquery condition.  No need for any joins - the join from inner query can be trimmed in  this case (but not in the above case as no a objects at all for given  B would then fail).
If content_type restriction isn't in the query's join condition,  then wrong results are produced here as the link to b will also match  (b and hs1 have equal pks).  Now if we don't have proper left join, we will not produce any  results at all here.  clear cached results  Note - 0 here would be a nicer result...  Finally test that filtering works.
deleting the Related cascades to the Content cascades to the Node,  where the pre_delete signal should fire and prevent deletion.
see 9258
-*- coding: utf-8 -*-
Override any settings on the model admin
Construct the admin, and ask it for a formfield
"unwrap" the widget wrapper, if needed
Check that we got a field of the right type
Return the formfield so that other tests can continue
Try posting with a non-existent pk in a raw id field: this  should result in an error message, not a server exception.
This should result in an error message, not a server exception.
Backslash in verbose_name to ensure it is JavaScript escaped.
Backslash in verbose_name to ensure it is JavaScript escaped.
pass attrs to widget
pass attrs to widget
WARNING: Don't use assertHTMLEqual in that testcase!  assertHTMLEqual will get rid of some escapes which are tested here!
Check that ForeignKeyRawIdWidget works with fields which aren't  related to the model's primary key.
FK to a model not registered with admin site. Raw ID widget should  have no magnifying glass link. See 16542
FK to self, not registered with admin site. Raw ID widget should have  no magnifying glass link. See 16542
see 9258
M2M relationship with model not registered with admin site. Raw ID  widget should have no magnifying glass link. See 16542
Used to fail with a name error.
Open a page that has a date and time picker widgets
First, with the date picker widget ---------------------------------  Check that the date picker is hidden  Click the calendar icon  Check that the date picker is visible  Press the ESC key  Check that the date picker is hidden again
Then, with the time picker widget ----------------------------------  Check that the time picker is hidden  Click the time icon  Check that the time picker is visible  Press the ESC key  Check that the time picker is hidden again
Open a page that has a date and time picker widgets
fill in the birth date.
Click the calendar icon
get all the tds within the calendar
make sure the first and last 6 cells have class nonday
Open a page that has a date and time picker widgets
fill in the birth date.
Click the calendar icon
get all the tds within the calendar
verify the selected cell
Open a page that has a date and time picker widgets
Click the calendar icon
get all the tds within the calendar
verify there are no cells with the selected class
Enter test data
Get month name translations for every locale
Get the expected caption
Test with every locale
Open a page that has a date picker widget  Click on the calendar icon  Make sure that the right month and year are displayed
If we are neighbouring a DST, we add an hour of error margin.
Click on the "today" and "now" shortcuts.
Check that there is a time zone mismatch warning.  Warning: This would effectively fail if the TIME_ZONE defined in the  settings has the same UTC offset as "Asia/Singapore" because the  mismatch warning would be rightfully missing from the page.
Submit the form.
Make sure that "now" in javascript is within 10 seconds  from "now" on the server side.
The above tests run with Asia/Singapore which are on the positive side of  UTC. Here we test with a timezone on the negative side.
Initial positions ---------------------------------------------------
Click 'Choose all' --------------------------------------------------  There 's no 'Choose all' button in vertical mode, so individually  select all options and click 'Choose'.
Click 'Remove all' --------------------------------------------------  There 's no 'Remove all' button in vertical mode, so individually  select all options and click 'Remove'.
Choose some options ------------------------------------------------
Check the title attribute is there for tool tips: ticket 20821
Check the tooltip is still there after moving: ticket 20821
Remove some options -------------------------------------------------
Choose some more options --------------------------------------------
Choose some more options --------------------------------------------
Confirm they're selected after clicking inactive buttons: ticket 26575
Unselect the options ------------------------------------------------
Choose some more options --------------------------------------------
Confirm they're selected after clicking inactive buttons: ticket 26575
Unselect the options ------------------------------------------------
Pressing buttons shouldn't change the URL.
Save and check that everything is properly stored in the database ---
Initial values
Typing in some characters filters out non-matching options
Clearing the text box makes the other options reappear
-----------------------------------------------------------------  Check that choosing a filtered option sends it properly to the  'to' box.
-----------------------------------------------------------------  Check that pressing enter on a filtered option sends it properly  to the 'to' box.
Save and check that everything is properly stored in the database ---
Navigate away and go back to the change form page.  Check that everything is still in place
self.selenium.refresh() or send_keys(Keys.F5) does hard reload and  doesn't replicate what happens when a user clicks the browser's  'Refresh' button.
No value has been selected yet
Open the popup window and click on a band
The field now contains the selected band's id
Reopen the popup window and click on another band
The field now contains the other selected band's id
No value has been selected yet
Help text for the field is displayed
Open the popup window and click on a band
The field now contains the selected band's id
Reopen the popup window and click on another band
The field now contains the two selected bands' ids
Click the Add User button to add new
The field now contains the new user
Click the Change User button to change it
Go ahead and submit the form to make sure it works
-*- coding: utf8 -*-
The pgpass temporary file needs to be encoded using the system locale.
Test for ticket 12059: TimeField wrongly handling datetime.datetime object.
MySQL backend does not support timezone-aware datetimes.
Verify we didn't break DateTimeField behavior  We need to test this way because datetime.datetime inherits  from datetime.date:
If the value for the field doesn't correspond to a valid choice,  the value itself is provided as a display value.
Although test runner calls migrate for several databases,  testing for only one of them is quite sufficient.  we need to test only one call of migrate
We connect receiver here and not in unit test code because we need to  connect receiver before test runner creates database.  That is, sequence of  actions would be:    1. Test runner imports this module.    2. We connect receiver.    3. Test runner calls migrate for create default database.    4. Test runner execute our unit test code.
The migration isn't applied backward.
-*- coding: utf-8 -*-
Call the "real" save() method
Call the "real" delete() method
Content-object field
Assume business logic forces every person to have at least one house.
Regression for 17439
Need a double
Without the ValueError, an author was deleted due to the implicit  save of the relation assignment.
Without the ValueError, a book was deleted due to the implicit  save of reverse relation assignment.
Ambiguous: Lookup was already seen with a different queryset.
Ambiguous: Lookup houses_lst doesn't yet exist when performing houses_lst__rooms.
Not ambiguous.
Control lookups.
Test lookups.
Control lookups.
Test lookups.
Control lookups.
Test lookups.
Control lookups.
Test lookups.
Control lookups.
Test lookups.
Control lookups.
Test lookups.
Control lookups.
Test lookups.
Control lookups.
Test lookups.
Test basic.
Test queryset filtering.
Test flattened.
Test inner select_related.
Test inner prefetch.
Test ForwardManyToOneDescriptor.
Test ReverseOneToOneDescriptor.
The custom queryset filters should be applied to the queryset  instance returned by the manager.
Regression test for 24873
Simulate a missing `_apply_rel_filters` method.  Also remove `get_queryset` as it rely on `_apply_rel_filters`.  Deleting `related_manager_cls` will force the creation of a new  class since it's a `cached_property`.
When we prefetch the teachers, and force the query, we don't want  the default manager on teachers to immediately get all the related  qualifications, since this will do one query per teacher.
1 for TaggedItem table, 1 for Book table, 1 for Reader table
1 for Comment table, 1 for Book table
We get 3 queries - 1 for main query, 1 for content_objects since they  all use the same table, and 1 for the 'read_by' relation.  If we limit to books, we know that they will have 'read_by'  attributes, so the following makes sense:
The custom queryset filters should be applied to the queryset  instance returned by the manager.
Regression for 18090: the prefetching query must include an IN clause.  Note that on Oracle the table name is upper case in the generated SQL,  thus the .lower() call.
The following two queries must be done in the same order as written,  otherwise 'primary_house' will cause non-prefetched lookups
Because we use select_related() for 'boss', it doesn't need to be  prefetched, but we can still traverse it although it contains some nulls
One for main employee, one for boss, one for serfs
Check that prefetch is done and it does not cause any errors.
Forward
Reverse
Forward
Reverse
parent link
child link
Implicit hinting
Explicit using on the same db.
Explicit using on a different db.
Set main_room for each house before creating the next one for  databases where supports_nullable_unique_constraints is False.
From uuid-pk model, prefetch <uuid-pk model>.<integer-pk model>:
From uuid-pk model, prefetch <integer-pk model>.<integer-pk model>.<uuid-pk model>.<uuid-pk model>:
From integer-pk model, prefetch <uuid-pk model>.<integer-pk model>:
From integer-pk model, prefetch <integer-pk model>.<uuid-pk model>:
From integer-pk model, prefetch <integer-pk model>.<uuid-pk model>.<uuid-pk model>:
Create a Reporter.  Create an Article.  Create an Article via the Reporter object.  Create an Article with no Reporter by passing "reporter=None".  Create another article and reporter
Article objects have access to their related Reporter objects.
Reporter objects have access to their related Article objects.
Need to reget a3 to refresh the cache  Accessing an article's 'reporter' attribute returns None  if the reporter is set to None.  To retrieve the articles with no reporters set, use "reporter__isnull=True".  We can achieve the same thing by filtering for the case where the  reporter is None.  Set the reporter for the Third article  Remove an article from the set, and check that it was removed.
Try to remove a4 from a set it does not belong to
Use manager.set() to allocate ForeignKey. Null is legal, so existing  members of the set that are not in the assignment set are set to null.  Use manager.set(clear=True)  Clear the rest of the set
Use descriptor assignment to allocate ForeignKey. Null is legal, so  existing members of the set that are not in the assignment set are  set to null.  Clear the rest of the set
Ensure that querysets used in reverse FK assignments are pre-evaluated  so their value isn't affected by the clearing operation in  RelatedManager.set() (19816).
QuerySet with an __init__() method that takes an additional argument.
Public methods are copied  Private methods are not copied
Methods with queryset_only=False are copied even if they are private.  Methods with queryset_only=True aren't copied even if they are public.
Test that specialized querysets inherit from our custom queryset.
Check that the fun manager DOESN'T remove boring people.  Check that the boring manager DOES remove boring people.
Check that the fun manager ONLY clears fun people.
Check that the fun manager DOESN'T remove boring people.
Check that the boring manager DOES remove boring people.
Check that the fun manager ONLY clears fun people.
Check that the fun manager DOESN'T remove boring people.
Check that the boring manager DOES remove boring people.
Check that the fun manager ONLY clears fun people.
Each model class gets a "_default_manager" attribute, which is a  reference to the first manager defined in the class.
alternate manager  explicit default manager  explicit base manager
If the hidden object wasn't seen during the save process,  there would now be two objects in the database.
All of the RestrictedModel instances should have been  deleted, since they *all* pointed to the RelatedModel. If  the default manager is used, only the public one will be  deleted.
The same test case as the last one, but for one-to-one  models, which are implemented slightly different internally,  so it's a different code path.
NB: be careful to delete any sessions created; stale sessions fill up  the /tmp (with some backends) and eventually overwhelm it after lots  of runs (think buildbots)
Need to reset these to pretend we haven't accessed it:
Submitting an invalid session key (either by guessing, or if the db has  removed the key) results in a new key being generated.  Some backends leave a stale cache entry for the invalid  session key; make sure that entry is manually deleted
Custom session expiry  A normal session has a max age equal to settings
So does a custom session with an idle expiration time of 0 (but it'll  expire at browser close)
Mock timezone.now, because set_expiry calls it on this code path.
Tests get_expire_at_browser_close with different settings and different  set_expiry calls
Ensure we can decode what we encode
check that the failed decode is logged
this doesn't work with JSONSerializer (serializing timedelta)
Regression test for 19200  With an expiry date in the past, the session expires instantly.
provided unknown key was cycled, not reused
Create new session.
Logout in another context.
Modify session in first context.  This should throw an exception as the session is deleted, not  resurrect the session.
Create a session
Change it  Clear cache, so that it will be retrieved from DB
One object in the future
One object in the past
Two sessions are in the database before clearsessions...  ... and one is deleted.
Set the account ID to be picked up by a custom session storage  and saved to a custom session model database column.
Make sure that the customized create_model_instance() was called.
Make the session "anonymous".
Make sure that save() on an existing session did the right job.
Some backends might issue a warning
21000 - CacheDB backend should respect SESSION_CACHE_ALIAS.
Don't need DB flushing for these tests, so can use unittest.TestCase as base class
Do file session tests in an isolated directory, and kill it after we're done.  Reset the file session backend's internal caches
Make sure the file backend checks for a good storage dir
Ensure we don't allow directory-traversal.  This is tested directly on _key_to_file, as load() will swallow  a SuspiciousOperation in the same way as an IOError - by creating  a new session, making it unclear whether the slashes were detected.
Ensure we don't allow directory-traversal
One object in the future
One object in the past
One object in the present without an expiry (should be deleted since  its modification time + SESSION_COOKIE_AGE will be in the past when  clearsessions runs).
Three sessions are in the filesystem before clearsessions...  ... and two are deleted.
Some backends might issue a warning
Re-initialize the session backend to make use of overridden settings.
Simulate a request the modifies the session
Handle the response through the middleware
Simulate a request the modifies the session
Handle the response through the middleware
Simulate a request the modifies the session
Handle the response through the middleware
Simulate a request the modifies the session
Handle the response through the middleware
Check that the value wasn't saved above.
Handle the response through the middleware. It will try to save the  deleted session which will cause an UpdateError that's caught and  results in a redirect to the original page.
Check that the response is a redirect.
Before deleting, there has to be an existing cookie
Simulate a request that ends the session
Handle the response through the middleware
Check that the cookie was deleted, not recreated.  A deleted cookie header looks like:   Set-Cookie: sessionid=; expires=Thu, 01-Jan-1970 00:00:00 GMT; Max-Age=0; Path=/
Before deleting, there has to be an existing cookie
Simulate a request that ends the session
Handle the response through the middleware
Check that the cookie was deleted, not recreated.  A deleted cookie header with a custom domain looks like:   Set-Cookie: sessionid=; Domain=.example.local;               expires=Thu, 01-Jan-1970 00:00:00 GMT; Max-Age=0; Path=/
Simulate a request that ends the session
Handle the response through the middleware
A cookie should not be set.  The session is accessed so "Vary: Cookie" should be set.
Set a session key and some data.  Handle the response through the middleware.  A cookie should be set, along with Vary: Cookie.
Empty the session data.  Handle the response through the middleware.  While the session is empty, it hasn't been flushed so a cookie should  still be set, along with Vary: Cookie.
Don't need DB flushing for these tests, so can use unittest.TestCase as base class
The cookie backend doesn't handle non-default expiry dates, see 19201
signed_cookies backend should handle unpickle exceptions gracefully  by creating a new session
Check each of the allowed method names
Check the case view argument is ok if predefined on the class...  ...but raises errors otherwise.
Let the cache expire and test again
we can't use self.rf.get because it always sets QUERY_STRING
the test_name key is inserted by the test classes parent
test that kwarg overrides values assigned higher up
Checks 'pony' key presence in dict returned by get_context_date
Checks 'object' key presence in dict returned by get_context_date 20234
Don't pass queryset as argument
Overwrite the view's queryset with queryset from kwarg
-*- coding: utf-8 -*-
TemplateView
DetailView  FormView
Create/UpdateView
ArchiveIndexView
ListView
YearArchiveView  Mixing keyword and positional captures below is intentional; the views  ought to be able to accept either.
MonthArchiveView
WeekArchiveView
DayArchiveView
TodayArchiveView
DateDetailView
Useful for testing redirects
Ensures get_context_object_name() doesn't reference self.object.
Dummy object, but attr is required by get_template_name()
-*- coding: utf-8 -*-
Test that short datasets ALSO result in a paginated view.
Custom pagination allows for 2 orphans on a page size of 5
Custom pagination allows for 2 orphans on a page size of 5
Regression test for 17535  1 query for authors  same as above + 1 query to test if authors exist + 1 query for pagination
test for 19240  tests that source exception's message is included in page
-*- coding: utf-8 -*-
Regression test for 18087  1 query for years list + 1 query for books  same as above + 1 query to test if books exist + 1 query to count them
Regression test for 18354
Since allow_empty=False, next/prev years must be valid (7164)
Since allow_empty=True, next/prev are allowed to be empty years (7164)
Create a new book in the future
Zebras comes after Dreaming by name, but before on '-pubdate' which is the default sorting
Regression test for 18354
Since allow_empty=False, next/prev months must be valid (7164)
allow_empty = False, empty month
allow_empty = True, empty month
Since allow_empty=True, next/prev are allowed to be empty months (7164)
allow_empty but not allow_future: next_month should be empty (7164)
allow_future = False, future month
allow_future = True, valid future month
Since allow_future = True but not allow_empty, next/prev are not  allowed to be empty months (7164)
allow_future, but not allow_empty, with a current month. So next  should be in the future (yup, 7164, again)
The following test demonstrates the bug  The bug does not occur here because a Book with pubdate of Sep 1 exists
Since allow_empty=False, next/prev weeks must be valid
allow_empty = False, empty week
allow_empty = True, empty month
Since allow_empty=True, next/prev are allowed to be empty weeks
allow_empty but not allow_future: next_week should be empty
January 7th always falls in week 1, given Python's definition of week numbers
Since allow_future = True but not allow_empty, next/prev are not  allowed to be empty weeks
allow_future, but not allow_empty, with a current week. So next  should be in the future
Regression for 14752
Since allow_empty=False, next/prev days must be valid.
allow_empty = False, empty month
allow_empty = True, empty month
Since it's allow empty, next/prev are allowed to be empty months (7164)
allow_empty but not allow_future: next_month should be empty (7164)
allow_future = False, future month
allow_future = True, valid future month
allow_future but not allow_empty, next/prev must be valid
allow_future, but not allow_empty, with a current month.
allow_future for yesterday, next_day is today (17192)
2008-04-02T00:00:00+03:00 (beginning of day) > 2008-04-01T22:00:00+00:00 (book signing event date)  2008-04-03T00:00:00+03:00 (end of day) > 2008-04-02T22:00:00+00:00 (book signing event date)
2008-04-02T00:00:00+03:00 (beginning of day) > 2008-04-01T22:00:00+00:00 (book signing event date)  2008-04-03T00:00:00+03:00 (end of day) > 2008-04-02T22:00:00+00:00 (book signing event date)
Also test with escaped chars in URL
Modification with both POST and PUT (browser compatible)
Also test with escaped chars in URL
Should raise exception -- No redirect URL provided, and no  get_absolute_url provided
Modification with both POST and PUT (browser compatible)
Deletion with POST
Deletion with browser compatible DELETE method
Also test with escaped chars in URL
Should raise exception -- No redirect URL provided, and no  get_absolute_url provided
-*- coding: utf-8 -*-
don't use the manager because we want to ensure the site exists  with pk=1, regardless of whether or not it already exists.
special urls for flatpage test cases
don't use the manager because we want to ensure the site exists  with pk=1, regardless of whether or not it already exists.
Site fields cache needs to be cleared after flatpages is added to  INSTALLED_APPS
don't use the manager because we want to ensure the site exists  with pk=1, regardless of whether or not it already exists.
no 'django.contrib.flatpages.middleware.FlatpageFallbackMiddleware'
no 'django.contrib.flatpages.middleware.FlatpageFallbackMiddleware'
don't use the manager because we want to ensure the site exists  with pk=1, regardless of whether or not it already exists.
don't use the manager because we want to ensure the site exists  with pk=1, regardless of whether or not it already exists.
This cleanup is necessary because contrib.sites cache  makes tests interfere with each other, see 11505
A package that raises an ImportError that can be shared among test apps and  excluded from test discovery.
M2M described on one of the models
Custom through link fields
field order is deliberately inverted. the target field is "invitee".
field order is deliberately inverted.
Since this isn't a symmetrical relation, Tony's friend link still exists.
Forward retrieval  Backward retrieval
don't default to False here, because we want to test that it defaults  to False if unspecified
Truncate the first character so that the hash is invalid.
Get a list of cookies, excluding ones with a max-age of 0 (because  they have been marked for deletion).
Set initial data.  Test that the message actually contains what we expect.
Test before the messages have been consumed
Test deletion of the cookie (storing with an empty value) after the messages have been consumed
Set initial (invalid) data.  Test that the message actually contains what we expect.
When storing as a cookie, the cookie has constant overhead of approx  54 chars, and each message has a constant overhead of about 37 chars  and a variable overhead of zero in the best case. We aim for a message  size which will fit 4 messages into the cookie, but not 5.  See also FallbackTest.test_session_fallback
Encode the messages using the current encoder.
Remove the is_safedata flag from the messages in order to imitate  the behavior of before 1.5 (monkey patching).
Decode the messages in the old format (without is_safedata)
Set initial data.  Test that the message actually contains what we expect.
Set initial cookie data.
Overwrite the _get method of the fallback storage to prove it is not  used (it would cause a TypeError: 'NoneType' object is not callable).
Test that the message actually contains what we expect.
Overwrite the _get method of the fallback storage to prove it is not  used (it would cause a TypeError: 'NoneType' object is not callable).
Test that the message actually contains what we expect.
Set initial cookie and session data.
Test that the message actually contains what we expect.
Set initial cookie and session data.
Test that the message actually contains what we expect.
Set initial cookie and session data.
When updating, previously used but no longer needed backends are  flushed.
Overwrite the _store method of the fallback storage to prove it isn't  used (it would cause a TypeError: 'NoneType' object is not callable).
see comment in CookieText.test_cookie_max_length
LEVEL_TAGS is a constant defined in the  django.contrib.messages.storage.base module, so after changing  settings.MESSAGE_TAGS, we need to update that constant too.
there shouldn't be any messages on second GET request
After iterating the storage engine directly, the used flag is set.  The data does not disappear because it has been iterated.
get_level works even with no storage on the request.
get_level returns the default level if it hasn't been set.
Only messages of sufficient level get recorded.
mixed Text and Char
mixed Text and Char wrapped
wrap the concat in something else to ensure that we're still  getting text rather than bytes
Check nodes counts
Multiple compilations should not alter the generated query.
if alias is null, set to first 5 lower characters of the name
Convert to target timezone before truncation
otherwise, truncate to year
If there was a daylight saving transition, then reset the timezone.
exact is implied and should be the same
date and datetime fields should behave the same
Load fixture 1. Single JSON file, with two objects.
Dump the current contents of the database as a JSON fixture
Try just dumping the contents of fixtures.Category
...and just fixtures.Article
...and both
Specify a specific model twice
Specify a dump that specifies Article both explicitly and implicitly
Specify a dump that specifies Article both explicitly and implicitly,  but lists the app first (22025).
Same again, but specify in the reverse order
Specify one model from one application, and an entire other application.
Load fixture 2. JSON file imported by default. Overwrites some existing objects
Load fixture 3, XML format.
Load fixture 6, JSON file with dynamic ContentType fields. Testing ManyToOne.
Load fixture 7, XML file with dynamic ContentType fields. Testing ManyToOne.
Load fixture 8, JSON file with dynamic Permission fields. Testing ManyToMany.
Load fixture 9, XML file with dynamic Permission fields. Testing ManyToMany.
object list is unaffected
By default, you get raw keys on dumpdata
But you can get natural keys if you ask for them and they are available
You can also omit the primary keys for models that we can get later with natural keys.
Dump the current contents of the database as a JSON fixture
Dump the current contents of the database as an XML fixture
Load fixture1 which has a site, two articles, and a category
Excluding fixtures app should only leave sites
Excluding fixtures.Article/Book should leave fixtures.Category
Excluding fixtures and fixtures.Article/Book should be a no-op
Excluding sites and fixtures.Article/Book should only leave fixtures.Category
Excluding a bogus app should throw an error
Excluding a bogus model should throw an error
Use the default manager  Dump using Django's base manager. Should return all objects,  even those normally filtered by the manager
Test no progress bar when verbosity = 0
Load fixture 4 (compressed), using format specification
Load fixture 5 (compressed), using format *and* compression specification
Load fixture 5 (compressed), only compression specification
The name "fixture5" is ambiguous, so loading it will raise an error
Load db fixtures 1 and 2. These will load using the 'default' database identifier implicitly
MySQL needs a little prodding to reject invalid data.  This won't affect other tests because the database connection  is closed at the end of each test.
Load db fixtures 1 and 2. These will load using the 'default' database identifier explicitly
Try to load db fixture 3. This won't load because the database identifier doesn't match
Load back in fixture 1, we need the articles from it
Try to load fixture 6 using format discovery
Dump the current contents of the database as a JSON fixture
Dump the current contents of the database as an XML fixture
Load fixture 1 again, using format discovery
Try to load fixture 2 using format discovery; this will fail  because there are two fixture2's in the fixtures directory
object list is unaffected
Dump the current contents of the database as a JSON fixture
Load fixture 4 (compressed), using format discovery
! -*- coding: utf-8 -*-
encodestring is a deprecated alias on Python 3
This file contains Chinese symbols and an accented char in the name.
The second value is normalized to an empty name by  MultiPartParser.IE_sanitize()
Empty filenames should be ignored
This test simulates possible directory traversal attacks by a  malicious uploader We have to do some monkeybusiness here to construct  a malicious payload with an invalid file name (containing os.sep or  os.pardir). This similar to what an attacker would need to do when  trying such an attack.
The filenames should have been sanitized by the time it got to the view.
field name, filename, expected
A small file (under the 5M quota)
A big file (over the quota)
Small file posting should work.
Large files don't go through.
AttributeError: You cannot alter upload handlers after the upload has been processed.
Check that the files got actually parsed.
Check that the fd closing logic doesn't trigger parsing of the stream
Maybe this is a little more complicated that it needs to be; but if  the django.test.client.FakePayload.read() implementation changes then  this test would fail.  So we need to know exactly what kind of error  it raises when there is an attempt to read more than the available bytes:
install the custom handler that tries to access request.POST
CustomUploadError is the error that should have been raised
Synthesize the contents of a file upload with a mixed case filename  so we don't have to carry such a file in the Django tests source code  tree.  The name of the file uploaded and the file stored in the server-side  shouldn't differ.
Create a file with the upload directory name  The test needs to be done on a specific string as IOError  is raised even without the patch (just not early enough)
We're not actually parsing here; just checking if the parser properly  instantiates with empty upload handlers.
If a file is posted, the dummy client should only post the file name,  not the full path.
Adding large file to the database should succeed
Check to see if unicode name came through properly.
Check to make sure the exotic characters are preserved even  through file save.
Cleanup the object with its exotic file name immediately.  (shutil.rmtree used elsewhere in the tests to clean up the  upload directory has been seen to choke on unicode  filenames on Windows.)
Regression test for 10183
Regression test for 10183
Check that the no template case doesn't mess with the template assertions
This page will redirect with code 301, not 302
Should redirect to get_view
The redirect target responds with a 301 code, not 200
The redirect target responds with a 301 code, not 200
The chain of redirects stops once the cycle is detected.
The chain of redirects will get back to the starting point, but stop there.
We can't use is_secure() or get_host()  because response.request is a dictionary, not an HttpRequest
This page will redirect with code 301, not 302
This page will redirect with code 301, not 302
For all possible True/False combinations of follow and secure  always redirects to https  the goal scheme is https
For testing field and non-field errors  For testing non-form errors
Create a second client, and log in.
Get a redirection page with the second client.
At this points, the self.client isn't logged in.  Check that assertRedirects uses the original client, not the  default client.
Try to access a login protected page.
This next operation should be successful; if it isn't we have a problem.
We need two different tests to check URLconf substitution -  one to check  it was changed, and another one (without self.urls) to check it was reverted on  teardown. This pair of tests relies upon the alphabetical ordering of test execution.
This test needs to run *after* UrlconfSubstitutionTests; the zz prefix in the  name is to ensure alphabetical ordering.
None, True and False are builtins of BaseContext, and present  in every Context without needing to be added.
Need to insert a context processor that assumes certain things about  the request instance. This triggers a bug caused by some ways of  copying RequestContext.
The session doesn't exist to start.
This request sets a session variable.
Check that the session has been modified
Log in
Session should still contain the modified value
A HEAD request doesn't return any content.
Regression test for 11371
Regression test for 11371
Regression test for 17797
Regression test for 21740
See: https://code.djangoproject.com/ticket/10571.  A GET-like request can pass a query string as data
A GET-like request can pass a query string as part of the URL
Data provided in the URL to a GET-like request is overridden by actual form data
A POST-like request can pass a query string as data
A POST-like request can pass a query string as part of the URL
POST data provided in the URL augments actual form data
Regression test for 10571
Regression test for 10571
Regression test for 10571
Regression test for 10571
apart from the next line the three tests are identical
This test is executed after the previous one
This just checks that the uploaded data is JSON
Special attribute that won't be present on a plain HttpRequest
GROUP BY on Oracle fails with TextField/BinaryField; see 24096.
There is a bug in sqlite < 3.7.0, where placeholder order is lost.  Thus, the above query returns  <condition_value> + <result_value>  for each matching case instead of <result_value> + 1 (24148).
fails on postgresql on Python 2.7 if output_field is not  set explicitly
fails on sqlite if output_field is not set explicitly on all  Values containing timedeltas
fails on postgresql if output_field is not set explicitly
fails on sqlite if output_field is not set explicitly on all  Values containing times
fails on sqlite if output_field is not set explicitly on all  Values containing UUIDs
Testing that:  1. There isn't any object on the remote side of the fk_rel     relation. If the query used inner joins, then the join to fk_rel     would remove o from the results. So, in effect we are testing that     we are promoting the fk_rel join to a left outer join here.  2. The default value of 3 is generated for the case expression.  Now 2 should be generated, as the fk_rel is null.
Testing that:  1. There isn't any object on the remote side of the fk_rel     relation. If the query used inner joins, then the join to fk_rel     would remove o from the results. So, in effect we are testing that     we are promoting the fk_rel join to a left outer join here.  2. The default value of 3 is generated for the case expression.  Now 2 should be generated, as the fk_rel is null.
The first o has 2 as its fk_rel__integer=1, thus it hits the  default=2 case. The other ones have 2 as the result as they have 2  fk_rel objects, except for integer=4 and integer=10 (created above).  The integer=4 case has one integer, thus the result is 1, and  integer=10 doesn't have any and this too generates 1 (instead of 0)  as ~Q() also matches nulls.
Need to use values before annotate so that Oracle will not group  by fields it isn't capable of grouping by.
Test related objects visibility.
A Restaurant can access its place.  A Place can access its restaurant, if available.  p2 doesn't have an associated restaurant.  The exception raised on attribute access when a related object  doesn't exist should be an instance of a subclass of `AttributeError`  refs 21563
Set the place using assignment notation. Because place is the primary  key on Restaurant, the save will create a new restaurant  Set the place back again, using assignment in the reverse direction.
Restaurant.objects.all() just returns the Restaurants, not the Places.  Place.objects.all() returns all Places, regardless of whether they  have Restaurants.
Add a Waiter to the Restaurant.
Query the waiters  Delete the restaurant; the waiter should also be removed
One-to-one fields still work if you create your own primary key
You can have multiple one-to-one fields on a model, too.  This will fail because each one-to-one field must be unique (and  link2=o1 was used for x1, above).
place should not cache restaurant
The bug in 9023: if you access the one-to-one relation *before*  setting to None and deleting, the cascade happens anyway.
Assigning None succeeds if field is null=True.
Assigning None doesn't throw AttributeError if there isn't a related  UndergroundBar.
Look up the objects again so that we get "fresh" objects
Accessing the related object again returns the exactly same object
But if we kill the cache, we get a new object
Reassigning the Restaurant object results in an immediate cache update  We can't use a new Restaurant because that'll violate one-to-one, but  with a new *instance* the is test below will fail if 6886 regresses.
Assigning None succeeds if field is null=True.
Assigning None will not fail: Place.restaurant is null=False
You also can't assign an object of the wrong type here
Creation using keyword argument should cache the related object.
Creation using keyword argument and unsaved related instance (8070).
Creation using attname keyword argument and an id will cause the related  object to be fetched.
Use a fresh object without caches
Use a fresh object without caches
When there's no instance of the origin of the one-to-one
When there's one instance of the origin  (p.undergroundbar used to return that instance)
Several instances of the origin are only possible if database allows  inserting multiple NULL rows for a unique constraint
When there are several instances of the origin
Assigning a reverse relation on an unsaved object is allowed.
However saving the object is not allowed.
Only one school is available via all() due to the custom default manager.
Only one director is available via all() due to the custom default manager.
Make sure the base manager is used so that the related objects  is still accessible even if the default manager doesn't normally  allow it.
Make sure the base manager is used so that an student can still access  its related school even if the default manager doesn't normally  allow it.
If the manager is marked "use_for_related_fields", it'll get used instead  of the "bare" queryset. Usually you'd define this as a property on the class,  but this approximates that in a way that's easier in tests.
The exception raised on attribute access when a related object  doesn't exist should be an instance of a subclass of `AttributeError`  refs 21563
Test that subquery using primary key and a query against the  same model works correctly.  Test that subquery using 'pk__in' instead of 'place_id__in' work, too.
'managed' is True by default. This tests we can set it explicitly.
To re-use the many-to-many intermediate table, we need to manually set up  things up.
Firstly, we need some models that will create the tables, purely so that the  tables are created. This is a test setup, not a requirement for unmanaged  models.
Unmanaged with an m2m to unmanaged: the intermediary table won't be created.
Here's an unmanaged model with an m2m to a managed one; the intermediary  table *will* be created (unless given a custom `through` as for C02 above).
Insert some data into one set of models.
... and pull it out via the other set.
-*- coding: utf-8 -*-
Default views
a view that raises an exception for the debug view
deprecated i18n views
i18n views
Static views
-*- coding: utf-8 -*-
-*- coding:utf-8 -*-
Special URLs for particular regression cases.
redirects, both temporary and permanent, with non-ASCII targets
json response
Make sure that a callable that raises an exception in the stack frame's  local vars won't hijack the technical 500 response. See:  http://code.djangoproject.com/ticket/15025
We need to inspect the HTML generated by the fancy 500 debug view but  the test client ignores it, so we send it explicitly.
We need to inspect the HTML generated by the fancy 500 debug view but  the test client ignores it, so we send it explicitly.
If we do not specify a template, we need to make sure the debug  view doesn't blow up.
The default logging config has a logging filter to ensure admin emails are  only sent with DEBUG=False, but since someone might choose to remove that  filter, we still want to be able to test the behavior of error emails  with DEBUG=True. So we need to remove the filter temporarily.
Make sure datetime and Decimal objects would be serialized properly
-*- coding:utf-8 -*-
response content must include a line like:  "this is to be translated": <value of trans_txt Python variable>  json.dumps() is used to be able to check unicode strings  Message with context (msgctxt)
Force a language via GET otherwise the gettext functions are a noop!
The test cases use fixtures & translations from these apps.
See ticket 14565
-*- coding:utf-8 -*-
we force saving language to a cookie rather than a session  by excluding session middleware and those which do require it
we force saving language to a cookie rather than a session  by excluding session middleware and those which do require it
The url() & view must exist for this to work as a regression test.
And reverse
And reverse
response content must include a line like:  "this is to be translated": <value of trans_txt Python variable>  json.dumps() is used to be able to check unicode strings  Message with context (msgctxt)
default plural function
Force a language via GET otherwise the gettext functions are a noop!
The test cases use fixtures & translations from these apps.
strip() to prevent OS line endings from causing differences
This is 24h before max Unix time. Remember to fix Django and  update this test well before 2038 :)
-*- coding: utf-8 -*-  This coding header is significant for tests, as the debug view is parsing  files to search for such a header to decode the source file content
Ensure that when DEBUG=True, technical_500_template() is called.
Ensure no 403.html template exists to test the default case.
Set up a test 403.html template.
We look for a HTML fragment of the form  '<div class="context" id="c38123208">', not '<div class="context" id="c38,123,208"'
May need a query to initialize MySQL connection
No template directories are configured, so no templates will be found.
Ensure that when DEBUG=True, technical_500_template() is called.
Raises a TemplateDoesNotExist exception and shows the debug view.
All variables are shown.
All POST parameters are shown.
Non-sensitive variable's name and value are shown.  Sensitive variable's name is shown but not its value.
All POST parameters' names are shown.  Non-sensitive POST parameters' values are shown.
Sensitive POST parameters' values are not shown.
Show variable names but not their values.
All POST parameters' names are shown.  No POST parameters' values are shown.
Frames vars are never shown in plain text email reports.
Frames vars are shown in html email reports.
All POST parameters are shown.
Frames vars are never shown in plain text email reports.
Frames vars are shown in html email reports.
All POST parameters' names are shown.  Non-sensitive POST parameters' values are shown.
Sensitive POST parameters' values are not shown.
Frames vars are never shown in plain text email reports.  All POST parameters' names are shown.  No POST parameters' values are shown.
Custom exception handler, just pass it into ExceptionReporter
Both messages are twice on page -- one rendered as html,  one as plain text (for pastebin)
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
This is the same as in the default project template
We only test things here that are difficult to test elsewhere  Many other tests are found in the main tests for builtin template tags  Test parsing via the printed parse tree
(False and False) or True == True   <- we want this one, like Python  False and (False or True) == False
True or (False and False) == True   <- we want this one, like Python  (True or False) and False == False
(1 or 1) == 2  -> False  1 or (1 == 2)  -> True   <- we want this one
-*- coding: utf-8 -*-
Check we don't double escape
Check we don't double escape
Test that the decorators preserve the decorated function's docstring, name and attributes.
The 'context' parameter must be present when takes_context is True
The 'context' parameter must be present when takes_context is True
Test that the decorators preserve the decorated function's docstring, name and attributes.
Test that the decorators preserve the decorated function's docstring, name and attributes.
The 'context' parameter must be present when takes_context is True
A test middleware that installs a temporary URLConf
explicit baking
response is not re-rendered without the render call
rebaking doesn't change the rendered content
but rendered content can be overridden by manually  setting content
unrendered response raises an exception on iteration
iteration works for rendered responses
unrendered response raises an exception when content is accessed
rendered response content can be accessed
content can be overridden
When the content is rendered, all the callbacks are invoked, too.
Create a template response. The context is  known to be unpicklable (e.g., a function).
But if we render the response, we can pickle it.
...and the unpickled response doesn't have the  template-related attributes, so it can't be re-rendered
...and requesting any of those attributes raises an exception
context processors should be overridden by passed-in context
Create a template response. The context is  known to be unpicklable (e.g., a function).
But if we render the response, we can pickle it.
...and the unpickled response doesn't have the  template-related attributes, so it can't be re-rendered
...and requesting any of those attributes raises an exception
Let the cache expire and test again
Let the cache expire and test again
-*- coding: utf-8 -*-
Test urls for testing reverse lookups
Unicode strings are permitted everywhere.
Test urls for namespaces and current_app
-*- coding: utf-8 -*-
Templates can be created from unicode strings.  Templates can also be created from bytestrings. These are assumed to  be encoded using UTF-8.
Contexts can be constructed from unicode or UTF-8 bytestrings.
Since both templates and all four contexts represent the same thing,  they all render the same (and are returned as unicode objects and  "safe" objects as well, for auto-escaping purposes).
Fake views for testing url reverse lookup
Unbalanced blocks
Using generator to mimic concurrency.  The generator is not passed to the 'for' loop, because it does a list(values)  instead, call gen.next() in the template to control the generator.  Simulate that another thread is now rendering.  When the IfChangeNode stores state at 'self' it stays at '3' and skip the last yielded value below.
Autoescape disabling and enabling nest in a predictable way.
Strings (ASCII or unicode) already marked as "safe" are not  auto-escaped
Objects which return safe strings as their __str__ method  won't get double-escaped.
Arguments to filters are 'safe' and manipulate their input unescaped.
coding: utf-8
Successes
Failures
{% url ... as var %}
Filters
Equality
Inequality
Comparison
Contains
AND
OR
NOT
Various syntax errors
A single equals sign is a syntax error.
For this to act as a regression test, it's important not to use  foo=True because True is (not None)
Raise exception if we don't have 3 args, last one an integer
18739: widthratio should handle None args consistently with  non-numerics
Widthratio with variable assignment
coding: utf-8
retrieving language information
blocktrans handling of variables which are not in the context.  this should work as if blocktrans was not there (19915)
trans tag with as var
Test whitespace in filter arguments
blocktrans tag with asvar
coding: utf-8
Check parsing of locale strings
{% load %} tag, importing individual tags
{% load %} tag errors
Test syntax errors
Inherit from a template with block wrapped in an {% if %} tag  (in parent), still gets overridden
The super block will still be found.
Ignore numpy deprecation warnings (23890)
SMART SPLITTING
NUMERIC RESOLUTION
FILTER EXPRESSIONS AS ARGUMENTS
Raise TemplateSyntaxError when trying to access a variable  containing an illegal character.
Don't get confused when parsing something that is almost, but not  quite, a template tag.
Literal strings are permitted inside variables, mostly for i18n  purposes.
12554 -- Make sure a silent_variable_failure Exception is  suppressed on dictionary and attribute lookup.
Something that starts like a number but has an extra lookup works  as a lookup.
Numbers are numbers even if their digits are in the context.
Raise exception if we don't have at least 2 args, first one integer.
Include syntax errors
Repeat to ensure it still works when loading from the cache
Sort the dictionary by key to guarantee the order for testing.
Sort the dictionary by key to guarantee the order for testing.
Filtered variables should reject access of attributes beginning with  underscores.
Translated strings are handled correctly.
Escaped quotes work correctly as well.
Variables should reject access of attributes beginning with  underscores.
Variables should raise on non string type
Correct number of arguments  One optional  Not supplying all
Render another path that uses the same templates from the cache
Template objects should not be duplicated.
Render a second time from cache
We can't access ``my_doodad.value`` in the template, because  ``my_doodad.__call__`` will be invoked first, yielding a dictionary  without a key ``value``.
We can confirm that the doodad has been called
But we can access keys on the dict that's returned  by ``__call__``, instead.
Since ``my_doodad.alters_data`` is True, the template system will not  try to call our doodad but will use string_if_invalid
Double-check that the object was really never called during the  template rendering.
Since ``my_doodad.do_not_call_in_templates`` is True, the template  system will not try to call our doodad.  We can access its attributes  as normal, and we don't have access to the dict that it returns when  called.
Double-check that the object was really never called during the  template rendering.
Double-check that the object was really never called during the  template rendering.
-*- coding: utf-8 -*-
Run a second time from cache
Run a second time from cache
fill the template cache
Retrieve a template specifying a template directory to check  Now retrieve the same template name, but from a different directory
The two templates should not have the same content
UTF-8 bytestrings are permitted.  Unicode strings are permitted.
when testing deprecation warnings, it's useful to run just one test since  the message won't be displayed multiple times
numerous tests make use of an inclusion tag  add this in here for simplicity
Make Engine.get_default() raise an exception to ensure that tests  are properly isolated from Django's global settings.  Set up custom template tag libraries if specified
These two classes are used to test auto-escaping of unicode output.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
lowercase e umlaut
Invalid uses that should fail silently.
Using a filter that forces a string back to unsafe:
Using a filter that forces safeness does not lead to double-escaping
Force to safe, then back (also showing why using force_escape too  early in a chain can lead to unexpected results).
ISO date formats
Timezone name
19370: Make sure |date doesn't blow up on a midnight time object
Default compare with datetime.now()
Compare to a given parameter
Regression for 7443
Ensures that differing timezones are calculated correctly.
Regression for 9065 (two date objects).
-*- coding: utf-8 -*-
Passing ';' to cut can break existing HTML entities, so those strings  are auto-escaped.
-*- coding: utf-8 -*-
Because the result of force_escape is "safe", an additional  escape filter has no effect (to be changed in Django 2.0).
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Default compare with datetime.now()
Compare to a given parameter
Check that timezone is respected
Tests for 7443
Ensures that differing timezones are calculated correctly.
Tests for 9065 (two date objects).
-*- coding: utf-8 -*-
This will lead to a nonsense result, but at least it won't be  exploitable for XSS purposes when auto-escaping is on.
mailto: testing for urlize
Quotes (single and double) and angle brackets shouldn't be considered  part of URLs.
Boolean return value from length_is should not be coerced to a string
Invalid uses that should fail silently.
-*- coding: utf-8 -*-
The above test fails because of Python 2's float handling. Floats  with many zeroes after the decimal point should be passed in as  another type such as unicode or Decimal.
It is only applied once, regardless of the number of times it  appears in a chain (to be changed in Django 2.0).
-*- coding: utf-8 -*-
uppercase E umlaut
11377 Test that joining with unsafe joiners doesn't result in  unsafe strings
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Test that push() limits access to the topmost dict
update only a
update both to check regression
make contexts equals again
The stack should now contain 3 items:  [builtins, supplied context, context processor, empty dict]
Create an engine without any context processors.
test comparing RequestContext to prevent problems if somebody  adds __eq__ in the future
View returning a template response
A view that can be hard to find...
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Scene/Character/Line models are used to test full text search. They're  populated with content from Monty Python and the Holy Grail.
Only create this model for postgres >= 9.4  create an object with this name so we don't have failing imports
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Ensure CreateExtension quotes extension names by creating one with a  dash in its name.
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
This checks that get_prep_value is deferred properly
Regression for 22907
The inner CharField is missing a max_length.
The inner CharField is missing a max_length.
See 22962
Only the CharField should have a LIKE index.  All fields should have regular indexes.
This should not raise a validation error
JSONField input is fine, name is too long
This time, the JSONField input is wrong  Appears once in the textarea and once in the error message
A OneToOneField is just a ForeignKey unique=True, so we don't duplicate  all the tests; just one smoke test to ensure on_delete works for it as  well.
This model is used to test a duplicate query regression (25685)
Testing DO_NOTHING is a bit harder: It would raise IntegrityError for a normal model,  so we connect to pre_delete and set the fk to a known value.
RelToBase should not be queried.
1 (select related `T` instances)  + 1 (select related `U` instances)  + 2 (delete `T` instances in batches)  + 1 (delete `s`)
Attach a signal to make sure we will not do fast_deletes.
Attach a signal to make sure we will not do fast_deletes.
The below doesn't make sense... Why do we need to null out  user.avatar if we are going to delete the user immediately after it,  and there are no more cascades.  1 query to find the users for the avatar.  1 query to delete the user  1 query to null out user.avatar, because we can't defer the constraint  1 query to delete the avatar
Calculate the number of queries needed.  The related fetches are done in batches.  One query for Avatar.objects.all() and then one related fast delete for  each batch.  The Avatar objects are going to be deleted in batches of GET_ITERATOR_CHUNK_SIZE
TEST_SIZE // batch_size (select related `T` instances)  + 1 (select related `U` instances)  + TEST_SIZE // GET_ITERATOR_CHUNK_SIZE (delete `T` instances in batches)  + 1 (delete `s`)
more complex example with multiple object types
One query for the Avatar table and a second for the User one.
1 query to fast-delete the user  1 query to delete the avatar
1 to delete f, 1 to fast-delete m2m for f
1 to delete t, 1 to fast-delete t's m_set
1 for self, 1 for parent  However, this doesn't work as child.parent access creates a query,  and this means we will be generating extra queries (a lot for large  querysets). This is not a fast-delete problem.  self.assertNumQueries(2, c.delete)  1 for self delete, 1 for fast delete of empty "child" qs.  1 for self delete, 1 for fast delete of empty "child" qs.
No problems here - we aren't going to cascade, so we will fast  delete the objects in a single query.  We don't hit parameter amount limits for a, so just one query for  that + fast delete of the related objs.
We should have the request object in the template.  Test is_secure.  Test path.
We should have the debug flag in the template.
And now we should not
Check we have not actually memoized connection.queries  Check queries for DB connection 'other'
We can filter on attribute relationships on same model obj, e.g.  find companies where the number of employees is greater  than the number of chairs.
We can set one field to have the value of another field  Make sure we have enough chairs
We can perform arithmetic operations in expressions  Make sure we have 2 spare chairs
Law of order of operations is followed
Law of order of operations can be overridden by parentheses
ForeignKey can become updated with the value of another ForeignKey.
F Expressions can also span joins
F expressions can be used to update attributes on single objects
We should be able to use Funcs when inserting new data
Aggregates are not allowed when inserting new data
F expressions cannot be used to update attributes which are foreign  keys, or attributes which involve joins.
F expressions cannot be used to update attributes on objects which do  not yet exist in the database
Test that reverse multijoin F() references and the lookup target  the same join. Pre 18375 the F() join was generated first, and the  lookup couldn't reuse that join.
The next query was dict-randomization dependent - if the "gte=1"  was seen first, then the F() will reuse the join generated by the  gte lookup, if F() was seen first, then it generated a join the  other lookups could not reuse.
Another similar case for F() than above. Now we have the same join  in two filter kwargs, one in the lhs lookup, one in F. Here pre  18375 the amount of joins generated was random if dict  randomization was enabled, that is the generated query dependent  on which clause was seen first.
Test that F() expressions do not reuse joins from previous filter.
Reuse the same F-object for another queryset  The original query still works correctly
LH Addition of floats and integers
LH Subtraction of floats and integers
Multiplication of floats and integers
LH Division of floats and integers
LH Modulo arithmetic on integers
LH Bitwise ands on integers
LH Bitwise or on integers
LH Powert arithmetic operation on floats and integers
Right hand operators
RH Addition of floats and integers
RH Subtraction of floats and integers
RH Multiplication of floats and integers
RH Division of floats and integers
RH Modulo arithmetic on integers
RH Powert arithmetic operation on floats and integers
Test data is set so that deltas and delays will be  strictly increasing.
e0: started same day as assigned, zero duration
e1: started one day after assigned, tiny duration, data  set so that end time has no fractional seconds, which  tests an edge case on sqlite. This Experiment is only  included in the test data when the DB supports microsecond  precision.
e2: started three days after assigned, small duration
e3: started four days after assigned, medium duration
e4: started 10 days after assignment, long duration
Ticket 21643
Ticket 21643 - Crash when compiling query more than once  Intentionally no assert
Exclude e1 which has very high precision so we can test this on all  backends regardless of whether or not it supports  microsecond_precision.
Simulate http.server.BaseHTTPRequestHandler.parse_request handling of raw request
On Python 3, %E9 is converted to the unicode replacement character by parse_qsl
If would be nicer if request.COOKIES returned unicode values.  However the current cookie parser doesn't do this and fixing it is  much more work than fixing 20557. Feel free to remove force_str()!
We don't test COOKIES content, as the result might differ between  Python version because parsing invalid content became stricter in  latest versions.
Expect "bad request" response
Regression test for 23173  Test first without PATH_INFO
This method intentionally doesn't work for all cases - part  of the test for ticket 20278
Save it into the database. You have to call save() explicitly.
You can also mix and match position and keyword arguments, but  be sure not to duplicate field information.
You can use 'in' to test for membership...  ... but there will often be more efficient ways if that is all you need:
In PostgreSQL, microsecond-level precision is available.
In MySQL, microsecond-level precision isn't always available. You'll  lose microsecond-level precision once the data is saved.
In MySQL, microsecond-level precision isn't always available. You'll  lose microsecond-level precision once the data is saved.
You can manually specify the primary key when creating a new object.
You can create saved objects in a single step
Edge-case test: A year lookup should retrieve all objects in  the given year, including Jan. 1 and Dec. 31.
Unicode data works, too.
Model instances have a hash function, so they can be used in sets  or as dictionary keys. Two models compare as equal if their primary  keys are equal.
The 'select' argument to extra() supports names with dashes in  them, as long as you use values().
If you use 'select' with extra() and names containing dashes on a  query that's *not* a values() query, those extra 'select' values  will silently be ignored.
test that assign + save works with Promise objects  test .update()  still test bulk_create()
Can't be instantiated
test for 15959
A hacky test for custom QuerySet subclass - refs 17271
Tests for ticket 17712
Tests for 19426
Value based on PK  No PK value -> unhashable (because save() would then change  hash)
Create an Article.  Save it into the database. You have to call save() explicitly.
Change values by changing the attributes, then calling save().
Article.objects.all() returns all the articles in the database.
Django provides a rich database lookup API.
The "__exact" lookup type can be omitted, as a shortcut.
Django raises an Article.DoesNotExist exception for get() if the  parameters don't match any object.  To avoid dict-ordering related errors check only one lookup  in single assert.
Lookup by a primary key is the most common case, so Django  provides a shortcut for primary-key exact lookups.  The following is identical to articles.get(id=a.id).
pk can be used as a shortcut for the primary key name in any query.
Model instances of the same type and same ID are considered equal.
Create a very similar object
Django raises an Article.MultipleObjectsReturned exception if the  lookup matches more than one object
Do not delete a directly - doing so alters its state.
Make sure the _update method below is in fact called.
This is not wanted behavior, but this is how Django has always  behaved for databases that do not return correct information  about matched rows for UPDATE.
MySQL < 5.6.4 removes microseconds from the datetimes which can cause  problems when comparing the original value to that loaded from DB
The old related instance was thrown away (the selfref_id has  changed). It needs to be reloaded on access, so one query  executed.
A model can't have multiple AutoFields  Refs 12467.
The limit_choices_to on the parent field says that a parent object's  number attribute must be 10, so this should fail validation.
Make sure the "commit=False and set field values later" idiom still  works with model validation.
Since a value for pub_date wasn't provided and the field is  blank=True, model-validation should pass.  Also, Article.clean() should be run, so pub_date will be filled after  validation, so the form should save cleanly even though pub_date is  not allowed to be null.
Even though pub_date is set to blank=True, an invalid value was  provided, so it should fail validation.
These two addresses are the same with different syntax
These two are different, because we are not doing IPv4 unpacking
These two are the same, because we are doing IPv4 unpacking
Regression test for 12560
Regression test for 12560
Regression test for 12132
Should work without errors
Should work without errors
-*- encoding: utf-8 -*-
Wrong format  Correct format but invalid date  Correct format but invalid date/time
Wrong format  Correct format but invalid time
Mock out get_wsgi_application so we know its return value is used
This is just to test finding, it doesn't have to be a real WSGI callable
Use a name that avoids collision with the built-in year lookup.
We will need to skip the extract part, and instead go  directly with the originating field, that is self.lhs.lhs  Note that we must be careful so that we have params in the  same order as we have the parts in the SQL.  We use PostgreSQL specific SQL here. Note that we must do the  conversions in SQL instead of in Python to support F() references.
Skip the YearTransform above us (no possibility for efficient  lookup otherwise).  Build SQL where the integer year is concatenated with last month  and day, then convert that to date. (We try to have SQL like:      WHERE somecol <= '2013-12-31')  but also make it work if the rhs_sql is field reference.
We need to be careful so that we get the params in right  places.
mult3__div3 always leads to 0
Same as age >= average_rating
The non-optimized version works, too.
This test will just check the generated SQL for __lte. This  doesn't require running on PostgreSQL and spots the most likely  error - not running YearLte SQL at all.
Two ways to add a customized implementation for different backends:  First is MonkeyPatch of the class.
The other way is to subclass the original lookup and register the subclassed  lookup instead of the original.  This method should be named "as_mysql" for MySQL, "as_postgresql" for postgres  and so on, but as we don't know which DB we are running on, we need to use  setattr.
Use a name that avoids collision with the built-in year lookup.
junk lookup - tries lookup, then transform, then fails  junk transform - tries transform only, then fails  Just getting the year (implied __exact) - lookup only  Just getting the year (explicit __exact) - lookup only
equals now.replace(tzinfo=utc)
Regression for 17414
Can be today or yesterday  Can be today or tomorrow
As 24h of difference they will never be the same
Regression for 18504  This is 2012-03-08HT19:30:00-06:00 in America/Chicago
Because of the DST change, 2 days and 6 hours after the chosen  date in naive arithmetic is only 2 days and 5 hours after in  aware arithmetic.
-*- coding: utf-8 -*-
Test multiple CC with multiple To
Testing with Bcc
Note that in Python 3, maximum line length has increased from 76 to 78
If we don't set the To header manually, it should default to the `to` argument to the constructor
Make sure MIME attachments also works correctly with other encodings than utf-8
Unicode in file name
filename, actual mimetype
Send using non-default connection
Regression for 13433 - Make sure that EmailMessage doesn't mangle  'From ' in message body.
Ticket 3472  Shouldn't use Base64 encoding at all
Ticket 11212  Shouldn't use quoted printable, should detect it can represent content with 7 bit data
Shouldn't use quoted printable, should detect it can represent content with 8 bit data
Ticket 18967  Shouldn't use base64 encoding for a child EmailMessage attachment.  Create a child message first
Now create a parent
Attach to parent as a string
Verify that the child message header is not base64 encoded
Feature test: try attaching email.Message object directly to the mail.
Verify that the child message header is not base64 encoded
Feature test: try attaching Django's EmailMessage object directly to the mail.
Verify that the child message header is not base64 encoded
Simple ASCII address - string form  Bytestrings are transformed to normal strings.
Simple ASCII address - tuple form
Unicode characters are are supported in RFC-6532.
Ticket 18861 - Validate emails when using the locmem backend
ignore decode error in SSL/TLS connection tests as we only care  whether the connection attempt was made
New kwarg added in Python 3.5; default switching to False in 3.6.
According to the spec, mailfrom does not necessarily match the  From header - on Python 3 this is the case where the local part  isn't encoded, so try to correct that.
Find the actual message
Ensure that the message only contains CRLF and not combinations of CRLF, LF, and CR.
Messages are stored in an instance variable for testing.
-*- coding: utf-8 -*-
don't use the manager because we want to ensure the site exists  with pk=1, regardless of whether or not it already exists.
no content_type field
missing object_id field
Whenever a test starts executing, only the "default" database is  connected. We explicitly connect to the "other" database here. If we  don't do it, then it will be implicitly connected later when we query  it, but in that case some database backends may automatically perform  extra queries upon connecting (notably mysql executes  "SET SQL_AUTO_IS_NULL = 0"), which will affect assertNumQueries().
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
At this point, a lookup for a ContentType should hit the DB
A second hit, though, won't hit the DB, nor will a lookup by ID  or natural key
Once we clear the cache, another lookup will again hit the DB
The same should happen with a lookup by natural key  And a second hit shouldn't hit the DB
Empty cache.
Partial cache
Full cache
Make sure deferred model are correctly handled
Make sure deferred model are correctly handled
Make sure stale ContentTypes can be fetched like any other object.  Before Django 1.6 this caused a NoneType error in the caching mechanism.  Instead, just return the ContentType object and let the app detect stale states.
Should display the login screen
View docstring
Overridden because non-trivial TEMPLATES settings aren't supported  but the page shouldn't crash (24125).
Overridden because non-trivial TEMPLATES settings aren't supported  but the page shouldn't crash (24125).
help text in fields
method docstrings
foreign keys
foreign keys with help text
many to many fields
"raw" and "include" directives are disabled
-*- encoding: utf-8 -*-
You can shorten this syntax with code like the following,  which is  especially useful if building the query in stages:
Q objects can be negated
This allows for more complex queries than filter() and exclude()  alone would allow
The 'complex_filter' method supports framework features such as  'limit_choices_to' which normally take a single dictionary of lookup  arguments but need to support arbitrary queries via Q objects too.
Passing "in" an empty list returns no results ...  ... but can return results if we OR it with another query.
Q arg objects are ANDed  Q arg AND order is irrelevant
Try some arg queries with operations other than filter.
Can't run this test under SQLite, because you can't  get two connections to an in-memory database.
Create a second connection to the default database
Close down the second connection.
Start a transaction on the main connection.
Delete something using another database connection.
In the same transaction on the main connection, perform a  queryset delete that covers the object deleted with the other  connection. This causes an infinite loop under MySQL InnoDB  unless we keep track of already deleted objects.
first two asserts are just sanity checks, this is the kicker:
first two asserts just sanity checks, this is the kicker:
Create an Image
Get the Image instance as a File
An Image deletion == File deletion
The Image deletion cascaded and *all* references to it are deleted.
Get the Image as a Photo
A Photo deletion == Image deletion == File deletion
The Photo deletion should have cascaded and deleted *all*  references to it.
A File deletion == Image deletion
The File deletion should have cascaded and deleted *all* references  to it.
Create an Image (proxy of File) and FooFileProxy (proxy of FooFile,  which has an FK to File)
Both Login objs should have same description so that only the one  having smaller PK will be deleted.  Assumed that l1 which is created first has smaller PK.
When a subquery is performed by deletion code, the subquery must be  cleared of all ordering. There was a but that caused _meta ordering  to be used. Refs 19720.
microseconds are lost during a round-trip in the database
interpret the naive datetime in local time to get the correct value
interpret the naive datetime in local time to get the correct value
This combination actually never happens.  interpret the naive datetime in local time to get the correct value  microseconds are lost during a round-trip in the database
interpret the naive datetime in local time to get the correct value
interpret the naive datetime in local time to get the correct value
Only min and max make sense for datetimes.
Only min and max make sense for datetimes.
Regression test for 17755
Regression test for 17742  This is 2011-09-02T01:30:00+03:00 in EAT
naive datetimes are interpreted in local time
naive datetimes are interpreted in local time
microseconds are lost during a round-trip in the database  naive datetimes are interpreted in local time
microseconds are lost during a round-trip in the database
naive datetimes are interpreted in local time
These two dates fall in the same day in EAT, but in different days,  years and months in UTC.
Only min and max make sense for datetimes.
Only min and max make sense for datetimes.
Regression test for 17755
Regression test for 17742  This is 2011-09-02T01:30:00+03:00 in EAT
Regression test for 17294
@skipIfDBFeature and @skipUnlessDBFeature cannot be chained. The  outermost takes precedence. Handle skipping manually instead.
Clear cached properties, after first accessing them to ensure they exist.
Clear cached properties, after first accessing them to ensure they exist.
Depending on the yaml dumper, '!timestamp' might be absent
Transform a list of keys in 'datetimes' to the expected template  output. This makes the definition of 'results' more readable.
Use a pytz timezone as local time
Use a pytz timezone as argument
Use a pytz timezone name as argument
bad datetime value  bad timezone value
Use a pytz timezone as argument
Use a pytz timezone name as argument
Regression for 17274
the actual value depends on the system time zone of the host
Regression for 17343
this is obviously a bug
this is obviously a bug
Datetime inputs formats don't allow providing a time zone.
re-fetch the object for backends that lose microseconds (MySQL)
re-fetch the object for backends that lose microseconds (MySQL)
-*- coding: utf-8 -*-
Use a utf-8 bytestring to ensure it works (see 11710)
we replicate Color to register with another ModelAdmin
a base class for Recommender and Recommendation
Proxy model to test overridden fields attrs on Post model so as not to  interfere with other tests.
`db_index=False` because MySQL cannot index large CharField (21196).
Models for 23329
Models for 23431
Models for 23604 and 23915
Model for 23839  Don't point any FK at this model.
Models for 23934
Models for 25622
-*- coding: utf-8 -*-
Post data for edit inline  inline data  there is no title in database, give one here or formset will fail.
inline data
started with 3 articles, one was deleted.
callables display the callable name.  lambdas display as "lambda" + index that they appear in list_display.
Let's make sure the ordering is right and that we don't get a  FieldError when we change to descending order
Sort by name, gender
Sort by gender descending, name
Test we can override with query string
Test ordering on Model Admin is respected, and overrides Model Meta
Check that we get the columns we expect if we have two columns  that correspond to the same ordering field
Should have 5 columns (including action checkbox col)
Check order
Check sorting - should be by name
Should have 3 columns including action checkbox col.  Check if the correct column was selected. 2 is the index of the  'order' column in the model admin's 'list_display' with 0 being  the implicit 'action_checkbox' and 1 being the column 'stuff'.  Check order of records.
ensure filter link exists  ensure link works  ensure changelist contains only valid objects
Spanning relationships through a nonexistent related object (Refs 16716)
Regression test for 18530
Filters are allowed if explicitly included in list_filter
Filters should be allowed if they involve a local field without the  need to whitelist them in list_filter or date_hierarchy.
Specifying a field that is not referred by any other model registered  to this admin site should raise an exception.
23839 - Primary key should always be allowed, even if the referenced model isn't registered.
23915 - Specifying a field referenced by another model though a m2m should be allowed.
23604, 23915 - Specifying a field referenced through a reverse m2m relationship should be allowed.
23329 - Specifying a field that is not referred by any other model directly registered  to this admin site but registered through inheritance should be allowed.
23431 - Specifying a field that is only referred to by a inline of a registered  model should be allowed.
25622 - Specifying a field of a model only referred by a generic  relation should raise DisallowedModelAdminToField.
We also want to prevent the add, change, and delete views from  leaking a disallowed field value.
Filters should be allowed if they are defined on a ForeignKey pointing to this model
Check the format of the shown object -- shouldn't contain a change link
Put this app's and the shared tests templates dirs in DIRS to take precedence  over the admin's templates dir.
Test custom change list template with custom extra context
Test custom add form template
Add an article so we can test delete, change, and history views
Test custom delete, change, and object history templates  Test custom change form template
this would be the usual behaviour  this is the overridden behaviour
Test with the admin's documented list of required context processors.
Setup permissions, for our users who can add, change, and delete.
User who can add Articles  User who can change Articles
User who can delete Articles
login POST dicts
Super User
Test if user enters email address  only correct passwords get a username hint  check to ensure if there are multiple email addresses a user doesn't get a 500
Add User
Change User
Delete User
Regular User should not be able to login.
Requests without username should not return 500 errors.
Regular User should not be able to login.
User with permissions should be able to login.
Staff should be able to login.
Establish a valid admin session
Logging in with non-admin user fails
Establish a valid admin session
Logging in with admin user while already logged in
Anonymous user should not be shown the hint
Non-staff user should be shown the hint
Change User should not have access to add articles  make sure the view removes test cookie  Try POST just to make sure
Add user may login and POST to add view, then redirect to admin root
Check that the addition was logged correctly
Super can add too, but is redirected to the change list view
8509 - if a normal user is already logged in, it is possible  to change user into the superuser without error  Check and make sure that if user expires, data still persists  make sure the view removes test cookie
add user should not be able to view the list of article or change any of them
change user can view all items and edit them
one error in form should produce singular error message, multiple errors plural
Test redirection when using row-level change permissions. Refs 11513.
Add user can perform "Save as new".
Change user cannot perform "Save as new" (no 'add' permission).
User with both add and change permissions should be redirected to the  change page for the newly created object.
add user should not be able to delete articles
Delete user can delete  test response contains link to related Article
add user should not be able to view the list of article or change any of them
change user can view all items and edit them
Test redirection when using row-level change permissions. Refs 11513.
The user can't add sections yet, so they shouldn't see the "add section" link.  Allow the user to add sections too. Now they can see the "add section" link.
The user can't change sections yet, so they shouldn't see the "change section" link.  Allow the user to change sections too. Now they can see the "change section" link.
The user can't delete sections yet, so they shouldn't see the "delete section" link.  Allow the user to delete sections too. Now they can see the "delete section" link.
the user has no module permissions, because this module doesn't exist
the user now has module permissions
Not logged in: we should see the login page.
Logged in? Redirect.  Can't use self.assertRedirects() because User.get_absolute_url() is silly.  Domain may depend on contrib.sites tests also run
Emulate Article creation for user with add-only permission.
User who can change Reports
we shouldn't get a 500 error caused by a NoReverseMatch
this URL now comes through reverse(), thus url quoting and iri_to_uri encoding
this URL now comes through reverse(), thus url quoting and iri_to_uri encoding
inline data
2 inputs per object(the field and the hidden id field) = 6  4 management hidden fields = 4  4 action inputs (3 regular checkboxes, 1 checkbox to select all)  main form submit button = 1  search field and search submit button = 2  CSRF field = 1  field to track 'select all' across paginated views = 1  6 + 4 + 4 + 1 + 2 + 1 + 1 = 19 inputs  1 select per object = 3 selects
Ticket 12707: Saving inline editable should not show admin  action warnings
test a filtered page
test a searched page
Same data as above: Forbidden because of unique_together!
Same data as above: Forbidden because of unique_together!
Same data also.
test if non-form errors are handled; ticket 12716
Ensure that the form processing understands this as a list_editable "Save"  and not an action "Go".
test if non-form errors are correctly handled; ticket 12878
NB: The order values must be changed so that the items are reordered.
Ensure that the form processing understands this as a list_editable "Save"  and not an action "Go".
Successful post will redirect
Check that the order values have been applied to the right objects
List editable changes should not be executed if the action "Go" button is  used to submit the form.
List editable changes should be executed if the "Save" button is  used to submit the form - any action choices should be ignored.
Only one hidden field, in a separate place than the table.
Only one hidden field, in a separate place than the table.
confirm the search returned 1 object
confirm the search returned one object
confirm the search returned zero objects
confirm the search returned one object
confirm the search returned zero objects
confirm the search returned one object
confirm the search returned one object
test the add case  make sure we have no duplicate HTML names
test the add case  inline data
make sure we have no duplicate HTML names
A POST request to delete protected objects should display the page  which says the deletion is prohibited.
No 500 caused by NoReverseMatch  The page shouldn't display a link to the nonexistent change page
Two different actions selected on the two forms...  ...but we clicked "go" on the top form.
Send mail, don't delete.
Insert some data
Hit the page once to get messages out of the queue message list  Ensure that data is still not visible on the page
create 2 Person objects
5 queries are expected: 1 for the session, 1 for the user,  2 for the counts and 1 for the objects on the page
model has __str__ method  Emulate model instance creation via the admin  Message should contain non-ugly model verbose name
model has no __str__ method  Emulate model instance creation via the admin  Message should contain non-ugly model verbose name
model has __str__ method  Emulate model instance creation via the admin  Message should contain non-ugly model verbose name
model has no __str__ method  Emulate model instance creation via the admin  Message should contain non-ugly model verbose name
model has __str__ method  Emulate model instance edit via the admin  Message should contain non-ugly model verbose name. Instance  representation is set by model's __str__()
model has no __str__ method  Emulate model instance edit via the admin  Message should contain non-ugly model verbose name. The ugly(!)  instance representation is set by six.text_type()
model has __str__ method  Emulate model instance edit via the admin  Message should contain non-ugly model verbose name. The instance  representation is set by model's __str__()
model has no __str__ method  Emulate model instance edit via the admin  Message should contain non-ugly model verbose name. The ugly(!)  instance representation is set by six.text_type()
Set up test Picture and Gallery.  These must be set up here instead of in fixtures in order to allow Picture  to use a NamedTemporaryFile.
First add a new inline
Check that the PK link exists on the rendered form
Now resave that inline
Now modify that inline
First add a new inline
Check that the PK link exists on the rendered form
Now resave that inline
Now modify that inline
First add a new inline
Check that the PK link exists on the rendered form
Now resave that inline
Now modify that inline
First add a new inline
Check that the PK link exists on the rendered form
Now resave that inline
Now modify that inline
First add a new inline
Check that the PK link exists on the rendered form
Now resave that inline
Now modify that inline
Create some objects with an initial ordering
NB: The order values must be changed so that the items are reordered.
Successful post will redirect
Check that the order values have been applied to the right objects
Main form ----------------------------------------------------------
Stacked inlines ----------------------------------------------------  Initial inline
Add an inline  50 characters maximum for slug1 field  60 characters maximum for slug2 field
Tabular inlines ----------------------------------------------------  Initial inline
Add an inline
Save and check that everything is properly stored in the database  75 characters in name field
Slugs are empty to start with.
The slugs got prepopulated since they were originally empty
Save the object
The slugs got prepopulated didn't change since they were originally not empty
First form field has a single widget
First form field has a MultiWidget
Click 'cancel' on the delete page.  Wait until we're back on the change page.
Click 'cancel' on the delete page.  Wait until we're back on the change page.
Change popup
Add popup
Select "parent2" in the popup.  The newly selected pk should appear in the raw id input.
3 fields + 2 submit buttons + 5 inline management form fields, + 2  hidden fields for inlines + 1 field for the inline + 2 empty form
Checks that multiline text in a readonly field gets <br /> tags  Remove only this last line when the deprecation completes.
Checking readonly field.  Checking readonly field in inline.
The reverse relation also works if the OneToOneField is null.
The allowed option should appear twice; the limited option should not appear.
Find the link
Handle relative links  Get the popup and verify the correct objects show up in the resulting  page. This step also tests integers, strings and booleans in the  lookup query string; in model we define inquisition field to have a  limit_choices_to option that includes a filter on a string field  (inquisition__actor__name), a filter on an integer field  (inquisition__actor__age), and a filter on a boolean field  (inquisition__expected).
Find the link
Handle relative links  Get the popup and verify the correct objects show up in the resulting  page. This step tests field__isnull=0 gets parsed correctly from the  lookup query string; in model we define defendant0 field to have a  limit_choices_to option that includes "actor__title__isnull=False".
Find the link
Handle relative links  Get the popup and verify the correct objects show up in the resulting  page. This step tests field__isnull=1 gets parsed correctly from the  lookup query string; in model we define defendant1 field to have a  limit_choices_to option that includes "actor__title__isnull=True".
Don't depend on a warm cache, see 17377.
Ensure no queries are skipped due to cached content type for Group.
The main form
The tabular inline
General index page
App index page
The builtin tag group exists
A builtin tag exists in both the index and detail
An app tag exists in both the index and detail
The admin list tag group exists
An admin list tag exists in both the index and detail
The builtin filter group exists
A builtin filter exists in both the index and detail
no day-level links
no day/month-level links
and make sure GET parameters still behave correctly
follow the redirect and test results.
Test equality.
Test inequality.
Ignore scheme and host.
Ignore ordering of querystring.
Ignore ordering of _changelist_filters.
Check the `change_view` link has the correct querystring.
Get the `change_view`.
Check the form action.
Check the history link.
Check the delete link.
Test redirect on "Save".
Test redirect on "Save and continue".
Test redirect on "Save and add new".
Get the `add_view`.
Check the form action.
Test redirect on "Save".
Test redirect on "Save and continue".
Test redirect on "Save and add new".
Test redirect on "Delete".
The form validation should fail because 'some_required_info' is  not included on the parent form, and the family_name of the parent  does not match that of the child
just verifying the parent form failed validation, as expected --  this isn't the regression test
actual regression test
The form validation should fail because 'some_required_info' is  not included on the parent form, and the family_name of the parent  does not match that of the child
just verifying the parent form failed validation, as expected --  this isn't the regression test
actual regression test
Restore the original values for the benefit of other tests.
we have registered two models from two different apps
admin_views.Article
auth.User
-*- coding: utf-8 -*-
A method with the same name as a reverse accessor.
Order by a field that isn't in list display, to be able to test  whether ordering is preserved.
Corner case: Don't call parent implementation
For Selenium Prepopulated tests -------------------------------------
Disable change_view, but leave other urls untouched
We intentionally register Promo and ChapterXtra1 but not Chapter nor ChapterXtra2.  That way we cover all four cases:      related ForeignKey object registered in admin      related ForeignKey object not registered in admin      related OneToOne object registered in admin      related OneToOne object not registered in admin  when deleting Book so as exercise all four troublesome (w.r.t escaping  and calling force_text to avoid problems on Python 2.3) paths through  contrib.admin.utils's get_deleted_objects function.
Register core models we need in our tests
Used to test URL namespaces
A custom index view.
used for testing password change on a user not in queryset
This cleanup is necessary because contrib.sites cache  makes tests interfere with each other, see 11505
Making sure there's only 1 `rss` element and that the correct  RSS version was specified.
Making sure there's only one `channel` element w/in the  `rss` element.
Find the last build date
Ensure the content of the channel is correct
Check feed_url is passed
Find the pubdate of the first feed item
Assert that <guid> does not have any 'isPermaLink' attribute
Making sure there's only 1 `rss` element and that the correct  RSS version was specified.
Making sure there's only one `channel` element w/in the  `rss` element.
Ensure the content of the channel is correct
Check feed_url is passed
this feed has a `published` element with the latest date
this feed has an `updated` element with the latest date
Naive date times passed in get converted to the local time zone, so  check the received zone offset against the local offset.
No last-modified when feed has no item_pubdate
Defining a template overrides any item_title definition
Provide a weird offset so that the test can know it's getting this  specific offset and not accidentally getting on from  settings.TIME_ZONE.
2 queries here as pool 3 has tournament 2, which is not cached  and the other direction
Testing choices= Iterable of Iterables    See: https://code.djangoproject.com/ticket/20430
Models created as unmanaged as these aren't ever queried
Models created as unmanaged as these aren't ever queried
Models created as unmanaged as these aren't ever queried
All our models should validate properly  Validation Tests:    * choices= Iterable of Iterables        See: https://code.djangoproject.com/ticket/20430    * related_name='+' doesn't clash with another '+'        See: https://code.djangoproject.com/ticket/21375
Oracle can have problems with a column named "date"
Set a new empty display value on AdminSite.
Test with list_editable fields  make sure that hidden fields are in the correct place
make sure that list editable fields are rendered in divs correctly
Test with list_editable fields
Add custom method with allow_tags attribute
There's only one Group instance
There's only one Group instance
There's only one Concert instance
There's only one Quartet instance
There's only one ChordsBand instance
Two children with the same name
Make sure distinct() was called
Make sure distinct() was called
There's only one Concert instance
Test default queryset
Test custom queryset
Test with user 'noparents'
Test with user 'parents'
Test default implementation
Add "show all" parameter to request
Test valid "show all" request (number of total objects is under max)  200 is the max we'll pass to ChangeList
Test invalid "show all" request (number of total objects over max)  falls back to paginated pages  30 is the max we'll pass to ChangeList for this test
just want to ensure it doesn't blow up during rendering  Reverse one-to-one relations should work.
Setup the test to reflect the DB state after step 2 where User2 has  edited the first swallow object's speed from '4' to '1'.
Send the POST from User1 for step 3. It's still using the changelist  ordering from before User2's edits in step 2.
The object User1 edited in step 3 is displayed on the changelist and  has the correct edits applied.  No new swallows were created.
When no order is defined at all, everything is ordered by '-pk'.
When an order field is defined but multiple records have the same  value for that field, make sure everything gets ordered by -pk as well.
When order fields are defined, including the pk itself, use them.
When no order is defined at all, use the model's default ordering (i.e. 'number')
When an order field is defined but multiple records have the same  value for that field, make sure everything gets ordered by -pk as well.
When order fields are defined, including the pk itself, use them.
Test with user 'noparents'
Test with user 'parents'
instantiating and setting up ChangeList object
assuming we have exactly `objects_count` objects
setting page number and calculating page range
The "Add" button inside the object-tools shouldn't appear.
Rendering should be u'' since this templatetag just logs,  it doesn't render any string.
Test amount of rows in the Changelist
Test current selection
Select a row and check again
Create an object for sitemap content.
This cleanup is necessary because contrib.sites cache  makes tests interfere with each other, see 11505
Retrieve the sitemap. Check that priorities  haven't been rendered in localized format
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Load fixture 1. Single JSON file, with two objects
Load fixture 2. JSON file imported by default. Overwrites some  existing objects
Load a fixture that doesn't exist
The master app registry is always ready when the tests run.  Non-master app registries are populated in __init__.
App label is case-sensitive, Model name is case-insensitive.
A single argument is accepted.
Construct a new model in a new app registry  Make sure it appeared in the right place!
When __name__ and __module__ match we assume the module  was reloaded and issue a warning. This use-case is  useful for REPL. Refs 23621.
If it doesn't appear to be a reloaded module then we expect  a RuntimeError.
Test models appearing twice, and models appearing consecutively
LazyModelA shouldn't be waited on since it's already registered,  and LazyModelC shouldn't be waited on until LazyModelB exists.
Test that multiple operations can wait on the same model
Now we are just waiting on LazyModelC.
Everything should be loaded - make sure the callback was executed properly.
We need nsapp to be top-level so our multiple-paths tests can add another  location for it (if its inside a normal package with an __init__.py that  isn't possible). In order to avoid cluttering the already-full tests/ dir  (which is on sys.path), we add these new entries to sys.path temporarily.
Temporarily add two directories to sys.path that both contain  components of the "nsapp" package.
Temporarily add two directories to sys.path that both contain  components of the "nsapp" package.
Avoid having two "id" fields in the Child1 subclass
Check for ticket 13839
An explicit link to the parent (we can control the attribute name).
The Student and Worker models both have 'name' and 'age' fields on  them and inherit the __str__() method, just as with normal Python  subclassing. This is useful if you want to factor out common  information for programming purposes, but still completely  independent separate models at the database level.
The children inherit the Meta class of their parents (if they don't  specify their own).
Since Student does not subclass CommonInfo's Meta, it has the effect  of completely overriding it. So ordering by name doesn't take place  for Students.
However, the CommonInfo class cannot be used as a normal model (it  doesn't exist as a model).
Even though p.supplier for a Place 'p' (a parent of a Supplier), a  Restaurant object cannot access that reverse relation, since it's not  part of the Place-Supplier Hierarchy.
The Post model has distinct accessors for the Comment and Link models.
The Post model doesn't have an attribute called  'attached_%(class)s_set'.
The Post model doesn't have a related query accessor based on  related_name (attached_comment_set).
Make sure Restaurant and ItalianRestaurant have the right fields in  the right order.
Low-level test for related_val  Higher level test for correct query values (title foof not  accidentally found).
Capture the expected query in a database agnostic way  Capture the queries executed when a subclassed model instance is saved.
Equality doesn't transfer in multitable inheritance.
Parent fields can be used directly in filters on the child model.
Filters against the parent model return objects of the parent's type.
Since the parent and child are linked by an automatically created  OneToOneField, you can get from the parent to the child by using the  child's name.
This won't work because the Demon Dogs restaurant is not an Italian  restaurant.
An ItalianRestaurant which does not exist is also a Place which does  not exist.
MultipleObjectsReturned is also inherited.
Related objects work just as they normally do.
This won't work because the Place we select is not a Restaurant (it's  a Supplier).
The update() command can update fields in parent and child classes at  once (although it executed multiple SQL queries to do so).
The values() command also works on fields from parent models.
select_related works with fields from the parent object as if they  were a normal part of the model.
Test that the field was actually deferred
Test that model fields where assigned correct values
Refs 12567
Refs 12567
Silence the django.server logger by replacing its StreamHandler with  NullHandler.
The correct level gets the message.
Incorrect levels shouldn't have any messages.
WSGIRequestHandler closes the output file; we need to make this a  no-op so we can still read its contents.
We don't need to check stderr, but we don't want it in test output  instantiating a handler runs the request as side effect
Backup original environment variable
Just the host is not accepted
The host must be valid
The list of ports must be in a valid format
Restore original environment variable
put it in a list to prevent descriptor lookups in test
skip it, as setUpClass doesn't call its parent either
We're out of ports, LiveServerTestCase correctly fails with  a socket error.  Unexpected error.
We've acquired a port, ensure our server threads acquired  different addresses.
Don't want to depend on Pillow in this test  field_image = models.ImageField("verbose field")
See ticket 16570.
See ticket 18389.
Set up a temp directory for file storage.
-*- coding: utf-8 -*-
When an extra clause exists, the boolean conversions are applied with  an offset (13293).
select_related('fk_field_name')
select_related()
If the field has a relation, there should be only one of the  4 cardinality flags available.
Test classes are what we expect
Ensure all m2m reverses are m2m
Test classes are what we expect
Ensure all o2m reverses are m2o
Test classes are what we expect
Ensure all m2o reverses are o2m
Test classes are what we expect
Ensure all o2o reverses are o2o
null isn't well defined for a ManyToManyField, but changing it to  True causes backwards compatibility problems (25320).
Regression test for 23098  Will raise ZeroDivisionError if lazy is evaluated
Pillow not available, create dummy classes (tests will be skipped anyway)
Person model to use for tests.  File class to use for file instances.
Check height/width attributes of field.
Check height/width fields of model, if defined.
Create two Persons with different mugshots.
Test again with an instance fetched from the db.
Instance from db should match the local instance.
Get a "clean" model instance  It won't have an opened file.
After asking for the size, the file should still be closed.
TestImageField value will default to being an instance of its  attr_class, a  TestImageFieldFile, with name == None, which will  cause it to evaluate as False.
Test setting a fresh created model instance.
If image assigned to None, dimension fields should be cleared.
A new file should update dimensions.
Field and dimensions should be cleared after a delete.
Dimensions should get set if file is saved.
Test dimensions after fetching from database.  Bug 11084: Dimensions should not get recalculated if file is  coming from the database.  We test this by checking if the file  was opened.  After checking dimensions on the image field, the file will have  opened.  Dimensions should now be cached, and if we reset was_opened and  check dimensions again, the file should not have opened.
If we assign a new image to the instance, the dimensions should  update.  Dimensions were recalculated, and hence file should have opened.
Clear the ImageFields one at a time.
We can use save=True when deleting the image field with null=True  dimension fields and the other field has an image.
Dimensions should get set for the saved file.
Test dimensions after fetching from database.  Bug 11084: Dimensions should not get recalculated if file is  coming from the database.  We test this by checking if the file  was opened.  After checking dimensions on the image fields, the files will  have been opened.  Dimensions should now be cached, and if we reset was_opened and  check dimensions again, the file should not have opened.
If we assign a new image to the instance, the dimensions should  update.  Dimensions were recalculated, and hence file should have opened.
Check that the two objects were correctly created.
regression for 24611
exercises ForeignKey.get_db_prep_value()
Need a TransactionTestCase to avoid deferring FK constraint checking.
This should not crash.
Other model with different datetime.
The following is equivalent to UTC 2014-03-12 18:34:23.24000.  The following is equivalent to UTC 2014-03-13 05:34:23.24000.  In Vancouver, we expect both results.  But in UTC, the __date only matches one of them.
Try setting float field to unsaved object  Set value to valid and save  Set field to object on saved instance  Try setting field to object on retrieved object
Resave (=update)  Test default value
-*- coding:utf-8 -*-
Several etags in If-None-Match is a bit exotic but why not?
see http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.htmlsec13.3.4
To test fix for 11263
To test fix for 7551
Tagging stuff.
Original list of tags:
shouldn't had changed the tag
Recall that the Mineral class doesn't have an explicit GenericRelation  defined. That's OK, because you can create TaggedItems explicitly.  However, excluding GenericRelations means your lookups have to be a  bit more explicit.
One update() query.
One save() for each object.
Ensure that querysets used in reverse GFK assignments are pre-evaluated  so their value isn't affected by the clearing operation in  ManyRelatedManager.set() (19816).
Test that GenericRelation by default isn't usable from  the reverse side.
Simple tests for multiple GenericForeignKeys  only uses one model, since the above tests should be sufficient.
Create directly
Create using GenericRelation
Filtering works
Filtering and deleting works
If we delete cheetah, Comparisons with cheetah as 'first_obj' will be  deleted since Animal has an explicit GenericRelation to Comparison  through first_obj. Comparisons with cheetah as 'other_obj' will not  be deleted.
GenericForeignKey should work with subclasses (see 8309)
GenericRelations to models that use multi-table inheritance work.  We're generating a slightly inefficient query for tags__tag - we  first join ValuableRock -> TaggedItem -> ValuableTaggedItem, and then  we fetch tag by joining TaggedItem from ValuableTaggedItem. The last  join isn't necessary, as TaggedItem <-> ValuableTaggedItem is a  one-to-one join.
GenericForeignKey should not use the default manager (which may filter objects) 16048
get_or_create should work with virtual fields (content_object)
update_or_create should work with virtual fields (content_object)
Create a Vegetable and Mineral with the same id.
AllowsNullGFK doesn't require a content_type, so None argument should  also be allowed.  TaggedItem requires a content_type but initializing with None should  be allowed.
second would clash with the __second lookup.
Exact query with value None returns nothing ("is NULL" in sql,  but every 'id' field has a value).
The same behavior for iexact query.
Excluding the previous result returns everything.
Valid query, but fails because foo isn't a keyword
Can't use None on anything other than __exact and __iexact
Related managers use __exact=None implicitly if the object hasn't been saved.
Ticket 13815: check if <reverse>_isnull=False does not produce  faulty empty lists
Sqlite had a problem where all the same-valued models were  collapsed to one insert.
SQLite had a problem with more than 500 UNIONed selects in single  query.
We can't assume much about the ID's created, except that the above  created IDs must exist.
Regression for 13227 -- having an attribute that  is unpicklable doesn't stop you from cloning queries  that use objects of that type as an argument.
A complex ordering specification. Should stress the system a bit.
Ticket 23721
db_table names have capital letters to ensure they are quoted in queries.
Create these out of order so that sorting by 'id' will be different to sorting  by 'info'. Helps detect some problems later.
Ordering by 'rank' gives us rank2, rank1, rank3. Ordering by the Meta.ordering  will be rank3, rank2, rank1.
It is possible to reuse U for the second subquery, no need to use W.  So, 'U0."id"' is referenced twice.
Checking that no join types are "left outer" joins.
Each filter call is processed "at once" against a single table, so this is  different from the previous example as it tries to find tags that are two  things at once (rather than two tags).
Make sure .distinct() works with slicing (this was broken in Oracle).
Merging two empty result sets shouldn't leave a queryset with no constraints  (which would match everything).
Create something with a duplicate 'name' so that we can test multi-column  cases (which require some tricky SQL transformations under the covers).
Excluding across a m2m relation when there is more than one related  object associated was problematic.
Excluding from a relation that cannot be NULL should not use outer joins.
Similarly, when one of the joins cannot possibly, ever, involve NULL  values (Author -> ExtraInfo, in the following), it should never be  promoted to a left outer join. So the following query should only  involve one "left outer" join (Author -> Item is 0-to-many).
The previous changes shouldn't affect nullable foreign key joins.
Combining querysets built on different models should behave in a well-defined  fashion. We raise an error.
Ordering on related tables should be possible, even if the table is  not otherwise involved.
Ordering on a related field should use the remote model's default  ordering as a final step.
Using remote model default ordering can span multiple models (in this  case, Cover is ordered by Item's default, which uses Note's default).
If the remote model does not have a default ordering, we order by its 'id'  field.
Ordering by a many-valued attribute (e.g. a many-to-many or reverse  ForeignKey) is legal, but the results might not make sense. That  isn't Django's problem. Garbage in, garbage out.
If we replace the default ordering, Django adjusts the required  tables automatically. Item normally requires a join with Note to do  the default ordering, but that isn't needed here.
This is also a good select_related() test because there are multiple  Note entries in the SQL. The two Note items should be different.
Ordering columns must be included in the output columns. Note that  this means results that might otherwise be distinct are not (if there  are multiple values in the ordering cols), as in this example. This  isn't a bug; it's a warning to be careful with the selection of  ordering columns.
The *_id version is returned by default.
You can also pass it in explicitly.
...or use the field name.
(First we need to know which order the keys fall in "naturally" on  your system, so we can put things in the wrong way around from  normal. A normal dict would thus fail.)
This slightly odd comparison works around the fact that PostgreSQL will  return 'one' and 'two' as strings, not Unicode objects. It's a side-effect of  using constants here and not a real concern.
Order by the number of tags attached to an item.
Dates with limits and/or counts
Dates with extra select columns
Nullable dates
Make sure semi-deprecated ordering by related models syntax still  works.
Make sure exclude() with multiple conditions continues to work.
More twisted cases, involving nested negations.
Make sure querysets with related fields can be pickled. If this  doesn't crash, it's a Good Thing.
We should also be able to pickle things that use select_related().  The only tricky thing here is to ensure that we do the related  selections properly after unpickling.
Check pickling of deferred-loading querysets
Complex objects should be converted to strings before being used in  lookups.
An EmptyQuerySet should not raise exceptions if it is filtered.
There were "issues" when ordering and distinct-ing on fields related  via ForeignKeys.
Pickling of QuerySets using datetimes() should work.
If a ValuesList or Values queryset is passed as an inner query, we  make sure it's only requesting a single value and use that as the  thing to select.
Multi-valued values() and values_list() querysets should raise errors.
qs.values_list(...).values(...) combinations should work.
When bailing out early because of an empty "__in" filter, we need  to set things up correctly internally so that subqueries can continue properly.
Testing an empty "__in" filter with a generator as the value.
The subquery result cache should not be populated
The subquery result cache should not be populated
The subquery result cache should not be populated
Excluding shouldn't eliminate NULL entries.
Ordering by related tables should accommodate nullable fields (this  test is a little tricky, since NULL ordering is database dependent.  Instead, we just count the number of results).
Empty querysets can be merged with others.
Make sure bump_prefix() (an internal Query method) doesn't (re-)break. It's  sufficient that this query runs without error.
Complex combinations of conjunctions, disjunctions and nullable  relations.
Querying direct fields with isnull should trim the left outer join.  It also should not create INNER JOIN.
Querying across several tables should strip only the last outer join,  while preserving the preceding inner joins.
Querying without isnull should not convert anything to left outer join.
Querying via indirect fields should populate the left outer join  join to dumbcategory ptr_id
Querying across several tables should strip only the last join, while  preserving the preceding left outer joins.
Querying across m2m field should not strip the m2m table from join.
Querying with isnull=False across m2m field should not create outer joins
Querying with isnull=True across m2m field should not create inner joins  and strip last outer join
Reverse querying with isnull should not strip the join
Querying with combined q-objects should also strip the left outer join
Combining queries should not re-populate the left outer join
A negated Q along with an annotated queryset failed in Django 1.4
A slight variation on the restricting the filtering choices by the  lookup constraints.
Custom lookups are registered to round float values correctly on gte  and lt IntegerField queries.
Count should work with a partially read result set.
This shouldn't create an infinite loop.
An error should be raised when QuerySet.datetimes() is passed the  wrong type of field.
Note: when combining the query we need to have information available  about the join type of the trimmed "creator__isnull" join. If we  don't have that information, then the join is created as INNER JOIN  and results will be incorrect.
Test that we correctly recreate joins having identical connections  in the rhs query, in case the query is ORed together. Related to  ticket 18748
Updates that are filtered on the model being updated are somewhat  tricky in MySQL. This exercises that case.
A values() or values_list() query across joined models must use outer  joins appropriately.  Note: In Oracle, we expect a null CharField to return '' instead of  None.
Similarly for select_related(), joins beyond an initial nullable join  must use outer joins so that all results are included.
Calling order_by() with no parameters removes any existing ordering on the  model. But it should still be possible to add new ordering after that.
It is possible to order by reverse of foreign key, although that can lead  to duplicate results.
Avoid raising an EmptyResultSet if an inner query is probably  empty (and hence, not executed).
Ordering by 'rank' gives us rank2, rank1, rank3. Ordering by the  Meta.ordering will be rank3, rank2, rank1.
Cross model ordering is possible in Meta, too.
Ordering of extra() pieces is possible, too and you can mix extra  fields and model fields in the ordering.
Despite having some extra aliases in the query, we can still omit  them in a values() query.
An empty values() call includes all aliases, including those from an  extra()
Extra tables used to crash SQL construction on the second use.  test passes if this doesn't raise an exception.
Make sure that the IDs from different tables don't happen to match.
Test different empty excludes.
Allow %%s to escape select clauses
Once upon a time, select_related() with circular relations would loop  infinitely if you forgot to specify "depth". Now we set an arbitrary  default upper bound.
The parent object should have been deleted as well.
Ordering by model related to nullable relations(!) should use outer  joins, so that all results are included.
Ordering by model related to nullable relations should not change  the join type of already existing joins.  The ordering by others__single__pk will add one new join (to single)  and that join must be LEFT join. The already existing join to related  objects must be kept INNER. So, we have both an INNER and a LEFT join  in the query.
For the purposes of this regression test, it's important that there is no  Join object related to the LeafA we create.
Checking that applying filters after a disjunction works correctly.
Test that parallel iterators work.
Nested queries should not evaluate the inner query as part of constructing the  SQL (so we should see a nested query here, indicated by two "SELECT" calls).
This example is tricky because the parent could be NULL, so only checking  parents with annotations omits some results (tag t1, in this case).
The annotation->tag link is single values and tag->children links is  multi-valued. So we have to split the exclude filter in the middle  and then optimize the inner query without losing results.
Nested queries are possible (although should be used with care, since  they have performance problems on backends like MySQL.
The all() method on querysets returns a copy of the queryset.
Test representation of raw query with one or few parameters passed as list
Using an empty generator expression as the rvalue for an "__in"  lookup is legal.
Regression tests for case-insensitive comparisons
Ok - so the exist query worked - but did it include too many columns?
Evaluate the Note queryset, populating the query cache  Use the note queryset in a query, and evaluate  that query in a way that involves cloning.
14366 -- Calling .values() on an empty QuerySet and then cloning  that should not cause an error
19151 -- Calling .values() or .values_list() on an empty QuerySet  should return an empty QuerySet and not cause an error.
testing for ticket 14930 issues
testing for ticket 14930 issues
Postgres doesn't allow constants in order by, so check for that.
testing for ticket 14930 issues
testing for 23259 issue
testing for 23259 issue
testing for ticket 14930 issues
testing for ticket 14930 issues
see 23443
Some more tests!
Using an offset without a limit is also possible.
People like to slice with '0' as the high-water mark.
ticket 12192
Reserved names are appropriately escaped
If you're not careful, it's possible to introduce infinite loops via  default ordering on foreign keys in a cycle. We detect that.
Note that this doesn't cause an infinite loop, since the default  ordering on the Tag model is empty (and thus defaults to using "id"  for the related field).
... but you can still order in a non-recursive fashion among linked  fields (the previous test failed because the default ordering was  recursive).
When grouping without specifying ordering, we add an explicit "ORDER BY NULL"  portion in MySQL to prevent unnecessary sorting.
Sqlite 3 does not support passing in more than 1000 parameters except by  changing a parameter at compilation time.  Test that the "in" lookup works with lists of 1000 items or more.  The numbers amount is picked to force three different IN batches  for Oracle, yet to be less than 2100 parameter limit for MSSQL.
Ticket 17056 -- affects Oracle
Create a few Orders.
Create some OrderItems for the first order with homogeneous  status_id values
Create some OrderItems for the second order with heterogeneous  status_id values
Create some OrderItems for the second order with heterogeneous  status_id values
Check that the inner queryset wasn't executed - it should be turned  into subquery above
Test for 19895 - second iteration over invalid queryset  raises errors.
The first Q-object is generating the match, the rest of the filters  should not remove the match even if they do not match anything. The  problem here was that b__name generates a LOUTER JOIN, then  b__c__name generates join to c, which the ORM tried to promote but  failed as that join isn't nullable.  We generate one INNER JOIN to D. The join is direct and not nullable  so we can use INNER JOIN for it. However, we can NOT use INNER JOIN  for the b->c join, as a->b is nullable.
Check the ~~Q() (or equivalently .exclude(~Q)) works like Q() for  join promotion.
Test OR + doubleneg. The expected result is that channel is LOUTER  joined, program INNER joined
Finally, a more complex case, one time in a way where each  NOT is pushed to lowest level in the boolean tree, and  another query where this isn't done.
Check that we don't accidentally trim reverse joins - we can't know  if there is anything on the other side of the join, so trimming  reverse joins can't be done, ever.
Pre-existing join, add two ORed filters to the same join,  all joins can be INNER JOINS.  Reverse the order of AND and OR filters.
Now we have two different joins in an ORed condition, these  must be OUTER joins. The pre-existing join should remain INNER.  Reverse case.
The ANDed a__f2 filter allows us to use keep using INNER JOIN  even inside the ORed case. If the join to a__ returns nothing,  the ANDed filter for a__f2 can't be true.
This one needs demotion logic: the first filter causes a to be  outer joined, the second filter makes it inner join again.
Demote needed for the "a" join. It is marked as outer join by  above filter (even if it is trimmed away).
Note that the above filters on a force the join to an  inner join even if it is trimmed.  So, now the a__f1 join doesn't need promotion.  But b__f1 does.  Now the join to a is created as LOUTER
channel contains 'program1', so all Identifiers except that one  should be returned
Evaluating the children query (which has parents as part of it) does  not change results for the parents query.
Check that if a values() queryset is used, then the given values  will be used instead of forcing use of the relation's field.
The query below should match o1 as it has related order_item  with id == status.
Test join trimming from ticket18785
The ORed condition below should have no effect on the query - the  ~Q(pk__in=[]) will always be True.
Passing incorrect object type
Passing an object of the class on which query is done.
proxy model objects
child objects
parent objects
QuerySet related object type checking shouldn't issue queries  (the querysets aren't evaluated here, hence zero queries) (23266).
Make sure the num and objecta field values match.
Load data so that assertNumQueries doesn't complain about the get  version's queries.
Make sure there is a left outer join without the filter.
Test filtering on a complicated q-object from ticket's report.  The query structure is such that we have multiple nested subqueries.  The original problem was that the inner queries weren't relabeled  correctly.  See also 24090.  True for a1 as field_b0 = 10000, field_c0=10000  False for a2 as no ticket23605b found  True for a1 (field_b1=True)  Same filters as above commented filters, but  double-negated (one for Q() above, one for  parentheses). So, again a1 match, a2 not.
Protect against annotations being passed to __init__ --  this'll make the test suite get angry if annotations aren't  treated differently than fields.
Check that all values on the model are equal  This includes checking that they are the same type
last_name isn't given, but it will be retrieved on demand
First Iteration
Second Iteration
Indexing on RawQuerySets
Because no Articles exist yet, earliest() raises ArticleDoesNotExist.
Get the earliest Article.  Get the earliest Article that matches certain filters.
Pass a custom field name to earliest() to change the field that's used  to determine the earliest object.
Ensure that earliest() overrides any other ordering specified on the  query. Refs 11283.
Ensure that error is raised if the user forgot to add a get_latest_by  in the Model.Meta
Because no Articles exist yet, latest() raises ArticleDoesNotExist.
Get the latest Article.  Get the latest Article that matches certain filters.
Pass a custom field name to latest() to change the field that's used  to determine the latest object.
Ensure that latest() overrides any other ordering specified on the query. Refs 11283.
Ensure that error is raised if the user forgot to add a get_latest_by  in the Model.Meta
You can still use latest() with a model that doesn't have  "get_latest_by" set -- just pass in the field name manually.
Note: by default PK ordering.
We know that we've broken the __iter__ method, so the queryset  should always raise an exception.
And it does not matter if there are any records in the DB.
XXX check Content-Length and truncate if too many bytes written?
Backport of http://hg.python.org/cpython/rev/d5af1b235dab. See 16241.  This can be removed when support for Python <= 2.7.3 is deprecated.
Return a blob of data that is 1.5 times the maximum chunk size.
SimpleTestCase can be decorated by override_settings, but not ut.TestCase
Should also work for a non-overridden setting
inner's __exit__ should have restored the settings of the outer  context manager, not those when the class was instantiated
File extension may by .py, .pyc, etc. Compare only basename.
Force evaluation of the lazy object.
-*- coding: utf-8 -*-
The XML serializer handles everything as strings, so comparisons  need to be performed on the stringified value
-*- coding: utf-8 -*-
SerializerDoesNotExist is instantiated with the nonexistent format
Serialize the test database to a stream
Serialize normally for a comparison
Check that the two are the same
Serialize then deserialize the test database
Check that the deserialized object contains data in only the serialized fields.
Prior to saving, old headline is in place
After saving, new headline is in place
Check the class instead of using isinstance() because model instances  with deferred fields (e.g. Author_Deferred_name) will pass isinstance.
Regression for 12524 -- dates before 1000AD get prefixed  0's on the year
The deserialization process needs to run in a transaction in order  to test forward reference handling.
Create all the objects defined in the test data  Serialize the test database
Assert that the deserialized data is the same  as the original source
Create the books.
Serialize the books.
Delete one book (to prove that the natural key generation will only  restore the primary keys of books found in the database via the  get_natural_key manager method).
Deserialize and test.
Dynamically register tests for each serializer
This isn't a raw save because:   1) we're testing inheritance, not field behavior, so none      of the field values need to be protected.   2) saving the child class and having the parent created      automatically is easier than manually creating both.
Define some data types. Each data type is  actually a pair of functions; one to create  and one to compare objects of that type
testing post- and pre-references and extra fields
(pk_obj, 620, DatePKData, datetime.date(2006, 6, 16)),  (pk_obj, 630, DateTimePKData, datetime.datetime(2006, 6, 16, 10, 42, 37)),  (pk_obj, 650, FilePKData, 'file:///foo/bar/whiz.txt'),  (XX, ImagePKData  (pk_obj, 700, NullBooleanPKData, True),  (pk_obj, 701, NullBooleanPKData, False),  (pk_obj, 760, TextPKData, """This is a long piece of text.  It contains line breaks.  Several of them.  The end."""),  (pk_obj, 770, TimePKData, datetime.time(10, 42, 37)),  (pk_obj, 790, XMLPKData, "<foo></foo>"),
Because Oracle treats the empty string as NULL, Oracle is expected to fail  when field.empty_strings_allowed is True and the value is None; skip these  tests.
Regression test for 8651 -- a FK to an object with PK of 0  This won't work on MySQL since it won't let you create an object  with an autoincrement primary key of 0,
Create all the objects defined in the test data
Get a count of the number of objects created for each class
Add the generic tagged objects to the object list
Serialize the test database
Assert that the deserialized data is the same  as the original source
Assert that the number of objects deserialized is the  same as the number that was serialized.
One to one field can't be null here, since it is a PK.
-*- coding: utf-8 -*-
clear out cached serializers to emulate yaml missing
clear out cached serializers to clean out BadSerializer instances
yaml.safe_load will return non-string objects for some  of the fields we are interested in, this ensures that  everything comes back as a string
-*- coding: utf-8 -*-
-*- coding: utf-8 -*-
Make sure this form doesn't pass validation.
Then make sure that it *does* pass validation and delete the object,  even though the data isn't actually valid.
Make sure this form doesn't pass validation.
Then make sure that it *does* pass validation and delete the object,  even though the data isn't actually valid.
exclude some required field from the forms
Regression test for 9171.
Regression test for 21472
-*- coding: utf-8 -*-
Such referer strings should not happen, but anyway, if it happens,  let's not crash
Such user agent strings should not happen, but anyway, if it happens,  let's not crash
URL with scheme and domain should also be ignored
URL with a different scheme should be ignored as well because bots  tend to use http:// in referers even when browsing HTTPS websites.
This is just an example for testing purposes...
-*- coding: utf-8 -*-
Use the name of the primary key, rather than pk.
pk can be used as a substitute for the primary key.  The primary key can be accessed via the pk property on the model.  Or we can use the real attribute name for the primary key:
Primary key may be unicode string
The primary key must also obviously be unique, so trying to create a  new object with the same primary key will fail.
Regression for 10785 -- Custom fields can be used for primary keys.
SQLite lets objects be saved with an empty primary key, even though an  integer is expected. So we can't check for an error being raised in that  case for SQLite. Remove it from the suite for this next bit.  The primary key must be specified, so an error is raised if you  try to create an object without it.
Based on tests/reserved_names/models.py
local_models should contain test dependent model classes that will be  automatically removed from the app cache on test tear down.
Delete any tables made for our models
SQLite has a different format for field_type  SQLite also doesn't error properly
Create the table  Check that it's there  Clean up that table  Check that it's gone
Create the table  Check that initial tables are there  Make sure the FK constraint is present  Repoint the FK constraint  Make sure the new FK constraint is present
Create the table
Create the table  Check that initial tables are there  Check that BookWeak doesn't have an FK constraint  Make a db_constraint=False FK  Make sure no FK constraint is present  Alter to one with a constraint  Make sure the new FK constraint is present  Alter to one without a constraint again  Make sure no FK constraint is present
Create the table  Check that initial tables are there  Make a db_constraint=False FK  Add the field  Make sure no FK constraint is present
Create the table  Ensure there's no age field  Add the new field  Ensure the field is right afterwards
Create the table  Ensure there's no age field  Add some rows of data  Add a not-null field  Ensure the field is right afterwards
Create the table  Ensure there's no age field  Add some rows of data  Add a not-null field  Ensure the field is right afterwards  BooleanField are stored as TINYINT(1) on MySQL.
Weird field that saves the count of items in its value
Create the table  Add some rows of data  Add the field with a default it needs to cast (to string in this case)  Ensure the field is there  Make sure the values were transformed correctly
Create the table  Add the new field  Ensure the field is right afterwards  MySQL annoyingly uses the same backend, so it'll come back as one of  these two types.
Create the table  Add the new field with default  Introspection treats BLOBs as TextFields
Create the table  Ensure the field is right to begin with  Alter the name field to a TextField  Ensure the field is right afterwards  Change nullability again  Ensure the field is right afterwards
Regression for "BLOB/TEXT column 'info' can't have a default value")  on MySQL.  Create the table
Make sure the field isn't nullable
Make sure the field isn't nullable
Make sure the field isn't nullable
Create the table  Ensure the field is right to begin with  Create some test data  Verify null value  Alter the height field to NOT NULL with default  Ensure the field is right afterwards  Verify default value
Create the table  Change the CharField to null
Create the table  Change the TextField to null
Create the table  Ensure the field is right to begin with  Alter the height field to NOT NULL keeping the previous default  Ensure the field is right afterwards
Create the table  Ensure the field is right to begin with  Make sure the FK constraint is present  Alter the FK  Ensure the field is right afterwards  Make sure the FK constraint is present
Create the tables  Ensure no FK constraint exists  Ensure FK constraint exists
Create the table  Ensure the field is right to begin with  Ensure the field is unique  Make sure the FK constraint is present  Alter the OneToOneField to ForeignKey  Ensure the field is right afterwards  Ensure the field is not unique anymore  Make sure the FK constraint is still present
Create the table  Ensure the field is right to begin with  Ensure the field is not unique  Make sure the FK constraint is present  Alter the ForeignKey to OneToOneField  Ensure the field is right afterwards  Ensure the field is unique now  Make sure the FK constraint is present
This will fail if DROP DEFAULT is inadvertently executed on this  field which drops the id sequence, at least on PostgreSQL.
model requires a new PK
Ensure unique constraint works.
Create the table  Ensure the field is right to begin with  Alter the name field's name  Ensure the field is right afterwards
Create the tables  Ensure there is now an m2m table there
Create the tables  Ensure there is now an m2m table there
Create the tables  Create an M2M field  Ensure there's no m2m table there  Add the field  Ensure there is now an m2m table there
"Alter" the field. This should not rename the DB table to itself.
Remove the M2M table again  Ensure there's no m2m table there
Make sure the model state is coherent with the table one now that  we've removed the tags field.
Create the tables  Ensure the m2m table is there  "Alter" the field's blankness. This should not actually do anything.  Ensure the m2m table is still there
Create the tables  Ensure the M2M exists and points to TagM2MTest  Repoint the M2M  Ensure old M2M is gone
This model looks like the new model and is used for teardown.  Ensure the new M2M exists and points to UniqueTest
Create the tables  Ensure the constraint exists  Alter the column to remove it  Alter the column to re-add it
Create the table  Ensure the field is unique to begin with  Alter the slug field to be non-unique  Ensure the field is no longer unique  Alter the slug field to be unique  Ensure the field is unique again  Rename the field  Ensure the field is still unique
Create the table  Ensure the fields are unique to begin with  Alter the model to its non-unique-together companion  Ensure the fields are no longer unique  Alter it back  Ensure the fields are unique again
Create the table  Ensure the fields are unique to begin with  Add the unique_together constraint  Alter it back
Create the tables  Ensure the fields aren't unique to begin with  Add the unique_together constraint  Alter it back
Create the table  Ensure there's no index on the year/slug columns first  Alter the model to add an index  Ensure there is now an index  Alter it back  Ensure there's no index
Create the table  Ensure the fields are unique to begin with  Add the unique_together constraint  Alter it back
Create the table  Ensure there is an index
Create the table  Ensure the table is there to begin with  Alter the table  Ensure the table is there afterwards  Alter the table again  Ensure the table is still there
Create the table  Ensure the table is there and has the right index  Alter to remove the index  Ensure the table is there and has no index  Alter to re-add the index  Ensure the table is there and has the index again  Add a unique column, verify that creates an implicit index  Remove the unique, check the index goes with it
Create the table  Ensure the table is there and has the right PK  Alter to change the PK  Ensure the PK changed
Create the table  Find the properly shortened column name
Ensure the table is there and has an index on the column
Create the initial tables  Add a second FK, this would fail due to long ref name before the fix
Create the table  Check that it's there  Clean up that table  Check that it's gone
Create the table  Ensure there's no surname field  Create a row  Add new CharField to ensure default will be used from effective_default  Ensure field was added with the right default
Create the table  Ensure there's no surname field  Create a row  Add new CharField with a default  Ensure field was added with the right default  And that the default is no longer set in the database.
Create the table  Create a row  The default from the new field is used in updating existing rows.  The database default should be removed.
Create the table  Create a row  Create a field that has an unhashable default
Create the table and verify no initial indexes.  Alter to add db_index=True and create 2 indexes.  Remove db_index=True to drop both indexes.
Create the table and verify no initial indexes.  Alter to add db_index=True and create 2 indexes.  Remove db_index=True to drop both indexes.
Create the table and verify initial indexes.  Alter to add unique=True (should add 1 index)  Alter to remove unique=True (should drop unique index)  XXX: bug!
Create the table and verify initial indexes.  Alter to add db_index=True  Alter to remove db_index=True
Create the table  Check auto_now/auto_now_add attributes are not defined  Create a row  Ensure fields were added with the correct defaults
Copy those methods from ManyToManyField because they don't call super() internally
Caution: this is only safe if you are certain that headline will be  in ASCII.
-*- coding: utf-8 -*-
On Python 2, the default str() output will be the UTF-8 encoded  output of __unicode__() -- or __str__() when the  python_2_unicode_compatible decorator is used.
Explicit call to __str__/__repr__ to make sure str()/repr() don't  coerce the returned value.
-*- coding: utf-8 -*-
(validator, value, expected),
Quoted-string format (CR not allowed)  Max length of domain name labels is 63 characters per RFC 1034.  Trailing newlines in username or domain not allowed
validate_ipv6_address uses django.utils.ipv6, which  is tested in much greater detail in its own testcase
Trailing newlines not accepted  Trailing junk does not take forever to reject
Add valid and invalid URL tests.  This only tests the validator without extended schemes.
assertRaises not used, so as to be able to produce an error message  containing the tested value
Delete all permissions and content_types
Re-run migrate. This will re-build the permissions and content types.
Check that content types and permissions exist for the swapped model,  but not for the swappable model.
Return the Transifex resource name
Output the approximate number of changed/added strings in the en catalog.
Subclass the literal_block to override the visit/depart event handlers
default literal block handler
default literal block handler
Latex document generator depart handler.
The 'snippet' directive that allows to add the filename (optional) of a code snippet in the document. This is modeled after CodeBlock.
Only one argument accepted for directive '{directive_name}::'.Comments should be provided as content, not as an extra argument..format(directive_name=self.name)
Subclass to add some extra things we need.
An interpreted text role to link docs to Trac tickets. To use: :ticket:`XXXXX` Based on code from psycopg2 by Daniele Varrazzo.
Explicitly set convert_charrefs to be False.This silences a deprecation warning on Python 3.4, but we can't do it at call time because Python 2.7 does not have the keyword argument.
Classic implementation of reader-writer lock with preference to writers. Readers can access a resource simultaneously. Writers get an exclusive access. API is self-descriptive: reader_enters() reader_leaves() writer_enters() writer_leaves()
Parses a string and return a datetime.date.Raises ValueError if the input is well formatted but not a valid date. Returns None if the input isn't well formatted.
Parses a string and return a datetime.time.This function doesn't support time zone offsets. Raises ValueError if the input is well formatted but not a valid time. Returns None if the input isn't well formatted, in particular if it contains an offset.
Parses a string and return a datetime.datetime.This function supports time zone offsets. When the input contains one, the output uses a timezone with a fixed offset from UTC. Raises ValueError if the input is well formatted but not a valid datetime. Returns None if the input isn't well formatted.
Providing iterator functions that are not in all version of Python we support. Where possible, we try to use the system-native version and only fall back to these implementations if necessary.
Returns the max-age from the response Cache-Control header as an integer (or ``None`` if it wasn't found or wasn't an integer.
Adds headers to a response to indicate that a page should never be cached.
Checks to see if the response has a given header name in its Vary header.
Returns a cache key from the headers given in the header list.
Returns a cache key for the header cache.
Returns a cache key based on the request URL and query. It can be used in the request phase because it pulls the list of headers to take into account from the global URL registry and uses those to build a cache key to check against. If there is no headerlist stored, the page needs to be rebuilt, so this function returns None.
Escape CR and LF characters.
A logging filter that checks the return value of a given callable (which takes the record-to-be-logged as its only parameter) to decide whether to log a record.
Returns the given text with ampersands, quotes and angle brackets encoded for use in HTML. This function always escapes its input, even if it's already escaped and marked as such. This may result in double-escaping. If this is a concern, use conditional_escape() instead.
Hex encodes characters for use in JavaScript strings.
Similar to escape(), except that it doesn't operate on pre-escaped strings. This function relies on the __html__ convention used both by Django's SafeData class and by third-party libraries like markupsafe.
Similar to str.format, but passes all arguments through conditional_escape, and calls 'mark_safe' on the result. This function should be used instead of str.format or % interpolation to build up small HTML fragments.
A wrapper of format_html, for the common case of a group of arguments that need to be formatted using the same format string, and then joined using 'sep'. 'sep' is also passed through conditional_escape. 'args_generator' should be an iterator that returns the sequence of 'args' that will be passed to format_html. Example: format_html_join('\n', "<li>{} {}</li>", ((u.first_name, u.last_name) for u in users))
Converts newlines into <p> and <br />s.
Internal tag stripping utility used by strip_tags.
Returns the given HTML with spaces between tags removed.
Avoid text wrapping in the middle of a phrase by adding non-breaking spaces where there previously were normal spaces.
Returns a tuple of the django version. If version argument is non-empty,then checks for correctness of the tuple provided.
Version of os.path.abspath that uses the unicode representation of the current working directory, thus avoiding a UnicodeDecodeError in join when the cwd has non-ASCII characters.
Always return a unicode path.
Always return a native path, that is unicode on Python 3 and bytestring on Python 2.
An iterator that yields the next character from "pattern_iter", respecting escape sequences. An escaped character is replaced by a representative of its class (e.g. \w -> "x"). If the escaped character is one that is skipped, it is not returned (the next character is returned instead). Yields the next character, along with a boolean indicating whether it is a raw (unescaped) character or not.
The iterator is currently inside a capturing group. We want to walk to the close of this group, skipping over any nested groups and handling escaped parentheses correctly.
Returns True if the "source" contains an instance of "inst". False, otherwise.
Return True if function 'func' accepts positional arguments *args.
Determine if the object instance is of a protected type.Objects of protected types are preserved as-is when passed to force_text(strings_only=True).
Converts a Uniform Resource Identifier(URI) into an Internationalized Resource Identifier(IRI). This is the algorithm from section 3.2 of RFC 3987. Takes an URI in ASCII bytes (e.g. '/I%20%E2%99%A5%20Django/') and returns unicode containing the encoded result (e.g. '/I \xe2\x99\xa5 Django/').
As per section 3.2 of RFC 3987, step three of converting a URI into an IRI, we need to re-percent-encode any octet produced that is not part of a strictly legal UTF-8 octet sequence.
The encoding of the default system locale but falls back to the given fallback encoding if the encoding is unsupported by python or could not be determined.  See tickets 10335 and 5846
Returns the last data value for this key, or [] if it's an empty list; raises KeyError if not found.
Returns the last data value for the passed key. If key doesn't exist or value is an empty list, then default is returned.
Returns the list of values for the passed key. If key doesn't exist, then a default value is returned.
Appends an item to the internal list associated with key.
Yields (key, value) pairs, where value is the last item in the list associated with the key.
Yields (key, list) pairs.
Yield the last value on every key list.
Returns a shallow copy of this object.
update() extends rather than replaces existing key lists. Also accepts keyword args.
Returns current object as a dict with singular values.
Add documentation to a function.
Import module, returning the module after the last dot.
Return true, if the named module is a package. We need this method to get correct spec objects with Python 3.4 (see PEP451)
Add an item to six.moves.
Remove item from six.moves.
Class decorator for creating a class with a metaclass.
Creates a TagURI. See http://web.archive.org/web/20110514113830/http://diveintomark.org/archives/2004/05/28/howto-atom-id
Return extra attributes to place on the root (i.e. feed/channel) element. Called from write().
Add elements in the root (i.e. feed/channel) element. Called from write().
Return extra attributes to place on each item (i.e. item/entry) element.
Add elements on each item (i.e. item/entry) element.
Outputs the feed in the given encoding to outfile, which is a file-like object. Subclasses should override this.
Returns the feed in the given encoding as a string.
Time, in 12-hour hours and minutes, with minutes left off if they're zero. Examples: '1', '1:30', '2:05', '2' Proprietary extension.
Difference to Greenwich time in hours; e.g. '+0200', '-0430'. If timezone information is not available, this method returns an empty string.
Time, in 12-hour hours, minutes and 'a.m.'/'p.m.', with minutes left off if they're zero and the strings 'midnight' and 'noon' if appropriate. Examples: '1 a.m.', '1:30 p.m.', 'midnight', 'noon', '12:30 p.m.' Proprietary extension.
ISO 8601 Format Example : '2008-01-02T10:30:00.000123'
Clear any cached formats.This method is provided primarily for testing purposes, so that the effects of cached formats can be removed.
Does the heavy lifting of finding format modules.
Returns a list of the format modules found
Formats a datetime.date or datetime.datetime object using a localizable format If use_l10n is provided and is not None, that will force the value to be localized (or not), overriding the value of settings.USE_L10N.
Formats a datetime.time object using a localizable format If use_l10n is provided and is not None, that will force the value to be localized (or not), overriding the value of settings.USE_L10N.
Formats a numeric value using localization settings If use_l10n is provided and is not None, that will force the value to be localized (or not), overriding the value of settings.USE_L10N.
Version of str(timedelta) which is not English specific.
UTC implementation taken from Python's docs. Used only when pytz isn't available.
Fixed offset in minutes east from UTC. Taken from Python's docs. Kept as close as possible to the reference version. __init__ was changed to make its arguments optional, according to Python's requirement that tzinfo subclasses can be instantiated without arguments.
Local time. Taken from Python's docs. Used only when pytz isn't available, and most likely inaccurate. If you're having trouble with this class, don't waste your time, just install pytz. Kept as close as possible to the reference version. __init__ was added to delay the computation of STDOFFSET, DSTOFFSET and DSTDIFF which is performed at import time in the example. Subclasses contain further improvements.
Slightly improved local time implementation focusing on correctness. It still crashes on dates before 1970 or after 2038, but at least the error message is helpful.
Returns the name of the default time zone.
Returns the currently active time zone as a tzinfo instance.
Returns the name of the currently active time zone.
Sets the time zone for the current thread. The ``timezone`` argument must be an instance of a tzinfo subclass or a time zone name. If it is a time zone name, pytz is required.
Unsets the time zone for the current thread. Django will then use the time zone defined by settings.TIME_ZONE.
Determines if a given datetime.datetime is aware. The concept is defined in Python's docs: http://docs.python.org/library/datetime.htmldatetime.tzinfo Assuming value.tzinfo is either None or a proper datetime.tzinfo, value.utcoffset() implements the appropriate logic.
Determines if a given datetime.datetime is naive. The concept is defined in Python's docs: http://docs.python.org/library/datetime.htmldatetime.tzinfo Assuming value.tzinfo is either None or a proper datetime.tzinfo, value.utcoffset() implements the appropriate logic.
Returns a randomly generated paragraph of lorem ipsum text. The paragraph consists of between 1 and 4 sentences, inclusive.
Returns a list of paragraphs as returned by paragraph(). If `common` is True, then the first paragraph will be the standard 'lorem ipsum' paragraph. Otherwise, the first paragraph will be random Latin text. Either way, subsequent paragraphs will be random Latin text.
This is called to create a new instance of this class when we need new Nodes (or subclasses) in the internal code in this class. Normally, it just shadows __init__(). However, subclasses with an __init__ signature that is not an extension of Node.__init__ might need to implement this method to allow a Node to create a new instance of them (if they have any extra setting up to do).
Utility method used by copy.deepcopy().
The size of a node if the number of children it has.
Returns True is 'other' is a direct child of this instance.
Import a dotted module path and return the attribute/class designated by the last name in the path. Raise ImportError if the import failed.
Truncates a string after a certain number of words. Takes an optional argument of what should be used to notify that the string has been truncated, defaulting to ellipsis (...).
Truncates a string after a certain number of words. Newlines in the string will be stripped.
Returns the given string converted to a string that can be used for a clean filename. Specifically, leading and trailing spaces are removed; other spaces are converted to underscores; and anything that is not a unicode alphanumeric, dash, underscore, or dot, is removed. >>> get_valid_filename("john's portrait in 2004.jpg") 'johns_portrait_in_2004.jpg'
Normalizes CRLF and CR newlines to just LF.
Generator that splits a string by spaces, leaving quoted phrases together. Supports both single and double quotes, and supports escaping quotes with backslashes. In the output, strings will keep their initial and trailing quote marks and escaped quotes will remain escaped (the results can then be further processed with unescape_string_literal()). >>> list(smart_split(r'This is "a person\'s" test.')) ['This', 'is', '"a person\\\'s"', 'test.'] >>> list(smart_split(r"Another 'person\'s' test.")) ['Another', "'person\\'s'", 'test.'] >>> list(smart_split(r'A "\"funky\" style" test.')) ['A', '"\\"funky\\" style"', 'test.']
Convert quoted string literals to unquoted strings with escaped quotes and backslashes unquoted:: >>> unescape_string_literal('"abc"') 'abc' >>> unescape_string_literal("'abc'") 'abc' >>> unescape_string_literal('"a \"bc\""') 'a "bc"' >>> unescape_string_literal("'\'ab\' c'") "'ab' c"
Convert to ASCII if 'allow_unicode' is False. Convert spaces to hyphens. Remove characters that aren't alphanumerics, underscores, or hyphens. Convert to lowercase. Also strip leading and trailing whitespace.
Unpack the tar or zip file at the specified path to the directory specified by to_path.
The external API class that encapsulates an archive implementation.
Returns true if all the paths have the same leading path name (i.e., everything is in one subdirectory in an archive)
Unpack an IPv4 address that was mapped in a compressed IPv6 address. This converts 0000:0000:0000:0000:0000:ffff:10.10.10.10 to 10.10.10.10. If there is nothing to sanitize, returns None. Args: ip_str: A string, the expanded IPv6 address. Returns: The unpacked IPv4 address, or None if there was nothing to unpack.
Returns your text, enclosed in ANSI graphics codes. Depends on the keyword arguments 'fg' and 'bg', and the contents of the opts tuple/list. Returns the RESET code if no parameters are given. Valid colors: 'black', 'red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white' Valid options: 'bold' 'underscore' 'blink' 'reverse' 'conceal' 'noreset' - string will not be auto-terminated with the RESET code Examples: colorize('hello', fg='red', bg='blue', opts=('blink',)) colorize() colorize('goodbye', opts=('underscore',)) print(colorize('first line', fg='red', opts=('noreset',))) print('this should be red too') print(colorize('and so should this')) print('this should not be red')
Returns a function with default parameters for colorize() Example: bold_red = make_style(opts=('bold',), fg='red') print(bold_red('hello')) KEYWORD = make_style(fg='yellow') COMMENT = make_style(fg='blue', opts=('bold',))
Apply a list/tuple of decorators if decorator is one. Decorator functions are applied so that the call order is the same as the order in which they appear in the iterable.
A base class that enables a context manager to also be used as a decorator.
A version of Python's urllib.quote() function that can operate on unicode strings. The url is first UTF-8 encoded before quoting. The returned string can safely be used as part of an argument to a subsequent iri_to_uri() call without double-quoting occurring.
A version of Python's urllib.quote_plus() function that can operate on unicode strings. The url is first UTF-8 encoded before quoting. The returned string can safely be used as part of an argument to a subsequent iri_to_uri() call without double-quoting occurring.
A wrapper for Python's urllib.unquote() function that can operate on the result of django.utils.http.urlquote().
A wrapper for Python's urllib.unquote_plus() function that can operate on the result of django.utils.http.urlquote_plus().
A version of Python's urllib.urlencode() function that can operate on unicode strings. The parameters are first cast to UTF-8 encoded strings and then encoded as per normal.
Formats the time to ensure compatibility with Netscape's cookie standard. Accepts a floating point number expressed in seconds since the epoch, in UTC - such as that outputted by time.time(). If set to None, defaults to the current time. Outputs a string in the format 'Wdy, DD-Mon-YYYY HH:MM:SS GMT'.
Formats the time to match the RFC1123 date format as specified by HTTP RFC7231 section 7.1.1.1. Accepts a floating point number expressed in seconds since the epoch, in UTC - such as that outputted by time.time(). If set to None, defaults to the current time. Outputs a string in the format 'Wdy, DD Mon YYYY HH:MM:SS GMT'.
Converts an integer to a base36 string
Encodes a bytestring in base64 for use in URLs, stripping any trailing equal signs.
Decodes a base64 encoded string, adding back any trailing equal signs that might have been stripped.
Wraps a string in double quotes escaping contents as necessary.
Unquote an ETag string; i.e. revert quote_etag().
Return ``True`` if the host is either an exact match or a match to the wildcard pattern. Any pattern beginning with a period matches a domain and all of its subdomains. (e.g. ``.example.com`` matches ``example.com`` and ``foo.example.com``). Anything else is an exact string match.
The purpose of this class is to store the actual translation function upon receiving the first call to that function. After this is done, changes to USE_I18N will have no effect to which function is served upon request. If your tests rely on changing USE_I18N, you can delete all the functions from _trans.__dict__. Note that storing the function with setattr will have a noticeable performance effect, as access to the function goes the normal path, instead of using __getattr__.
Lazy variant of string concatenation, needed for translations that are constructed from multiple parts.
Reset global state when LANGUAGES setting has been changed, as some languages should no longer be accepted.
Turns a locale name (en_US) into a language name (en-us).
Returns a mergeable gettext.GNUTranslations instance. A convenience wrapper. By default gettext uses 'fallback=False'. Using param `use_null_fallback` to avoid confusion with any other references to 'fallback'.
Creates a base catalog using global django translations.
Merges translations from each installed app.
Merges translations defined in LOCALE_PATHS.
Returns the translation language.
Returns the translation language name.
Returns a translation object in the default 'django' domain.
Fetches the translation object for a given language and installs it as the current translation object for the current thread.
Deinstalls the currently active translation object so that further _ calls will resolve against the default translation object, again.
Makes the active translation object a NullTranslations() instance. This is useful when we want delayed translations to appear as the original string for some reason.
Returns selected language's BiDi layout. * False = left-to-right layout * True = right-to-left layout
Returns the current active catalog for further processing. This can be used if you need to modify the catalog or want to access the whole message catalog instead of just translating one string.
Returns a string of the translation of the message. Returns a string on Python 3 and an UTF-8-encoded bytestring on Python 2.
Marks strings for translation but doesn't translate them now. This can be used to store strings in global variables that should stay in the base language (because they might be used externally) and will be translated later.
Returns a string of the translation of either the singular or plural, based on the number. Returns a string on Python 3 and an UTF-8-encoded bytestring on Python 2.
Returns a unicode strings of the translation of either the singular or plural, based on the number.
Returns a list of paths to user-provides languages files.
Cache of settings.LANGUAGES in an OrderedDict for easy lookups by key.
Returns the language-code if there is a valid language-code found in the `path`. If `strict` is False (the default), the function will look for an alternative country-specific variant when the currently checked is not found.
Analyzes the request to find what language the user wants the system to show. Only languages listed in settings.LANGUAGES are taken into account. If the user requests a sublanguage where we have a main language, we send out the main language. If check_path is True, the URL path prefix will be checked for a language code, otherwise this is skipped for backwards compatibility.
Decorator that converts a method with a single self argument into a property cached on the instance. Optional ``name`` argument allows you to make cached properties of other methods. (e.g.  url = cached_property(get_absolute_url, name='url') )
This is just a base class for the proxy class created in the closure of the lazy function. It can be used to recognize promises in code.
A decorator that allows a function to be called with one or more lazy arguments. If none of the args are lazy, the function is evaluated immediately, otherwise a __proxy__ is returned that will evaluate the function when needed.
A decorator for functions that accept lazy arguments and return text.
Used to unpickle lazy objects. Just return its argument, which will be the wrapped object.
Pass in a callable that returns the object to be wrapped. If copies are made of the resulting SimpleLazyObject, which can happen in various circumstances within Django, then you must ensure that the callable can be safely run more than once and will return the same value.
A property that works with subclasses by wrapping the decorated functions of the base class.
Returns True if the two strings are equal, False otherwise. The time taken is independent of the number of characters that match. For the sake of simplicity, this function executes in constant time only when the two strings have the same length. It short-circuits when they have different lengths. Since Django only uses it to compare hashes of known expected length, this is acceptable.
Convert a binary string into a long integer This is a clever optimization for fast xor vector math
Convert a long integer into a binary string. hex_format_string is like "%020x" for padding 10 characters.
Implements PBKDF2 with the same API as Django's existing implementation, using the stdlib. This is used in Python 2.7.8+ and 3.4+.
Implements PBKDF2 as defined in RFC 2898, section 5.2 HMAC+SHA256 is used as the default pseudo random function. As of 2014, 100,000 iterations was the recommended default which took 100ms on a 2.7Ghz Intel i7 with an optimized implementation. This is probably the bare minimum for security given 1000 iterations was recommended in 2001. This code is very well optimized for CPython and is about five times slower than OpenSSL's implementation. Look in django.contrib.auth.hashers for the present default, it is lower than the recommended 100,000 because of the performance difference between this and an optimized implementation.
A specification for a token class.
Create a regex from a space-separated list of literal `choices`. If provided, `prefix` and `suffix` will be attached to each choice individually.
Lexically analyze `text`. Yields pairs (`name`, `tokentext`).
Report cache statistics
A byte string that should be HTML-escaped when output.
A unicode string object that should be HTML-escaped when output.
Returns the html representation of a string for interoperability. This allows other template engines to understand Django's SafeData.
Concatenating a safe byte string with another safe byte string or safe unicode string is safe. Otherwise, the result is no longer safe.
Wrap a call to a normal unicode method up so that we return safe results. The method that is being wrapped is passed in the 'method' argument.
Concatenating a safe unicode string with another safe byte string or safe unicode string is safe. Otherwise, the result is no longer safe.
Wrap a call to a normal unicode method up so that we return safe results. The method that is being wrapped is passed in the 'method' argument.
Explicitly mark a string as safe for (HTML) output purposes. The returned object can be used everywhere a string or unicode object is appropriate. Can be called multiple times on a single string.
Return a compiled regular expression based on the activate language.
A URL resolver that always matches the active language code as URL prefix. Rather than taking a regex argument, we just override the ``regex`` function to always return the active language-code as regex.
Set the script prefix for the current thread.
Return the currently active script prefix. Useful for client code that wishes to construct their own URLs manually (although accessing the request instance is normally going to be a lot cleaner).
Unset the script prefix for the current thread.
Set the URLconf for the current thread (overriding the default one in settings). If urlconf_name is None, revert back to the default.
Return the root URLconf to use for the current thread if it has been changed from the default one.
Return True if the given path resolves against the default URL resolver, False otherwise. This is a convenience method to make working with "is this a match?" cases easier, avoiding try...except blocks.
Returns a HttpResponse whose content is filled with the result of calling django.template.loader.render_to_string() with the passed arguments.
Returns a HttpResponse whose content is filled with the result of calling django.template.loader.render_to_string() with the passed arguments.
Returns an HttpResponseRedirect to the appropriate URL for the arguments passed. The arguments could be: * A model: the model's `get_absolute_url()` function will be called. * A view name, possibly with arguments: `urls.reverse()` will be used to reverse-resolve the name. * A URL, which will be used as-is for the redirect location. By default issues a temporary redirect; pass permanent=True to issue a permanent redirect
Uses get() to return an object, or raises a Http404 exception if the object does not exist. klass may be a Model, Manager, or QuerySet object. All other passed arguments and keyword arguments are used in the get() query. Note: Like with get(), an MultipleObjectsReturned will be raised if more than one object is found.
Uses filter() to return a list of objects, or raise a Http404 exception if the list is empty. klass may be a Model, Manager, or QuerySet object. All other passed arguments and keyword arguments are used in the filter() query.
No more reads are allowed from this device.
Cleanup filename from Internet Explorer full paths.
Every LazyStream must have a producer when instantiated. A producer is an iterable that returns a string each time it is called.
Used when the exact number of bytes to read is unimportant. This procedure just returns whatever is chunk is conveniently returned from the iterator instead. Useful to avoid unnecessary bookkeeping if performance is an issue.
Used to invalidate/disable this lazy stream. Replaces the producer with an empty list. Any leftover bytes that have already been read will still be reported upon read() and/or next().
Places bytes back onto the front of the lazy stream. Future calls to read() will return those bytes first. The stream position and thus tell() will be rewound.
Updates the unget history as a sanity check to see if we've pushed back the same number of bytes in one chunk. If we keep ungetting the same number of bytes many times (here, 50), we're mostly likely in an infinite loop of some sort. This is usually caused by a maliciously-malformed MIME request.
An iterable that will yield chunks of data. Given a file-like object as the constructor, this object will yield chunks of read operations from that object.
A Producer that will iterate over boundaries.
Completely exhausts an iterator or stream. Raise a MultiPartParserError if the argument is not a stream or an iterable.
HTTP headers as a bytestring.
Case-insensitive check for a header.
Sets a header unless it has already been set.
Full HTTP message, including headers, as a bytestring.
A streaming HTTP response class optimized for files.
An HTTP response class that consumes data to be serialized to JSON. :param data: Data to be dumped into json. By default only ``dict`` objects are allowed to be passed due to a security flaw before EcmaScript 5. See the ``safe`` parameter for more information. :param encoder: Should be an json encoder class. Defaults to ``django.core.serializers.json.DjangoJSONEncoder``. :param safe: Controls if only ``dict`` objects may be serialized. Defaults to ``True``. :param json_dumps_params: A dictionary of kwargs passed to json.dumps().
You cannot access raw_post_data from a request that has multipart/* POST data if it has been accessed via POST, FILES, etc..
Return the port number for the request as a string.
Attempts to return a signed cookie. If the signature fails or the cookie has expired, raises an exception... unless you provide the default argument in which case that value will be returned instead.
Return an absolute URI from variables available in this request. Skip allowed hosts protection, so may return insecure URI.
Hook for subclasses like WSGIRequest to implement. Returns 'http' by default.
Sets the encoding used for GET/POST accesses. If the GET or POST dictionary has already been created, it is removed and recreated on the next access (so that it is decoded correctly).
Returns a tuple of (POST QueryDict, FILES MultiValueDict).
Returns a mutable copy of this object.
Converts basestring objects to unicode, using the given encoding. Illegally encoded input characters are replaced with Unicode "unknown" codepoint (\ufffd). Returns any non-basestring objects without change.
Context manager that disables the default implicit wait.
Run tests and record everything but don't display anything. The implementation matches the unpythonic coding style of unittest2.
Run a suite of tests with a RemoteTestRunner and return a RemoteTestResult. This helper lives at module-level and its arguments are wrapped in a tuple because of the multiprocessing module's requirements.
Distribute test cases across workers. Return an identifier of each test case with its result in order to use imap_unordered to show results as soon as they're available. To minimize pickling errors when getting results from workers: - pass back numeric indexes in self.subsuites instead of tests - make tracebacks picklable with tblib, if available Even with tblib, errors may still occur for dynamically created exception classes such Model.DoesNotExist which cannot be unpickled.
Destroys all the non-mirror databases.
Run the unit tests for all the test labels in the provided list. Test labels should be dotted Python paths to test modules, test classes, or test methods. A list of 'extra' tests may also be provided; these tests will be added to the test suite. Returns the number of tests that failed.
Check if a test label points to a python package or file directory. Relative labels like "." and ".." are seen as directories.
Reorders a test suite by test type. `classes` is a sequence of types All tests of type classes[0] are placed first, then tests of type classes[1], etc. Tests with no match in classes are placed last. If `reverse` is True, tests within classes are sorted in opposite order, but test classes are not reversed.
Partitions a test suite by test type. Also prevents duplicated tests. classes is a sequence of types bins is a sequence of TestSuites, one more than classes reverse changes the ordering of tests within bins Tests of type classes[i] are added to bins[i], tests with no match found in classes are place in bins[-1]
Partitions a test suite by test case, preserving the order of tests.
The test client has been asked to follow a redirect loop.
A wrapper around BytesIO that restricts what can be read since data from the network can't be seeked and cannot be read outside of its content length. This makes sure that views can't do anything under the test client that wouldn't work in Real Life.
Simulate the behavior of most Web servers by removing the content of responses for HEAD requests, 1xx, 204, and 304 responses. Ensures compliance with RFC 7230, section 3.3.3.
Stores templates and contexts that are rendered. The context is copied so that it is an accurate representation at the time of rendering.
Stores exceptions when they are generated by a view.
Obtains the current session variables.
Requests a response from the server using GET.
Requests a response from the server using POST.
Request a response from the server using HEAD.
Request a response from the server using OPTIONS.
Send a resource to the server using PUT.
Send a resource to the server using PATCH.
Send a DELETE request to the server.
Send a TRACE request to the server.
Sets the Factory to appear as if it has successfully logged into a site. Returns True if login is possible; False if the provided credentials are incorrect.
Removes the authenticated user's cookies and session object. Causes the authenticated user to be logged out.
Flattened keys of subcontexts.
An instrumented Template render method, providing a signal that can be intercepted by the test system Client
A base class that can either be used as a context manager during tests or as a test function or unittest.TestCase subclass decorator to perform temporary alterations. `attr_name`: attribute assigned the return value of enable() if used as a class decorator. `kwarg_name`: keyword argument passing the return value of enable() if used as a function decorator.
Acts as a decorator. Overrides list of registered system checks. Useful when you override `INSTALLED_APPS`, e.g. if you exclude `auth` app, you also need to exclude its system checks.
Strip quotes of doctests output values: >>> strip_quotes("'foo'") "foo" >>> strip_quotes('"foo"') "foo"
Context manager that captures queries executed by the specified connection.
Context manager to temporarily add paths to sys.path.
Clear the cache of an LRU cache object on entering and exiting.
Puts value into a list if it's not already one. Returns an empty list if value is None.
Wrapper around default __call__ method to perform common Django test set up. This means that user-defined Test Cases aren't required to include a call to super().setUp().
Performs any pre-test setup. This includes:* Creating a test client. * Clearing the mail test outbox.
Perform any post-test things.
A context manager that temporarily sets a setting and reverts to the original value when exiting the context.
A context manager that temporarily applies changes a list setting and reverts back to the original value when exiting the context.
Asserts that a response indicates that some content was retrieved successfully, (i.e., the HTTP status code was as expected), and that ``text`` occurs ``count`` times in the content of the response. If ``count`` is None, the count doesn't matter - the assertion is true if the text occurs at least once in the response.
Asserts that a response indicates that some content was retrieved successfully, (i.e., the HTTP status code was as expected), and that ``text`` doesn't occurs in the content of the response.
Asserts that two HTML snippets are semantically the same. Whitespace in most cases is ignored, and attribute ordering is not significant. The passed-in arguments must be valid HTML.
Asserts that two HTML snippets are not semantically equivalent.
Asserts that the JSON fragments raw and expected_data are equal. Usual JSON non-significant whitespace rules apply as the heavyweight is delegated to the json library.
Asserts that the JSON fragments raw and expected_data are not equal. Usual JSON non-significant whitespace rules apply as the heavyweight is delegated to the json library.
Asserts that two XML snippets are semantically the same. Whitespace in most cases is ignored, and attribute ordering is not significant. The passed-in arguments must be valid XML.
Asserts that two XML snippets are not semantically equivalent. Whitespace in most cases is ignored, and attribute ordering is not significant. The passed-in arguments must be valid XML.
Performs any pre-test setup. This includes:* If the class has an 'available_apps' attribute, restricting the app registry to these applications, then firing post_migrate -- it must run with the correct set of applications for the test case. * If the class has a 'fixtures' attribute, installing these fixtures.
Returns True if all connections support transactions.
Helper method to open atomic blocks for multiple databases
Rollback atomic blocks opened through the previous method
Load initial data for the TestCase
Descriptor class for deferred condition checking
Skip a test if a database has at least one of the named features.
Skip a test unless a database has all the named features.
Skip a test unless a database has any of the named features.
Just a regular WSGIRequestHandler except it doesn't log to the standard output any of the requests received, so as to not clutter the output for the tests' results.
Checks if the path should be handled. Ignores the path if: * the host is provided as part of the base_url * the request's path isn't under the media path (or equal)
Returns the relative path to the file on disk for the given URL.
Handler for serving static files. A private class that is meant to be used solely as a convenience by LiveServerThread.
Handler for serving the media files. A private class that is meant to be used solely as a convenience by LiveServerThread.
Mixin to enforce serialization of TestCases that share a common resource. Define a common 'lockfile' for each set of TestCases to serialize. This file must exist on the filesystem. Place it early in the MRO in order to isolate setUpClass / tearDownClass.
Returns a dict containing the data in ``instance`` suitable for passing as a Form's ``initial`` keyword argument. ``fields`` is an optional list of field names. If provided, only the named fields will be included in the returned dict. ``exclude`` is an optional list of field names. If provided, the named fields will be excluded from the returned dict, even if they are listed in the ``fields`` argument.
Calls the instance's validate_unique() method and updates the form's validation errors if any were raised.
Returns the number of forms that are required in this FormSet.
If the field is a related field, fetch the concrete field's (that is, the ultimate pointed-to field's) to_python.
Saves and returns a new model instance for the given form.
Saves and returns an existing model instance for the given form.
Deletes an existing model instance.
Saves model instances for every form, adding and changing instancesas necessary, and returns the list of instances.
Returns ``limit_choices_to`` for this form field. If it is a callable, it will be invoked and the result will be returned.
This method is used to convert objects into strings; it's used to generate the labels for the choices presented by this object. Subclasses can override this method to customize the display of the choices.
Yields the forms in the order they should be rendered
Returns the form at the given index, based on the rendering order
Returns the ManagementForm instance for this FormSet.
Return additional keyword arguments for each individual formset form. index will be None if the form being constructed is a new empty form.
Return a list of all the initial forms in this formset.
Return a list of all the extra forms in this formset.
Returns a list of form.cleaned_data dicts for every form in self.forms.
Returns an ErrorList of errors that aren't associated with a particular form -- i.e., from formset.clean(). Returns an empty ErrorList if there are none.
Returns a list of form.errors for every form in self.forms.
Returns the number of errors across all forms in the formset.
Returns whether or not the form was marked for deletion.
Hook for doing any extra formset-wide cleaning after Form.clean() has been called on every form. Any ValidationError raised by this method will not be associated with a particular form; it will be accessible via formset.non_form_errors()
Returns true if data in any form differs from initial.
Returns True if the formset needs to be multipart, i.e. it has FileInput. Otherwise, False.
Returns True if the form has no errors. Otherwise, False. If errors are being ignored, returns False.
Returns the field name with a prefix appended, if this Form has a prefix set. Subclasses may wish to override.
Add a 'initial' prefix for checking dynamic initial values
Returns an ErrorList of errors that aren't associated with a particular field -- i.e., from Form.clean(). Returns an empty ErrorList if there are none.
An internal hook for performing additional cleaning after form cleaning is complete. Used for model validation in model forms.
Hook for doing any extra form-wide cleaning after Field.clean() has been called on every field. Any ValidationError raised by this method will not be associated with a particular field; it will have a special-case association with the field named '__all__'.
Returns True if data differs from initial.
Provide a description of all media required to render the widgets on this form
Returns True if the form needs to be multipart-encoded, i.e. it has FileInput. Otherwise, False.
Returns a list of all the BoundField objects that are hidden fields. Useful for manual form layout in templates.
Returns a list of BoundField objects that aren't hidden fields. The opposite of the hidden_fields() method.
Given a relative or absolute path to a static asset, return an absolute path. An absolute path will be returned unchanged while a relative path will be passed to django.templatetags.static.static().
Metaclass for classes that can have media definitions.
Some widgets are made of multiple HTML elements -- namely, RadioSelect. This is a class that represents the "inner" HTML element of a widget.
Yields all "subwidgets" of this widget. Used only by RadioSelect to allow template access to individual <input type="radio"> buttons. Arguments are the same as for render().
Returns this Widget rendered as HTML, as a Unicode string. The 'value' given is not guaranteed to be valid input, so subclass implementations should program defensively.
Given a dictionary of data and this widget's name, returns the value of this widget. Returns None if it's not provided.
Returns the HTML ID attribute of this Widget for use by a <label>, given the ID of the field. Returns None if no ID is available. This hook is necessary because some widgets have multiple HTML elements and, thus, multiple IDs. In that case, this method should return an ID value that corresponds to the first ID in the widget's tags.
Given the name of the file input, return the name of the clear checkbox input.
Given the name of the clear checkbox input, return the HTML id for it.
Return whether value is considered to be initial value.
Return value-related substitutions.
A Select Widget intended to be used with NullBooleanField.
Outputs a <ul> for this set of choice fields. If an id was given to the field, it is applied to the <ul> (each item in the list will get an id of `$id_$i`).
Returns an instance of the renderer.
Given a list of rendered widgets (as strings), returns a Unicode string representing the HTML for the whole lot. This hook allows you to format the HTML design of the widgets, if needed.
Returns a list of decompressed values for the given compressed value. The given value can be assumed to be valid, but not necessarily non-empty.
A Widget that splits datetime input into two <input type="text"> boxes.
A Widget that splits datetime input into two <input type="hidden"> inputs.
Converts 'first_name' to 'First name'
Convert a dictionary of attributes to a single string. The returned string will contain a leading space followed by key="value", XML-style pairs. In the case of a boolean value, the key will appear without a value. It is assumed that the keys do not need to be XML-escaped. If the passed dictionary is empty, then return an empty string. The result is passed through 'mark_safe' (by way of 'format_html_join').
A collection of errors that knows how to display itself in various formats. The dictionary keys are the field names, and the values are the errors.
When time zone support is enabled, convert naive datetimes entered in the current time zone to aware datetimes.
Validates the given value and returns its "cleaned" value as an appropriate Python object. Raises ValidationError for any errors.
Return the value that should be shown for this field on render of a bound form, given the submitted POST data for the field and the initial data, if any. For most fields, this will simply be data; FileFields need to handle it a bit differently.
Given a Widget instance (*not* a Widget class), returns a dictionary of any HTML attributes that should be added to the Widget, based on this Field.
Return a BoundField instance that will be used when accessing the form field in a template.
Validates that float() can be called on the input. Returns the result of float(). Returns None for empty values.
Validates that the input is a decimal number. Returns a Decimal instance. Returns None for empty values. Ensures that there are no more than max_digits in the number, and no more than decimal_places digits after the decimal point.
Validates that the input can be converted to a date. Returns a Python datetime.date object.
Validates that the input can be converted to a time. Returns a Python datetime.time object.
Validates that the input can be converted to a datetime. Returns a Python datetime.datetime object.
regex can be either a string or a compiled regular expression object. error_message is an optional error message to use, if 'Enter a valid value' is too generic for you.
Explicitly checks for the string 'True' and 'False', which is what a hidden field will submit for True and False, for 'true' and 'false', which are likely to be returned by JavaScript serializations of forms, and for '1' and '0', which is what a RadioField will submit. Unlike the Booleanfield we need to explicitly check for True, because we are not using the bool() function
Validates that the input is in self.choices.
Validate that the value can be coerced to the right type (if not empty).
Validates that the values are in self.choices and can be coerced to the right type.
Validates the given value against all of self.fields, which is a list of Field instances.
Returns a single value for the given list of values. The values can be assumed to be valid. For example, if this MultiValueField was instantiated with fields=(DateField(), TimeField()), this might return a datetime object created by combining the date and time in data_list.
Renders this field as an HTML widget.
Yields rendered strings that comprise all widgets in this BoundField. This really is only useful for RadioSelect widgets, so that you can iterate over individual radio buttons in a template.
Returns an ErrorList for this field. Returns an empty ErrorList if there are none.
Renders the field by rendering the passed widget, adding any HTML attributes passed as attrs.  If no widget is specified, then the field's default widget will be used.
Returns a string of HTML for representing this as an <input type="text">.
Returns a string of HTML for representing this as an <input type="hidden">.
Returns the data for this BoundField, or None if it wasn't given.
Returns a string of space-separated CSS classes for this field.
Calculates and returns the ID attribute for this BoundField, if the associated Form has specified auto_id. Returns an empty string otherwise.
Converts a datetime to local time in the active time zone. This only makes sense within a {% localtime off %} block.
Converts a datetime to UTC.
Template node class used by ``localtime_tag``.
Template node class used by ``timezone_tag``.
Template node class used by ``get_current_timezone_tag``.
Forces or prevents conversion of datetime objects to local time, regardless of the value of ``settings.USE_TZ``. Sample usage:: {% localtime off %}{{ value_in_utc }}{% endlocaltime %}
Enables a given time zone just for this block. The ``timezone`` argument must be an instance of a ``tzinfo`` subclass, a time zone name, or ``None``. If is it a time zone name, pytz is required. If it is ``None``, the default time zone is used within the block. Sample usage:: {% timezone "Europe/Paris" %} It is {{ now }} in Paris. {% endtimezone %}
Forces a value to be rendered as a localized value, regardless of the value of ``settings.USE_L10N``.
Forces a value to be rendered as a non-localized value, regardless of the value of ``settings.USE_L10N``.
This will store the language information dictionary for the given language code in a context variable. Usage:: {% get_language_info for LANGUAGE_CODE as l %} {{ l.code }} {{ l.name }} {{ l.name_translated }} {{ l.name_local }} {{ l.bidi|yesno:"bi-directional,uni-directional" }}
This will store a list of language information dictionaries for the given language codes in a context variable. The language codes can be specified either as a list of strings or a settings.LANGUAGES style list (or any sequence of sequences whose first items are language codes). Usage:: {% get_language_info_list for LANGUAGES as langs %} {% for l in langs %} {{ l.code }} {{ l.name }} {{ l.name_translated }} {{ l.name_local }} {{ l.bidi|yesno:"bi-directional,uni-directional" }} {% endfor %}
This will mark a string for translation and will translate the string for the current language. Usage:: {% trans "this is a test" %} This will mark the string for translation so it will be pulled out by mark-messages.py into the .po files and will run the string through the translation engine. There is a second form:: {% trans "this is a test" noop %} This will only mark for translation, but will return the string unchanged. Use it when you need to store values into forms that should be translated later on. You can use variables instead of constant strings to translate stuff you marked somewhere else:: {% trans variable %} This will just try to translate the contents of the variable ``variable``. Make sure that the string in there is something that is in the .po file. It is possible to store the translated string into a variable:: {% trans "this is a test" as var %} {{ var }} Contextual translations are also supported:: {% trans "this is a test" context "greeting" %} This is equivalent to calling pgettext instead of (u)gettext.
This will translate a block of text with parameters. Usage:: {% blocktrans with bar=foo|filter boo=baz|filter %} This is {{ bar }} and {{ boo }}. {% endblocktrans %} Additionally, this supports pluralization:: {% blocktrans count count=var|length %} There is {{ count }} object. {% plural %} There are {{ count }} objects. {% endblocktrans %} This is much like ngettext, only in template syntax. The "var as value" legacy format is still supported:: {% blocktrans with foo|filter as bar and baz|filter as boo %} {% blocktrans count var|length as count %} The translated string can be stored in a variable using `asvar`:: {% blocktrans with bar=foo|filter boo=baz|filter asvar var %} This is {{ bar }} and {{ boo }}. {% endblocktrans %} {{ var }} Contextual translations are supported:: {% blocktrans with bar=foo|filter context "greeting" %} This is {{ bar }}. {% endblocktrans %} This is equivalent to calling pgettext/npgettext instead of (u)gettext/(u)ngettext.
Populates a template variable with the static prefix, ``settings.STATIC_URL``. Usage:: {% get_static_prefix [as varname] %} Examples:: {% get_static_prefix %} {% get_static_prefix as static_prefix %}
Populates a template variable with the media prefix, ``settings.MEDIA_URL``. Usage:: {% get_media_prefix [as varname] %} Examples:: {% get_media_prefix %} {% get_media_prefix as media_prefix %}
Class method to parse prefix node and return a Node.
Joins the given path with the STATIC_URL setting. Usage:: {% static path [as varname] %} Examples:: {% static "myapp/css/base.css" %} {% static variable_with_path %} {% static "myapp/css/base.css" as admin_base_css %} {% static variable_with_path as varname %}
Disconnect receiver from sender for signal. If weak references are used, disconnect need not be called. The receiver will be remove from dispatch automatically. Arguments: receiver The registered receiver to disconnect. May be none if dispatch_uid is specified. sender The registered sender to disconnect dispatch_uid the unique identifier of the receiver to disconnect
Send signal from sender to all connected receivers. If any receiver raises an error, the error propagates back through send, terminating the dispatch loop. So it's possible that all receivers won't be called if an error is raised. Arguments: sender The sender of the signal. Either a specific object or None. named Named arguments which will be passed to receivers. Returns a list of tuple pairs [(receiver, response), ... ].
A decorator for connecting receivers to signals. Used by passing in the signal (or list of signals) and keyword arguments to connect:: @receiver(post_save, sender=MyModel) def signal_receiver(sender, **kwargs): ... @receiver([post_save, post_delete], sender=MyModel) def signals_receiver(sender, **kwargs): ...
Proxy for accessing the default DatabaseWrapper object's attributes. If you need to access the DatabaseWrapper object itself, use connections[DEFAULT_DB_ALIAS] instead.
Apply (if callable) or prepend (if a string) upload_to to the filename, then delegate further processing of the name to the storage backend. Until the storage layer, all file paths are expected to be Unix style (with forward slashes).
Return "app_label.model_label.field_name".
Displays the module, class and name of the field.
Custom format for select clauses. For example, GIS columns need to be selected as AsText(table.col) on MySQL as the table.col data can't be used by Django.
Uses deconstruct() to clone a new copy of this Field. Will not preserve any class attachments/attribute names.
Hook to generate new PK values on save. This method is called when saving instances with no primary key value set. If this method returns something else than None, then the returned value is used when saving the new instance.
Converts the input value into the expected Python data type, raising django.core.exceptions.ValidationError if the data can't be converted. Returns the converted value. Subclasses should override this.
Some validators can't be created at field initialization time. This method provides a way to delay their creation until required.
Convert the value's type and run validation. Validation errors from to_python and validate are propagated. The correct value is returned if no error is raised.
Return the database column check constraint for this field, for the provided connection. Works the same way as db_type() for the case that get_internal_type() does not map to a preexisting model field.
Return the data type that a related field pointing to this field should use. For example, this method is called by ForeignKey and OneToOneField to determine its data type.
Extension of db_type(), providing a range of different return values (type, checks). This will look at db_type(), allowing custom model fields to override it.
Return a dict that when passed as kwargs to self.model.filter(), would yield all instances having the same value for this field as obj has.
Returns field's value just before saving.
Perform preliminary non-db specific value checks and conversions.
Returns field's value prepared for interacting with the databasebackend. Used by the default implementations of get_db_prep_save().
Returns field's value prepared for saving into a database.
Returns a boolean of whether this field has a default value.
Returns the default value for this field.
return smart_text(self.value_from_object(obj)) def _get_flatchoices(self): Flattened version of choices tuple.
Returns the value of this field in the given model instance.
Return the data type that a related field pointing to this field should use. In most cases, a foreign key pointing to a positive integer primary key will have an integer column data type but some databases (e.g. MySQL) have an unsigned integer type. In that case (related_fields_match_type=True), the primary key should return its db_type.
Binary data is serialized as base64
Get the related objects through the reverse relation. With the example above, when getting ``parent.children``: - ``self`` is the descriptor managing the ``children`` attribute - ``instance`` is the ``parent`` instance - ``instance_type`` in the ``Parent`` class (we don't need it)
Set the related objects through the reverse relation. With the example above, when setting ``parent.children = children``: - ``self`` is the descriptor managing the ``children`` attribute - ``instance`` is the ``parent`` instance - ``value`` in the ``children`` sequence on the right of the equal sign
Filter the queryset for the instance this manager is bound to.
Filter the queryset for the instance this manager is bound to.
When filtering against this relation, returns the field on the remote model against which the filtering should happen.
Return choices with a default blank choices included, for use as SelectField choices for this field. Analog of django.db.models.fields.Field.get_choices(), provided initially for utilization by RelatedFieldListFilter.
Return the Field in the 'to' object to which this relationship is tied.
Used by OneToOneField to store information about the relation. ``_meta.get_fields()`` returns this class to provide access to the field flags for the reverse relation.
Schedule `function` to be called once `model` and all `related_models` have been imported and registered with the app registry. `function` will be called with the newly-loaded model classes as its positional arguments, plus any optional keyword arguments. The `model` argument must be a model class. Each subsequent positional argument is another model, or a reference to another model - see `resolve_relation()` for the various forms these may take. Any relative references will be resolved relative to `model`. This is a convenience wrapper for `Apps.lazy_model_operation` - the app registry model used is the one found in `model._meta.apps`.
Return the keyword arguments that when supplied to self.model.object.filter(), would select all instances related through this field to the remote obj. This is used to build the querysets returned by related descriptors. obj is an instance of self.related_field.model.
Complement to get_forward_related_filter(). Return the keyword arguments that when passed to self.related_field.model.object.filter() select all instances of self.related_field.model related through this field to obj. obj is an instance of self.model.
Return ``limit_choices_to`` for this model field. If it is a callable, it will be invoked and the result will be returned.
Define the name that can be used to identify this related object in a table-spanning query.
When filtering against this relation, returns the field on the remote model against which the filtering should happen.
Return an extra filter condition for related object fetching when user does 'instance.fieldname', that is the extra filter is used in the descriptor of the field. The filter should be either a dict usable in .filter(**kwargs) call or a Q-object. The condition will be ANDed together with the relation's joining columns. A parallel method is get_extra_restriction() which is used in JOIN and subquery conditions.
Return a pair condition used for joining and subquery pushdown. The condition is something that responds to as_sql(compiler, connection) method. Note that currently referring both the 'alias' and 'related_alias' will not work in some conditions, like subquery pushdown. A parallel method is get_extra_descriptor_filter() which is used in instance.fieldname related object fetching.
Get path from this field to the related model.
Get path from the related model to this field's model.
Get path from the related model to this field's model.
Called by both direct and indirect m2m traversal.
Function that can be curried to provide the m2m table name for this relation.
Function that can be curried to provide the source accessor or DB column name for the m2m table.
Return the value of this field in the given model instance.
A proxy for the _order database field that is used when Meta.order_with_respect_to is specified.
Return Sunday=1 through Saturday=7. To replicate this in Python: (mydatetime.isoweekday() % 7) + 1
Use the MAX function on SQLite.
Use the MIN function on SQLite.
Returns the number of characters in the expression
expression: the name of a field, or an expression returning a string pos: an integer > 0, or an expression returning an integer length: an optional number of characters to return
Decorator that calls urls.reverse() to return a URL using parameters returned by the decorated function "func". "func" should be a function that returns a tuple in one of the following formats: (viewname, viewargs) (viewname, viewargs, viewkwargs)
The query passed to raw isn't a safe query to use with raw.
A type that indicates the contents are an SQL fragment and the associate parameters. Can be used to pass opaque data to a where-clause, for example.
Check if the field value can be fetched from a parent field already loaded in the instance. This can be done if the to-be fetched field is a primary key field.
Remove given lookup from cls lookups. For use in tests only as it's not thread-safe.
Returns True if this field should be used to descend deeper for select_related() purposes. Used by both the query construction code (sql.query.fill_related_selections()) and the model instance creation code (query.get_klass_info()). Arguments: * field - the field to be checked * restricted - a boolean field, indicating if the field list has been manually restricted using a requested clause) * requested - The select_related() dictionary. * load_fields - the set of fields to be loaded on this model * reverse - boolean, True if we are checking a reverse select related
A helper method to check if the lookup_parts contains references to the given annotations set. Because the LOOKUP_SEP is contained in the default annotation names we must check each prefix of the lookup_parts for a match.
Provides pickling support. Normally, this just dispatches to Python's standard handling. However, for models with deferred field loading, we need to do things manually, as they're dynamically created classes and only module-level classes can be pickled by the default path.
Returns a set containing names of deferred fields for this instance.
Returns the value of the field name for this instance. If the field is a foreign key, returns the id value, instead of the object. If there's no Field object with this name on the model, the model attribute's value is returned directly. Used to serialize a field's value (in the serializer, or form output, for example). Normally, you would just access the attribute directly and not use this method.
Do an INSERT. If update_pk is defined then this method should return the new pk for the model.
Hook for doing any extra model-wide validation after clean() has been called on every field by self.clean_fields. Any ValidationError raised by this method will not be associated with a particular field; it will have a special-case association with the field defined by NON_FIELD_ERRORS.
Checks unique constraints on the model and raises ``ValidationError`` if any failed.
Check if the swapped model exists.
Perform all manager checks.
Perform all field checks.
Check the value of "index_together" option.
Check the value of "unique_together" option.
Iterable returned by QuerySet.values_list(flat=True) that yields single values.
Deep copy of a QuerySet doesn't populate the cache
The queryset iterator protocol uses three nested iterators in the default case: 1. sql.compiler:execute_sql() - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE) using cursor.fetchmany(). This part is responsible for doing some column masking, and returning the rows in chunks. 2. sql/compiler.results_iter() - Returns one row at time. At this point the rows are still just tuples. In some cases the return values are converted to Python values at this location. 3. self.iterator() - Responsible for turning the rows into model objects.
Retrieves an item or slice from the set of results.
An iterator over the results from applying this QuerySet to the database.
Performs a SELECT COUNT() and returns the number of records as an integer. If the QuerySet is already fully cached this simply returns the length of the cached results set to avoid multiple SELECT COUNT(*) calls.
Performs the query and returns a single object matching the given keyword arguments.
Creates a new object with the given kwargs, saving it to the database and returning the created object.
Looks up an object with the given kwargs, updating one with defaults if it exists, otherwise creates a new one. Returns a tuple (object, created), where created is a boolean specifying whether an object was created.
Tries to create an object using passed params. Used by get_or_create and update_or_create
Prepares `lookup` (kwargs that are valid model attributes), `params` (for creating a model instance) based on given kwargs; for use by get_or_create and update_or_create.
Returns the latest object, according to the model's 'get_latest_by' option or optional given field_name.
Returns the first object of a query, returns None if no match is found.
Returns the last object of a query, returns None if no match is found.
Returns a dictionary mapping each of the given IDs to the object with that ID. If `id_list` isn't provided, the entire QuerySet is evaluated.
Deletes objects found from the given queryset in single direct SQL query. No signals are sent, and there is no protection for cascades.
Updates all elements in the current QuerySet, setting all the given fields to the appropriate values.
A version of update that accepts field objects instead of field names. Used primarily for model saving and not intended for use by general code (it requires too much poking around at model internals to be useful at that level).
Returns a list of date objects representing all available dates for the given field_name, scoped to 'kind'.
Returns a list of datetime objects representing all available datetimes for the given field_name, scoped to 'kind'.
Returns an empty QuerySet.
Returns a new QuerySet that is a copy of the current one. This allows a QuerySet to proxy for a model manager in some cases.
Returns a new QuerySet instance with the args ANDed to the existing set.
Returns a new QuerySet instance with NOT (args) ANDed to the existing set.
Returns a new QuerySet instance with filter_obj added to the filters. filter_obj can be a Q object (or anything with an add_to_query() method) or a dictionary of keyword lookup arguments. This exists to support framework features such as 'limit_choices_to', and usually it will be more natural to use other methods.
Returns a new QuerySet instance that will select objects with a FOR UPDATE lock.
Returns a new QuerySet instance that will select related objects. If fields are specified, they must be ForeignKey fields and only those related objects are included in the selection. If select_related(None) is called, the list is cleared.
Returns a new QuerySet instance that will prefetch the specified Many-To-One and Many-To-Many related objects when the QuerySet is evaluated. When prefetch_related() is called more than once, the list of lookups to prefetch is appended to. If prefetch_related(None) is called, the list is cleared.
Returns a new QuerySet instance with the ordering changed.
Returns a new QuerySet instance that will select only distinct results.
Adds extra SQL fragments to the query.
Reverses the ordering of the QuerySet.
Defers the loading of data for certain fields until they are accessed. The set of fields to defer is added to any existing set of deferred fields. The only exception to this is if None is passed in as the only parameter, in which case all deferrals are removed (None acts as a reset option).
Selects which database this QuerySet should execute its query against.
Returns True if the QuerySet is ordered -- i.e. has an order_by() clause or a default ordering on the model.
Inserts a new record for the given model. This provides an interface to the InsertQuery class and is how Model.save() is implemented.
A little helper method for bulk_insert to insert the bulk one batch at a time. Inserts recursively a batch from the front of the bulk and then _batched_insert() the remaining objects again.
Indicates that the next filter call and the one following that should be treated as a single filter. This is only important when it comes to determining when to reuse tables for many-to-many filters. Required so that we can filter naturally on the results of related managers. This doesn't return a clone of the current QuerySet (it returns "self"). The method is only used internally and should be immediately followed by a filter() that does create a clone.
Checks that we are merging two comparable QuerySet classes.
Keep track of all known related objects from either QuerySet instance.
Checks if this QuerySet has any filtering going on. Note that this isn't equivalent for checking if all objects are present in results, for example qs[1:]._has_filters() -> False.
Marker class usable for checking if a queryset is empty by .none(): isinstance(qs.none(), EmptyQuerySet) -> True
Resolve the init field names and value positions
Selects which database this Raw QuerySet should execute its query against.
A dict mapping column names to model field names.
Helper function that normalize lookups into Prefetch objects.
An expression that can be combined with other expressions.
Arguments: * name: the name of the field this expression references
Arguments: * value: the value this expression represents. The value will be added into the sql parameter list and properly quoted. * output_field: an instance of the model field type that this expression will return, such as IntegerField() or CharField().
An expression that can wrap another expression so that it can provide extra context to the inner expression, such as the output_field.
An SQL searched CASE expression: CASE WHEN n > 0 THEN 'positive' WHEN n < 0 THEN 'negative' ELSE 'zero' END
Creates a clone of the tree. Must only be called on root nodes (nodes with empty subtree_parents). Childs must be either (Contraint, lookup, value) tuples, or objects supporting .clone().
A node that matches nothing.
The BaseTable class is used for base table references in FROM clause. For example, the SQL "foo" in SELECT * FROM "foo" WHERE somecond could be generated by this class.
Returns the query as a string of SQL with the parameter values substituted in (use sql_with_params() to see the unsubstituted string). Parameter values won't necessarily be quoted correctly, since that is done by the database interface at execution time.
Returns the query as an SQL string and the parameters that will be substituted into the query.
Returns the Options instance (the model._meta) from which to start processing. Normally, this is self.model._meta, but it can be changed by subclasses.
Performs a COUNT() query using the current filter constraints.
Increases the reference count for this alias.
Decreases the reference count for this alias.
Change join type from LOUTER to INNER for all joins in aliases. Similarly to promote_joins(), this method must ensure no join chains containing first an outer, then an inner join are generated. If we are demoting b->c join in chain a LOUTER b LOUTER c then we must demote a->b automatically, or otherwise the demotion of b->c doesn't actually change anything in the query results. .
This method will reset reference counts for aliases so that they match the value passed in :param to_counts:.
Generates a sequence of characters in alphabetical order: -> 'A', 'B', 'C', ... When the alphabet is finished, the sequence will continue with the Cartesian product: -> 'AA', 'AB', 'AC', ...
Returns the first alias for this query, after increasing its reference count.
Returns the number of tables in this query with a non-zero reference count. Note that after execution, the reference counts are zeroed, so tables added in compiler will not be seen by this method.
Adds a single annotation expression to the Query
Solve the lookup type from the lookup (eg: 'foobar__id__icontains')
Checks whether the object passed while querying is of the correct type. If not, it raises a ValueError specifying the wrong object.
Helper method for build_lookup. Tries to fetch and initialize a transform for name parameter from lhs.
Adds a Q-object to the current filter.
The 'target' parameter is the final field being joined to, 'joins' is the full list of join aliases. The 'path' contain the PathInfos used to create the joins. Returns the final target field and table alias and the new active joins. We will always trim any direct join if we have the target column available already in the previous table. Reverse joins can't be trimmed as we don't know if there is anything on the other side of the join.
Adjusts the limits on the rows retrieved. We use low/high to set these, as it makes it more Pythonic to read and write. When the SQL query is created, they are converted to the appropriate offset and limit values. Any limits passed in here are applied relative to the existing constraints. So low is added to the current low value and both will be clamped to any existing high value.
Clears any existing limits.
Returns True if adding filters to this instance is still possible. Typically, this means no limits or offsets have been put on the results.
Removes all fields from SELECT clause.
Clears the list of fields to select (but not extra_select columns). Some queryset types completely replace any existing list of select columns.
Adds and resolves the given fields to the query's "distinct on" clause.
Adds items from the 'ordering' sequence to the query's "order by" clause. These items are either field names (not column names) -- possibly with a direction prefix ('-' or '?') -- or OrderBy expressions. If 'ordering' is empty, all ordering is cleared from the query.
Removes any ordering settings. If 'force_empty' is True, there will be no ordering in the resulting query (not even the model's default).
Expands the GROUP BY clause required by the query. This will usually be the set of all non-aggregate fields in the return data. If the database backend supports grouping by the primary key, and the query would be equivalent, the optimization will be made automatically.
Sets up the select_related data structure so that we only select certain related models (as opposed to all models, when self.select_related=True).
Remove any fields from the deferred loading set.
Callback used by get_deferred_field_names().
Set the mask of extra select items that will be returned by SELECT, we don't actually remove them from the Query since they might be used later
The OrderedDict of aggregate columns that are not masked, and shouldbe used in the SELECT clause. This result is cached for optimization purposes.
Returns the field name and direction for an order specification. For example, '-foo' is returned as ('foo', 'DESC'). The 'default' param is used to indicate which way no prefix (or a '+' prefix) should sort. The '-' prefix always sorts the opposite way.
A helper function to add "value" to the set of values for "key", whether or not "key" already exists.
A little helper to check if the given field is reverse-o2o. The field is expected to be some sort of relation field or related object.
Add single vote per item to self.votes. Parameter can be any iterable.
Does any necessary class setup immediately prior to producing SQL. This is for things that can't necessarily be done in __init__ because we might not have all the pieces in place at that time.
A wrapper around connection.ops.quote_name that doesn't quote aliases for table names. This avoids problems with some SQL dialects that treat quoted strings specially (e.g. PostgreSQL).
Returns a quoted list of fields to use in DISTINCT ON part of the query. Note that this method can alter the tables in the query, and thus it must be called before get_from_clause().
A helper method for get_order_by and get_distinct. Note that get_ordering and get_distinct must produce same target columns on same input, as the prefixes of get_ordering and get_distinct must match. Executing SQL where this is not true is an error.
Converts the self.deferred_loading data structure to mapping of table names to sets of column names which are to be loaded. Returns the dictionary.
Returns an iterator over the results from executing this query.
Get the given field's value off the given obj. pre_save() is used for things like auto_now on DateTimeField. Skip it if this is a raw query.
Creates the SQL for this query. Returns the SQL string and list of parameters.
Execute the specified update. Returns the number of rows affected by the primary update query. The "primary update query" is the first non-empty query that is executed. Row counts for any subsequent, related queries are not available.
Runs on initialization and after cloning. Any attributes that would normally be set in __init__ should go in here, instead, so that they are also set up after a clone() call.
Convert a dictionary of field name to value mappings into an update query. This is the entry point for the public update() method on querysets.
Append a sequence of (field, model, value) triples to the internal list that will be used to generate the UPDATE query. Might be more usefully called add_update_targets() to hint at the extra information here.
Adds (name, value) to an update query for an ancestor model. Updates are coalesced so that we only run one update query per ancestor.
Returns a list of query objects: one for each update required to an ancestor model. Each query will have the same filtering conditions as the current query but will only update a single table.
Set up the insert query from the 'insert_values' dictionary. The dictionary gives the model field names and their target values. If 'raw_values' is True, the values in the 'insert_values' dictionary are inserted directly into the query, rather than passed as SQL parameters. This provides a way to insert NULL and DEFAULT keywords into the query, for example.
An AggregateQuery takes another query as a parameter to the FROM clause and only selects the elements in the provided list.
Does the internal setup so that the current model is a proxy for "target".
Return True if the model can/should be migrated on the `connection`. `connection` can be either a real connection or a connection alias.
There are a few places where the untranslated verbose name is needed (so that we get the same value regardless of currently active locale).
Returns a list of all concrete fields on the model and its parents. Private API intended only to be used by Django itself; get_fields() combined with filtering of field properties is the public API for obtaining this field list.
Returns a list of all concrete fields on the model. Private API intended only to be used by Django itself; get_fields() combined with filtering of field properties is the public API for obtaining this field list.
Returns a list of all many to many fields on the model and its parents. Private API intended only to be used by Django itself; get_fields() combined with filtering of field properties is the public API for obtaining this list.
Returns all related objects pointing to the current model. The related objects can come from a one-to-one, one-to-many, or many-to-many field relation type. Private API intended only to be used by Django itself; get_fields() combined with filtering of field properties is the public API for obtaining this field list.
Return a list of parent classes leading to `model` (ordered from closest to most distant ancestor). This has to handle the case where `model` is a grandparent or even more distant relation.
Returns all the ancestors of this model as a list ordered by MRO. Useful for determining if something is an ancestor, regardless of lineage.
Returns a list of fields associated to the model. By default, includes forward and reverse fields, fields derived from inheritance, but not hidden fields. The returned fields can be changed using the parameters: - include_parents: include fields derived from inheritance - include_hidden:  include fields that have a related_name that starts with a "+"
Return "app_label.model_label.manager_name".
Sets the creation counter value for this instance and increments the class-level copy.
Returns a new QuerySet object.  Subclasses can override this method to easily customize the behavior of the Manager.
RegisterLookupMixin() is first so that get_lookup() and get_transform() first examine self and then check output_field.
Some lookups require Field.get_db_prep_value() to be called on each value in an iterable.
Allow floats to work as query values for IntegerField. Without this, the decimal portion of the float would always be discarded.
Schedules a field update. 'objs' must be a homogeneous iterable collection of model instances (e.g. a QuerySet).
Returns the objs in suitably sized batches for the used connection.
Gets a QuerySet of objects related to ``objs`` via the relation ``related``.
Returns a list of table and view names in the current database.
Returns a dictionary of {field_name: (field_name_other_table, other_table)} representing all relationships to the given table.
Retrieves the storage engine for a given table. Returns the default storage engine if the table doesn't exist.
"ORDER BY NULL" prevents MySQL from implicitly ordering by grouped columns. If no ordering would otherwise be applied, we don't want any implicit sorting going on.
MySQL requires special cases for ^ operators in query expressions
MySQL doesn't accept default values for TEXT and BLOB types, and implicitly treats these columns as nullable.
MySQL can remove an implicit FK index on a field when that field is covered by another index like a unique_together. "covered" here means that the more complex index starts like the simpler one. http://bugs.mysql.com/bug.php?id=37910 / Django ticket 24757 We check here before removing the [unique|index]_together if we have to recreate a FK index.
Keep the null property of the old field. If it has changed, it will be handled separately.
Disables foreign key checks, primarily for use in adding rows with forward references. Always returns True, to indicate constraint checks need to be re-enabled.
SELECT REFERRING.`%s`, REFERRING.`%s` FROM `%s` as REFERRING LEFT JOIN `%s` as REFERRED ON (REFERRING.`%s` = REFERRED.`%s`) WHERE REFERRING.`%s` IS NOT NULL AND REFERRED.`%s` IS NULL  % (
Given a cursor object that has just performed an INSERT...RETURNING statement into a table that has an auto-incrementing ID, return the list of newly created IDs.
Returns the maximum length of an identifier. Note that the maximum length of an identifier is 63 by default, but can be changed by recompiling PostgreSQL after editing the NAMEDATALEN macro in src/include/pg_config_manual.h . This implementation simply returns 63, but can easily be overridden by a custom database backend that inherits most of its behavior from this one.
Makes ALTER TYPE with SERIAL make sense.
To check constraints, we set constraints to immediate. Then, when, we're done we must ensure they are returned to deferred.
Escape a fragment of a PostgreSQL .pgpass file.
Returns a list of table and view names in the current database.
Returns a dictionary of {field_name: field_index} for the given table. Indexes are 0-based.
Implements the interval functionality for expressions format for Oracle: INTERVAL '3 00:03:20.000000' DAY(1) TO SECOND(6)
Transform a date value to an object compatible with what is expected by the backend driver for date columns. The default implementation transforms the date to text, but that is not necessary for Oracle.
Get the properly shortened and uppercased identifier as returned by quote_name(), but without the actual quotes.
Generates temporary names for workarounds that need temp columns
This is analogous to other backends' `_nodb_connection` property, which allows access to an "administrative" connection which can be used to manage the test databases. For Oracle, the only connection that can be used for that purpose is the main (non-test) connection.
Oracle doesn't have the concept of separate databases under the same user. Thus, we use a separate user (see _create_test_db). This method is used to switch to that user. We will need the main user again for clean-up when we end testing, so we keep its credentials in SAVED_USER/SAVED_PASSWORD entries in the settings dict.
Set this database up to be used in testing as a mirror of a primary database whose settings are given
Return a value from the test settings dict, or a given default, or a prefixed entry from the main settings dict
We need to return the 'production' DB name to get the test DB creation machinery to work. This isn't a great deal in this case because DB names as handled by Django haven't real counterparts in Oracle.
EXTRACT(day from %(expressions)s) * 86400 + EXTRACT(hour from %(expressions)s) * 3600 + EXTRACT(minute from %(expressions)s) * 60 + EXTRACT(second from %(expressions)s)
To check constraints, we set constraints to immediate. Then, when, we're done we must ensure they are returned to deferred.
An adapter class for cursor variables that prevents the wrapped object from being converted into a string when used to instantiate an OracleParam. This can be used generally for any other object that should be passed into Cursor.execute as-is.
Cursor iterator wrapper that invokes our custom row factory.
Some versions of Oracle -- we've seen this on 11.2.0.1 and suspect it goes back -- have a weird bug where, when an integer column is added to an existing table with a default, its precision is later reported on introspection as 0, regardless of the real precision. For Django introspection, this means that such columns are reported as IntegerField even if they are really BigIntegerField or BooleanField. The bug is solved in Oracle 11.2.0.2 and up.
A late-binding cursor variable that can be passed to Cursor.execute as a parameter, in order to receive the id of the row created by an insert statement.
A datetime object, with an additional class attribute to tell cx_Oracle to save the microseconds too.
This class encapsulates all backend-specific validation.
Hook for a database backend to use the cursor description tomatch a Django field type to a database column. For Oracle, the column data_type on its own is insufficient to distinguish between a FloatField and IntegerField, for example.
Apply a conversion to the name for the purposes of comparison.The default table name converter is for case sensitive comparison.
Apply a conversion to the column name for the purposes of comparison. Uses table_name_converter() by default.
Returns a list of names of all tables that exist in the database. The returned table list is sorted by Python's default sorting. We do NOT use database's ORDER BY here to avoid subtle differences in sorting order between databases.
Returns an unsorted list of TableInfo named tuples of all tables and views that exist in the database.
Returns a list of all table names that have associated Django models and are in INSTALLED_APPS. If only_existing is True, the resulting list will only include the tables that actually exist in the database.
Backends can override this to return a list of (column_name, referenced_table_name, referenced_column_name) for all key columns in given table.
Returns the name of the primary key column for the given table.
Returns a dictionary of indexed fieldname -> infodict for the given table, where each infodict is in the format: {'primary_key': boolean representing whether it's the primary key, 'unique': boolean representing whether it's a unique index} Only single-column indexes are introspected.
Returns any SQL needed to support auto-incrementing primary keys, or None if no SQL is necessary. This SQL is executed when a table is created.
Returns the maximum allowed batch size for the backend. The fields are the fields going to be inserted in the batch, the objs contains all the objects to be inserted.
Returns an SQL query that retrieves the first cache key greater than the n smallest. This is used by the 'db' cache backend to determine where to start culling.
Given a field instance, returns the SQL necessary to cast the result of a union to that type. Note that the resulting string should contain a '%s' placeholder for the expression being cast.
Given a lookup_type of 'year', 'month' or 'day', returns the SQL that extracts a value from the given date field field_name.
Implements the date interval functionality for expressions
Given a lookup_type of 'year', 'month' or 'day', returns the SQL that truncates the given date field field_name to a date object with only the given specificity.
Returns the SQL necessary to cast a datetime value to date value.
Given a lookup_type of 'year', 'month', 'day', 'hour', 'minute' or 'second', returns the SQL that extracts a value from the given datetime field field_name, and a tuple of parameters.
Given a lookup_type of 'year', 'month', 'day', 'hour', 'minute' or 'second', returns the SQL that truncates the given datetime field field_name to a datetime object with only the given specificity, and a tuple of parameters.
Given a lookup_type of 'hour', 'minute' or 'second', returns the SQL that extracts a value from the given time field field_name.
Returns the SQL necessary to make a constraint "initially deferred" during a CREATE TABLE statement.
Returns an SQL DISTINCT clause which removes duplicate rows from the result set. If any fields are given, only the given fields are being checked for duplicates.
Returns the SQL command that drops a foreign key.
Returns any SQL necessary to drop the sequence for the given table. Returns None if no SQL is necessary.
Given a cursor object that has just performed an INSERT...RETURNING statement into a table that has an auto-incrementing ID, returns the newly created ID.
Given a column type (e.g. 'BLOB', 'VARCHAR'), and an internal type (e.g. 'GenericIPAddressField'), returns the SQL necessary to cast it before using it in a WHERE statement. Note that the resulting string should contain a '%s' placeholder for the column being searched against.
Returns a list used in the "ORDER BY" clause to force no ordering at all. Returning an empty list means that nothing will be included in the ordering.
Returns the FOR UPDATE SQL clause to lock rows for an update operation.
Given a cursor object that has just performed an INSERT statement into a table that has an auto-incrementing ID, returns the newly created ID. This method also receives the table name and the name of the primary-key column.
Returns the string to use in a query when performing lookups ("contains", "like", etc.). The resulting string should contain a '%s' placeholder for the column being searched against.
Returns the maximum number of items that can be passed in a single 'IN' list condition, or None if the backend does not impose a limit.
Returns the maximum length of table and column names, or None if there is no limit.
Returns the value to use for the LIMIT when we are wanting "LIMIT infinity". Returns None if the limit clause can be omitted in this case.
Returns the value to use during an INSERT statement to specify that the field should use its default value.
Takes an SQL script that may contain multiple lines and returns a list of statements to feed to successive cursor.execute() calls. Since few databases are able to process raw SQL scripts in a single cursor.execute() call and PEP 249 doesn't talk about this use case, the default implementation is conservative.
Returns the value of a CLOB column, for backends that return a locator object that requires additional processing.
For backends that support returning the last insert ID as part of an insert query, this method returns the SQL and params to append to the INSERT query. The returned fragment should contain a format string to hold the appropriate column.
Returns the SQLCompiler class corresponding to the given name, in the namespace corresponding to the `compiler_module` attribute on this backend.
Returns a quoted version of the given table, index or column name. Does not quote the given name if it's already been quoted.
Returns an SQL expression that returns a random value.
Returns the string to use in a query when performing regular expression lookups (using "regex" or "iregex"). The resulting string should contain a '%s' placeholder for the column being searched against. If the feature is not supported (or part of it is not supported), a NotImplementedError exception can be raised.
Returns the SQL for starting a new savepoint. Only required if the "uses_savepoints" feature is True. The "sid" parameter is a string for the savepoint id.
Returns the SQL for committing the given savepoint.
Returns the SQL for rolling back the given savepoint.
Returns the SQL that will set the connection's time zone. Returns '' if the backend doesn't support time zones.
Returns a list of SQL statements required to remove all data from the given database tables (without actually removing the tables themselves). The returned value also includes SQL statements required to reset DB sequences passed in :param sequences:. The `style` argument is a Style object as returned by either color_style() or no_style() in django.core.management.color. The `allow_cascade` argument determines whether truncation may cascade to tables with foreign keys pointing the tables being truncated. PostgreSQL requires a cascade even if these tables are empty.
Returns a list of the SQL statements required to reset sequences passed in :param sequences:. The `style` argument is a Style object as returned by either color_style() or no_style() in django.core.management.color.
Returns the SQL statement required to start a transaction.
Returns the SQL statement required to end a transaction.
Returns the SQL that will be used in a query to define the tablespace. Returns '' if the backend doesn't support tablespaces. If inline is True, the SQL is appended to a row; otherwise it's appended to the entire CREATE TABLE or CREATE INDEX statement.
Certain backends do not accept some values for "serial" fields (for example zero in MySQL). This method will raise a ValueError if the value is invalid, otherwise returns validated value.
Transforms a date value to an object compatible with what is expected by the backend driver for date columns.
Transforms a datetime value to an object compatible with what is expected by the backend driver for datetime columns.
Transforms a time value to an object compatible with what is expected by the backend driver for time columns.
Transforms a decimal.Decimal value to an object compatible with what is expected by the backend driver for decimal (numeric) columns.
Transforms a string representation of an IP address into the expected type for the backend driver.
Returns a two-elements list with the lower and upper bound to be used with a BETWEEN operator to query a DateField value using a year lookup. `value` is an int, containing the looked-up year.
Returns a two-elements list with the lower and upper bound to be used with a BETWEEN operator to query a DateTimeField value using a year lookup. `value` is an int, containing the looked-up year.
Get a list of functions needed to convert field data. Some field types on some backends do not provide data in the correct format, this is the hook for converter functions.
Check that the backend supports the provided expression. This is used on specific backends to rule out known expressions that have problematic or nonexistent implementations. If the expression has a known problem, the backend should raise NotImplementedError.
Combine a list of subexpressions into a single expression, usingthe provided connecting operator. This is required because operators can vary between backends (e.g., Oracle with %% and &) and between subexpression types (e.g., date expressions)
Some backends require special syntax to insert binary content (MySQL for example uses '_binary %s').
Allow modification of insert parameters. Needed for Oracle Spatialbackend due to 10888.
Given an integer field internal type (e.g. 'PositiveIntegerField'), returns a tuple of the (min_value, max_value) form representing the range of the column type bound to the field.
Some backends don't accept default values for certain columns types (i.e. MySQL longtext and longblob).
Only used for backends which have requires_literal_defaults feature
Renames the table a model points to.
Moves a model's table between tablespaces
Hook to specialize column type alteration for different backends, for cases when a creation type is different to an alteration type (e.g. SERIAL in PostgreSQL, PostGIS fields). Should return two things; an SQL fragment of (sql, params) to insert into an ALTER TABLE statement, and a list of extra (sql, params) tuples to run once the field is altered.
Return the SQL statement to create the index for one or several fields. `sql` can be specified if the syntax differs from the standard (GIS indexes, ...).
Return all index SQL statements (field indexes, index_together) for the specified model, as a list.
Used to be defined here, now moved to DatabaseWrapper.
Set this database up to be used in testing as a mirror of a primary database whose settings are given
Reloads the database with data from a string generated by the serialize_db_to_string method.
Return display string for a database for use in various actions.
Internal implementation - returns the name of the test DB that will be created. Only useful when called from create_test_db() and _create_test_db() and when no external munging is done with the 'NAME' settings.
Internal implementation - duplicate the test db tables.
SQL to append to the end of the test table creation statements.
Name of the time zone of the database connection.
Returns a dict of parameters suitable for get_new_connection.
Opens a connection to the database.
Initializes the database connection settings.
Creates a cursor, opening a connection if necessary.
Creates a savepoint inside the current transaction. Returns an identifier for the savepoint that will be used for the subsequent rollback or commit. Does nothing if savepoints are not supported.
Releases a savepoint. Does nothing if savepoints are not supported.
Check the autocommit state.
Enable or disable autocommit. The usual way to start a transaction is to turn autocommit off. SQLite does not properly start a transaction when disabling autocommit. To avoid this buggy behavior and to actually enter a new transaction, an explcit BEGIN is required. Using force_begin_transaction_with_broken_autocommit=True will issue an explicit BEGIN with SQLite. This option will be ignored for other backends.
Get the "needs rollback" flag -- for *advanced use* only.
Set or unset the "needs rollback" flag -- for *advanced use* only.
Raise an error if an atomic block is active.
Context manager that disables foreign key constraint checking.
Backends can implement as needed to temporarily disable foreign key constraint checking. Should return True if the constraints were disabled and will need to be reenabled.
Backends can implement as needed to re-enable foreign key constraint checking.
Tests if the database connection is usable. This function may assume that self.connection is not None. Actual implementations should take care not to raise exceptions as that may prevent Django from recycling unusable connections.
Hook to do any database check or preparation, generally called before migrating a project or an app.
Context manager and decorator that re-throws backend-specific database exceptions using Django's common wrappers.
Creates a cursor that logs all queries in self.queries_log.
Creates a cursor without debug logging.
Context manager that ensures that a connection is established, and if it opened one, closes it to avoid leaving a dangling connection. This is useful for operations outside of the request-response cycle. Provides a cursor: with self.temporary_connection() as cursor: ...
Return an alternative connection to be used when there is no need to access the main database, specifically for test db creation/deletion. This also prevents the production database from being exposed to potential child threads while (or after) the test database is destroyed. Refs 10868, 17786, 16969.
Only required when autocommits_when_autocommit_is_off = True.
Returns a new instance of this backend's SchemaEditor.
Confirm support for transactions.
Confirm support for STDDEV and related stats functions.
What is the type returned when the backend introspects a BooleanField? The optional arguments may be used to give further details of the field to be introspected; in particular, they are provided by Django's test suite: field -- the field definition created_separately -- True if the field was added via a SchemaEditor's AddField, False if the field was created with the model Note that return value from this function is compared by tests against actual introspection results; it should provide expectations, not run an introspection itself.
SQLite has a compile-time default (SQLITE_LIMIT_VARIABLE_NUMBER) of 999 variables per query. If there is just single field to insert, then we can hit another limit, SQLITE_MAX_COMPOUND_SELECT which defaults to 500.
Do nothing here, we will handle it in the custom function.
Deals with a model changing its index_together. Note: The input index_togethers must be doubly-nested, not the single- nested ["foo", "bar"] format.
Deals with a model changing its unique_together. Note: The input unique_togethers must be doubly-nested, not the single- nested ["foo", "bar"] format.
SELECT REFERRING.`%s`, REFERRING.`%s` FROM `%s` as REFERRING LEFT JOIN `%s` as REFERRED ON (REFERRING.`%s` = REFERRED.`%s`) WHERE REFERRING.`%s` IS NOT NULL AND REFERRED.`%s` IS NULL
Start a transaction explicitly in autocommit mode. Staying in autocommit mode works around a bug of sqlite3 that breaks savepoints when autocommit is disabled.
Django uses "format" style placeholders, but pysqlite2 uses "qmark" style. This fixes it -- but note that if you want to use a literal "%s" in a query, you'll need to use "%%s".
Confirm support for STDDEV and related stats functionsSQLite supports STDDEV as an extension package; so connection.ops.check_expression_support() can't unilaterally rule out support for STDDEV. We need to manually check whether the call works.
Take either a model class or an "app_label.ModelName" string and return (app_label, object_name).
Drops a model's table.
Changes the value of unique_together to the target one. Input value of unique_together must be a set of tuples.
Changes the value of index_together to the target one. Input value of index_together must be a set of tuples.
Alters the model's managers
Returns a 3-tuple of class import path (or just name if it lives under django.db.migrations), positional arguments, and keyword arguments.
Takes the state from the previous migration, and mutates it so that it matches what this migration would perform.
Performs the mutation on the database schema in the normal (forwards) direction.
Performs the mutation on the database schema in the reverse direction - e.g. if this were CreateModel, it would in fact drop the model's table.
Outputs a brief summary of what the action does.
Returns True if there is a chance this operation references the given model name (as a string), with an optional app label for accuracy. Used for optimization. If in doubt, return True; returning a false positive will merely make the optimizer a little less efficient, while returning a false negative may result in an unusable optimized migration.
Returns True if there is a chance this operation references the given field name, with an optional app label for accuracy. Used for optimization. If in doubt, return True.
Returns if we're allowed to migrate the model. This is a thin wrapper around router.allow_migrate_model() that preemptively rejects any proxy, swapped out, or unmanaged model.
Return either a list of operations the actual operation should be replaced with or a boolean that indicates whether or not the specified operation can be optimized across.
Runs some raw SQL. A reverse SQL statement may be provided. Also accepts a list of operations that represent the state change effected by this SQL change, in case it's custom column/table creation/deletion.
Removes a field from a model.
Alters a field's database column (e.g. null, max_length) to the provided new field
Special subclass of string which actually references a current settings value. It's treated as the value in memory, but serializes out to a settings.NAME attribute reference.
Looks through the loaded graph and detects any conflicts - apps with more than one leaf migration. Returns a dict of the app labels that conflict with the migration names that conflict.
Returns a set of (app, name) of applied migrations.
Records that a migration was applied.
Records that a migration was unapplied.
Transition dummy to a normal node and clean off excess attribs. Creating a Node object from scratch would be too much of a hassle as many dependendies would need to be remapped.
This may create dummy nodes if they don't yet exist. If `skip_validation` is set, validate_consistency should be called afterwards.
Ensure there are no dummy nodes remaining in the graph.
Returns all root nodes - that is, nodes with no dependencies inside their app. These are the starting point for an app.
Returns all leaf nodes - that is, nodes with no dependents in their app. These are the "most current" version of an app's schema. Having more than one per app is technically an error, but one that gets handled further up, in the interactive command - it's usually the result of a VCS merge and needs some user input.
Given a migration node or nodes, returns a complete ProjectState for it. If at_end is False, returns the state before the migration has run. If nodes is not provided, returns the overall most current project state.
Takes a ProjectState and returns a new one with the migration's operations applied to it. Preserves the original object state by default and will return a mutated state from a copy.
Subclass of tuple so Django can tell this was originally a swappable dependency when it reads the migration file.
Raised when more than one migration matches a name prefix.
Raised when there's a bad migration (unreadable/bad format/etc.).
Raised when there's an impossible-to-resolve circular dependency.
Raised when an applied migration has some of its dependencies not applied.
Raised when a model's base classes can't be resolved.
Raised when a irreversible migration is about to be reversed.
Raised when an attempt on a node is made that is not available in the graph.
Takes a migration plan and returns a list of collected SQL statements that represent the best-efforts version of that plan.
Mark replacement migrations applied if their replaced set all are. We do this unconditionally on every migrate, rather than just when migrations are applied or unapplied, so as to correctly handle the case when a new squash migration is pushed to a deployment that already had all its replaced migrations applied. In this case no new migration will be applied, but we still want to correctly maintain the applied state of the squash migration.
Return all models that have a direct or indirect relationship to the given model. Relationships are either defined by explicit relational fields, like ForeignKey, ManyToManyField or OneToOneField, or by inheriting from another model (a superclass is related to its subclasses, but not vice versa). Note, however, that a model inheriting from a concrete model is also related to its superclass through the implicit *_ptr OneToOneField on the subclass.
Main entry point to produce a list of applicable changes. Takes a graph to base names on and an optional set of apps to try and restrict to (restriction is not guaranteed)
Return a definition of the fields that ignores field names and what related fields actually relate to. Used for detecting renames (as, of course, the related fields change during renames)
Prepare field lists, and prepare a list of the fields that used through models in the old state so we can make dependencies from the through model deletion to the field that uses it.
Through model map generation
Sorting key function that places potential swappable models first in lists of created models (only real way to solve 22783)
Finds any renamed models, and generates the operations for them, and removes the old entry from the model lists. Must be run before other model-level generation.
Makes DeleteModel statements for proxy models.
Fields that have been added
Fields that have been removed.
Given a set of operations, suggests a name for the migration they might represent. Names are not guaranteed to be unique, but we put some effort in to the fallback name to avoid VCS conflicts if we can.
This exception is thrown when transaction management is used improperly.
Get a database connection by name, or the default database connection if no name is provided. This is a private API.
Get the autocommit status of the connection.
Set the autocommit status of the connection.
Commits a transaction.
Rolls back a transaction.
Creates a savepoint (if supported and required by the backend) inside the current transaction. Returns an identifier for the savepoint that will be used for the subsequent rollback or commit.
Rolls back the most recent savepoint (if one exists). Does nothing if savepoints are not supported.
Commits the most recent savepoint (if one exists). Does nothing if savepoints are not supported.
Resets the counter used to generate unique savepoint ids in this thread.
Gets the "needs rollback" flag -- for *advanced use* only.
Sets or unsets the "needs rollback" flag -- for *advanced use* only. When `rollback` is `True`, it triggers a rollback when exiting the innermost enclosing atomic block that has `savepoint=True` (that's the default). Use this to force a rollback without raising an exception. When `rollback` is `False`, it prevents such a rollback. Use this only after rolling back to a known-good state! Otherwise, you break the atomic block and data corruption may occur.
Register `func` to be called when the current transaction is committed. If the current transaction is rolled back, `func` will not be called.
wrapper is a database wrapper. It must have a Database attribute defining PEP-249 exceptions.
databases is an optional dictionary of database definitions (structured like settings.DATABASES).
Puts the defaults into the settings dictionary for a given connection where no settings is provided.
Makes sure the test settings are available in the 'TEST' sub-dictionary.
If routers is not specified, will default to settings.DATABASE_ROUTERS.
Validates that the input matches the regular expression if inverse_match is False, otherwise raises ValidationError.
Depending on the given parameters returns the appropriate validators for the GenericIPAddressField. This code is here, because it is exactly the same for the model and the form field.
Retrieves the specified file from storage.
Returns a filename, based on the provided filename, that's suitable for use in the target storage system.
Deletes the specified file from the storage system.
Returns True if a file referenced by the given name already exists in the storage system, or False if the name is available for a new file.
Lists the contents of the specified path, returning a 2-tuple of lists; the first item being directories, the second item being files.
Returns the total size, in bytes, of the file specified by name.
Returns an absolute URL where the file's contents can be accessed directly by a Web browser.
Returns the last accessed time (as datetime object) of the file specified by name. Deprecated: use get_accessed_time() instead.
Returns the creation time (as datetime object) of the file specified by name. Deprecated: use get_created_time() instead.
Returns the last modified time (as datetime object) of the file specified by name. Deprecated: use get_modified_time() instead.
Reset setting based property values.
Returns the full path of this file.
A mixin for use alongside django.core.files.base.File, which provides additional features for dealing with images.
Get a filedescriptor from something which could be a file or an fd.
Read the file and yield chunks of ``chunk_size`` bytes (defaults to ``UploadedFile.DEFAULT_CHUNK_SIZE``).
Returns ``True`` if you can expect multiple chunks. NB: If a particular file representation is in memory, subclasses should always return ``False`` -- there's no good reason to read from memory in chunks.
Return True if line (a text or byte string) ends with '\r'.
Return True if line (a text or byte string) ends with '\n'.
A mixin class used to forward file methods to an underlaying file object.  The internal file object has to be called "file":: class FileProxy(FileProxyMixin): def __init__(self, file): self.file = file
Any error having to do with uploading files.
If ``connection_reset`` is ``True``, Django knows will halt the upload without consuming the rest of the upload. This will cause the browser to show a "connection reset" error.
This exception is raised by an upload handler that wants to skip a given file.
Upload handers that have handled a file and do not want future handlers to run should raise this exception instead of returning None.
Handle the raw input from the client. Parameters: :input_data: An object that supports reading via .read(). :META: ``request.META``. :content_length: The (integer) value of the Content-Length header from the client. :boundary: The boundary from the Content-Type header. Be sure to prepend two '--'.
Signal that a new file has been started. Warning: As with any data from the client, you should not trust content_length (and sometimes won't even get it).
Receive data from the streamed upload parser. ``start`` is the position in the file of the chunk.
Signal that a file has completed. File size corresponds to the actual size accumulated by all the chunks. Subclasses should return a valid ``UploadedFile`` object.
Signal that the upload is complete. Subclasses should perform cleanup that is necessary for this handler.
Create the file object to append to as data is coming in.
Add the data to the BytesIO file.
Return a file object if we're activated.
Signature does not match
Signature timestamp is older than required max_age
Simple wrapper around json to be used in signing.dumps and signing.loads.
Built-in tags for internal checks.
Return a list containing a warning that the pattern is invalid. describe_pattern() cannot be used here, because we cannot rely on the urlpattern having regex or name attributes.
Format the URL pattern for display in warning messages.
Check that include is not used with a regex ending with a dollar.
Check that the pattern does not begin with a forward slash.
Collate the objects to be serialized. If count_only is True, just count the number of objects to be serialized.
Return a list of fixture directories. The list contains the 'fixtures' subdirectory of each installed application, if it exists, the directories in FIXTURE_DIRS, and the current directory.
Splits fixture name in name, serialization format, compression format.
Returns the default WSGI handler for the runner.
Runs the server, using the autoreloader if needed
Converts a module namespace to a Python dictionary.
Path to a file which is being fed into GNU gettext pipeline. This may be either a translatable or its preprocessed version.
Preprocess (if necessary) a translatable file before passing it to xgettext GNU gettext utility.
Build pot files and apply msguniq to them.
Check if the given path should be ignored or not.
Group translatable files by locale directory and run pot file build process for each group.
Pre-parse the command line to extract the value of the --testrunner option. This allows a test runner to define additional command line arguments.
Start IPython pre-0.11
Start IPython pre-1.0.0
Start IPython >= 1.0
Given a path to a management directory, returns a list of all the command names that are available. Returns an empty list if no commands are defined.
Given a command name and an application name, returns the Command class instance. All errors raised by the import process (ImportError, AttributeError) are allowed to propagate.
Returns a dictionary mapping command names to their callback applications. This works by looking for a management.commands package in django.core, and in each installed application -- if a commands package exists, all commands in that package are registered. Core commands are always included. If a settings module has been specified, user-defined commands will also be included. The dictionary is in the format {command_name: app_name}. Key-value pairs from this dictionary can then be used in calls to load_command_class(app_name, command_name) If a specific version of a command must be loaded (e.g., with the startapp command), the instantiated module can be placed in the dictionary in place of the application name. The dictionary is cached on the first call and reused on subsequent calls.
Like os.path.splitext, but takes off .tar, too
Extracts the given file to a temporarily and returns the path of the directory with the extracted content.
Returns True if the name looks like a URL
Exception class indicating a problem while executing a management command. If this exception is raised during the execution of a management command, it will be caught and turned into a nicely-printed error message to the appropriate output stream (i.e., stderr); as a result, raising this exception (with a sensible description of the error) is the preferred way to indicate that something has gone wrong in the execution of a command.
The system check framework detected unrecoverable errors.
Include any default options that all commands should accept here so that ManagementUtility can handle them before searching for user commands.
Wrapper around stdout/stderr
Return the Django version, which should be correct for all built-in Django commands. User-supplied commands can override this method to return their own version.
Return a brief description of how to use this command, by default from the attribute ``self.help``.
Create and return the ``ArgumentParser`` which will be used to parse the arguments to this command.
Entry point for subclassed commands to add custom arguments.
Print the help message for this command, derived from ``self.usage()``.
The actual logic of the command. Subclasses must implement this method.
Perform the command's actions for app_config, an AppConfig instance corresponding to an application label given on the command line.
Returns a Style object with no color scheme.
Friendly wrapper around Popen. Returns stdout output, stderr output and OS status code.
Organizes multiple extensions that are separated with commas or passed by using --extension/-e multiple times. For example: running 'django-admin makemessages -e js,txt -e xhtml -a' would result in an extension list: ['.js', '.txt', '.xhtml'] >>> handle_extensions(['.html', 'html,js,py,py,py,.py', 'py,.py']) {'.html', '.js', '.py'} >>> handle_extensions(['.html, txt,.tpl']) {'.html', '.tpl', '.txt'}
Return a 50 character random string usable as a SECRET_KEY setting value.
Returns a list of the SQL statements used to flush the database. If only_django is True, then only table names that have associated Django models and are in INSTALLED_APPS will be included.
A Cache Handler to manage access to Cache instances. Ensures only one instance of each alias exists per thread.
Proxy access to the default Cache object's attributes. This allows the legacy `cache` object to be thread-safe using the new ``caches`` API.
Convert a key into a cache file path. Basically this is the root cache path joined with the md5sum of the key and a suffix.
Remove all the cache files.
Implements transparent thread-safe access to a memcached client.
A context manager that does nothing special.
A class that will quack like a Django model _meta class.This allows cache operations to be controlled by the router
Default function to generate keys. Constructs the key used by all other methods. By default it prepends the `key_prefix'. KEY_FUNCTION can be used to specify an alternate function with custom key making behavior.
Function to decide which key function to use. Defaults to ``default_key_func``.
Constructs the key used by all other methods. By default ituses the key_func to generate a key (which, by default, prepends the `key_prefix' and 'version'). A different key function can be provided at the time of cache construction; alternatively, you can subclass the cache backend to provide custom key making behavior.
Set a value in the cache if the key does not already exist. If timeout is given, that timeout will be used for the key; otherwise the default cache timeout will be used. Returns True if the value was stored, False otherwise.
Fetch a given key from the cache. If the key does not exist, return default, which itself defaults to None.
Set a value in the cache. If timeout is given, that timeout will be used for the key; otherwise the default cache timeout will be used.
Delete a key from the cache, failing silently.
Fetch a bunch of keys from the cache. For certain backends (memcached, pgsql) this can be *much* faster when fetching multiple values. Returns a dict mapping each key in keys to its value. If the given key is missing, it will be missing from the response dict.
Returns True if the key is in the cache and has not expired.
Add delta to value in the cache. If the key does not exist, raise a ValueError exception.
Subtract delta from value in the cache. If the key does not exist, raise a ValueError exception.
Set a bunch of values in the cache at once from a dict of key/value pairs.  For certain backends (memcached), this is much more efficient than calling set() multiple times. If timeout is given, that timeout will be used for the key; otherwise the default cache timeout will be used.
Delete a bunch of values in the cache at once. For certain backends (memcached), this is much more efficient than calling delete() multiple times.
Remove *all* values from the cache at once.
Warn about keys that would not be portable to the memcached backend. This encourages (but does not force) writing backend-portable cache code.
Adds delta to the cache version for the supplied key. Returns thenew version.
Subtracts delta from the cache version for the supplied key. Returnsthe new version.
Validates the given 1-based page number.
Returns a Page object for the given 1-based page number.
Returns an instance of a single page. This hook can be used by subclasses to use an alternative to the standard :cls:`Page` object.
Returns the total number of pages.
The requested model field does not exist
The django.apps registry is not populated yet
The requested object does not exist
The query returned multiple objects when only one was expected.
Suspect MIME request in multipart form data
A Suspicious filesystem operation was attempted
HTTP_HOST header contains invalid value
Redirect to scheme not in allowed list
The number of fields in a POST request exceeded settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.
The size of the request (excluding any file uploads) exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.
The user did not have permission to do that
The requested view does not exist
This middleware is not used in this server configuration
Django is somehow improperly configured
Some kind of problem with a model field.
Returns the HTTP request's PATH_INFO as a unicode string.
Get a value from the WSGI environ dictionary as str. key and default should be str objects. Under Python 2 they may also be unicode objects provided they only contain ASCII characters.
Pass the exception to the exception middleware. If no middleware return a response for this exception, raise it.
Load an email backend and return an instance of it.If backend is None (default) settings.EMAIL_BACKEND is used. Both fail_silently and other keyword arguments are used in the constructor of the backend.
Easy wrapper for sending a single message to a recipient list. All members of the recipient list will see the other recipients in the 'To' field. If auth_user is None, the EMAIL_HOST_USER setting is used. If auth_password is None, the EMAIL_HOST_PASSWORD setting is used. Note: The API for this method is frozen. New code wanting to extend the functionality should use the EmailMessage class directly.
Given a datatuple of (subject, message, from_email, recipient_list), sends each message to each recipient list. Returns the number of emails sent. If from_email is None, the DEFAULT_FROM_EMAIL setting is used. If auth_user and auth_password are set, they're used to log in. If auth_user is None, the EMAIL_HOST_USER setting is used. If auth_password is None, the EMAIL_HOST_PASSWORD setting is used. Note: The API for this method is frozen. New code wanting to extend the functionality should use the EmailMessage class directly.
Sends a message to the admins, as defined by the ADMINS setting.
Sends a message to the managers, as defined by the MANAGERS setting.
Return a unique file name.
Dummy email backend that does nothing.
Open a network connection.This method can be overwritten by backend implementations to open a network connection. It's up to the backend implementation to track the status of a network connection if it's needed by the backend. This method can be called by applications to force a single network connection to be used when sending mails. See the send_messages() method of the SMTP backend for a reference implementation. The default implementation does nothing.
Close a network connection.
Forbids multi-line headers, to prevent header injection.
Return the entire formatted message as a string.Optional `unixfrom' when True, means include the Unix From_ envelope header. This overrides the default as_string() implementation to not mangle lines that begin with 'From '. See bug 13433 for details.
Return the entire formatted message as bytes.Optional `unixfrom' when True, means include the Unix From_ envelope header. This overrides the default as_bytes() implementation to not mangle lines that begin with 'From '. See bug 13433 for details.
Initialize a single email message (which can be sent to multiple recipients). All strings used to create the message can be unicode strings (or UTF-8 bytestrings). The SafeMIMEText class will handle any necessary encoding conversions.
Returns a list of all recipients of the email (includes direct addressees as well as Cc and Bcc entries).
Attaches a file with the given filename and content. The filename can be omitted and the mimetype is guessed, if not provided. If the first parameter is a MIMEBase subclass it is inserted directly into the resulting message attachments.
Converts the filename, content, mimetype triple into a MIME attachment object.
Initialize a single email message (which can be sent to multiple recipients). All strings used to create the message can be unicode strings (or UTF-8 bytestrings). The SafeMIMEText class will handle any necessary encoding conversions.
Attach an alternative content representation.
Loads and returns the WSGI application as configured by the user in ``settings.WSGI_APPLICATION``. With the default ``startproject`` layout, this will be the ``application`` object in ``projectname/wsgi.py``. This function, and the ``WSGI_APPLICATION`` setting itself, are only useful for Django's internal server (runserver); external WSGI servers should just be configured to point to the correct application object directly. If settings.WSGI_APPLICATION is not set (is ``None``), we just return whatever ``django.core.wsgi.get_wsgi_application`` returns.
Override server_bind to store the server name.
Start serialization -- open the XML document and the root element.
End serialization -- end the document.
Called as each object is handled.
Called after handling all fields for an object.
Helper to output the <field> element for relational fields
Create a hardened XML parser (no custom/external entities).
Helper to look up a model from a <object model=...> or a <field rel=... to=...> node.
Base exception.
Document type definition is forbidden.
Entity definition is forbidden.
Resolving an external reference is forbidden.
Stub serializer to hold exception raised during registration This allows the serializer registration to cache serializers and if there is an error raised in the process of creating a serializer it will be raised and passed along to the caller when the serializer is used.
Register a new serializer.``serializer_module`` should be the fully qualified module name for the serializer. If ``serializers`` is provided, the registration will be added to the provided dictionary. If ``serializers`` is not provided, the registration will be made directly into the global register of serializers. Adding serializers directly is not a thread-safe operation.
Serialize a queryset (or any iterator that returns database objects) using a certain serializer.
Deserialize a stream or a string. Returns an iterator that yields ``(obj, m2m_relation_dict)``, where ``obj`` is an instantiated -- but *unsaved* -- object, and ``m2m_relation_dict`` is a dictionary of ``{m2m_field_name : list_of_related_objects}``.
Register built-in and settings-defined serializers. This is done lazily so that user code has a chance to (e.g.) set up custom settings without needing to be careful of import order.
The requested serializer was not found.
Something bad happened during serialization.
Factory method for creating a deserialization error which has a more explanatory message.
Called when serializing of the queryset starts.
Called when serializing of the queryset ends.
Called when serializing of an object starts.
Called when serializing of an object ends.
Called to handle each individual (non-relational) field on an object.
Called to handle a ForeignKey field.
Called to handle a ManyToManyField.
Return the fully serialized queryset (or None if the output stream is not seekable).
Init this serializer given a stream or a string
Iteration iterface -- return the next item in the stream
Build a model instance. If the model instance doesn't have a primary key and the model supports natural keys, try to retrieve it from the database.
Loads and returns a template for the given name. Raises TemplateDoesNotExist if no such template exists.
Loads and returns a template for one of the given names. Tries names in order and returns the first template found. Raises TemplateDoesNotExist if no such template exists.
Loads a template and renders it with a context. Returns a string. template_name may be a string or a list of strings.
Returns a compiled Template object for the given template code, handling template inheritance recursively.
Adds slashes before quotes. Useful for escaping strings in CSV, for example. Less useful for escaping JavaScript; use the ``escapejs`` filter instead.
Capitalizes the first character of the value.
Escapes an IRI value for use in a URL.
Converts a string into all lowercase.
Returns the value turned into a list. For an integer, it's a list of digits. For a string, it's a list of characters.
Converts to ASCII. Converts spaces to hyphens. Removes characters that aren't alphanumerics, underscores, or hyphens. Converts to lowercase. Also strips leading and trailing whitespace.
Formats the variable according to the arg, a string formatting specifier. This specifier uses Python string formating syntax, with the exception that the leading "%" is dropped. See https://docs.python.org/3/library/stdtypes.htmlprintf-style-string-formatting for documentation of Python string formatting.
Converts a string into titlecase.
Converts a string into all uppercase.
Escapes a value for use in a URL. Takes an optional ``safe`` parameter used to determine the characters which should not be escaped by Django's ``urlquote`` method. If not provided, the default safe characters will be used (but an empty string can be provided when *all* characters should be escaped).
Converts URLs in plain text into clickable links.
Converts URLs into clickable links, truncating URLs to the given character limit, and adding 'rel=nofollow' attribute to discourage spamming. Argument: Length to truncate URLs to.
Returns the number of words.
Wraps words at specified line length. Argument: number of characters to wrap the text at.
Left-aligns the value in a field of a given width. Argument: field size.
Right-aligns the value in a field of a given width. Argument: field size.
Centers the value in a field of a given width.
Removes all values of arg from the given string.
Escapes a string's HTML. This returns a new string containing the escaped characters (as opposed to "escape", which marks the content for later possible escaping).
Replaces line breaks in plain text with appropriate HTML; a single newline becomes an HTML line break (``<br />``) and a new line followed by a blank line becomes a paragraph break (``</p>``).
Converts all newlines in a piece of plain text to HTML line breaks (``<br />``).
Marks the value as a string that should not be auto-escaped.
A "safe" filter for sequences. Marks each element in the sequence, individually, as safe, after converting them to unicode. Returns a list with the results.
Strips all [X]HTML tags.
When arg is convertible to float, behave like operator.itemgetter(arg) Otherwise, behave like Variable(arg).resolve >>> _property_resolver(1)('abc') 'b' >>> _property_resolver('1')('abc') Traceback (most recent call last): ... TypeError: string indices must be integers >>> class Foo: ...     a = 42 ...     b = 3.14 ...     c = 'Hey!' >>> _property_resolver('b')(Foo()) 3.14
Takes a list of dicts, returns that list sorted by the property given in the argument.
Takes a list of dicts, returns that list sorted in reverse order by the property given in the argument.
Returns the first item in a list.
Returns the length of the value - useful for lists.
Returns a boolean of whether the value's length is the argument.
Returns a random item from the list.
Recursively takes a self-nested list and returns an HTML unordered list -- WITHOUT opening and closing <ul> tags. The list is assumed to be in the proper format. For example, if ``var`` contains: ``['States', ['Kansas', ['Lawrence', 'Topeka'], 'Illinois']]``, then ``{{ var|unordered_list }}`` would return:: <li>States <ul> <li>Kansas <ul> <li>Lawrence</li> <li>Topeka</li> </ul> </li> <li>Illinois</li> </ul> </li>
Adds the arg to the value.
Formats a date according to the given format.
Formats a time according to the given format.
Formats a date as the time since that date (i.e. "4 days, 6 hours").
Formats a date as the time until that date (i.e. "4 days, 6 hours").
If value is unavailable, use given default.
If value is None, use given default.
Returns True if the value is divisible by the argument.
Takes a phone number and converts it in to its numerical equivalent.
Return a collation of template tag libraries from installed applications and the supplied custom_libraries argument.
Create a new TemplateDoesNotExist. Preserve its declared attributes and template debug data but discard __traceback__, __context__, and __cause__ to make this object suitable for keeping around (in a cache, for example).
Reraise TemplateDoesNotExist while maintaining template debug information.
Recursively yield template tag libraries defined in submodules of a package.
Initializes the template engine. Receives the configuration settings as a dict.
Creates and returns a template for the given source code. This method is optional.
A container to hold debug information as described in the template API documentation.
Returns what to display in error messages for this node
Implements the actions of the autoescape tag.
Ignores everything between ``{% comment %}`` and ``{% endcomment %}``.
Outputs a whole load of debugging information, including the current context and imported modules. Sample usage:: <pre> {% debug %} </pre>
Outputs the first variable passed that is not False, without escaping. Outputs nothing if all the passed variables are False. Sample usage:: {% firstof var1 var2 var3 as myvar %} This is equivalent to:: {% if var1 %} {{ var1|safe }} {% elif var2 %} {{ var2|safe }} {% elif var3 %} {{ var3|safe }} {% endif %} but obviously much cleaner! You can also use a literal string as a fallback value in case all passed variables are False:: {% firstof var1 var2 var3 "fallback value" %} If you want to disable auto-escaping of variables you can use:: {% autoescape off %} {% firstof var1 var2 var3 "<strong>fallback value</strong>" %} {% autoescape %} Or if only some variables should be escaped, you can use:: {% firstof var1 var2|safe var3 "<strong>fallback value</strong>"|safe %}
Loops over each item in an array. For example, to display a list of athletes given ``athlete_list``:: <ul> {% for athlete in athlete_list %} <li>{{ athlete.name }}</li> {% endfor %} </ul> You can loop over a list in reverse by using ``{% for obj in list reversed %}``. You can also unpack multiple values from a two-dimensional array:: {% for key,value in dict.items %} {{ key }}: {{ value }} {% endfor %} The ``for`` tag can take an optional ``{% empty %}`` clause that will be displayed if the given array is empty or could not be found:: <ul> {% for athlete in athlete_list %} <li>{{ athlete.name }}</li> {% empty %} <li>Sorry, no athletes in this list.</li> {% endfor %} <ul> The above is equivalent to -- but shorter, cleaner, and possibly faster than -- the following:: <ul> {% if athlete_list %} {% for athlete in athlete_list %} <li>{{ athlete.name }}</li> {% endfor %} {% else %} <li>Sorry, no athletes in this list.</li> {% endif %} </ul> The for loop sets a number of variables available within the loop: ==========================  ================================================ Variable                    Description ==========================  ================================================ ``forloop.counter``         The current iteration of the loop (1-indexed) ``forloop.counter0``        The current iteration of the loop (0-indexed) ``forloop.revcounter``      The number of iterations from the end of the loop (1-indexed) ``forloop.revcounter0``     The number of iterations from the end of the loop (0-indexed) ``forloop.first``           True if this is the first time through the loop ``forloop.last``            True if this is the last time through the loop ``forloop.parentloop``      For nested loops, this is the loop "above" the current one ==========================  ================================================
Outputs the contents of the block if the two arguments equal each other. Examples:: {% ifequal user.id comment.user_id %} ... {% endifequal %} {% ifnotequal user.id comment.user_id %} ... {% else %} ... {% endifnotequal %}
Outputs the contents of the block if the two arguments are not equal. See ifequal.
Checks if a value has changed from the last iteration of a loop. The ``{% ifchanged %}`` block tag is used within a loop. It has two possible uses. 1. Checks its own rendered contents against its previous state and only displays the content if it has changed. For example, this displays a list of days, only displaying the month if it changes:: <h1>Archive for {{ year }}</h1> {% for date in days %} {% ifchanged %}<h3>{{ date|date:"F" }}</h3>{% endifchanged %} <a href="{{ date|date:"M/d"|lower }}/">{{ date|date:"j" }}</a> {% endfor %} 2. If given one or more variables, check whether any variable has changed. For example, the following shows the date every time it changes, while showing the hour if either the hour or the date has changed:: {% for date in days %} {% ifchanged date.date %} {{ date.date }} {% endifchanged %} {% ifchanged date.hour date.date %} {{ date.hour }} {% endifchanged %} {% endfor %}
Return a subset of tags and filters from a library.
Displays the date, formatted according to the given string. Uses the same format as PHP's ``date()`` function; see http://php.net/date for all the possible values. Sample usage:: It is {% now "jS F Y H:i" %}
Removes whitespace between HTML tags, including tab and newline characters. Example usage:: {% spaceless %} <p> <a href="foo/">Foo</a> </p> {% endspaceless %} This example would return this HTML:: <p><a href="foo/">Foo</a></p> Only space between *tags* is normalized -- not space between tags and text. In this example, the space around ``Hello`` won't be stripped:: {% spaceless %} <strong> Hello </strong> {% endspaceless %}
Return an absolute URL matching the given view with its parameters. This is a way to define links that aren't tied to a particular URL configuration:: {% url "url_name" arg1 arg2 %} or {% url "url_name" name1=value1 name2=value2 %} The first argument is a django.conf.urls.url() name. Other arguments are space-separated values that will be filled in place of positional and keyword arguments in the URL. Don't mix positional and keyword arguments. All arguments for the URL must be present. For example, if you have a view ``app_name.views.client_details`` taking the client's id and the corresponding line in a URLconf looks like this:: url('^client/(\d+)/$', views.client_details, name='client-detail-view') and this app's URLconf is included into the project's URLconf under some path:: url('^clients/', include('app_name.urls')) then in a template you can create a link for a certain client like this:: {% url "client-detail-view" client.id %} The URL will look like ``/clients/client/123/``. The first argument may also be the name of a template variable that will be evaluated to obtain the view name or the URL name, e.g.:: {% with url_name="client-detail-view" %} {% url url_name client.id %} {% endwith %}
Stops the template engine from rendering the contents of this block tag. Usage:: {% verbatim %} {% don't process this %} {% endverbatim %} You can also designate a specific closing tag block (allowing the unrendered use of ``{% endverbatim %}``):: {% verbatim myblock %} ... {% endverbatim myblock %}
For creating bar charts and such, this tag calculates the ratio of a given value to a maximum value, and then applies that ratio to a constant. For example:: <img src="bar.png" alt="Bar" height="10" width="{% widthratio this_value max_value max_width %}" /> If ``this_value`` is 175, ``max_value`` is 200, and ``max_width`` is 100, the image in the above example will be 88 pixels wide (because 175/200 = .875; .875 * 100 = 87.5 which is rounded up to 88). In some cases you might want to capture the result of widthratio in a variable. It can be useful for instance in a blocktrans like this:: {% widthratio this_value max_value max_width as width %} {% blocktrans %}The width is: {{ width }}{% endblocktrans %}
The exception used when a template does not exist. Accepts the following optional arguments: backend The template backend class used when raising this exception. tried A list of sources that were tried when finding the template. This is formatted as a list of tuples containing (origin, status), where origin is an Origin object or duck type and status is a string with the reason the template wasn't found. chain A list of intermediate TemplateDoesNotExist exceptions. This is used to encapsulate multiple exceptions when loading templates from multiple engines.
Returns a new context with the same properties, but with only the values given in 'values' stored.
Returns self.dicts as one dictionary
A stack container for storing Template state. RenderContext simplifies the implementation of template Nodes by providing a safe place to store state between invocations of a node's `render` method. The RenderContext also provides scoping rules that are more sensible for 'template local' variables. The render context stack is pushed before each template is rendered, creating a fresh scope with nothing in it. Name resolution fails if a variable is not found at the top of the RequestContext stack. Thus, variables are local to a specific template and don't affect the rendering of other templates as they would if they were stored in the normal template context.
Parse and compile the template source into a nodelist. If debug is True and an exception occurs during parsing, the exception is is annotated with contextual line information where it occurred in the template source.
A token representing a string from the template. token_type One of TOKEN_TEXT, TOKEN_VAR, TOKEN_BLOCK, or TOKEN_COMMENT. contents The token source string. position An optional tuple containing the start and end index of the token in the template source. This is used for traceback information when debug is on. lineno The line number the token appears on in the template source. This is used for traceback information and gettext files.
Return a list of tokens from a given template_string.
Split a template string into tokens and annotates each token with its start and end position in the source. This is slower than the default lexer so we only use it when debug is True.
Return an exception annotated with the originating token. Since the parser can be called recursively, check if a token is already set. This ensures the innermost token is highlighted if an exception occurs, e.g. a compile error within the body of an if statement.
Convenient wrapper for FilterExpression
Return the node rendered as a string.
Render the node. If debug is True and an exception occurs during rendering, the exception is annotated with contextual line information where it occurred in the template. For internal usage this method is preferred over using the render method directly.
Return a list of all nodes (within this node and its nodelist) of the given type
Converts any value to a string to become part of a rendered template. This means escaping, if required, and conversion to a unicode object. If value is a string, it is expected to have already been translated.
Perform the caching that gives this loader its name. Often many of the templates attempted will be missing, so memory use is of concern here. To keep it in check, caching behavior is a little complicated when a template is not found. See ticket 26306 for more details. With template debugging disabled, cache the TemplateDoesNotExist class for every missing template and raise a new instance of it after fetching it from the cache. With template debugging enabled, a unique TemplateDoesNotExist object is cached for each missing template to preserve debug data. When raising an exception, Python sets __traceback__, __context__, and __cause__ attributes on it. Those attributes can contain references to all sorts of objects up the call chain and caching them creates a memory leak. Thus, unraised copies of the exceptions are cached and copies of those copies are raised after they're fetched from the cache.
Generate a cache key for the template name, dirs, and skip. If skip is provided, only origins that match template_name are included in the cache key. This ensures each template is only parsed and cached once if contained in different extend chains like: x -> a -> a y -> a -> a z -> a -> a
RemovedInDjango20Warning: This is an internal property used by the ExtendsNode during the deprecation of non-recursive loaders.
RemovedInDjango20Warning: An internal method to lookup the template name in all the configured loaders.
Wrapper for loading templates from a plain Python dict.
Wrapper for loading templates from "templates" directories in INSTALLED_APPS packages.
An iterator that yields possible matching template paths for a template name.
RemovedInDjango20Warning: Returns a tuple containing the source and origin for the given template name.
Resets any state maintained by the loader instance (e.g. cached templates or cached loader modules).
templates is an optional list of template engine definitions (structured like settings.TEMPLATES).
Signal that this template extends a parent template. This tag may be used in two ways: ``{% extends "base" %}`` (with quotes) uses the literal value "base" as the name of the parent template to extend, or ``{% extends variable %}`` uses the value of ``variable`` as either the name of the parent template to extend (if it evaluates to a string) or as the parent template itself (if it evaluates to a Template object).
Register a callable as an inclusion tag: @register.inclusion_tag('results.html') def show_results(poll): choices = poll.choice_set.all() return {'choices': choices}
Base class for tag helper nodes such as SimpleNode and InclusionNode. Manages the positional and keyword arguments to be passed to the decorated function.
Pickling support function.Ensures that the object can't be pickled before it has been rendered, and that the pickled state only includes rendered data, not the data used to construct the response.
Returns the freshly rendered content for the template and contextdescribed by the TemplateResponse. This *does not* set the final content of the response. To set the response content, you must either call render(), or set the content explicitly using the value of this property.
Adds a new post-rendering callback.If the response has already been rendered, invoke the callback immediately.
Renders (thereby finalizing) the content of the response.If the content has already been rendered, this is a no-op. Returns the baked response instance.
Adds static-related context variables to the context.
Adds media-related context variables to the context.
Returns an extra keyword arguments dictionary that is used when initializing the feed generator.
Returns an extra keyword arguments dictionary that is used with the `add_item` call of the feed generator.
Returns a dictionary to use as extra context if either ``self.description_template`` or ``self.item_template`` are used. Default implementation preserves the old behavior of using {'obj': item, 'site': current_site} as the context.
Returns a feedgenerator.DefaultFeed object, fully populated, for this feed. Raises FeedDoesNotExist for invalid parameters.
Given the OGRGeomType for a geometry and its associated GeometryField, determine whether the geometry should be turned into a GeometryCollection.
Goes through the given sources and returns a 3-tuple of the application label, module name, and field name of every GeometryField encountered in the sources. If no sources are provided, then all models.
This method is overridden so the appropriate `geo_format` attribute is placed on each URL element.
Return a dictionary of city information for the given IP address or Fully Qualified Domain Name (FQDN). Some information in the dictionary may be undefined (None).
Compresses the KML content and returns as KMZ (using the correct MIME type).
Custom argparse action for the `ogrinspect` `layer_key` keyword option which may be an integer or a string.
Custom argparse action for `ogrinspect` keywords that require a string list. If the string is 'True'/'true' then the option value will be a boolean instead.
Initializes a GEvent object. Parameters: event: string for the event, such as 'click'. The event must be a valid event for the object in the Google Maps API. There is no validation of the event type within Django. action: string containing a Javascript function, such as 'function() { location.href = "newurl";}' The string must be a valid Javascript function. Again there is no validation fo the function within Django.
Generates the JavaScript necessary for displaying this Google Map.
Generates the JavaScript for the collection of Google Maps in this set.
By default, Decimal values are converted to str by the SQLite backend, which is not acceptable by the GIS functions expecting numeric values.
Returns the centroid of the geographic field in a `centroid` attribute on each element of this GeoQuerySet.
Returns the spatial difference of the geographic field in a `difference` attribute on each element of this GeoQuerySet.
Returns the distance from the given geographic field name to the given geometry in a `distance` attribute on each element of the GeoQuerySet. Keyword Arguments: `spheroid`  => If the geometry field is geodetic and PostGIS is the spatial database, then the more accurate spheroid calculation will be used instead of the quicker sphere calculation. `tolerance` => Used only for Oracle. The tolerance is in meters -- a default of 5 centimeters (0.05) is used.
Returns a Geometry representing the bounding box of the Geometry field in an `envelope` attribute on each element of the GeoQuerySet.
Returns a modified version of the Polygon/MultiPolygon in which all of the vertices follow the Right-Hand-Rule.  By default, this is attached as the `force_rhr` attribute on each element of the GeoQuerySet.
Returns a GeoJSON representation of the geometry field in a `geojson` attribute on each element of the GeoQuerySet. The `crs` and `bbox` keywords may be set to True if the user wants the coordinate reference system and the bounding box to be included in the GeoJSON representation of the geometry.
Returns a GeoHash representation of the given field in a `geohash` attribute on each element of the GeoQuerySet. The `precision` keyword may be used to custom the number of _characters_ used in the output GeoHash, the default is 20.
Returns GML representation of the given field in a `gml` attribute on each element of the GeoQuerySet.
Returns the spatial intersection of the Geometry field in an `intersection` attribute on each element of this GeoQuerySet.
Returns KML representation of the geometry field in a `kml` attribute on each element of this GeoQuerySet.
Returns the length of the geometry field as a `Distance` object stored in a `length` attribute on each element of this GeoQuerySet.
Returns the memory size (number of bytes) that the geometry field takes in a `mem_size` attribute  on each element of this GeoQuerySet.
Returns the number of geometries if the field is a GeometryCollection or Multi* Field in a `num_geom` attribute on each element of this GeoQuerySet; otherwise the sets with None.
Returns the number of points in the first linestring in the Geometry field in a `num_points` attribute on each element of this GeoQuerySet; otherwise sets with None.
Returns the perimeter of the geometry field as a `Distance` object stored in a `perimeter` attribute on each element of this GeoQuerySet.
Returns a Point geometry guaranteed to lie on the surface of the Geometry field in a `point_on_surface` attribute on each element of this GeoQuerySet; otherwise sets with None.
Reverses the coordinate order of the geometry, and attaches as a `reverse` attribute on each element of this GeoQuerySet.
Scales the geometry to a new size by multiplying the ordinates with the given x,y,z scale factors.
Snap all points of the input geometry to the grid.  How the geometry is snapped to the grid depends on how many arguments were given: - 1 argument : A single size to snap both the X and Y grids to. - 2 arguments: X and Y sizes to snap the grid to. - 4 arguments: X, Y sizes and the X, Y origins.
Returns SVG representation of the geographic field in a `svg` attribute on each element of this GeoQuerySet. Keyword Arguments: `relative`  => If set to True, this will evaluate the path in terms of relative moves (rather than absolute). `precision` => May be used to set the maximum number of decimal digits used in output (defaults to 8).
Returns the symmetric difference of the geographic field in a `sym_difference` attribute on each element of this GeoQuerySet.
Translates the geometry to a new location using the given numeric parameters as offsets.
Transforms the given geometry field to the given SRID.  If no SRID is provided, the transformation will default to using 4326 (WGS84).
DRY routine for setting up a GeoQuerySet method that attaches a Geometry attribute (e.g., `centroid`, `point_on_surface`).
DRY routine for setting up a GeoQuerySet method that attaches a Geometry attribute and takes a Geoemtry parameter.  This is used for geometry set-like operations (e.g., intersection, difference, union, sym_difference).
Wrapper for GML to be used by Oracle to ensure Database.LOB conversion.
Returns the placeholder for the spatial column for the given value.
Prepare the value for saving in the database.
Return a GDALRaster if conversion is successful, otherwise return None.
Returns a distance number in units of the field.  For example, if `D(km=1)` was passed in and the units of the field were in meters, then 1000 would be returned.
The overlaps_left operator returns true if A's bounding box overlaps or is to the left of B's bounding box.
The 'overlaps_right' operator returns true if A's bounding box overlaps or is to the right of B's bounding box.
The 'overlaps_below' operator returns true if A's bounding box overlaps or is below B's bounding box.
The 'overlaps_above' operator returns true if A's bounding box overlaps or is above B's bounding box.
The 'left' operator returns true if A's bounding box is strictly to the left of B's bounding box.
The 'right' operator returns true if A's bounding box is strictly to the right of B's bounding box.
The 'strictly_below' operator returns true if A's bounding box is strictly below B's bounding box.
The 'strictly_above' operator returns true if A's bounding box is strictly above B's bounding box.
The "~=" operator is the "same as" operator. It tests actual geometric equality of two features. So if A and B are the same feature, vertex-by-vertex, the operator returns true.
The 'bbcontains' operator returns true if A's bounding box completely contains by B's bounding box.
The 'bboverlaps' operator returns true if A's bounding box overlaps B's bounding box.
The 'contained' operator returns true if A's bounding box is completely contained by B's bounding box.
Proxy initializes on the given Geometry or Raster class (not an instance) and the corresponding field.
Returns the name of the metadata column used to store the feature table name.
Returns the name of the metadata column used to store the feature geometry column.
The 'spatial_ref_sys' table from PostGIS. See the PostGIS documentation at Ch. 4.2.1.
This method allows escaping the binary in the style required by the server's `standard_conforming_string` setting.
Returns a 4-tuple extent for the `Extent` aggregate by converting the bounding box text returned by PostGIS (`box` argument), for example: "BOX(-90.0 30.0, -85.0 40.0)".
Returns a 6-tuple extent for the `Extent3D` aggregate by converting the 3d bounding-box text returned by PostGIS (`box3d` argument), for example: "BOX3D(-90.0 30.0 1, -85.0 40.0 2)".
Converts the geometry returned from PostGIS aggregates.
Return the version of PROJ.4 used by PostGIS as a tuple of the major, minor, and subminor release numbers.
Pack data into hex string with little endian format.
Unpack little endian hexlified binary string into a list.
Split a string into two parts at the input index.
Returns the name of the metadata column used to store the feature table name.
Returns the name of the metadata column used to store the feature geometry column.
Oracle requires that polygon rings are in proper orientation. This affects spatial operations and an invalid orientation may cause failures. Correct orientations are: * Outer ring - counter clockwise * Inner ring(s) - clockwise
Returns the geometry database type for Oracle.  Unlike other spatial backends, no stored procedure is necessary and it's the same for all geometry types.
Returns a tuple of the ellipsoid parameters: (semimajor axis, semiminor axis, and inverse flattening).
Return a tuple of (unit_value, unit_name) for the given WKT without using any of the database fields.
This provides an adaptor for Geometries sent to the MySQL and Oracle database backends.
Returns the database column type for the geometry field on the spatial backend.
Returns the distance parameters for the given geometry field, lookup value, and lookup type.
Returns the placeholder for the given geometry field with the given value.  Depending on the spatial backend, the placeholder may contain a stored procedure call to the transformation function of the spatial backend.
Class encapsulating the behavior specific to a GIS operation (used by lookups).
Returns the name of the metadata column used to store the feature table name.
Returns the name of the metadata column used to store the feature geometry column.
The 'spatial_ref_sys' table from SpatiaLite.
Sublcass that includes updates the `base_data_types_reverse` dict for geometry field types.
Determine the version of the SpatiaLite library.
Convert the polygon data received from Spatialite to min/max values.
Converts geometry WKT returned from a SpatiaLite aggregate.
Returns None because geometry columns are added via the `AddGeometryColumn` stored procedure on SpatiaLite.
Returns the distance parameters for the given geometry field, lookup value, and lookup type.  SpatiaLite only supports regular cartesian-based queries (no spheroid/sphere calculations for point geometries like PostGIS).
Helper routine for calling SpatiaLite functions and returning their result. Any error occurring in this method should be handled by the caller.
Return the version of LWGEOM library used by SpatiaLite.
Returns the SpatiaLite version as a tuple (version string, major, minor, subminor).
Return the unit value and the default units specified from the given keyword arguments dictionary.
Retrieves the unit attribute name for the given unit string. For example, if the given unit string is 'metre', 'm' would be returned. An exception is raised if an attribute cannot be found.
Create a coordinate sequence, set X, Y, [Z], and create point
Returns a clone because the copy of a GEOSGeometry may contain an invalid pointer location if the original is garbage collected.
The `deepcopy` routine is used by the `Node` class of django.utils.tree; thus, the protocol routine needs to be implemented to return correct copies (clones) of these GEOS objects, which use C pointers.
Equivalence testing, a Geometry may be compared with another Geometry or a WKT representation.
Returns a boolean indicating whether the set of points in this Geometry are empty.
Return True if the DE-9IM Intersection Matrix for the two geometries is T*****FF*, *T****FF*, ***T**FF*, or ****T*FF*. If either geometry is empty, return False.
Returns true if the DE-9IM intersection matrix for the two Geometries is T*T****** (for a point and a curve,a point and an area or a line and an area) 0******** (for two curves).
Returns true if the DE-9IM intersection matrix for the two Geometries is FF*FF****.
Returns true if the DE-9IM intersection matrix for the two Geometries is T*F**FFF*.
Returns true if the two Geometries are exactly equal, up to a specified tolerance.
Returns true if the DE-9IM intersection matrix for the two Geometries is T*T***T** (for two points or two surfaces) 1*T***T** (for two curves).
Returns true if the elements in the DE-9IM intersection matrix for the two Geometries match the elements in pattern.
Returns true if the DE-9IM intersection matrix for the two Geometries is FT*******, F**T***** or F***T****.
Returns the EWKT (SRID + WKT) of the Geometry.
Returns the EWKB of this Geometry in hexadecimal form.  This is an extension of the WKB specification that includes SRID value that are a part of this geometry.
Returns GeoJSON representation of this Geometry.
Returns the WKB (Well-Known Binary) representation of this Geometry as a Python buffer.  SRID and Z values are not included, use the `ewkb` property instead.
Return the EWKB representation of this Geometry as a Python buffer. This is an extension of the WKB specification that includes any SRID value that are a part of this geometry.
Returns a geometry that represents all points whose distance from this Geometry is less than or equal to distance. Calculations are in the Spatial Reference System of this Geometry. The optional third parameter sets the number of segment used to approximate a quarter circle (defaults to 8). (Text from PostGIS documentation at ch. 6.1.3)
The centroid is equal to the centroid of the set of component Geometries of highest dimension (since the lower-dimension geometries contribute zero "weight" to the centroid).
Returns the smallest convex Polygon that contains all the points in the Geometry.
Returns a Geometry representing the points making up this Geometry that do not make up other.
Returns the Geometry, simplified using the Douglas-Peucker algorithm to the specified tolerance (higher tolerance => less points).  If no tolerance provided, defaults to 0. By default, this function does not preserve topology - e.g. polygons can be split, collapse to lines or disappear holes can be created or disappear, and lines can cross. By specifying preserve_topology=True, the result will have the same dimension and number of components as the input. This is significantly slower.
Returns a set combining the points in this Geometry not in other, and the points in other not in this Geometry.
Returns the distance between the closest points on this Geometry and the other. Units will be in those of the coordinate system of the Geometry.
Returns the extent of this geometry as a 4-tuple, consisting of (xmin, ymin, xmax, ymax).
Returns the length of this Geometry (e.g., 0 for point, or the circumference of a Polygon).
Return the line merge of this Geometry.
In GeoRSS coordinate pairs are ordered by lat/lon and separated by a single white space.  Given a tuple of coordinates, this will return a unicode GeoRSS representation.
Adds a GeoRSS point with the given coords using the given handler. Handles the differences between simple GeoRSS and the more popular W3C Geo specification.
This is a subclass of the `Feed` from `django.contrib.syndication`. This allows users to define a `geometry(obj)` and/or `item_geometry(item)` methods on their own subclasses so that geo-referenced information may placed in the feed.
Call the flush method on the Band's parent raster and force a refresh of the statistics attribute when requested the next time.
Returns the description string of the band.
Width (X axis) in pixels of the band.
Height (Y axis) in pixels of the band.
Returns the total number of pixels in this band.
Return the minimum pixel value for this band.
Return the maximum pixel value for this band.
Return the mean of all pixel values of this band.
Return the standard deviation of all pixel values of this band.
Sets the nodata value for this band.
Returns the GDAL Pixel Datatype for this band.
Short-hand representation because WKB may be very large.
Returns the name of this raster. Corresponds to filename for file-based rasters.
Returns the GDAL Driver used for this raster.
Width (X axis) in pixels.
Height (Y axis) in pixels.
Returns the SpatialReference used in this GDALRaster.
Sets the spatial reference used in this GDALRaster. The input can be a SpatialReference or any parameter accepted by the SpatialReference constructor.
Shortcut to access the srid of this GDALRaster.
Shortcut to set this GDALRaster's srs from an srid.
Coordinates of the raster origin.
Pixel scale in units of the raster projection.
Skew of pixels (rotation parameters).
The attribute value for the given target node (e.g. 'PROJCS'). The index keyword specifies an index of the child node to return.
This method inspects the WKT of this SpatialReference, and will add EPSG authority nodes where an EPSG identifier is applicable.
Returns a tuple of the ellipsoid parameters: (semimajor axis, semiminor axis, and inverse flattening)
Returns True if this SpatialReference is geographic (root node is GEOGCS).
Returns the number of GDAL/OGR data source drivers registered.
GDAL uses OFTReals to represent OFTIntegers in created shapefiles -- forcing the type here since the underlying field type may actually be OFTReal.
Similar functionality to `check_string`, but does not free the pointer.
The error code is returned in the last argument, by reference. Check its value with `check_err` before returning the result.
Check the error code returned (c_int).
Creates a function prototype for the OSR routines that take the OSRSpatialReference object and
Generates a ctypes prototype for the given function with the given C arguments that returns a pointer to an OGR Spatial Reference System.
Does an equivalence test on the OGR type with the given other OGRGeomType, the short-hand string, or the integer.
Returns True if the envelopes are equivalent; can compare against other Envelopes and 4-tuples.
Returns a list of string names corresponding to each of the Fields available in this Layer.
Returns a list of the types of fields in this Layer.  For example, the list [OFTInteger, OFTReal, OFTString] would be returned for an OGR layer that had an integer, a floating-point, and string fields.
Returns a list containing the given field name for every Feature in the Layer.
Returns a list containing the OGRGeometry for every Feature in the Layer.
Returns the GeoJSON representation of this Geometry.
Returns the smallest convex Polygon that contains all the points in this Geometry.
Returns a new geometry consisting of the region which is the difference of this geometry and the other.
Returns a new geometry consisting of the region of intersection of this geometry and the other.
Returns a new geometry which is the symmetric difference of this geometry and the other.
Internal routine that returns a sequence (list) corresponding with the given function.
Initializes Feature from a pointer and its Layer object.
Gets the Field object at the specified index, which may be either an integer or the Field's string label.  Note that the Field object is not the field's _value_ -- use the `get` method instead to retrieve the value (e.g. an integer) instead of a Field instance.
Returns the value of the field, instead of an instance of the Field object.  May take a string of the field name or a Field object as parameters.
Callable with the same interface as the storage classes. This isn't just default_storage = import_string(settings.MESSAGE_STORAGE) to avoid accessing the settings at the module level.
Either sets the cookie with the encoded data if there is any data to store, or deletes the cookie.
Creates an HMAC/SHA1 hash based on the value and the project setting's SECRET_KEY, modified to make it unique for the present purpose.
Returns an encoded version of the messages list which can be stored as plain text. Since the data will be retrieved from the client-side, the encoded data also contains a hash to ensure that the data was not tampered with.
Prepares the message for serialization by forcing the ``message`` and ``extra_tags`` to unicode in case they are lazy translations. Known "safe" types (None, int, etc.) are not converted (see Django's ``force_text`` implementation for details).
Returns a list of loaded messages, retrieving them first if they have not been loaded yet.
Retrieves a list of stored messages. Returns a tuple of the messages and a flag indicating whether or not all the messages originally intended to be stored in this storage were, in fact, stored and retrieved; e.g., ``(messages, all_retrieved)``. **This method must be implemented by a subclass.** If it is possible to tell if the backend was not used (as opposed to just containing no messages) then ``None`` should be returned in place of ``messages``.
Stores a list of messages, returning a list of any messages which could not be stored. One type of object must be able to be stored, ``Message``. **This method must be implemented by a subclass.**
Prepares a list of messages for storage.
Stores all unread messages. If the backend has yet to be iterated, previously stored messages will be stored again. Otherwise, only messages added after the last iteration will be stored.
Returns the minimum recorded level. The default level is the ``MESSAGE_LEVEL`` setting. If this is not found, the ``INFO`` level is used.
Sets a custom minimum recorded level. If set to ``None``, the default level will be used (see the ``_get_level`` method).
Retrieves a list of messages from the request's session.  This storage always stores everything it is given, so return True for the all_retrieved flag.
Stores a list of messages to the request's session.
Adds a success message on successful form submission.
Attempts to add a message to the request using the 'messages' app.
Returns the message storage on the request if it exists, otherwise returns an empty list.
Returns the minimum level of messages to be recorded. The default level is the ``MESSAGE_LEVEL`` setting. If this is not found, the ``INFO`` level is used.
Sets the minimum level of messages to be recorded, returning ``True`` if the level was recorded successfully. If set to ``None``, the default level will be used (see the ``get_level`` method).
Adds a message with the ``DEBUG`` level.
Adds a message with the ``INFO`` level.
Adds a message with the ``SUCCESS`` level.
Adds a message with the ``WARNING`` level.
If self.change_message is a JSON structure, interpret it as a change string, properly translated.
The admin's JavaScript should be compatible with CSP.
Helper function that blocks the execution of the tests until the specified callback returns a value that is not falsy. This function can be called, for example, after clicking a link or submitting a form. See the other public methods that call this function for more details.
Block until `num_windows` are present (usually 2, but can be overridden in the case of pop-ups opening other pop-ups).
Helper function that blocks until a CSS selector is found on the page.
Helper function that blocks until the text is found in the CSS selector.
Helper function that blocks until the value is found in the CSS selector.
Block until the element described by the CSS selector is visible.
Block until the element described by the CSS selector is invisible.
Helper function to log into the admin.
Helper function that returns the value for the CSS attribute of an DOM element specified by the given selector. Uses the jQuery that ships with Django.
Returns the <OPTION> with the value `value` inside the <SELECT> widget identified by the CSS selector `selector`.
Asserts that the <SELECT> widget identified by `selector` has the options with the given `values`.
Asserts that the <SELECT> widget identified by `selector` has the selected options with the given `values`.
With no file paths given this script will automaticallycompress all jQuery-based files of the admin app. Requires the Google Closure Compiler library and Java version 6 or later.
Simple AppConfig which does not do automatic discovery.
The default AppConfig for admin which does autodiscovery.
Returns True if some choices would be output for this filter.
Returns choices ready to be output in the template. `changelist` is the ChangeList to be displayed.
Returns the filtered queryset.
Returns the list of parameter names that are expected from the request's query string and that will be used by this filter.
Returns the value (in string format) provided in the request's query string for this filter, if any. If the value wasn't provided then returns None.
Must be overridden to return a list of tuples (value, verbose value)
Return True if a "(None)" choice should be included, which filters out everything except empty relationships.
A custom authentication form used in the admin app.
A wrapper around an inline formset for use in the admin system.
Stores all errors for the form/formsets in an add/change stage view.
Creates a list of prepopulated_fields that should render Javascript for the prepopulated fields for both the admin form and inlines.
Displays the row of buttons for delete and save.
Generates an individual page index link in a paginated list.
Coerce a field_name (which may be a callable) to a string.
Displays the headers and data list together
Displays a search form for searching the list.
Unregisters the given model(s). If a model isn't already registered, this will raise NotRegistered.
Check if a model class is registered with this `AdminSite`.
Register an action to be available globally.
Disable a globally-registered action. Raises KeyError for invalid names.
Explicitly get a registered global action whether it's enabled or not. Raises KeyError for invalid names.
Get all the enabled actions as an iterable of (name, func).
Returns True if the given HttpRequest has permission to view *at least one* page in the admin site.
Returns a dictionary of variables to put in the template context for *every* page in the admin site. For sites running on a subpath, use the SCRIPT_NAME value if site_url hasn't been customized.
Handles the "change password" task -- both form display and validation.
Displays the "success" page after a password change.
Displays the i18n JavaScript that the Django admin requires.
Displays the main admin index page, which lists all of the installed apps that have been registered in this site.
A SelectMultiple with a JavaScript filter interface. Note that the resulting JavaScript assumes that the jsi18n catalog has been loaded in the page
Outputs a <ul> for this set of radio fields.
Converts the type of lookups specified in a ForeignKey limit_choices_to attribute to a dictionary of query parameters
Registers the given model(s) classes and wrapped ModelAdmin class with admin site: @register(Author) class AuthorAdmin(admin.ModelAdmin): pass A kwarg of `site` can be passed as the admin site, otherwise the default admin site will be used.
If the ModelAdmin specifies ordering, the queryset should respect that ordering.  Otherwise don't specify the queryset, let the field decide (returns None in that case).
Get a form Field for a ForeignKey.
Return the empty_value_display set on ModelAdmin or AdminSite.
Hook for specifying fields.
Hook for specifying fieldsets.
Hook for specifying custom readonly fields.
Hook for specifying custom prepopulated fields.
Returns True if the given request has permission to add an object. Can be overridden by the user in subclasses.
Returns True if the given request has permission to change the given Django model instance, the default implementation doesn't examine the `obj` parameter. Can be overridden by the user in subclasses. In such case it should return True if the given request has permission to change the `obj` model instance. If `obj` is None, this should return True if the given request has permission to change *any* object of the given type.
Returns True if the given request has permission to change the given Django model instance, the default implementation doesn't examine the `obj` parameter. Can be overridden by the user in subclasses. In such case it should return True if the given request has permission to delete the `obj` model instance. If `obj` is None, this should return True if the given request has permission to delete *any* object of the given type.
Returns True if the given request has any permission in the given app label. Can be overridden by the user in subclasses. In such case it should return True if the given request has permission to view the module on the admin index page and access the module's index page. Overriding it does not restrict access to the add, change or delete views. Use `ModelAdmin.has_(add|change|delete)_permission` for that.
Returns a dict of all perms for this model. This dict has the keys ``add``, ``change``, and ``delete`` mapping to the True/False for each of those actions.
Returns the ChangeList class for use on the changelist page.
Returns an instance matching the field and value provided, the primary key is used if no field is provided. Returns ``None`` if no match is found or the object_id fails validation.
Returns a Form class for use in the Formset on the changelist page.
Returns a FormSet class for use on the changelist page if list_editable is used.
Yields formsets and the corresponding inlines.
Log that an object has been successfully added. The default implementation creates an admin LogEntry object.
Log that an object has been successfully changed. The default implementation creates an admin LogEntry object.
Log that an object will be deleted. Note that this method must be called before the deletion. The default implementation creates an admin LogEntry object.
A list_display column containing a checkbox widget.
Return a list of choices for use in a form object.  Each choice is a tuple (name, description).
Return a sequence containing the fields to be displayed on the changelist.
Returns a sequence containing the fields to be displayed as filters in the right sidebar of the changelist page.
Returns a list of fields to add to the select_related() part of the changelist items query.
Returns a sequence containing the fields to be searched whenever somebody submits a search query.
Returns the preserved filters querystring.
Construct a JSON structure describing changes from a changed object. Translations are deactivated so that strings are stored untranslated. Translation happens later on LogEntry access.
Given a ModelForm return an unsaved instance. ``change`` is True if the object is being changed, and False if it's being added.
Given a model instance save it to the database.
Given a model instance delete it from the database.
Given an inline formset save it to the database.
Given the ``HttpRequest``, the parent ``ModelForm`` instance, the list of inline formsets and a boolean value based on whether the parent is being added or changed, save the related objects to the database. Note that at this point save_form() and save_model() have already been called.
Figure out where to redirect after the 'Save' button has been pressed when adding a new object.
Figure out where to redirect after the 'Save' button has been pressed when editing an existing object.
Determines the HttpResponse for the delete_view stage.
Hook for customizing the number of extra inline forms.
Hook for customizing the min number of inline forms.
Hook for customizing the max number of extra inline forms.
if c in :/_#?;@&=+$,"[]<>%\n\\:
Undo the effects of quote(). Based heavily on urllib.unquote().
flat = [] for field in fields: if isinstance(field, (list, tuple)): flat.extend(field) else: flat.append(field) return flat def flatten_fieldsets(fieldsets): Returns a list of field names from an admin fieldsets structure.
Return the graph as a nested list.
We always want to load the objects into memory so that we can display them to the user in confirm page.
Return a `dict` with keys 'verbose_name' and 'verbose_name_plural', typically for use with string formatting. `obj` may be a `Model` instance, `Model` subclass, or `QuerySet` instance.
Return the appropriate `verbose_name` or `verbose_name_plural` value for `obj` depending on the count `n`. `obj` may be a `Model` instance, `Model` subclass, or `QuerySet` instance. If `obj` is a `QuerySet` instance, `n` is optional and the length of the `QuerySet` is used.
Return list of Fields given path relative to model.e.g. (ModelX, "user__groups__name") -> [ <django.db.models.fields.related.ForeignKey object at 0x...>, <django.db.models.fields.related.ManyToManyField object at 0x...>, <django.db.models.fields.CharField object at 0x...>, ]
Check that form subclasses BaseModelForm.
Check that filter_vertical is a sequence of field names.
Check that filter_horizontal is a sequence of field names.
try: field = model._meta.get_field(field_name) except FieldDoesNotExist: return refer_to_missing_field(field=field_name, option=label, model=model, obj=obj, id='admin.E019') else: if not field.many_to_many: return must_be('a many-to-many field', option=label, obj=obj, id='admin.E020') else: return [] def _check_radio_fields(self, obj):  Check that `radio_fields` is a dictionary.
try: field = model._meta.get_field(field_name) except FieldDoesNotExist: return refer_to_missing_field(field=field_name, option=label, model=model, obj=obj, id='admin.E022') else: if not (isinstance(field, models.ForeignKey) or field.choices): return [ checks.Error( "The value of '%s' refers to '%s', which is not an " "instance of ForeignKey, and does not have a 'choices' definition." % ( label, field_name ), obj=obj.__class__, id='admin.E023', ) ] else: return [] def _check_radio_fields_value(self, obj, val, label):  Check type of a value of `radio_fields` dictionary.
Check that readonly_fields refers to proper attribute or field.
Check save_as is a boolean.
Check save_on_top is a boolean.
Check all inline model admin classes.
Check one inline model admin.
Check that list_select_related is a boolean, a list or a tuple.
Check that list_per_page is an integer.
Check that list_max_show_all is an integer.
Check search_fields is a sequence.
Check that date_hierarchy refers to DateField or DateTimeField.
Check that extra is an integer.
Check that max_num is an integer.
Check that min_num is an integer.
Check formset is a subclass of BaseModelFormSet.
Return a somewhat-helpful data type given a function name
Returns the description for a given field type, if it exists,Fields' descriptions can contain format strings, which will be interpolated against the values of field.__dict__ before being output.
Return a list of views from a list of urlpatterns. Each object in the returned list is a two-tuple: (view_func, regex)
Parse out the parts of a docstring.  Return (title, body, metadata).
Return the given session dictionary serialized and encoded as a string.
Django provides full support for anonymous sessions. The session framework lets you store and retrieve arbitrary data on a per-site-visitor basis. It stores data on the server side and abstracts the sending and receiving of cookies. Cookies contain a session ID -- not the data itself. The Django sessions framework is entirely cookie-based. It does not fall back to putting session IDs in URLs. This is an intentional design decision. Not only does that behavior make URLs ugly, it makes your site vulnerable to session-ID theft via the "Referer" header. For complete documentation on using Sessions in your code, consult the sessions documentation that is shipped with Django (also available on the Django Web site).
To create a new key, we simply make sure that the modified flag is set so that the cookie is set on the client for the current request.
To save, we get the session key as a securely signed string and then set the modified flag so that the cookie is set on the client for the current request.
This method makes sense when you're talking to a shared resource, but it doesn't matter when you're storing the information in the client's cookie.
To delete, we clear the session key and the underlying data structure and set the modified flag so that the cookie is set on the client for the current request.
Keeps the same data but with a new key.  To do this, we just have to call ``save()`` and it will automatically save a cookie with a new key at the end of the request.
Most session backends don't need to override this method, but we do, because instead of generating a random string, we want to actually generate a secure url-safe Base64-encoded string of data as our session key.
Return a new instance of the session model object, which represents the current session state. Intended to be used for saving the session data to the database.
Saves the current session data to the database. If 'must_create' is True, a database error will be raised if the saving operation doesn't create a *new* entry (as opposed to possibly updating an existing entry).
Return the modification time of the file storing the session's content.
Return the expiry time of the file storing the session's content.
Used internally as a consistent exception type to catch from save (see the docstring for SessionBase.save() for details).
Occurs if Django tries to update a session that was deleted.
Key must be truthy and at least 8 characters long. 8 characters is an arbitrary lower bound for some minimal key security.
Validate session key on assignment. Invalid values will set to None.
Lazily loads session from storage (unless "no_load" is True, when only an empty dict is stored) and stores it in the current instance.
Returns ``True`` if the session is set to expire when the browser closes, and ``False`` if there's an expiry date. Use ``get_expiry_date()`` or ``get_expiry_age()`` to find the actual expiry date/age, if there is one.
Removes the current session data from the database and regenerates the key.
Returns True if the given session_key already exists.
Creates a new session instance. Guaranteed to create a new object with a unique key and will have saved the result once (with empty data) before the method returns.
Saves the session data. If 'must_create' is True, a new session object is created (otherwise a CreateError exception is raised). Otherwise, save() only updates an existing object and does not create one (an UpdateError is raised).
Deletes the session data under this key. If the key is None, the current session key value is used.
Loads the session data and returns a dictionary.
Simple wrapper around pickle to be used in signing.dumps and signing.loads.
Use the i18n enabled defaultfilters.floatformat if possible
A validator designed for HStore to require/restrict keys.
A small dict-like wrapper for a given cache backend instance.
A static file system storage backend which also saves hashed copies of the files it saves.
A static file system storage backend which also saves hashed copies of the files it saves.
Set instance variables based on an options dict
Small log helper
Returns the static files serving handler wrapping the default handler, if static files should be served. Otherwise just returns the default handler.
Checks if the path should be handled. Ignores the path if: * the host is provided as part of the base_url * the request's path isn't under the media path (or equal)
Returns the relative path to the media file on disk for the given URL.
Actually serves the request path.
Return True or False depending on whether the ``path`` should be ignored (if it matches any pattern in ``ignore_patterns``).
Recursively walk the storage directories yielding the paths of all files that should be copied.
Given a relative file path this ought to find an absolute file path. If the ``all`` parameter is ``False`` (default) only the first found file path will be returned; if set to ``True`` a list of all found files paths is returned.
Given an optional list of paths to ignore, this should return a two item iterable consisting of the relative path and storage instance.
Looks for files in the extra locations as defined in ``STATICFILES_DIRS``.
Finds a requested static file in a location, returning the found absolute path (or ``None`` if no match).
List all files in all locations.
Looks for files in the app directories.
Looks for files in the default file storage, if it's local.
List all files of the storage.
A static files finder that uses the default storage backend.
Validates that the given value contains no whitespaces to prevent common typos.
Returns the current Site based on the SITE_ID in the project's settings. If SITE_ID isn't defined, it returns the site with domain matching request.get_host(). The ``Site`` object is cached the first time it's retrieved from the database.
Clears the ``Site`` object cache.
A class that shares the primary interface of Site (i.e., it has ``domain`` and ``name`` attributes) but gets its data from a Django HttpRequest object rather than from a database. The save() and delete() methods raise NotImplementedError.
Return self.__field_name or 'site' or 'sites'.
Middleware that sets `site` attribute to request object.
Clear out the content-type cache. This needs to happen during database flushes to prevent caching of "stale" content type IDs (see django.contrib.contenttypes.management.update_contenttypes for where this gets called).
Returns an object of this type for the keyword arguments given. Basically, this is a proxy around this object_type's get_object() model method. The ObjectNotExist exception, if thrown, will not be caught, so code that calls this method should catch it.
Returns all objects of this type for the keyword arguments given.
A formset for generic inline objects to a parent.
See corresponding method on Field
See corresponding method on RelatedField
Check if field named `field_name` in model `model` exists and is a valid content_type field (is a ForeignKey to ContentType).
Used by GenericRelation to store information about the relation.
Return True if field is a GenericForeignKey whose content type and object id fields correspond to the equivalent attributes on this GenericRelation.
Return the content type associated with this field's model.
Return all objects related to ``objs`` via this ``GenericRelation``.
Accessor to the related objects manager on the one-to-many relation created by GenericRelation. In the example:: class Post(Model): comments = GenericRelation(Comment) ``post.comments`` is a ReverseGenericManyToOneDescriptor instance.
Filter the queryset for the instance this manager is bound to.
Public interface to the flat page view. Models: `flatpages.flatpages` Templates: Uses the template defined by the ``template_name`` field, or :template:`flatpages/default.html` if template_name is not defined. Context: flatpage `flatpages.flatpages` object
Returns a token that can be used once to do a password reset for the given user.
A signal receiver which updates the last_login date for the user logging in.
The permissions system provides a way to assign permissions to specific users and groups of users. The permission system is used by the Django admin site, but may also be useful in your own code. The Django admin site uses permissions as follows: - The "add" permission limits the user's ability to view the "add" form and add an object. - The "change" permission limits a user's ability to view the change list, view the "change" form and change an object. - The "delete" permission limits the ability to delete an object. Permissions are set globally per type of object, not per specific object instance. It is possible to say "Mary may change news stories," but it's not currently possible to say "Mary may change news stories, but only the ones she created herself" or "Mary may only change news stories that have a certain status or publication date." Three basic permissions -- add, change and delete -- are automatically created for each Django model.
The manager for the auth's Group model.
Groups are a generic way of categorizing users to apply permissions, or some other label, to those users. A user can belong to any number of groups. A user in a group automatically has all the permissions granted to that group. For example, if the group Site editors has the permission can_edit_home_page, any user in that group will have that permission. Beyond permissions, groups are a convenient way to categorize users to apply some label, or extended functionality, to them. For example, you could create a group 'Special users', and you could write code that would do special things to those users -- such as giving them access to a members-only portion of your site, or sending them members-only email messages.
Creates and saves a User with the given username, email and password.
A backend can raise `PermissionDenied` to short-circuit permission checking.
A backend can raise `PermissionDenied` to short-circuit permission checking.
Returns a list of permission strings that this user has through their groups. This method queries all available auth backends. If an object is passed in, only permissions matching this object are returned.
Returns True if the user has each of the specified permissions. If object is passed, it checks if the user has all required perms for this object.
Returns the first_name plus the last_name, with a space in between.
Sends an email to this User.
Users within the Django authentication system are represented by this model. Username, password and email are required. Other fields are optional.
Validate whether the password meets all validator requirements. If the password is valid, return ``None``. If the password is invalid, raise ValidationError with all error messages.
Inform all validators that have implemented a password_changed() method that the password has been changed.
Return a list of all help texts of all configured validators.
Return an HTML string with all help texts of all configured validators in an <ul>.
Validate whether the password is of a minimum length.
Validate whether the password is sufficiently different from the user's attributes. If no specific attributes are provided, look at a sensible list of defaults. Attributes that don't exist are ignored. Comparison is made to not only the full attribute value, but also its components, so that, for example, a password is validated against either part of an email address, as well as the full address.
Validate whether the password is a common password. The password is rejected if it occurs in a provided list, which may be gzipped. The list Django ships with contains 1000 common passwords, created by Mark Burnett: https://xato.net/passwords/more-top-worst-passwords/
Validate whether the password is alphanumeric.
Override this method to override the login_url attribute.
Override this method to override the permission_denied_message attribute.
Override this method to override the redirect_field_name attribute.
CBV mixin which verifies that the current user is authenticated.
Override this method to override the permission_required attribute. Must return an iterable.
Override this method to customize the way permissions are checked.
Override this method to use a different test_func method.
Cleans a dictionary of credentials of potentially sensitive info before sending to less secure functions. Not comprehensive - intended for user_login_failed signal
Returns the User model that is active in this project.
Returns the codename of the permission for the specified action.
Updating a user's password logs out all sessions for the user. This function takes the current request and the updated user object from which the new session hash will be derived and updates the session hash appropriately to prevent a password change from logging out the session from which the password was changed.
Normalize the email address by lowercasing the domain part of it.
Generate a random password with the given length and given allowed_chars. The default value of allowed_chars does not have "I" or "O" or letters and digits that look similar -- just to avoid confusion.
Always return False. This is a way of comparing User objects to anonymous users.
Always return True. This is a way to tell if the user has been authenticated in templates.
Handle deprecation of the current_app parameter of the views.
Displays the login form and handles the login action.
Logs out the user if they are logged in. Then redirects to the log-in page.
Use special form during user creation
A form that creates a user, with no privileges, from the given username and password.
Controls whether the given User may log in. This is a policy setting, independent of end-user authentication. This default behavior is to allow login by active users, and reject login by inactive users. If the given user cannot log in, this method should raise a ``forms.ValidationError``. If the given user may log in, this method should return None.
Given an email, return matching user(s) who should receive a reset.This allows subclasses to more easily customize the default policies that prevent inactive users and users with unusable passwords from resetting their password.
Generates a one-use only link for resetting password and sends to the user.
A form that lets a user change set their password without entering the old password
Validates that the old_password field is correct.
Saves the new password.
Override this method if you want to customize data inputs or validation exceptions.
Returns (codename, name) for all permissions in the given opts.
Returns (codename, name) for all autogenerated permissions. By default, this is ('add', 'change', 'delete')
Reject users with is_active=False. Custom user models that don't have that attribute are allowed.
Returns the permissions of `user_obj` from `from_name`. `from_name` can be either "group" or "user" to return permissions from `_get_group_permissions` or `_get_user_permissions` respectively.
Returns a set of permission strings the user `user_obj` has from their `user_permissions`.
Returns a set of permission strings the user `user_obj` has from the groups they belong.
Returns True if user_obj has any permissions in the given app_label.
Performs any cleaning on the "username" prior to using it to get or create the user object.  Returns the cleaned username. By default, returns the username unchanged.
Configures a user after creation and returns the updated user. By default, returns the user unmodified.
Authorizes a user based on groups
Turn a plain-text password into a hash for database storage Same as encode() but generates a new random salt. If password is None then a concatenation of UNUSABLE_PASSWORD_PREFIX and a random string will be returned which disallows logins. Additional random string reduces chances of gaining access to staff or superuser accounts. See ticket 20079 for more info.
Returns an instance of a loaded password hasher. If algorithm is 'default', the default hasher will be returned. This function will also lazy import hashers specified in your settings file if needed.
Returns the given hash, with only the first ``show`` number shown. The rest are masked with ``char`` for security reasons.
Generates a cryptographically secure nonce salt in ASCII
Checks if the given password is correct
Creates an encoded database value The result is normally formatted as "algorithm$salt$hash" and must be fewer than 128 characters.
Returns a summary of safe values The result is a dictionary and will be used where the password field must be displayed to construct a safe representation of the password.
Bridge the runtime gap between the work factor supplied in `encoded` and the work factor suggested by this hasher. Taking PBKDF2 as an example, if `encoded` contains 20000 iterations and `self.iterations` is 30000, this method should run password through another 10000 iterations of PBKDF2. Similar approaches should exist for any hasher that has a work factor. If not, this method should be defined as a no-op to silence the warning.
Secure password hashing using the PBKDF2 algorithm (recommended) Configured to use PBKDF2 + HMAC + SHA256. The result is a 64 byte binary string.  Iterations may be changed safely but you must rename the algorithm if you change SHA256.
Alternate PBKDF2 hasher which uses SHA1, the default PRF recommended by PKCS 5. This is compatible with other implementations of PBKDF2, such as openssl's PKCS5_PBKDF2_HMAC_SHA1().
Secure password hashing using the bcrypt algorithm This is considered by many to be the most secure algorithm but you must first install the bcrypt library.  Please be warned that this library depends on native C code and might cause portability issues. This hasher does not first hash the password which means it is subject to the 72 character bcrypt password truncation, most use cases should prefer the BCryptSHA256PasswordHasher. See: https://code.djangoproject.com/ticket/20138
The SHA1 password hashing algorithm (not recommended)
The Salted MD5 password hashing algorithm (not recommended)
Very insecure algorithm that you should *never* use; stores SHA1 hashes with an empty salt. This class is implemented because Django used to accept such password hashes. Some older Django installs still have these values lingering around so we need to handle and upgrade them properly.
Incredibly insecure algorithm that you should *never* use; stores unsalted MD5 hashes without the algorithm prefix, also accepts MD5 hashes with an empty salt. This class is implemented because Django used to store passwords this way and to accept such password hashes. Some older Django installs still have these values lingering around so we need to handle and upgrade them properly.
Decorator for views that checks that the user is logged in, redirecting to the log-in page if necessary.
Formerly, a middleware for invalidating a user's sessions that don't correspond to the user's current session authentication hash. However, it caused the "Vary: Cookie" header on all responses. It's now a shim to allow a single settings file to more easily support multiple versions of Django. Will be RemovedInDjango20Warning.
Adds the language code prefix to every URL pattern within this function. This may only be used in the root URLconf, not in an included URLconf.
Return a tuple of two booleans: ( `True` if LocaleRegexURLResolver` is used in the `urlconf`, `True` if the default language should be prefixed )
Load the settings module pointed to by the environment variable. This is used the first time we need any settings at all, if the user has not previously configured the settings manually.
Called to manually configure the settings. The 'default_settings' parameter sets where to retrieve any unspecified values from (its argument must support attribute access (__getattr__)).
Returns True if the settings have already been configured.
Common logic for settings whether set by a module or by the user.
Requests for configuration variables not in this class are satisfied from the module specified in default_settings (if possible).
Decorator that adds headers to a response so that it will never be cached.
Modifies a view function so its response has the X-Frame-Options HTTP header set to 'DENY' as long as the response doesn't already have that header set. e.g. @xframe_options_deny def some_view(request): ...
Modifies a view function so its response has the X-Frame-Options HTTP header set to 'SAMEORIGIN' as long as the response doesn't already have that header set. e.g. @xframe_options_sameorigin def some_view(request): ...
Modifies a view function by setting a response variable that instructs XFrameOptionsMiddleware to NOT set the X-Frame-Options HTTP header. e.g. @xframe_options_exempt def some_view(request): ...
A view decorator that adds the specified headers to the Vary header of the response. Usage: @vary_on_headers('Cookie', 'Accept-language') def index(request): ... Note that the header names are not case-sensitive.
A view decorator that adds "Cookie" to the Vary header of a response. This indicates that a page's contents depends on cookies. Usage: @vary_on_cookie def index(request): ...
Indicates which variables used in the decorated function are sensitive, so that those variables can later be treated in a special way, for example by hiding them when logging unhandled exceptions. Two forms are accepted: * with specified variable names: @sensitive_variables('user', 'password', 'credit_card') def my_function(user): password = user.pass_word credit_card = user.credit_card_number ... * without any specified variable names, in which case it is assumed that all variables are considered sensitive: @sensitive_variables() def my_function() ...
Indicates which POST parameters used in the decorated view are sensitive, so that those parameters can later be treated in a special way, for example by hiding them when logging unhandled exceptions. Two forms are accepted: * with specified parameters: @sensitive_post_parameters('password', 'credit_card') def my_view(request): pw = request.POST['password'] cc = request.POST['credit_card'] ... * without any specified parameters, in which case it is assumed that all parameters are considered sensitive: @sensitive_post_parameters() def my_view(request) ...
Return the list of items for this view. The return value must be an iterable and may be an instance of `QuerySet` in which case `QuerySet` specific behavior will be enabled.
Return the field or fields to use for ordering the queryset.
Paginate the queryset, if needed.
Get the number of items to paginate by, or ``None`` for no pagination.
Return an instance of the paginator for this view.
Returns the maximum number of orphans extend the last page by when paginating.
Returns ``True`` if the view should display empty lists, and ``False`` if a 404 should be raised instead.
Get the name of the item to be used in the context.
Get the context for this view.
Returns the initial data to use for forms on this view.
Returns the prefix to use for forms on this view
Returns the form class to use in this view
Returns an instance of the form to be used in this view.
Returns the keyword arguments for instantiating the form.
If the form is valid, redirect to the supplied URL.
If the form is invalid, re-render the context data with the data-filled form and errors.
Insert the form into the context dict.
Returns the keyword arguments for instantiating the form.
Returns the supplied URL.
If the form is valid, save the associated model.
Handles GET requests and instantiates a blank version of the form.
Base view for creating an new object instance. Using this base class requires subclassing to provide a response mixin.
View for creating a new object instance, with a response rendered by template.
Base view for updating an existing object. Using this base class requires subclassing to provide a response mixin.
View for updating an object, with a response rendered by template.
Get a year format string in strptime syntax to be used to parse the year from url variables.
Return the year for which this view should display data.
Get the next valid year.
Get the previous valid year.
Return the start date of the next interval. The interval is defined by start date <= item date < next start date.
Return the start date of the current interval.
Get a month format string in strptime syntax to be used to parse the month from url variables.
Return the month for which this view should display data.
Get the next valid month.
Get the previous valid month.
Return the start date of the next interval. The interval is defined by start date <= item date < next start date.
Return the start date of the previous interval.
Get a day format string in strptime syntax to be used to parse the day from url variables.
Return the day for which this view should display data.
Get the next valid day.
Get the previous valid day.
Return the start date of the next interval. The interval is defined by start date <= item date < next start date.
Return the start date of the current interval.
Get a week format string in strptime syntax to be used to parse the week from url variables.
Return the week for which this view should display data
Get the next valid week.
Get the previous valid week.
Return the start date of the next interval. The interval is defined by start date <= item date < next start date.
Return the start date of the current interval.
Get the name of the date field to be used to filter by.
Return `True` if the date field is a `DateTimeField` and `False` if it's a `DateField`.
Convert a date into a datetime when the date field is a DateTimeField. When time zone support is enabled, `date` is assumed to be in the current time zone, so that displayed items are consistent with the URL.
Obtain the list of dates and items.
Returns the field or fields to use for ordering the queryset; uses the date field by default.
Get the aggregation period for the list of dates: 'year', 'month', or 'day'.
Get a date list by calling `queryset.dates/datetimes()`, checking along the way for empty lists that aren't allowed.
Return (date_list, items, extra_context) for this request.
Top-level archive of date-based items.
Return `True` if this view should contain the full list of objects in the given year.
List of objects published in a given year.
Return (date_list, items, extra_context) for this request.
List of objects published in a given month.
Return (date_list, items, extra_context) for this request.
List of objects published in a given week.
Return (date_list, items, extra_context) for this request.
Do the actual heavy lifting of getting the dated items; this accepts a date object so that TodayArchiveView can be trivial.
List of objects published on a given day.
Return (date_list, items, extra_context) for this request.
List of objects published today.
Detail view of a single object on a single date; this differs from the standard DetailView by accepting a year/month/day in the URL.
Helper: get a datetime.date object given a format string and a year, month, and day (only year is mandatory). Raise a 404 for an invalid date.
A default context mixin that passes the keyword arguments received by get_context_data as the template context.
Handles responding to requests for the OPTIONS HTTP verb.
Returns a response, using the `response_class` for this view, with a template rendered with the given context. If any keyword arguments are provided, they will be passed to the constructor of the response class.
Returns a list of template names to be used for the request. Must return a list. May not be called if render_to_response is overridden.
A view that renders a template.  This view will also pass into the context any keyword arguments passed by the URLconf.
Return the URL redirect to. Keyword arguments from the URL pattern match generating the redirect request are provided as kwargs to this method.
Return the `QuerySet` that will be used to look up the object. Note that this method is called by the default implementation of `get_object` and may not be called if `get_object` is overridden.
Get the name of a slug field to be used to look up by slug.
Get the name to use for the object.
Insert the single object into the context dict.
A base view for displaying a single object
Returns "identity" versions of the JavaScript i18n functions -- i.e., versions that don't actually do anything.
Returns the selected language catalog as a javascript library. Receives the list of packages to check for translations in the packages parameter either from an infodict or as a +-delimited string from the request. Default is 'django.conf'. Additionally you can override the gettext domain for this view, but usually you don't want to do that, as JavaScript messages go to the djangojs domain. But this might be needed if you deliver your JavaScript source from Django templates.
Object to wrap callable appearing in settings* Not to call in the debug page (21345). * Not to break the debug page if the callable forbidding to set attributes (23070).
Create a technical server error response. The last three arguments are the values returned from sys.exc_info() and friends.
Base for all exception reporter filter classes. All overridable hooks contain lenient default behaviors.
This filter is to add safety in production environments (i.e. DEBUG is False). If DEBUG is True then your site is not safe anyway. This hook is provided as a convenience to easily activate or deactivate the filter on a per request basis.
Replaces the keys in a MultiValueDict marked as sensitive with stars. This mitigates leaking sensitive POST parameters if something like request.POST['nonexistent_key'] throws an exception (21098).
Return the same data as from traceback.format_exception.
{{ frame.context_line|escape }}</pre>{% if not is_email %} <span>...</span>{% endif %}</li></ol>
Raises an exception if all apps haven't been imported yet.
Raises an exception if all models haven't been imported yet.
Imports applications and returns an iterable of app configs.
Returns a list of all installed models. By default, the following models aren't included: - auto-created models for many-to-many relations without an explicit intermediate table, - models created to satisfy deferred attribute queries, - models that have been swapped out. Set the corresponding keyword argument to True to include such models.
Returns the model matching the given app_label and model_name. As a shortcut, this function also accepts a single argument in the form <app_label>.<model_name>. model_name is case-insensitive. Raises LookupError if no application exists with this label, or no model exists with this name in the application. Raises ValueError if called with a single argument that doesn't contain exactly one dot.
Checks whether an application with this name exists in the registry. app_name is the full name of the app eg. 'django.contrib.admin'.
Look for an app config containing a given object. object_name is the dotted Python path to the object. Returns the app config for the inner application in case of nesting. Returns None if the object isn't in any registered app config.
Similar to get_model(), but doesn't require that an app exists with the given app_label. It's safe to call this method at import time, even while the registry is being populated.
Restricts the set of installed apps used by get_app_config[s]. available must be an iterable of application names. set_available_apps() must be balanced with unset_available_apps(). Primarily used for performance optimization in TransactionTestCase. This method is safe is the sense that it doesn't trigger any imports.
Cancels a previous call to set_available_apps().
Enables a different set of installed apps for get_app_config[s]. installed must be an iterable in the same format as INSTALLED_APPS. set_installed_apps() must be balanced with unset_installed_apps(), even if it exits with an exception. Primarily used as a receiver of the setting_changed signal in tests. This method may trigger new imports, which may add new models to the registry of all imported models. They will stay in the registry even after unset_installed_apps(). Since it isn't possible to replay imports safely (eg. that could lead to registering listeners twice), models are registered when they're imported and never removed.
Cancels a previous call to set_installed_apps().
Raises an exception if models haven't been imported yet.
Returns the model with the given case-insensitive model_name. Raises LookupError if no model exists with this name.
Returns an iterable of models. By default, the following models aren't included: - auto-created models for many-to-many relations without an explicit intermediate table, - models created to satisfy deferred attribute queries, - models that have been swapped out. Set the corresponding keyword argument to True to include such models. Keyword arguments aren't documented; they're a private API.
Returns the view to be used for CSRF rejections
Given a secret (assumed to be a string of CSRF_ALLOWED_CHARS), generate a token by adding a salt and using it to encrypt the secret.
Returns the CSRF token required for a POST form. The token is an alphanumeric value. A new token is created if one is not already set. A side effect of calling this function is to make the csrf_protect decorator and the CsrfViewMiddleware add a CSRF cookie and a 'Vary: Cookie' header to the outgoing response.  For this reason, you may need to use this function lazily, as is done by the csrf context processor.
Changes the CSRF token in use for a request - should be done on login for security purposes.
Gets the value to set for the X_FRAME_OPTIONS header. By default this uses the value from the X_FRAME_OPTIONS Django settings. If not found in settings, defaults to 'SAMEORIGIN'. This method can be overridden if needed, allowing it to vary based on the request or response.
Return True if settings.APPEND_SLASH is True and appending a slash to the request path turns an invalid path into a valid one.
Return the full path of the request with a trailing slash appended. Raise a RuntimeError if settings.DEBUG is True and request.method is POST, PUT, or PATCH.
Send broken link emails for relevant 404 NOT FOUND responses.
Handles conditional GET operations. If the response has an ETag or Last-Modified header, and the request has If-None-Match or If-Modified-Since, the response is replaced by an HttpNotModified. Also sets the Date and Content-Length response-headers.
Reverse lookups This demonstrates the reverse lookup features of the database API.
Regression test for 10153: foreign key __gte lookups.
Regression test for 10153: foreign key __lte lookups.
21430 -- Verifies a warning is raised for models that are unpickled without a Django version
21430 -- Verifies a warning is raised for models that are unpickled with a different Django version than the current
Regression test for 7957: Combining extra() calls should leave the corresponding parameters associated with the right extra() bit. I.e. internal dictionary must remain sorted.
Regression test for 7961: When not using a portion of an extra(...) in a query, remove any corresponding parameters from the query as well.
Regression test for 8063: limiting a query shouldn't discard any extra() bits.
Regression test for 8039: Ordering sometimes removed relevant tables from extra(). This test is the critical case: ordering uses a table, but then removes the reference because of an optimization. The table should still be present because of the extra() call.
Regression test for 8819: Fields in the extra(select=...) list should be available to extra(order_by=...).
When calling the dates() method on a queryset with extra selection columns, we can (and should) ignore those columns. They don't change the result and cause incorrect SQL to be produced otherwise.
Regression for 10847: the list of extra columns can always be accurately evaluated. Using an inner query ensures that as_sql() is producing correct output without requiring full evaluation and execution of the inner query.
Returns the paths for any external backend packages.
A series of tests for django-admin.py when using a settings.py file thatcontains the test application.
A series of tests for django-admin.py when using a settings.py file thatcontains the test application specified using a full path.
A series of tests for django-admin.py when using a settings.py file thatdoesn't contain the test application.
A series of tests for django-admin.py when using a settings filewith a name other than 'settings.py'.
A series of tests for django-admin.py when multiple settings files(including the default 'settings.py') are available. The default settings file is insufficient for performing the operations described, so the alternate settings must be used by the running script.
A series of tests for django-admin.py when the settings file is in a directory. (see 9751).
A series of tests for manage.py when using a settings.py file thatcontains the test application.
A series of tests for manage.py when using a settings.py file thatcontains the test application specified using a full path.
A series of tests for manage.py when using a settings.py file thatdoesn't contain the test application.
A series of tests for manage.py when using a settings filewith a name other than 'settings.py'.
A series of tests for manage.py when multiple settings files(including the default 'settings.py') are available. The default settings file is insufficient for performing the operations described, so the alternate settings must be used by the running script.
import error: manage.py builtin commands shows useful diagnostic info when settings with import errors is provided (14130).
manage.py builtin commands does not swallow attribute error due to bad settings (18845).
Test listing available commands output note when only core commands are available.
self.write_settings( 'settings.py', apps=[ 'admin_scripts.app_with_import', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sites', ], sdict={'DEBUG': True}, ) args = ['check'] out, err = self.run_manage(args) self.assertNoOutput(err) self.assertEqual(out, 'System check identified no issues (0 silenced).\n') def test_output_format(self):  All errors/warnings should be sorted by level and by message.
Ensure runserver.check_migrations doesn't choke on empty DATABASES.
Test that non-ASCII message of CommandError does not raise any UnicodeDecodeError in run_from_argv.
Apps listed first in INSTALLED_APPS have precedence.
Options passed after settings are correctly handled.
Short options passed after settings are correctly handled.
Options passed before settings are correctly handled.
Short options passed before settings are correctly handled.
The startproject management command is able to use a different project template from a tarball via a URL.
Make sure an exception is raised when the provided destination directory doesn't exist
The startproject management command is able to render templates with non-ASCII content.
Runs without error and emits settings diff.
Regression for #20509Test would raise an exception rather than printing an error message.
python -m django works like django-admin.
Regression tests for defer() / only() behavior.
Many-to-many relationships via an intermediary table For many-to-many relationships that need extra fields on the intermediary table, use an intermediary model. In this example, an ``Article`` can have multiple ``Reporter`` objects, and each ``Article``-``Reporter`` combination (a ``Writer``) has a ``position`` field, which specifies the ``Reporter``'s position for the given article (e.g. "Staff writer").
Base model with a natural_key and a manager with `get_by_natural_key`
Test for ticket 9279 -- Error is raised for entries in the serialized data for fields that have been removed from the database when not ignored.
Test for ticket 9279 -- Ignores entries in the serialized data for fields that have been removed from the database.
Test for ticket 19998 -- Ignore entries in the XML serialized data for fields that have been removed from the model definition.
Regression test for ticket 6436 -- os.path.join will throw away the initial parts of a path if it encounters an absolute path. This means that if a fixture is specified as an absolute path, we need to make sure we don't discover the absolute path in every fixture directory.
Test for ticket 4371 -- Loading data of an unknown format should fail Validate that error conditions are caught correctly
Test that failing serializer import raises the proper error
Test for ticket 4371 -- Loading a fixture file with invalid data using explicit filename. Test for ticket 18213 -- warning conditions are caught correctly
Test for ticket 4371 -- Loading a fixture file with invalid data without file extension. Test for ticket 18213 -- warning conditions are caught correctly
Test for ticket 18213 -- Loading a fixture file with no data output a warning. Previously empty fixture raises an error exception, see ticket 4371.
Regression for 9011 - error message is correct. Change from error to warning for ticket 18213.
Test for ticket 7565 -- PostgreSQL sequence resetting checks shouldn't ascend to parent models when inheritance is used (since they are treated individually).
Test for tickets 8298, 9942 - Field values should be coerced into the correct type by the deserializer, not as part of the database write.
[{"pk": %d, "model": "fixtures_regress.widget", "fields": {"name": "grommet"}}]
Regression for 3615 - Forward references cause fixtures not to load in MySQL (InnoDB)
Regression for 3615 - Ensure data with nonexistent child key references raises error
Regression for 17530 - should be able to cope with forward references when the fixtures are not in the same files or directories.
Regression for 7043 - Error is quickly reported when no fixtures is provided in the command line.
Regression for ticket 20820 -- loaddata on a model that inherits from a model with a M2M shouldn't blow up.
Regression for ticket 22421 -- loaddata on a model that inherits from a grand-parent model with a M2M but via an abstract parent shouldn't blow up.
Regression test for ticket 17946.
settings.FIXTURE_DIRS cannot contain duplicates in order to avoid repeated fixture loading.
settings.FIXTURE_DIRS cannot contain a default fixtures directory for application (app/fixtures) in order to avoid repeated fixture loading.
Test for ticket 13030 - Python based parser version natural keys deserialize with fk to inheriting model
Test for ticket 13030 - XML version natural keys deserialize with fk to inheriting model
Now lets check the dependency sorting explicitly It doesn't matter what order you mention the models Store *must* be serialized before then Person, and both must be serialized before Book.
Check that normal primary keys still work on a model with natural key capabilities
M2M relations without explicit through models SHOULD count as dependencies Regression test for bugs that could be caused by flawed fixes to 14226, namely if M2M checks are removed from sort_dependencies altogether.
Resolving circular M2M relations without explicit through models should fail loudly
Circular M2M relations with explicit through models should be serializable This test tests the circularity with explicit natural_key.dependencies
Test serializing and deserializing back models with simple M2M relations
Test that fixtures can be rolled back (ticket #11101).
Import errors in tag modules should be reraised with a helpful message.
Failing to import a backend keeps raising the original import error. Regression test for 24265.
Failing to initialize a backend keeps raising the original exception. Regression test for 24265.
Instances with deferred fields look the same as normal instances when we examine attribute values. Therefore, this method returns the number of deferred fields on returned instances.
Ensure select_related together with only on a proxy model behaves as expected. See 17876.
When an inherited model is fetched from the DB, its PK is also fetched. When getting the PK of the parent model it is useful to use the already fetched parent model PK if it happens to be available. Tests that this is done.
Create a Thing instance and notify about it.
If outer transaction fails, no hooks from within it run.
Helper method that instantiates a Paginator object from the passed params and then checks that its attributes match the passed output.
Helper method that checks a single attribute and gives a nice error message upon test failure.
Helper method that instantiates a Paginator object from the passed params and then checks that the start and end indexes of the passed page_num match those given as a 2-tuple in indexes.
Tests that a paginator page acts like a standard sequence.
Tests that a Paginator subclass can use the ``_get_page`` hook to return an alternative to the standard Page class.
Paginator.page_range should be an iterator.
Helper to create a complete tree.
Normally, accessing FKs doesn't fill in related objects
A select_related() call will fill in those related objects without any extra queries
select_related() also of course applies to entire lists, not just items. This test verifies the expected behavior without select_related.
select_related() also of course applies to entire lists, not just items. This test verifies the expected behavior with select_related.
Passing a relationship field lookup specifier to select_related() will stop the descent at a particular level. This can be used on lists as well.
The optional fields passed to select_related() control which related models we pull in. This allows for smaller queries. In this case, we explicitly say to select the 'genus' and 'genus.family' models, leading to the same number of queries as before.
In this case, we explicitly say to select the 'genus' and 'genus.family' models, leading to the same number of queries as before.
Running select_related() after calling values() raises a TypeError
Running select_related() after calling values_list() raises a TypeError
select_related() should thrown an error on fields that do not exist and non-relational fields.
Tests for the update() queryset method that allows in-place, multi-object updates.
Test that update changes the right number of rows for a nonempty queryset
Test that update changes the right number of rows for an empty queryset
Test that update changes the right number of rows for an empty queryset when the update affects only a base table
Test that update changes the right number of rows for an empty queryset when the update affects only a base table
Test that update works using <field>_id for foreign keys
Objects are updated by first filtering the candidates into a queryset and then calling the update() method. It executes immediately and returns nothing.
We can update multiple objects at once.
Foreign key fields can also be updated, although you can only update the object referred to, not anything inside the related object.
Multiple fields can be updated at once
In the rare case you want to update every instance of a model, update() is also a manager method.
We do not support update on already sliced query sets.
Models module can be loaded from an app in an egg
Loading an app from an egg that has no models returns no models (and no error)
Models module can be loaded from an app located under an egg's top-level package
Loading an app with no models from under the top-level egg package generates no error
Loading an app from an egg that has an import error in its models module raises that error
The last choice is for the None value.
Any filter must define a title.
Any SimpleListFilter must define a parameter_name.
A SimpleListFilter lookups method can return None but disables the filter completely.
Ensure that when a filter's queryset method fails, it fails loudly and the corresponding exception doesn't get swallowed (17828).
Ensure choices are set the selected class when using non-string values for lookups in SimpleListFilters (19318).
Ensure SimpleListFilter lookups pass lookup_allowed checks when parameter_name attribute contains double-underscore value (19182).
Ensure SimpleListFilter can access self.value() inside the lookup.
A list filter that filters the queryset by default gives the correct full_result_count.
Remove all entries named 'name' from the ModelAdmin instance URL patterns list
Ensure GET on the add_view works.
Ensure GET on the add_view plus specifying a field value in the query string works.
Ensure POST on add_view works.
Ensures that ModelAdmin.response_post_save_add() controls the redirection after the 'Save' button has been pressed when adding a new object. Refs 8001, 18310, 19505.
Ensures that ModelAdmin.response_post_save_change() controls the redirection after the 'Save' button has been pressed when editing an existing object. Refs 8001, 18310, 19505.
Many-to-many relationships between the same two tables In this example, a ``Person`` can have many friends, who are also ``Person`` objects. Friendship is a symmetrical relationship - if I am your friend, you are my friend. Here, ``friends`` is an example of a symmetrical ``ManyToManyField``. A ``Person`` can also have many idols - but while I may idolize you, you may not think the same of me. Here, ``idols`` is an example of a non-symmetrical ``ManyToManyField``. Only recursive ``ManyToManyField`` fields may be non-symmetrical, and they are symmetrical by default. This test validates that the many-to-many table is created using a mangled name if there is a name clash, and tests that symmetry is preserved where appropriate.
Using properties on models Use properties on models just like on any other Python object.
Test intentionally the automatically created through model.
24831 -- Cached properties on ManyToOneRel created in QuerySet.delete() caused subsequent QuerySet pickling to fail.
21430 -- Verifies a warning is raised for querysets that are unpickled without a Django version
Test plurals with ungettext. French differs from English in that 0 is singular.
Test that the language restored is the one used when the function was called, not the one used when the decorator was initialized. refs 23381
six.text_type(string_concat(...)) should not raise a TypeError - 4796
Empty value must stay empty after being translated (23196).
Translating a string requiring no auto-escaping shouldn't change the "safe" status.
Translations on files with mac or dos end of lines will be converted to unix eof in .po catalogs, and they have to match when retrieved
Tests the to_locale function and the special case of Serbian Latin (refs 12230 and r11299)
Test the to_language function
Error in translation file should not crash template rendering (%(person)s is translated as %(personne)s in fr.po) Refs 16516.
Error in translation file should not crash template rendering (%(person) misses a 's' in fr.po, causing the string formatting to fail) Refs 18393.
Check if sublocales fall back to the main locale
Tests the iter_format_modules function always yields format modules in a stable and correct order in presence of both base ll and ll_CC formats.
Tests the {% localize %} templatetag
Tests if form input with 'as_hidden' or 'as_text' is correctly localized. Ticket 18777
With a non-English LANGUAGE_CODE and if the active language is English or one of its variants, the untranslated string should be returned (instead of falling back to LANGUAGE_CODE) (See 24413).
Some browsers (Firefox, IE, etc.) use deprecated language codes. As these language codes will be removed in Django 1.9, these will be incorrectly matched. For example zh-tw (traditional) will be interpreted as zh-hans (simplified), which is wrong. So we should also accept these deprecated language codes. refs 18419 -- this is explicitly for browser compatibility
Some languages may have special fallbacks that don't follow the simple 'fr-ca' -> 'fr' logic (notably Chinese codes).
14170 after setting LANGUAGE, cache should be cleared and languages previously valid should not be used.
get_language_info return the first fallback language info if the lang_info struct does not contain the 'name' key.
With i18n_patterns(..., prefix_default_language=False), the default language (settings.LANGUAGE_CODE) should be accessible without a prefix.
A language non present in default Django languages can still be installed/used by a Django project.
Tests if the `i18n_patterns` is adding the prefix correctly.
Check that if no i18n_patterns is used in root URLconfs, then no language activation happens based on url prefix.
Tests if the translations are still working within namespaces.
Tests that 'Accept-Language' is not added to the Vary header when using prefixed URLs.
Tests the redirect when the requested URL doesn't end with a slash (`settings.APPEND_SLASH=False`).
Tests if the response has the right language-code.
21579 - LocaleMiddleware should respect the script prefix.
Tests using the French translations of the sampleproject.
Tests the extracted string found in the gettext catalog. Ensures that percent signs are python formatted. These tests should all have an analogous translation tests below, ensuring the python formatting does not persist through to a rendered template.
self.assertLocationCommentPresent('django.po', 42, 'dirA', 'dirB', 'foo.py') verifies that the django.po file has a gettext-style location comment of the form `: dirA/dirB/foo.py:42` (or `: .\dirA\dirB\foo.py:42` on Windows) None can be passed for the line_number argument to skip checking of the :42 suffix part. A string token can also be passed as line_number, in which case it will be searched in the template, and its line number will be used. A msgid is a suitable candidate.
Check the opposite of assertLocationComment()
Assert that file was recently modified (modification time was less than 10 seconds ago).
Assert that file was not recently modified (modification time was more than 10 seconds ago).
Value of locale-munging option used by the command is the right one
test xgettext warning about multiple bare interpolation placeholders
Test that find_files only discover files having the proper extensions.
Update of PO file doesn't corrupt it with non-UTF-8 encoding on Python3+Windows (#23271)
Regression test for 23583.
Regression test for 23717.
Ticket #20311.
Behavior is correct if --no-location switch is specified. See #16903.
Ensure no leaky paths in comments, e.g. : path\to\file.html.py:123 Refs 21209/26341.
Set access and modification times to the Unix epoch time for all the .po files.
For ModelChoiceField and ModelMultipleChoiceField tests.
A coerce function which results in a value not present in choices should raise an appropriate error (21397).
URLField correctly validates IPv6 (#18779).
Setting min_length or max_length to something that is not a number raises an exception.
Values have whitespace stripped but not if strip=False.
A localized IntegerField's widget renders to a text input without any number input specific attributes.
Class-defined widget is not overwritten by __init__() (22245).
A coerce function which results in a value not present in choices should raise an appropriate error (21397).
A localized DecimalField's widget renders to a text input without number input specific attributes.
A localized FloatField's widget renders to a text input without any number input specific attributes.
If insufficient data is provided, None is substituted.
Test when the first widget's data has changed.
Test when the last widget's data has changed. This ensures that it is not short circuiting while testing the widgets.
Quacks like a FieldFile (has a .url and unicode representation), but doesn't require us to care about storages etc.
A ClearableFileInput instantiated with no initial value does not render a clear checkbox.
ClearableFileInput.value_from_datadict returns False if the clear checkbox is checked, if not required.
ClearableFileInput.value_from_datadict never returns False if the field is required.
A ClearableFileInput should not mask exceptions produced while checking that it has a value.
Boolean values are rendered to their string forms ("True" and "False").
`attrs` passed to render() get precedence over those passed to the constructor
Rendering the None or '' values should yield the same output.
Using any value that's not in ('', None, False, True) will check the checkbox and set the 'value' attribute.
Integers are handled by value, not as booleans (17114).
You can pass 'check_test' to the constructor. This is a callable that takes the value and returns True if the box should be checked.
Calling check_test() shouldn't swallow exceptions (17888).
The CheckboxInput widget will return False if the key is not found in the data dictionary (because HTML form submission doesn't send any result for unchecked checkboxes).
When choices are set for this widget, we want to pass those along to the Select widget.
The choices for this widget are the Select widget's choices.
needs_multipart_form should be True if any widgets need it.
needs_multipart_form should be False if no widgets need it.
<ul id="media"> <li> <label for="media_0"><input id="media_0" name="nestchoice" type="checkbox" value="unknown" /> Unknown</label> </li> <li>Audio<ul id="media_1"> <li> <label for="media_1_0"> <input checked="checked" id="media_1_0" name="nestchoice" type="checkbox" value="vinyl" /> Vinyl </label> </li> <li> <label for="media_1_1"><input id="media_1_1" name="nestchoice" type="checkbox" value="cd" /> CD</label> </li> </ul></li> <li>Video<ul id="media_2"> <li> <label for="media_2_0"><input id="media_2_0" name="nestchoice" type="checkbox" value="vhs" /> VHS</label> </li> <li> <label for="media_2_1"> <input checked="checked" id="media_2_1" name="nestchoice" type="checkbox" value="dvd" /> DVD </label> </li> </ul></li> </ul>
<ul id="abc"> <li> <label for="abc_0"><input checked="checked" type="checkbox" name="letters" value="a" id="abc_0" /> A</label> </li> <li><label for="abc_1"><input type="checkbox" name="letters" value="b" id="abc_1" /> B</label></li> <li> <label for="abc_2"><input checked="checked" type="checkbox" name="letters" value="c" id="abc_2" /> C</label> </li> </ul>
Should be able to initialize from a string value.
Use 'format' to change the way a value is displayed.
The microseconds are trimmed on display, by default.
We should be able to initialize from a unicode value.
Use 'format' to change the way a value is displayed.
<ul id="media"> <li> <label for="media_0"><input id="media_0" name="nestchoice" type="radio" value="unknown" /> Unknown</label> </li> <li>Audio<ul id="media_1"> <li> <label for="media_1_0"><input id="media_1_0" name="nestchoice" type="radio" value="vinyl" /> Vinyl</label> </li> <li><label for="media_1_1"><input id="media_1_1" name="nestchoice" type="radio" value="cd" /> CD</label></li> </ul></li> <li>Video<ul id="media_2"> <li><label for="media_2_0"><input id="media_2_0" name="nestchoice" type="radio" value="vhs" /> VHS</label></li> <li> <label for="media_2_1"> <input checked="checked" id="media_2_1" name="nestchoice" type="radio" value="dvd" /> DVD </label> </li> </ul></li> </ul>
<ul id="foo"> <li> <label for="foo_0"><input checked="checked" type="radio" id="foo_0" value="J" name="beatle" /> John</label> </li> <li><label for="foo_1"><input type="radio" id="foo_1" value="P" name="beatle" /> Paul</label></li> <li><label for="foo_2"><input type="radio" id="foo_2" value="G" name="beatle" /> George</label></li> <li><label for="foo_3"><input type="radio" id="foo_3" value="R" name="beatle" /> Ringo</label></li> </ul>
The render_value argument lets you specify whether the widget should render its value. For security reasons, this is off by default.
The microseconds are trimmed on display, by default.
Use 'format' to change the way a value is displayed.
Test for issue 10405
Test for issue 10405
Make a ChoiceFormset from the given formset_data. The data should be given as a list of (choice, votes) tuples.
Test that custom kwargs set on the formset instance are passed to the underlying forms.
Test that form kwargs can be passed dynamically in a formset.
Formsets with no forms should still evaluate as true. Regression test for 15722
Formset should also work with SplitDateTimeField(initial=datetime.datetime.now). Regression test for 18709.
Test that an empty formset still calls clean()
Make sure media is available on empty formset, refs #19545
Quacks like a FieldFile (has a .url and unicode representation), but doesn't require us to care about storages etc.
Python 2 issue: Test that rendering a BoundField with bytestring content doesn't lose it's safe string status (22950).
17922 - required_css_class is added to the label_tag() of required fields.
19298 -- MultiValueField needs to override the default as it needs to deep-copy subfields:
23674 -- invalid initial data should not break form.changed_data()
If a widget has no id, label_tag just returns the text with no surrounding <label>.
If an id is provided in `Widget.attrs`, it overrides the generated ID, unless it is `None`.
BoundField label_suffix (if provided) overrides Form label_suffix
#5749 - `field_name` may be used as a key in _html_output().
`css_classes` may be used as a key in _html_output() (empty classes).
`css_classes` may be used as a key in _html_output() (class comes from required_css_class in this case).
BaseForm._html_output() should merge all the hidden input fields and put them in the last row.
BaseForm._html_output() should merge all the hidden input fields and put them in the last row ended with the specific row ender.
#21962 - adding html escape flag to ErrorDict
BaseForm.__repr__() should contain some basic information about the form.
BaseForm.__repr__() shouldn't trigger the form validation.
Fixes 23883 -- Check that flatatt does not modify the dict passed in
<link href="http://media.example.com/static/path/to/css1" type="text/css" media="all" rel="stylesheet" /><link href="/path/to/css2" type="text/css" media="all" rel="stylesheet" /> <link href="/path/to/css3" type="text/css" media="all" rel="stylesheet" /> <link href="/some/form/css" type="text/css" media="all" rel="stylesheet" />
If we execute the exact same statement twice, the second time, it won't create a Person.
If you don't specify a value or default value for all required fields, you will get an error.
If you have a field named defaults and want to use it as an exact lookup, you need to use 'defaults__exact'.
If you specify an existing primary key, but different other fields, then you will get an error and data will not be updated.
Regression test for 20463: the database connection should still be usable after a DataError or ProgrammingError in .get_or_create().
Regression test for 16137: get_or_create does not require kwargs.
Regression test for 15117. Requires a TransactionTestCase on databases that delay integrity checks until the end of transactions, otherwise the exception is never raised.
If you don't specify a value or default value for all required fields, you will get an error.
If you specify an existing primary key, but different other fields, then you will get an error and data will not be updated.
update_or_create should raise IntegrityErrors with the full traceback. This is tested by checking that a known method call is in the traceback. We cannot use assertRaises/assertRaises here because we need to inspect the actual traceback. Refs 16340.
Should be able to use update_or_create from the related manager to create a book. Refs 23611.
Should be able to use update_or_create from the related manager to update a book. Refs 23611.
Should be able to use update_or_create from the m2m related manager to create a book. Refs 23611.
Should be able to use update_or_create from the m2m related manager to update a book. Refs 23611.
If you have a field named defaults and want to use it as an exact lookup, you need to use 'defaults__exact'.
Warn if SESSION_COOKIE_SECURE is off and "django.contrib.sessions" is in INSTALLED_APPS.
Warn if SESSION_COOKIE_SECURE is off and "django.contrib.sessions.middleware.SessionMiddleware" is in MIDDLEWARE.
If SESSION_COOKIE_SECURE is off and we find both the session app and the middleware, provide one common warning.
If SESSION_COOKIE_SECURE is on, there's no warning about it.
Warn if SESSION_COOKIE_HTTPONLY is off and "django.contrib.sessions" is in INSTALLED_APPS.
Warn if SESSION_COOKIE_HTTPONLY is off and "django.contrib.sessions.middleware.SessionMiddleware" is in MIDDLEWARE.
If SESSION_COOKIE_HTTPONLY is off and we find both the session app and the middleware, provide one common warning.
If SESSION_COOKIE_HTTPONLY is on, there's no warning about it.
Warn if CsrfViewMiddleware isn't in MIDDLEWARE.
Warn if CsrfViewMiddleware is in MIDDLEWARE but CSRF_COOKIE_SECURE isn't True.
No warning if CsrfViewMiddleware isn't in MIDDLEWARE, even if CSRF_COOKIE_SECURE is False.
Warn if CsrfViewMiddleware is in MIDDLEWARE but CSRF_COOKIE_HTTPONLY isn't True.
No warning if CsrfViewMiddleware isn't in MIDDLEWARE, even if CSRF_COOKIE_HTTPONLY is False.
Warn if SecurityMiddleware isn't in MIDDLEWARE.
Warn if SECURE_HSTS_SECONDS isn't > 0.
Don't warn if SECURE_HSTS_SECONDS isn't > 0 and SecurityMiddleware isn't installed.
Warn if SECURE_HSTS_INCLUDE_SUBDOMAINS isn't True.
Don't warn if SecurityMiddleware isn't installed.
Don't warn if SECURE_HSTS_SECONDS isn't set.
Warn if XFrameOptionsMiddleware isn't in MIDDLEWARE.
Warn if XFrameOptionsMiddleware is in MIDDLEWARE but X_FRAME_OPTIONS isn't 'DENY'.
No error if XFrameOptionsMiddleware isn't in MIDDLEWARE even if X_FRAME_OPTIONS isn't 'DENY'.
Warn if SECURE_CONTENT_TYPE_NOSNIFF isn't True.
Don't warn if SECURE_CONTENT_TYPE_NOSNIFF isn't True and SecurityMiddleware isn't in MIDDLEWARE.
Warn if SECURE_BROWSER_XSS_FILTER isn't True.
Don't warn if SECURE_BROWSER_XSS_FILTER isn't True and SecurityMiddleware isn't in MIDDLEWARE.
Warn if SECURE_SSL_REDIRECT isn't True.
Don't warn if SECURE_SSL_REDIRECT is False and SecurityMiddleware isn't installed.
Warn if DEBUG is True.
`database` checks are only run when their tag is specified.
Error if template loaders are specified and APP_DIRS is True.
Don't error if 'default' is present in CACHES setting.
Error if 'default' not present in CACHES setting.
Routes to the 'other' database if the model name starts with 'Other'.
An annotation not included in values() before an aggregate should be excluded from the group by clause.
An annotation included in values() before an aggregate should be included in the group by clause.
Test that .dates() returns a distinct set of dates when applied to a QuerySet with aggregation. Refs 18056. Previously, .dates() would return distinct (date_kind, aggregation) sets, in this case (year, num_authors), so 2008 would be returned twice because there are books from 2008 with a different number of authors.
Check that aggregation over sliced queryset works correctly.
No fields passed to modelformset_factory() should result in no fields on returned forms except for the id (14119).
Regression for 13095 and 17683: Using base forms with widgets defined in Meta should not raise errors and BaseModelForm should respect the specified pk widget.
A formset mix-in that lets a form decide if it's to be deleted. Works for BaseFormSets. Also works for ModelFormSets with 14099 fixed. form.should_delete() is called. The formset delete field is also suppressed.
delete form if odd PK
Add test data to database via formset
should warn on invalid separator
The symbol django.core.files.NamedTemporaryFile is assigned as a different class on different operating systems. In any case, the result should minimally mock some of the API of tempfile.NamedTemporaryFile from the Python standard library.
File objects should yield lines when iterated over. Refs 22107.
8149 - File objects with \r\n line endings should yield lines when iterated over.
8149 - File objects with \r line endings should yield lines when iterated over.
Other examples of unnamed files may be tempfile.SpooledTemporaryFile or urllib.urlopen()
Test that the constructor of ContentFile accepts 'name' (16590).
Test that ContentFile can accept both bytes and unicode and that the retrieved content is of the same type.
Open files passed into get_image_dimensions() should stay opened.
Multiple calls of get_image_dimensions() should return the same size.
Regression test for 19457 get_image_dimensions fails on some pngs, while Image.size is working good on them
get_image_dimensions() should return (None, None) for the dimensions of invalid images (24441). brokenimg.png is not a valid image and it has been generated by: $ echo "123" > brokenimg.png
get_image_dimensions() should catch struct.error while feeding the PIL Image parser (24544). Emulates the Parser feed error. Since the error is raised on every feed attempt, the resulting image size should be invalid: (None, None).
When the redirect target is '', return a 410
can_delete should be passed to inlineformset factory.
Bug #13174.
A model form with a form field specified (TitleForm.title1) should have its label rendered in the tabular inline.
#18263 -- Make sure hidden fields don't get a column in tabular inlines
Ensure that multiple inlines with related_name='+' have correct form prefixes. Bug 16838.
Ensure that the "View on Site" link is correct for locales that use thousand separators
Ensure that the "View on Site" link is correct for models with a custom primary key field. Bug 18433.
Ensure that an object can be created with inlines when it inherits another class. Bug 19524.
Ensure that min_num and extra determine number of forms.
Ensure that get_min_num is called and used correctly.
Regression for #9362The problem depends only on InlineAdminForm and its "original" argument, so we can safely set the other arguments to None/{}. We just need to check that the content_type argument of Child isn't altered by the internals of the inline form.
Ensure that the "Add another XXX" link correctly adds items to the stacked formset.
RegexURLResolver should raise an exception when no urlpatterns exist.
Verifies lazy object returned by reverse_lazy is coerced to text by resolve(). Previous to 21043, this would raise a TypeError.
Verifies that we raise a Resolver404 if what we are resolving doesn't meet the basic requirements of a path to match - i.e., at the very least, it matches the root pattern '^/'. We must never return None from resolve, or we will get a TypeError further down the line. Regression for 10834.
Overriding request.urlconf with None will fall back to the default URLconf.
Test reversing an URL from the *overridden* URLconf from inside a response middleware.
Test reversing an URL from the *default* URLconf from inside a response middleware.
Test reversing an URL from the *overridden* URLconf from inside a streaming response.
Test reversing an URL from the *default* URLconf from inside a streaming response.
Tests for handler400, handler404 and handler500
Tests for handler404 and handler500 if ROOT_URLCONF is None
Test QueryDict with one key/value pair
A copy of a QueryDict is mutable.
Test QueryDict with two key/value pairs with same keys.
QueryDicts must be able to handle invalid input encoding (in this case, bad UTF-8 encoding), falling back to ISO-8859-1 decoding. This test doesn't apply under Python 3 because the URL is a string and not a bytestring.
Regression test for #8278: QueryDict.update(QueryDict)
#13572 - QueryDict with a non-default encoding
Test for bug 14020: Make HttpResponse.get work like dict.get
Make sure HttpResponseRedirect works with lazy strings.
Test that we can still preserve semi-colons and commas
Test that we haven't broken normal encoding
Test that a single non-standard cookie name doesn't affect all cookies. Ticket 13007.
Test that a repeated non-standard name doesn't affect all cookies. Ticket 15852
Test that we can use httponly attribute on cookies that we load
Regression test for 18403
Proxy model inheritance across apps can result in migrate not creating the table for the proxied model (as described in 12286).  This test creates two dummy apps and calls migrate, then verifies that the table has been created.
Deleting an instance of a model proxying a multi-table inherited subclass should cascade delete down the whole inheritance chain (see 18083).
There's a bug in Django/cx_Oracle with respect to string handling under Python 3 (essentially, they treat Python 3 strings as Python 2 strings rather than unicode). This makes some tests here fail under Python 3, so we mark them as expected failures until someone fixes them in 23843.
Deferred attributes can be referenced by an annotation, but they are not themselves deferred, and cannot be deferred.
Fields on an inherited model can be referenced by an annotated field.
Test that annotating None onto a model round-trips
Test that columns are aligned in the correct order for resolve_columns. This test will fail on mysql if column ordering is out. Column fields should be aligned as: 1. extra_select 2. model_fields 3. annotation_fields 4. model_related_fields
Test if backend specific checks are performed.
25723 - Referenced model registration lookup should be run against the field's model registry.
25723 - Referenced model registration lookup should be run against the field's model registry.
25723 - Through model registration lookup should be run against the field's model registry.
Ref #22047.
Tests that ManyToManyField accepts the ``through_fields`` kwarg only if an intermediary table is specified.
Tests that mixing up the order of link fields to ManyToManyField.through_fields triggers validation errors.
Tests that providing invalid field names to ManyToManyField.through_fields triggers validation errors.
Tests that if ``through_fields`` kwarg is given, it must specify both link fields of the intermediary table.
Ensure that a lookup_allowed allows a parameter whose field lookup doesn't exist. Refs 21129.
Ensure that the `exclude` kwarg passed to `ModelAdmin.get_form()` overrides all other declarations. Refs 8999.
Ensure that the `exclude` kwarg passed to `InlineModelAdmin.get_formset()` overrides all other declarations. Refs 8999.
Same as assertIsInvalid but treats the given msg as a regexp.
Validate the comma-separated list of requested browsers.
Test a middleware that implements process_view.
Test a middleware that implements process_view, operating on a callable class.
Test that gen_filenames() yields the built-in Django locale files.
Test that gen_filenames also yields from LOCALE_PATHS locales.
Test that gen_filenames also yields from the current directory (project root).
Test that gen_filenames also yields from locale dirs in installed apps.
If i18n machinery is disabled, there is no need for watching the locale files.
When a file is deleted, gen_filenames() no longer returns it.
When a file containing an error is imported in a function wrapped by check_errors(), gen_filenames() returns it.
When a file containing an error is imported in a function wrapped by check_errors(), gen_filenames(only_new=True) returns it.
Test that force_bytes knows how to convert to bytes an exception containing non-ASCII characters in its args.
Test get_tag_uri() correctly generates TagURIs.
Test that get_tag_uri() correctly generates TagURIs from URLs with port numbers.
Test rfc2822_date() correctly formats datetime objects.
Test rfc2822_date() correctly formats datetime objects with tzinfo.
Test rfc2822_date() correctly formats date objects.
Test rfc3339_date() correctly formats datetime objects.
Test rfc3339_date() correctly formats datetime objects with tzinfo.
Test rfc3339_date() correctly formats date objects.
Test to make sure Atom MIME type has UTF8 Charset parameter set
The Custom Loader test is exactly the same as the EggLoader, butit uses a custom defined Loader and Finder that is intentionally split into two classes. Although the EggLoader combines both functions into one class, this isn't required.
A simple class with just one attribute.
Wrap the given object into a LazyObject
A base class with a funky __reduce__ method, meant to simulate the __reduce__ method of Model, which sets self._django_version.
A class that inherits from BaseBaz and has its own __reduce_ex__ method.
A class that acts as a proxy for Baz. It does some scary mucking about with dicts, which simulates some crazy things that people might do with e.g. proxy models.
Test that lazy also finds base class methods in the proxy object
Test that lazy finds the correct (overridden) method implementation
Here is the docstring...
Microseconds and seconds are ignored.
Test other units.
Test multiple units.
If the two differing units aren't adjacent, only the first unit is displayed.
When the second date occurs before the first, we should always get 0 minutes.
When using two different timezones.
Both timesince and timeuntil should work on date objects (#17937).
Timesince should work with both date objects (#9672)
Check that function(value) equals output.  If output is None, check that function(value) equals value.
Wrapper for Decimal which prefixes each amount with the € symbol.
normalize_newlines should be able to handle bytes too
15346, 15573 - create_default_site() creates an example site only if none exist.
16353, 16828 - The default site creation should respect db routing.
17415 - Another site can be created right after the default one. On some backends the sequence needs to be reset after saving with an explicit ID. Test that there isn't a sequence collisions by saving another site. This test is only meaningful with databases that use sequences for automatic primary keys such as PostgreSQL and Oracle.
23641 - Sending the ``post_migrate`` signal triggers creation of the default site.
24488 - The pk should default to 1 if no ``SITE_ID`` is configured.
24075 - A Site shouldn't be created if the model isn't available.
Tests for basic system checks of 'exclude' option values (12689)
Regression test for 9932 - exclude in InlineModelAdmin should not contain the ForeignKey field used in ModelAdmin.model
Regression test for 22034 - check that generic inlines don't look for normal ForeignKey relations.
Ensure that a model without a GenericForeignKey raises problems if it's included in an GenericInlineModelAdmin definition.
A GenericInlineModelAdmin raises problems if the ct_field points to a field that isn't part of a GenericForeignKey.
A GenericInlineModelAdmin raises problems if the ct_fk_field points to a field that isn't part of a GenericForeignKey.
Regression test for 15669 - Include app label in admin system check messages
Regression test for 11709 - when testing for fk excluding (when exclude is given) make sure fk_name is honored or things blow up when there is more than one fk to the parent model.
Regression test for 12203/12237 - Fail more gracefully when a M2M field that specifies the 'through' option is included in the 'fields' or the 'fieldsets' ModelAdmin options.
Regression test for 12209 -- If the explicitly provided through model is specified as a string, the admin should still be able use Model.m2m_field.through
Regression for ensuring ModelAdmin.fields can contain non-model fields that broke with r11737
Regression for ensuring ModelAdmin.field can handle first elem being a non-model field (test fix for UnboundLocalError introduced with r16225).
TRACE a view
Check that the returned response has a ``request`` attribute with the originating environ dict and a ``wsgi_request`` with the originating ``WSGIRequest`` instance.
The response contains a ResolverMatch instance.
The response ResolverMatch instance contains the correct information when following redirects.
The response ResolverMatch instance contains the correct information when accessing a regular view.
<?xml version="1.0" encoding="utf-8"?><library><book><title>Blink</title><author>Malcolm Gladwell</author></book></library>
An inactive user may login if the authenticate backend allows it.
A nested test client request shouldn't clobber exception signals from the outer client request.
A test case can specify a custom class for self.client.
The request factory implements all the HTTP/1.1 methods.
The request factory returns a templated response for a GET request.
A simple view that expects a TRACE request and echoes its status line. TRACE requests should not have an entity; the view will return a 400 status response if it is present.
A view that expects a POST, and returns a different template dependingon whether any POST data is available
A view which just raises an exception, simulating a broken view.
A view that uses a nested client to call another view and then raises an exception.
Test that an unknown command raises CommandError
find_command should still work when the PATH environment variable doesn't exist (22256).
Test that management commands can also be loaded from Python eggs.
When passing the long option name to call_command, the available option key is the option dest name (22985).
It should be possible to pass non-string arguments to call_command.
By default, call_command should not trigger the check framework, unless specifically asked.
Tests that need to run by simulating the command line, not by call_command.
This command returns a URL from a reverse() call.
Transactions Django handles transactions in three different ways. The default is to commit each transaction upon a write, but you can decorate a function to get commit-on-success behavior. Alternatively, you can manage the transaction manually.
All basic tests for atomic should also pass within an existing transaction.
Ensure headers sent by the default MIDDLEWARE don't inadvertently change. For example, we never want "Vary: Cookie" to appear in the list since it prevents the caching of responses.
Check that login_required is assignable to callable objects.
Check that login_required is assignable to normal views.
Check that login_required works on a simple view wrapped in a login_required decorator.
Check that login_required works on a simple view wrapped in a login_required decorator with a login_url set.
Check that login_required works on a simple view wrapped in a login_required decorator.
Ensure that we can make a token and that it is valid
Tests requests where no remote user is specified and insures that no users get created.
Backend that doesn't create unknown users.
Contains the same tests as RemoteUserTest, but using a custom auth backend class that doesn't create unknown users.
Backend that allows inactive users.
Grabs username before the @ character.
Sets user's email address.
The strings passed in REMOTE_USER should be cleaned and the known users should not have been configured with an email address.
The unknown user created should be configured with an email address.
Middleware that overrides custom HTTP auth user header.
Tests a custom RemoteUserMiddleware subclass with custom HTTP auth user header.
Hasher that counts how many times it computes a hash.
Regressiontest for #12462
17903 -- Anonymous users shouldn't have permissions in ModelBackend.get_(all|user|group)_permissions().
17903 -- Inactive users shouldn't have permissions in ModelBackend.get_(all|user|group)_permissions().
A superuser has all permissions. Refs #14795.
An inactive user can't authenticate.
A custom user without an `is_active` field is allowed to authenticate.
Tests for the ModelBackend using the custom ExtensionUser model. This isn't a perfect test, because both the User and ExtensionUser are synchronized to the database, which wouldn't ordinary happen in production. As a result, it doesn't catch errors caused by the non- existence of the User table. The specific problem is queries on .filter(groups__user) et al, which makes an implicit assumption that the user model is called 'User'. In production, the auth.User table won't exist, so the requested join won't exist either; in testing, the auth.User *does* exist, and so does the join. However, the join table won't contain any useful data; for testing, we check that the data we expect actually does exist.
Tests for the ModelBackend using the CustomPermissionsUser model. As with the ExtensionUser test, this isn't a perfect test, because both the User and CustomPermissionsUser are synchronized to the database, which wouldn't ordinary happen in production.
Tests that the model backend can accept a credentials kwarg labeled with custom user model's USERNAME_FIELD.
A custom user with a UUID primary key should be able to login.
Tests for AnonymousUser delegating to backend.
Tests that an appropriate error is raised if no auth backends are provided.
Tests for an inactive user
Always raises PermissionDenied in `authenticate`, `has_perm` and `has_module_perms`.
Always raises TypeError.
Tests that a TypeError within a backend is propagated properly. Regression test for ticket 18171
23925 - The backend path added to the session should be the same as the one defined in AUTHENTICATION_BACKENDS setting.
Inactive users may authenticate with the AllowAllUsersModelBackend.
Create a user and return a tuple (user_object, username, email).
Test nonexistent email address. This should not fail because it would expose information about registered users.
Preserve the case of the user name (before the @ in the email address) when creating a user (5605).
Test that inactive user cannot receive password reset email.
Test the PasswordResetForm.save() method with no html_email_template_name parameter passed in. Test to ensure original behavior is unchanged after the parameter was added.
Test the PasswordResetFOrm.save() method with html_email_template_name parameter specified. Test to ensure that a multipart email is sent with both text/plain and text/html parts.
Makes sure specifying no plain password with a valid encoded password returns `False`.
A user with a non-unique username. This model is not invalid if it is used with a custom authentication backend which supports non-unique usernames.
The CustomPermissionsUser users email as the identifier, but uses the normal Django permissions model. This allows us to check that the PermissionsMixin includes everything that is needed to interact with the ModelBackend.
A user with a UUID as primary key
Creates and saves a User with the given email and password.
A context manager to temporarily remove the groups and user_permissions M2M fields from the AbstractUser class, so they don't clash with the related_name sets.
Assert that error is found in response.context['form'] errors
Given two URLs, make sure all their components (the ones given by urlparse) are equal, only comparing components that are present in both URLs. If `parse_qs` is True, then the querystrings are parsed with QueryDict. This is useful if you don't want the order of parameters to matter. Otherwise, the query strings are compared as-is.
extra_email_context should be available in the email template context.
A multipart email with text/plain and text/html is sent if the html_email_template parameter is passed to the view
To avoid reusing another user's session, ensure a new, empty session is created if the existing session corresponds to a different authenticated user.
Session without django.contrib.auth.HASH_SESSION_KEY should login without an exception.
Tests for settings.LOGIN_URL.
Tests for settings.LOGIN_REDIRECT_URL.
Tests for the redirect_to_login view
Stay on the login page by default.
If not logged in, stay on the same page.
If logged in, go to default redirected URL.
If logged in, go to custom redirected URL.
If next is specified as a GET parameter, go there.
Detect a redirect loop if LOGIN_REDIRECT_URL is not correctly set, with and without custom parameters.
The logout() view should send "no-cache" headers for reasons described in 25490.
A fake stdin object that pretends to be a TTY to be used in conjunction with mock_inputs.
A CommandError should be thrown by handle() if the user enters in mismatched passwords three times.
A CommandError should be raised if the user enters in passwords which fail validation three times.
changepassword --database should operate on the specified DB.
A fake stdin object that has isatty() return False.
You can pass a stdin object as an option and it should be available on self.stdin. If no such option is passed, it defaults to sys.stdin.
changepassword --database should operate on the specified DB.
Calling user.save() twice should trigger password_changed() once.
Simple test case for ticket 20541
This object makes sure __eq__ will not be called endlessly.
No endless loops if accessed with 'in' - refs 18979.
Tests that the session is not accessed simply by including the auth context processor
Tests that the session is accessed if the auth context processor is used and relevant attributes accessed.
REQUIRED_FIELDS should be a list.
USERNAME_FIELD should not appear in REQUIRED_FIELDS.
A non-unique USERNAME_FIELD should raise an error only if we use the default authentication backend. Otherwise, an warning should be raised.
<User Model>.is_anonymous/is_authenticated must not be methods.
Set up the listeners and reset the logged in/logged out counters
Disconnect the listeners
With SQLite, foreign keys can be added with different syntaxes.
Test that multicolumn indexes are not included in the introspection results.
Test the RequireDebugFalse filter class.
Test the RequireDebugTrue filter class.
The 'django' base logger only output anything when DEBUG=True.
Ensure that newlines in email reports' subjects are escaped to avoid AdminErrorHandler to fail. Refs 17281.
23593 - AdminEmailHandler should allow Unicode characters in the request.
Test that calling django.setup() initializes the logging configuration.
[loggers] keys=root [handlers] keys=stream [formatters] keys=simple [logger_root] handlers=stream [handler_stream] class=StreamHandler formatter=simple args=(sys.stdout,) [formatter_simple] format=%(message)s
Many-to-many and many-to-one relationships to the same table Make sure to set ``related_name`` if you use relationships to the same table.
Regression test for 6755
Regression test for 11764
Regression test for 7853 If the parent class has a self-referential link, make sure that any updates to that link via the child update the right table.
Regression tests for 8076 get_(next/previous)_by_date should work
Regression test for 10362 It is possible to call update() and only change a field in an ancestor model.
Test that a model which has different primary key for the parent model passes unique field checking correctly. Refs 17615.
WSGI squashes multiple successive slashes in PATH_INFO, WSGIRequest should take that into account when populating request.path and request.META['SCRIPT_NAME']. Refs 17133.
Ensure that the FORCE_SCRIPT_NAME setting takes precedence over the request's SCRIPT_NAME environment parameter. Refs 20169.
Setting a cookie after deletion should clear the expiry date.
Reading from request is allowed after accessing request contents as POST or body.
Construction of POST or body is not allowed after reading from request.
Test a POST with non-utf-8 payload encoding.
If wsgi.input.read() raises an exception while trying to read() the POST, the exception should be identifiable (not a generic IOError).
If wsgi.input.read() raises an exception while trying to read() the FILES, the exception should be identifiable (not a generic IOError).
Index names should be deterministic.
Test indexes are not created for related objects
A storage class that implements get_modified_time().
Base finder test mixin. On Windows, sometimes the case of the path we ask the finders for and the path(s) they find can differ. Compare them using os.path.normcase() to avoid false negatives.
Test FileSystemFinder.
Test AppDirectoriesFinder.
Test DefaultStorageFinder.
We can't determine if STATICFILES_DIRS is set correctly just by looking at the type, but we can determine if it's definitely wrong.
Test case with a couple utility assertions.
Can find a file in a STATICFILES_DIRS directory.
Can find a file in a subdirectory of a STATICFILES_DIRS directory.
File in STATICFILES_DIRS has priority over file in app.
Can find a file in an app static/ directory.
Can find a file with non-ASCII character in an app static/ directory.
Test static asset serving view.
Test serving static files disabled when DEBUG is False.
Make sure no files were create in the destination directory.
Test that findstatic returns all candidate files if run without --first and -v0.
Even if the STATIC_ROOT setting is not set, one can still call the `manage.py help collectstatic` command.
Test that -i patterns are ignored.
Common ignore patterns (*~, .*, CVS) are ignored.
Test the ``--clear`` option of the ``collectstatic`` management command.
With --no-default-ignore, common ignore patterns (*~, .*, CVS) are not ignored.
A custom ignore_patterns list, ['*.css'] in this case, can be specified in an AppConfig definition.
Test ``--dry-run`` option for ``collectstatic`` management command.
Run collectstatic, and capture and return the output. We want to run the command at highest verbosity, which is why we can't just call e.g. BaseCollectionTestCase.run_collectstatic()
There isn't a warning if there isn't a duplicate destination.
Tests for 15035
With ``--link``, symbolic links are created.
Test broken symlink gets deleted.
Test the CachedStaticFilesStorage backend.
Like test_template_tag_absolute, but for a file in STATIC_ROOT (26249).
Test that post_processing behaves correctly. Files that are alterable should always be post-processed; files that aren't should be skipped. collectstatic has already been called once in setUp() for this testcase, therefore we check by verifying behavior on a second run.
Test that post_processing indicates the origin of the error when it fails. Regression test for 18986.
Handle cache key creation correctly, see 17861.
Test the CachedStaticFilesStorage backend.
Used in TestStaticFilePermissions
Tests that the postgis version check parses correctly the version numbers
Test creating a model with a raster field on a db without raster support.
Test adding a raster field on a db without raster support.
Test the RemoveField operation with a raster-enabled column.
Test annotation with Extent GeoAggregate.
Make sure data is 3D and has expected Z values -- shouldn't change because of coordinate system.
Test the creation of polygon 3D models.
Testing GeoQuerySet.translate() on Z values.
Base testing object, turns keyword args into attributes.
Each attribute of this object is a list of `TestGeom` instances.
Test a layer containing utf-8-encoded name
Test the geo-enabled inspectdb command.
Testing if extent supports limit.
Test that using a queryset inside a geo lookup is working (using a subquery) (14483).
'geojson' should be listed in available serializers.
The fields option allows to define a subset of fields to be present in the 'properties' of the generated output.
Testing GoogleMap.scripts() output. See 20773.
Test that GoogleMap doesn't crash with non-ASCII content.
Testing `transform` method (no SRID or negative SRID)
Testing `transform` method (GDAL not available)
Geometry classes should be deconstructible.
Testing the GEOS version regular expression.
Distance and Area objects to allow for sensible and convenient calculation and conversions. Here are some tests.
Test creating a model where the RasterField has a null value.
RasterField should accept a positional verbose name argument.
Cast a geography to a geometry field for an aggregate function that expects a geometry input.
Testing Distance() support on non-point geography fields.
geodjango_point.layers.base = new OpenLayers.Layer.OSM("OpenStreetMap (Mapnik)");,
"http://vmap0.tiles.osgeo.org/wms/vmap0", {layers: 'basic', format: 'image/jpeg'});,
Skip a test unless a database supports all of gis_lookups.
Test initialization of distance models.
Test a simple distance query, with projected coordinates and without transformation.
Make sure the MapWidget js is passed in the form media and a MapWidget is actually created
get_storage_class returns the class for a storage backend name/path.
get_storage_class raises an error if the requested import don't exist.
get_storage_class raises an error if the requested class don't exist.
Makes sure an exception is raised if the location is empty
Standard file access options are available, and work as expected.
File storage returns a Datetime object for the last accessed time of a file.
File storage returns a datetime for the last accessed time of a file.
File storage returns a datetime for the creation time of a file.
File storage returns a datetime for the creation time of a file.
File storage returns a datetime for the last modified time of a file.
File storage returns a datetime for the last modified time of a file.
File storage extracts the filename from the content object if no name is given explicitly.
Saving a pathname should create intermediate directories as necessary.
File storage returns the full path of a file
File storage returns a tuple containing directories and files.
File storage prevents directory traversal (files can only be accessed if they're below the storage location).
Test behavior when file.chunks() is raising an error
Calling delete with an empty name should not try to remove the base storage directory, but fail loudly (20660).
Properties using settings values as defaults should be updated on referenced settings change while specified values should be unchanged.
Append numbers to duplicate files rather than underscores, like Trac.
When Storage.save() wraps a file-like object in File, it should include the name argument so that bool(file) evaluates to True (26495).
Regression test for #9610.If the directory name contains a dot and the file name doesn't, make sure we still mangle the file name instead of the directory name.
File names with a dot as their first character don't have an extension, and the underscore should get added to the end.
Test that ContentFile can be saved correctly with the filesystem storage, both if it was initialized with string or unicode content
This method is important to test that Storage.save() doesn't replace '\' with '/' (rather FileSystemStorage.save() does).
This is the method that's important to override when using S3 so that os.path() isn't called, which would break S3 keys.
See: https://code.djangoproject.com/ticket/21566
Test that it's possible to create a working related field that doesn't use any joining columns, as long as an extra restriction is supplied.
Define some extra Field methods so this Rel acts more like a Field, which lets us use ReverseManyToOneDescriptor in both directions.
Makes ReverseManyToOneDescriptor work in both directions.
This model is designed to yield no join conditions and raise an exception in ``Join.as_sql()``.
The set of articletranslation should not set any local fields.
This field will allow querying and fetching the currently active translation for Article from ArticleTranslation.
Ensure that a regex lookup does not fail on null/None values
Ensure that a regex lookup does not fail on non-string fields
Ensure that a regex lookup does not trip on non-ASCII characters.
Ensure that a lookup query containing non-fields raises the proper exception.
A view that uses {% csrf_token %}
A view that doesn't use the token, but does use the csrf view processor.
A version of HttpRequest that allows us to change some things more easily
If the token is longer than expected, it is ignored and a new token is created.
If the token contains non-alphanumeric characters, it is ignored and a new token is created.
Check that if no CSRF cookies is present, the middleware rejects the incoming request.  This will stop login CSRF.
Check that if a CSRF cookie is present but no token, the middleware rejects the incoming request.
Check that if both a cookie and a token is present, the middleware lets it through.
Check that if a CSRF cookie is present and no token, but the csrf_exempt decorator has been applied to the view, the middleware lets it through
Check that we can pass in the token in a header instead of in the form
settings.CSRF_HEADER_NAME can be used to customize the CSRF header name
Tests that HTTP PUT and DELETE methods have protection
Check that CsrfTokenNode works when no CSRF cookie is set
Check that we get a new token if the csrf_cookie is the empty string
Check that CsrfTokenNode works when a CSRF cookie is set
Check that get_token still works for a view decorated with 'csrf_exempt'.
Check that get_token works for a view decorated solely with requires_csrf_token
Check that CsrfTokenNode works when a CSRF cookie is created by the middleware (when one was not already present)
The csrf token used in posts is changed on every request (although stays equivalent). The csrf cookie should not change on accepted requests. If it appears in the response, it should keep its value.
The csrf token is reset from a bare secret.
Test that a POST HTTPS request with a bad referer is rejected
A POST HTTPS request with a good referer is accepted.
A POST HTTPS request is accepted when USE_X_FORWARDED_PORT=True.
A POST HTTPS request with a referer added to the CSRF_TRUSTED_ORIGINS setting is accepted.
A POST HTTPS request with a referer that matches a CSRF_TRUSTED_ORIGINS wildcard is accepted.
A POST HTTPS request with a good referer should be accepted from a subdomain that's allowed by CSRF_COOKIE_DOMAIN.
A POST HTTPS request with a good referer should be accepted from a subdomain that's allowed by CSRF_COOKIE_DOMAIN and a non-443 port.
A POST HTTPS request from an insecure referer should be rejected.
HttpRequest that can raise an IOError when accessing POST data
Testing the django.test.skipIfDBFeature decorator.
Test that assertQuerysetEqual checks the number of appearance of each item when used with option ordered=False.
assertRaisesMessage shouldn't interpret RE special chars.
Overriding the MEDIA_ROOT setting should be reflected in the base_location attribute of django.core.files.storage.default_storage.
Overriding the MEDIA_URL setting should be reflected in the base_url attribute of django.core.files.storage.default_storage.
Overriding the FILE_UPLOAD_PERMISSIONS setting should be reflected in the file_permissions_mode attribute of django.core.files.storage.default_storage.
Overriding the FILE_UPLOAD_DIRECTORY_PERMISSIONS setting should be reflected in the directory_permissions_mode attribute of django.core.files.storage.default_storage.
Overriding DATABASE_ROUTERS should update the master router.
Overriding the STATIC_URL setting should be reflected in the base_url attribute of django.contrib.staticfiles.storage.staticfiles_storage.
Overriding the STATIC_ROOT setting should be reflected in the location attribute of django.contrib.staticfiles.storage.staticfiles_storage.
Overriding the STATICFILES_STORAGE setting should be reflected in the value of django.contrib.staticfiles.storage.staticfiles_storage.
Overriding the STATICFILES_FINDERS setting should be reflected in the return value of django.contrib.staticfiles.finders.get_finders.
Overriding the STATICFILES_DIRS setting should be reflected in the locations attribute of the django.contrib.staticfiles.finders.FileSystemFinder instance.
'manage.py help test' works after r16352.
Test that setup_databases() doesn't fail with dummy database backend.
Test that setup_datebases() doesn't fail when 'default' is aliased
Here we test creating the same model two times in different test methods, and check that both times they get "1" as their PK value. That is, we test that AutoField values start from 1 for each transactional test case.
Test that an empty default database in settings does not raise an ImproperlyConfigured error when running a unit test that does not use a database.
If the test label is empty, discovery should happen on the current working directory.
When given a dotted path to a module, unittest discovery searches not just the module, but also the directory containing the module. This results in tests from adjacent modules being run when they should not. The discover runner avoids this behavior.
Tests shouldn't be discovered twice when discovering on overlapping paths.
Reverse should reorder tests while maintaining the grouping specified by ``DiscoverRunner.reorder_by``.
Regression for 12168: models split into subpackages still get M2M tables.
Regression for 12245 - Models can exist in the test package, too.
Media that can associated to any object.
A smoke test to ensure GET on the add_view works.
A smoke test to ensure GET on the change_view works.
Create a model with an attached Media object via GFK. We can't load content via a fixture (since the GenericForeignKey relies on content type IDs, which will vary depending on what other tests have been run), thus we do it here.
With one initial form, extra (default) at 3, there should be 4 forms.
With extra=0, there should be one form.
With extra=5 and max_num=2, there should be only 2 forms.
With extra=3 and min_num=2, there should be five forms.
Ensure that the custom ModelForm's `Meta.exclude` is respected when used in conjunction with `GenericInlineModelAdmin.readonly_fields` and when no `ModelAdmin.exclude` is defined.
Regression for #19733
Regression for #23451
24377 - If we're adding a new object, a parent's auto-generated pk from the model field default should be ignored as it's regenerated on the save request. Tests the case where both the parent and child have a UUID primary key.
24377 - Inlines with a model field default should ignore that default value to avoid triggering validation on empty forms.
24958 - Variant of test_inlineformset_factory_nulls_default_pks for the case of a parent object with a UUID primary key and a child object with an AutoField primary key.
24958 - Variant of test_inlineformset_factory_nulls_default_pks for the case of a parent object with an AutoField primary key and a child object with a UUID primary key.
24958 - Variant of test_inlineformset_factory_nulls_default_pks for the case of a parent object with a UUID primary key and a child object with an editable natural key for a primary key.
24958 - Variant of test_inlineformset_factory_nulls_default_pks for the case of a parent object with a UUID alternate key and a child object that relates to that alternate key.
inspectdb can inspect a subset of tables by passing the table names as arguments.
Call inspectdb and return a function to validate a field type in its output
Introspection of table names containing special characters, unsuitable for Python identifiers
Test that by default the command generates models with `Meta.managed = False` (#14305)
Introspection of columns with a custom field (21090)
Test that empty DATABASES setting default to the dummy backend.
Check that auto_increment fields are created with the AUTOINCREMENT keyword in order to be monotonically increasing. Refs 10164.
19360: Raise NotImplementedError when aggregating on date/time fields.
A named in-memory db should be allowed where supported.
Test PostgreSQL version parsing from `SELECT version()` output
Test the custom ``django_date_trunc method``, in particular against fields which clash with strings passed to it (e.g. 'year') - see 12818__. __: http://code.djangoproject.com/ticket/12818
Test the custom ``django_date_extract method``, in particular against fields which clash with strings passed to it (e.g. 'day') - see 12818__. __: http://code.djangoproject.com/ticket/12818
last_executed_query should not raise an exception even if no previous query has been run.
Test that last_executed_query() returns an Unicode string
Test creation of model with long name and long pk name doesn't error. Ref #8901
An m2m save of a model with a long name and a long m2m field name doesn't error (8901).
Test that DatabaseOperations initialization doesn't query the database. See 17656.
Test that creating an existing table returns a DatabaseError
Zero as id for AutoField should raise exception in MySQL, because MySQL does not allow zero for autoincrement primary key.
Test the format_number converter utility
Used in the tests for the database argument in signals (13552)
A router that sends all writes to the other database.
Regression test for #16039: migrate with --database option.
Regression test for #16039: migrate with --database option.
Vaguely behave like primary/replica, but the databases aren't assumed to propagate changes.
The default ordering should be by name, as specified in the inner Meta class.
Let's use a custom ModelAdmin that changes the ordering dynamically.
The default ordering should be by name, as specified in the inner Meta class.
Let's check with ordering set to something different than the default.
DB-API Shortcuts ``get_object_or_404()`` is a shortcut function to be used in view functions for performing a ``get()`` lookup and raising a ``Http404`` exception if a ``DoesNotExist`` exception was raised during the ``get()`` call. ``get_list_or_404()`` is a shortcut function to be used in view functions for performing a ``filter()`` lookup and raising a ``Http404`` exception if a ``DoesNotExist`` exception was raised during the ``filter()`` call.
Testing signals before/after saving and deleting.
Test that signals that disconnect when being called don't mess future dispatching.
Filtering on an aggregate annotation with Decimal values should work. Requires special handling on SQLite (18247).
Annotate *args ordering should be preserved in values_list results. **kwargs comes after *args. Regression test for 23659.
Check that splitting a q object to parts for where/having doesn't alter the original q-object.
Check that an F() object referring to related column works correctly in group by.
Regression 18333 - Ensure annotated column name is properly quoted.
HttpResponseBase.setdefault() should not change an existing header and should be case insensitive.
HttpResponse should parse charset from content_type.
HttpResponse should encode based on charset.
A DecimalField with decimal_places=0 should work (22272).
An object that migration doesn't know how to serialize.
A model that is in a migration-less app (which this app is if its migrations directory has not been repointed)
Applying a non-atomic migration works as expected.
An atomic operation is properly rolled back inside a non-atomic migration.
Really all we need is any object with a debug-useful repr.
Minimize unnecessary rollbacks in connected apps. When you say "./manage.py migrate appA 0001", rather than migrating to just after appA-0001 in the linearized migration plan (which could roll back migrations in other apps that depend on appA 0001, but don't need to be rolled back since we're not rolling back appA 0001), we migrate to just before appA-0002.
Minimize rollbacks when target has multiple in-app children. a: 1 <---- 3 <--\ \ \- 2 <--- 4 \       \ b:      \- 1 <--- 2
Make sure compiled regex can be serialized.
Ticket 22679: makemigrations generates invalid code for (an empty tuple) default_permissions = ()
Ticket 22436: You cannot use a function straight from its body (e.g. define the method and use it in the same body)
Neither py2 or py3 can serialize a reference in a local scope.
Make sure user is seeing which module/function is the issue
24155 - Tests ordering of imports.
django.db.models shouldn't be imported if unused.
When the default manager of the model is a custom manager, it needs to be added to the model state.
StateApps.bulk_update() should update apps.ready to False and reset the value afterwards.
24147 - Tests that managers refer to the correct version of a historical model
24483 - ProjectState.from_apps should not destructively consume Field.choices iterators.
Tests that rendering a model state doesn't alter its internal fields.
Tests making a ProjectState from an Apps with a swappable model
A router that doesn't have an opinion regarding migrating.
A router that doesn't allow migrating.
A router that always allows migrating.
A router that allows migrating depending on a hint.
Test when router doesn't have an opinion (i.e. CreateModel should run).
Test when router returns False (i.e. CreateModel shouldn't run).
Test when router returns True (i.e. CreateModel should run).
Test multiple routers.
A custom deconstructible object.
24537 - Tests that the order of fields in a model does not influence the RenameModel detection.
Field instances are handled correctly by nested deconstruction.
This is a wee bit crazy, but it's just to show that run_before works.
Makes a test state using set_up_test_model and returns the original state and the state after the migration is applied.
Column names that are SQL keywords shouldn't cause problems when used in migrations (22168).
Tests the RemoveField operation on a foreign key.
24098 - Tests no-op RunSQL operations.
24282 - Tests that model changes to a FK reverse side update the model on the FK side as well.
A model with BigAutoField can be created.
A field may be migrated from AutoField to BigAutoField.
24098 - Tests no-op RunPython operations.
Handy shortcut for getting results + number of loops
Tests that the optimizer does nothing on a single operation, and that it does it in just one pass.
CreateModel and DeleteModel should collapse into nothing.
CreateModel should absorb RenameModels.
RenameModels should absorb themselves.
CreateModel, AlterModelTable, AlterUniqueTogether/AlterIndexTogether/ AlterOrderWithRespectTo, and DeleteModel should collapse into nothing.
Two AlterUniqueTogether/AlterIndexTogether/AlterOrderWithRespectTo should collapse into the second.
AddField should optimize into CreateModel.
AddField should NOT optimize into CreateModel if it's an FK to a model that's between them.
AlterField should optimize into CreateModel.
RenameField should optimize into CreateModel.
RenameField should optimize into AddField
RenameField should optimize to the other side of AlterField, and into itself.
RemoveField should optimize into CreateModel.
AlterField should optimize into AddField.
RemoveField should cancel AddField
RemoveField should absorb AlterField
Checks that field-level through checking is working. This should manage to collapse model Foo to nonexistence, and model Bar to a single IntegerField called "width".
Makes sure the '__first__' migrations build correctly.
Undefined MIGRATION_MODULES implies default migration module.
MIGRATION_MODULES allows disabling of migrations for a particular app.
Makes sure that migrate exits if it detects a conflict.
Tests --plan output of showmigrations command without migrations
Tests --plan output of showmigrations command with squashed migrations.
Makes sure that sqlmigrate does something.
Transaction wrappers aren't shown for non-atomic migrations.
Running migrate with some migrations applied before their dependencies should not be allowed.
Makes sure that makemigrations exits if it detects a conflict.
Makes sure that makemigrations exits if in merge mode with no conflicts.
Makes sure that makemigrations exits if a non-existent app is specified.
Makes sure that makemigrations exits if no app is specified with 'empty' mode.
makemigrations raises a nice error when migrations are disabled for an app.
Makes sure that makemigrations exits when there are no changes and no apps are specified.
Makes sure that makemigrations exits when there are no changes to an app.
makemigrations should detect initial is needed on empty migration modules if no app provided.
Makes sure that makemigrations announces the migration at the default verbosity level.
Makes sure that makemigrations fails to merge migrations with no common ancestor.
Tests that non-interactive makemigrations fails when a default is missing on a new not-null field.
Tests that non-interactive makemigrations fails when a default is missing on a field changed to not-null.
Makes sure that makemigrations adds and removes a possible model rename in non-interactive mode.
Makes sure that makemigrations adds and removes a possible field rename in non-interactive mode.
Makes sure that makemigrations properly merges the conflicting migrations with --noinput.
Makes sure that makemigrations respects --dry-run option when fixing migration conflicts (24427).
Makes sure that makemigrations does not raise a CommandError when an unspecified app has conflicting migrations.
makemigrations --exit should exit with sys.exit(1) when there are no changes to an app.
makemigrations --check should exit with a non-zero status when there are changes to an app requiring migrations.
makemigrations should print the relative paths to the migrations unless they are outside of the current tree, in which case the absolute path should be shown.
makemigrations should raise InconsistentMigrationHistory exception if there are some migrations applied before their dependencies.
Tests that squashmigrations squashes migrations.
Tests that squashmigrations optimizes operations.
Makes sure that squashmigrations --no-optimize really doesn't optimize operations.
squashmigrations accepts a starting migration.
Tests for forwards/backwards_plan of nonexistent node.
Tests a complex dependency graph: app_a:        0001 <- \ app_b:        0001 <- x 0002 <- /               \ app_c:   0001<-  <------------- x 0002 And apply squashing on app_c.
Exception is raised when trying to register an abstract model. Refs 12004.
Tests the register decorator in admin.decorators For clarity: @register(Person) class AuthorAdmin(ModelAdmin): pass is functionally equal to (the way it is written in these tests): AuthorAdmin = register(Person)(AuthorAdmin)
Delete and recreate cache table with legacy behavior (explicitly specifying the table name).
A router that puts the cache table on the 'other' database.
#20613/#18541 -- Ensures pickling is done outside of the lock.
The default expiration time of a cache key is 5 minutes.This value is defined inside the __init__() method of the :class:`django.core.cache.backends.base.BaseCache` type.
Caches that have the TIMEOUT parameter undefined in the defaultsettings will use the default 5 minute timeout.
Memory caches that have the TIMEOUT parameter set to `None` in thedefault settings with have `None` as the default timeout. This means "no timeout".
Memory caches that have the TIMEOUT parameter unset will set cachekeys having the default 5 minute timeout.
Memory caches that have the TIMEOUT parameter set to `None` will seta non expiring key by default.
get_cache_key keys differ by fully-qualified URL instead of path
Attempting to retrieve the same alias should yield the same instance.
Requesting the same alias from separate threads should yield separate instances.
A simple Article model for testing
21846 -- Check that `NestedObjects.collect()` doesn't trip (AttributeError) on the special notation for relations on abstract models (related_name that contains %(app_label)s and/or %(class)s).
Regression test for 12654: lookup_field
Tests for label_for_field
Regression test for 13963
Regression test for 18051
LogEntry.action_time is a timestamp of the date when the entry was created. It shouldn't be updated on a subsequent save().
LogEntry.change_message is stored as a dumped JSON structure to be able to get the message dynamically translated at display time.
LogEntry.get_edited_object() returns the edited object of a LogEntry object.
LogEntry.get_admin_url returns a URL to edit the entry's object or None for non-existent (possibly deleted) models.
Test the send_robust() function
Test suite for receiver.
Regression tests for the resolve_url function.
Tests that passing a URL path to ``resolve_url`` will result in the same url.
Tests that passing a relative URL path to ``resolve_url`` will result in the same url.
Tests that passing a full URL to ``resolve_url`` will result in the same url.
Tests that passing a model to ``resolve_url`` will result in ``get_absolute_url`` being called on that model instance.
Tests that passing a view function to ``resolve_url`` will result in the URL path mapping to that view name.
Tests that passing the result of reverse_lazy is resolved to a real URL string.
Tests that passing a view name to ``resolve_url`` will result in the URL path mapping to that view.
Tests that passing a domain to ``resolve_url`` returns the same domain.
A simple concrete base class.
A simple abstract base class, to be used for error checking.
A proxy subclass, this should not get a new table. Overrides the default manager.
A class with the default manager from Person, plus an secondary manager.
The MyPerson model should be generating the same database queries as the Person model (when the same manager is used in each case).
The StatusPerson models should have its own table (it's using ORM-level inheritance).
Creating a Person makes them accessible through the MyPerson proxy.
Person is not proxied by StatusPerson subclass.
A new MyPerson also shows up as a standard Person.
Correct type when querying a proxy of proxy
Proxy models are included in the ancestors for a model's DoesNotExist and MultipleObjectsReturned
Test save signals for proxy models
Proxy objects can be deleted
We can still use `select_related()` to include related models in our querysets.
Test if admin gives warning about cascade deleting models referenced to concrete model by deleting proxy object.
Test that the backend's FOR UPDATE variant appears in generated SQL when select_for_update is invoked.
Test that the backend's FOR UPDATE NOWAIT variant appears in generated SQL when select_for_update is invoked.
If nowait is specified, we expect an error to be raised rather than blocking.
If a SELECT...FOR UPDATE NOWAIT is run on a database backend that supports FOR UPDATE but not NOWAIT, then we should find that a DatabaseError is raised.
Test that a TransactionManagementError is raised when a select_for_update query is executed outside of a transaction.
Test that no TransactionManagementError is raised when select_for_update is invoked outside of a transaction - only when the query is executed.
An argument of fields=() to fields_for_model should return an empty dictionary
No fields on a ModelForm should actually result in no fields.
No fields should be set on a model instance if construct_instance receives fields=().
A ModelForm with a model with a field set to blank=False and the form field set to required=False should allow the field to be unset.
ModelForm with an extra field
ModelForm with an extra field
Using 'fields' *and* 'exclude'. Not sure why you'd want to dothis, but uh, "be liberal in what you accept" and all.
Subclassing without specifying a Meta on the class will usethe parent's Meta (or the first parent in the MRO if there are multiple parent classes).
We can also subclass the Meta inner class to change the fieldslist.
A form that replaces the model's url field with a custom one. This should prevent the model field's validation from being called.
A form that replaces the model's url field with a custom one. This should prevent the model field's validation from being called.
ModelForm test of unique_together constraint
When the same field is involved in multiple unique_together constraints, we need to make sure we don't remove the data for it before doing all the validation checking (not just failing after the first one).
Test for primary_key being in the form and failing validation.
Ensure keys and blank character strings are tested for uniqueness.
Test that ModelMultipleChoiceField does O(1) queries instead of O(n) (10156).
Test that ModelMultipleChoiceField run given validators (14144).
Test support of show_hidden_initial by ModelMultipleChoiceField.
If the ``clean`` method on a non-required FileField receives False as the data (meaning clear the field value), it returns False, regardless of the value of ``initial``.
If the ``clean`` method on a required FileField receives False as the data, it has the same effect as None: initial is returned if non-empty, otherwise the validation catches the lack of a required value.
Integration happy-path test that a model FileField can actually be set and cleared via a ModelForm.
If the user submits a new file upload AND checks the clear checkbox, they get a validation error, and the bound redisplay of the form still includes the current file and the clear checkbox.
Regression test for 8842: FilePathField(blank=True)
If the http:// prefix is omitted on form input, the field adds it again. (Refs 13613)
<p><label for="id_name">Name:</label> <input id="id_name" type="text" name="name" maxlength="50" required /></p><p><label for="id_colours">Colours:</label> <select multiple="multiple" name="colours" id="id_colours" required> <option value="%(blue_pk)s">Blue</option> </select></p>
Regression test for 12960. Make sure the cleaned_data returned from ModelForm.clean() is applied to the model instance.
Regression test for https://code.djangoproject.com/ticket/22510.
A ForeignKey relation can use ``limit_choices_to`` as a callable, re 2554.
A ManyToMany relation can use ``limit_choices_to`` as a callable, re 2554.
Regression test for 23795: Make sure a custom field with a `queryset` attribute but no `limit_choices_to` still works.
Regression for #13095: Using base forms with widgets defined in Meta should not raise errors.
Regression for #19733
Regression for #19733
Test that a custom formfield_callback is used if provided
Expected __doc__
Tests that django decorators set certain attributes of the wrapped function.
Test that the user_passes_test decorator can be applied multiple times (9474).
Test that we can call cache_page the new way
Test for the require_safe decorator. A view returns either a response or an exception. Refs 15637.
@method_decorator can be used to decorate a class and its methods.
@method_decorator on a non-callable attribute raises an error.
@method_decorator on a nonexistent method raises an error.
Ensures @xframe_options_deny properly sets the X-Frame-Options header.
Ensures @xframe_options_sameorigin properly sets the X-Frame-Options header.
Callable defaults You can pass callable objects as the ``default`` parameter to a field. When the object is created without an explicit value passed in, Django will call the method to determine the default value. This example uses ``datetime.datetime.now`` as the default for the ``pub_date`` field.
Specifying ordering Specify default ordering for a model using the ``ordering`` attribute, which should be a list or tuple of field names. This tells Django how to order ``QuerySet`` results. If a field name in ``ordering`` starts with a hyphen, that field will be ordered in descending order. Otherwise, it'll be ordered in ascending order. The special-case field name ``"?"`` specifies random order. The ordering attribute is not required. If you leave it off, ordering will be undefined -- not random, just undefined.
Override ordering with order_by, which is in the same format as the ordering attribute in models.
Only the last order_by has any effect (since they each override any previous ordering).
Use the 'stop' part of slicing notation to limit the results.
Use the 'stop' and 'start' parts of slicing notation to offset the result list.
Use '?' to order randomly.
Ordering can be reversed using the reverse() method on a queryset. This allows you to extract things like "the last two items" (reverse and then take the first two).
Ordering can be based on fields included from an 'extra' clause
If the extra clause uses an SQL keyword for a name, it will be protected by quoting.
Ensure that 'pk' works as an ordering option in Meta. Refs 8291.
Ensure that ordering by a foreign key by its attribute name prevents the query from inheriting its related model ordering option. Refs 19195.
A column may only be included once (the first occurrence) so we check to ensure there are no duplicates by inspecting the SQL.
Regression test for bug 8106. Same sort of problem as the previous test, but this time there are more extra tables to pull in as part of the select_related() and some of them could potentially clash (so need to be kept separate).
Regression test for bug 8036 the first related model in the tests below ("state") is empty and we try to select the more remotely related state__country. The regression here was not skipping the empty column results for country before getting status.
Exercising select_related() with multi-table model inheritance.
Regression test for 1661 and 1662 Check that string form referencing of models works, both as pre and post reference, on all RelatedField types.
Regression tests for 5087 make sure we can perform queries on TextFields.
Test that GenericRelations on inherited classes use the correct content type.
Test that the correct column name is used for the primary key on the originating model of a query.  See 12664.
Test that ordering over a generic relation does not include extraneous duplicate results, nor excludes rows not participating in the relation.
Test for #13085 -- __len__() returns 0
A single car tire. This to test that a user can only select their own cars.
A model with a FK to a model that won't be registered with the admin (Honeycomb) so the corresponding raw ID widget won't have a magnifying glass link to select related honeycomb instances.
A model with a FK to itself. It won't be registered with the admin, so the corresponding raw ID widget won't have a magnifying glass link to select related instances (rendering will be called programmatically in this case).
A model with a m2m to a model that won't be registered with the admin (Company) so the corresponding raw ID widget won't have a magnifying glass link to select related company instances.
Test that widget instances in formfield_overrides are not shared between different fields. (19423)
Overriding the widget for DateTimeField doesn't overrides the default form_class for that field (26449).
formfield_overrides works for a custom field class.
m2m fields help text as it applies to admin app (#9321).
Ensure the user can only see their own cars in the foreign key dropdown.
File widgets should render as a link when they're marked "read only."
This is a basic model to test saving and loading boolean and date-related types, which in the past were problematic for some database backends.
Year boundary tests (ticket #3689)
Specifying 'choices' for a field Most fields take a ``choices`` parameter, which should be a tuple of tuples specifying which are the valid values for that field. For each field that has ``choices``, a model instance gets a ``get_fieldname_display()`` method, where ``fieldname`` is the name of the field. This method returns the "human-readable" value of the field.
Regression test for 20242 - QuerySet "in" didn't work the first time when using prefetch_related. This was fixed by the removal of chunked reads from QuerySet iteration in 70679243d1786e03557c28929f9762a119e3ac14.
Test that we can clear the behavior by calling prefetch_related()
Test we can follow an m2m relation after a relation like ForeignKey that doesn't have many objects
Test that we can follow a m2m relation after going through the select_related reverse of an o2o.
Helper method that returns a list containing a list of the objects in the obj_iter. Then for each object in the obj_iter, the path will be recursively travelled and the found objects are added to the return value.
Nested prefetch_related() shouldn't trigger duplicate queries for the same lookup. Before, prefetch queries were for 'addresses', 'first_time_authors', and 'first_time_authors__addresses'. The last query is the duplicate.
Many-to-one relationships that can be null To define a many-to-one relationship that can have a null foreign key, use ``ForeignKey()`` with ``null=True`` .
Test a custom Manager method.
The custom manager __init__() argument has been set.
Custom manager method is only available on the manager and not on querysets.
Queryset method doesn't override the custom manager method.
The related managers extend the default manager.
The default manager, "objects", doesn't exist, because a custom one was provided.
Custom managers respond to usual filtering methods
A custom manager may be defined on an abstract model. It will be inherited by the abstract model's children.
BaseManager.get_queryset() should use kwargs rather than args to allow custom kwargs (24911).
A session model with a column for an account ID.
A database session store, that handles updating the account ID column inside the custom session model.
Falsey values (Such as an empty string) are rejected.
Strings shorter than 8 characters are rejected.
Strings of length 8 and up are accepted and stored.
Test we can use Session.get_decoded to retrieve data stored in normal way
This test tested exists() in the other session backends, but that doesn't make sense for us.
This test tested cycle_key() which would create a new session key for the same session data. But we can't invalidate previously signed cookies (other than letting them expire naturally) so testing for this behavior is meaningless.
A simple view with a docstring.
Test that a view can't be accidentally instantiated before deployment
Test that a view can't be accidentally instantiated before deployment
The edge case of a http request that spoofs an existing method name is caught.
Test a view which only allows GET doesn't allow other methods.
Test a view which supplies a GET method also responds correctly to HEAD.
Test a view which supplies no GET method responds to HEAD with HTTP 405.
Test a view which only allows both GET and POST.
Test a view can only be called once.
Test that the callable returned from as_view() has proper docstring, name and module.
Test that attributes set by decorators on the dispatch method are also present on the closure.
Test that views respond to HTTP OPTIONS requests with an Allow header appropriate for the methods implemented by the view class.
Test that a view implementing GET allows GET and HEAD.
Test that a view implementing GET and POST allows GET, HEAD, and POST.
Test that a view implementing POST allows POST.
Test a view only has args, kwargs & request once `as_view` has been called.
It should be possible to use the view by directly instantiating it without going through .as_view() (21564).
Test a view that simply renders a template on GET
Test a TemplateView responds correctly to HEAD
Test a view that renders a template on GET with the template name as an attribute on the class.
Test a completely generic view that renders a template on GET with the template name as an argument at instantiation.
A template view must provide a template name.
A template view may provide a template engine.
A generic template view passes kwargs as context.
A template view can be customized to return extra context.
It should be possible to use the view without going through .as_view() (21564).
A ListView that doesn't use a model.
date_list should be sorted descending in index
date_list should be sorted ascending in year view
date_list should be sorted ascending in month view
Ensure that custom querysets are used when provided to BaseDateDetailView.get_object() Refs 16918.
Test instance independence of initial data dict (see #16138)
Test prefix can be set (see #18872)
A form can be marked invalid in the form_valid() method (25548).
AuthorCustomDetail overrides get() and ensures that SingleObjectMixin.get_context_object_name() always uses the obj parameter instead of self.object.
Existing flatpages can be edited in the admin form without triggering the url-uniqueness validation.
Relating an object to itself, many-to-one To define a many-to-one relationship between a model and itself, use ``ForeignKey('self', ...)``. In this example, a ``Category`` is related to itself. That is, each ``Category`` has a parent ``Category``. Set ``related_name`` to designate what the reverse relationship is called.
Tests that relations with intermediary tables with multiple FKs to the M2M's ``to`` model are possible.
dummy message-store to test the api methods
Tests that a complex nested data structure containing Message instances is properly encoded/decoded by the custom JSON encoder/decoder classes.
Tests that a message containing SafeData is keeping its safe status when retrieved from the message storage.
Sets the messages into the backend request's session and remove the backend's loaded data cache.
Return the storage totals from both cookie and session backends.
Adds 6 messages from different levels (including a custom one) to a storage instance.
Returns the storage backend, setting its loaded data to the ``data`` argument. This method avoids the storage ``_get`` method from getting called so that other parts of the storage backend can be tested independent of the message retrieval logic.
With the message middleware enabled, tests that messages are properly stored and then retrieved across the full request/redirect/response cycle.
Tests that messages persist properly when multiple POSTs are made before a GET.
Tests that, when the middleware is disabled, an exception is raised when one attempts to store a message.
Tests that, when the middleware is disabled, an exception is not raised if 'fail_silently' = True
Returns the number of messages being stored after a ``storage.update()`` call.
Tests for built in Function expressions.
If the truncated datetime transitions to a different offset (daylight saving) then the returned value will have that new timezone/offset.
Fixtures. Fixtures are a way of loading data into the database in bulk. Fixure data can be stored in any serializable format (including JSON and XML). Fixtures are identified by name, and are stored in either a directory named 'fixtures' in the application directory, or in one of the directories named in the ``FIXTURE_DIRS`` setting.
Make sure that subclasses can remove fixtures from parent class (21089).
A warning is displayed if a proxy model is dumped without its concrete parent.
A warning isn't displayed if a proxy model is dumped with its concrete parent.
Verifies that the --app option works.
If no fixtures match the loaddata command, constraints checks on the database shouldn't be disabled. This is performance critical on MSSQL.
Test receiving file upload when filename is encoded with RFC2231 (22971).
Test receiving file upload when filename is encoded with RFC2231 (22971).
Uploaded files may have content type parameters available.
If passed an incomplete multipart message, MultiPartParser does not attempt to read beyond the end of the stream, and simply will handle the part that can be parsed gracefully.
If passed an empty multipart message, MultiPartParser will return an empty QueryDict.
Permission errors are not swallowed
Simple view to echo back info about uploaded files for tests.
Simple view to echo back the content of uploaded files for tests.
Dynamically add in an upload handler.
You can't change handlers after reading FILES; this view shouldn't work.
Check the .getlist() function to ensure we receive the correct number of files.
Check adding the file to the database will preserve the filename case.
Simple view to echo back extra content-type parameters.
A handler that raises an exception.
Template assertions work when a template is rendered multiple times.
Test that specifying a full URL as assertRedirects expected_url still work as backwards compatible behavior until Django 2.0.
Checks that an assertion is raised if the form's non field errors doesn't contain the provided error.
URLconf is reverted to original value after modification in a TestCaseThis will not find a match as the default ROOT_URLCONF is empty.
response.context is not lost when view call another view.
Logout should work whether the user is logged in or not (#9978).
Logout should send user_logged_out signal if user was logged in.
Logout should send user_logged_out signal if custom user was logged in.
Logout should send signal even if user not authenticated.
Login should send user_logged_in signal on successful login.
Login shouldn't send signal if user wasn't logged in
Regression tests for 8551 and 17067: ensure that environment variables are set correctly in RequestFactory.
A view that takes a string argumentThe purpose of this view is to check that if a space is provided in the argument, the test framework unescapes the %20 before passing the value to the view.
A view that uses test client to call another view.
A simple cookie-based session storage implementation. The session key is actually the session data, pickled and encoded. This means that saving the session will change the session key.
Using SQL reserved names Need to use a reserved SQL name as a column name or table name? Need to include a hyphen in a column or table name? No problem. Django quotes names appropriately behind the scenes, so your database won't complain about reserved-name usage.
get_absolute_url() functions as a normal method.
ABSOLUTE_URL_OVERRIDES should override get_absolute_url().
ABSOLUTE_URL_OVERRIDES should work even if the model doesn't have a get_absolute_url() method.
Regression test for 1064 and 1506 Check that we create models via the m2m relation if the remote model has a OneToOneField.
Regression test for 7173 Check that the name of the cache for the reverse object is correct.
Regression test for 9968 filtering reverse one-to-one relations with primary_key=True was misbehaving. We test both (primary_key=True & False) cases here to prevent any reappearance of the problem.
Regression for 13839 and 17439. DoesNotExist on a reverse one-to-one relation is cached.
Regression for 13839 and 17439. The target of a one-to-one relation is always cached.
Regression for 13839 and 17439. The target of a one-to-one relation is always cached.
The intermediary table between two unmanaged models should not be created.
An abstract article Model so that we can create article models with and without a get_absolute_url method (for create_update generic views tests).
An Article class with a get_absolute_url defined.
Dummy index page
Ignores all the filtering done by its parent class.
The json_catalog returns the language catalog and settings as JSON.
The javascript_catalog shouldn't load the fallback language in the case that the current selected language is actually the one translated from, and hence missing translation files completely. This happens easily when you're translating from English to other languages and you've set settings.LANGUAGE_CODE to some other language than English.
Same as above for the json_catalog view. Here we also check for the expected JSON format.
Let's make sure that the fallback language is still working properly in cases where the selected language cannot be found.
Check if the Javascript i18n view returns an empty language catalog if the default language is non-English, the selected language is English and there is not 'en' translation available. See 13388, 3594 and 13726 for more details.
Same as above with the difference that there IS an 'en' translation available. The Javascript i18n view must return a NON empty language catalog with the proper English translations. See 13726 for more details.
Makes sure that the fallback language is still working properly in cases where the selected language cannot be found.
Non-BMP characters should not break the javascript_catalog (21725).
Check if the JavaScript i18n view returns a complete language catalog if the default language is en-us, the selected language has a translation available and a catalog composed by djangojs domain translations of multiple Python packages is requested. See 13388, 3594 and 13514 for more details.
Similar to above but with neither default or requested language being English.
Test that 404.html and 500.html templates are picked by their respective handler.
Content-Type of the default error responses is text/html. Refs 20822.
Return language code for a language which is not activated.
The set_language view can be used to change the session language. The user is redirected to the 'next' argument if provided.
The set_language view only redirects to the 'next' argument if it is "safe".
The set_language view redirects to the URL in the referer header when there isn't a "next" parameter.
The set_language view redirects to '/' when there isn't a referer or "next" parameter.
The set_language view redirects to the "next" parameter for AJAX calls.
The set_language view doesn't redirect to the HTTP referer header for AJAX calls.
The set_language view returns 204 for AJAX calls by default.
The fallback to root URL for the set_language view works for AJAX calls.
The json_catalog returns the language catalog and settings as JSON.
The javascript_catalog shouldn't load the fallback language in the case that the current selected language is actually the one translated from, and hence missing translation files completely. This happens easily when you're translating from English to other languages and you've set settings.LANGUAGE_CODE to some other language than English.
Same as above for the json_catalog view. Here we also check for the expected JSON format.
Let's make sure that the fallback language is still working properly in cases where the selected language cannot be found.
Check if the Javascript i18n view returns an empty language catalog if the default language is non-English, the selected language is English and there is not 'en' translation available. See 13388, 3594 and 13726 for more details.
Same as above with the difference that there IS an 'en' translation available. The Javascript i18n view must return a NON empty language catalog with the proper English translations. See 13726 for more details.
Makes sure that the fallback language is still working properly in cases where the selected language cannot be found.
Non-BMP characters should not break the javascript_catalog (21725).
Check if the JavaScript i18n view returns a complete language catalog if the default language is en-us, the selected language has a translation available and a catalog composed by djangojs domain translations of multiple Python packages is requested. See 13388, 3594 and 13514 for more details.
Similar to above but with neither default or requested language being English.
Handle bogus If-Modified-Since values gracefullyAssume that a file is modified since an invalid timestamp as per RFC 2616, section 14.25.
Handle even more bogus If-Modified-Since values gracefullyAssume that a file is modified since an invalid timestamp as per RFC 2616, section 14.25.
Test case to make sure the static URL pattern helper works as expected
Tests for not existing file
Make sure if you don't specify a template, the debug view doesn't blow up.
Make sure that the default URLconf template is shown shown instead of the technical 404 page, if the user has not altered their URLconf yet.
Regression test for bug 21530. If the admin app include is replaced with exactly one url pattern, then the technical 404 template should be displayed. The bug here was that an AttributeError caused a 500 response.
Ensure the debug view works when a database exception is raised by performing an invalid query and passing the exception to the debug view.
Test that the ExceptionReporter supports Unix, Windows and Macintosh EOL markers
importlib is not a frozen app, but its loader thinks it's frozen which results in an ImportError on Python 3. Refs 21443.
Don't trip over exceptions generated by crafted objects when evaluating them while cleansing (24455).
Ensure that everything (request info and frame variables) can bee seen in the default error reports for non-sensitive requests.
Ensure that sensitive POST parameters and frame variables cannot be seen in the default error reports for sensitive requests.
Ensure that no POST parameters and frame variables can be seen in the default error reports for "paranoid" requests.
21098 -- Ensure that sensitive POST parameters cannot be seen in the error reports for if request.POST['nonexistent_key'] throws an error.
Ensure that it's possible to assign an exception reporter filter to the request to bypass the one set in DEFAULT_EXCEPTION_REPORTER_FILTER.
Ensure that the sensitive_variables decorator works with object methods. Refs 18379.
Ensure that sensitive variables don't leak in the sensitive_variables decorator's frame, when those variables are passed as arguments to the decorated function. Refs 19453.
Ensure that sensitive variables don't leak in the sensitive_variables decorator's frame, when those variables are passed as keyword arguments to the decorated function. Refs 19453.
Callable settings should not be evaluated in the debug page (21345).
Callable settings which forbid to set attributes should not break the debug page (23070).
A dict setting containing a non-string key should not break the debug page (12744).
The debug page should not show some sensitive settings (password, secret key, ...).
The debug page should filter out some sensitive information found in dict settings.
Ensure that request info can bee seen in the default error reports for non-sensitive requests.
Ensure that sensitive POST parameters cannot be seen in the default error reports for sensitive requests.
Ensure that no POST parameters can be seen in the default error reports for "paranoid" requests.
Ensure that it's possible to assign an exception reporter filter to the request to bypass the one set in DEFAULT_EXCEPTION_REPORTER_FILTER.
Tests that a non-ASCII argument to HttpRedirect is handled properly.
Test that an invalid request is rejected with a localized error message.
Test that an invalid request is rejected with a localized error message.
Referer header is strictly checked for POST over HTTPS. Trigger the exception by sending an incorrect referer.
The CSRF cookie is checked for POST. Failure to send this cookie should provide a nice error message.
The CSRF view doesn't depend on the TEMPLATES configuration (24388).
A custom CSRF_FAILURE_TEMPLATE_NAME is used.
9005 -- url tag shouldn't require settings.SETTINGS_MODULE to be set.
19827 -- url tag should keep original strack trace when reraising exception.
Test that inclusion tag passes down `use_l10n` of context to the Context of the included/rendered template as well.
23441 -- InclusionNode shouldn't modify its nodelist at render time.
24555 -- InclusionNode should push and pop the render_context stack when rendering. Otherwise, leftover values such as blocks from extending can interfere with subsequent rendering.
Checks whether index of error is calculated correctly in template debugger in for loops. Refs ticket 5831
Mixed named and unnamed endblocks
Test one parameter given to ifchanged.
Test multiple parameters to ifchanged.
Test a date+hour like construct, where the hour of the last day is the same but the date had changed, so print the hour anyway.
Logically the same as above, just written with explicit ifchanged for the day.
Test the else clause of ifchanged.
Test whitespace in filter arguments
19890. The content of ifchanged template tag was rendered twice.
The "safe" and "escape" filters cannot work due to internal implementation details (fortunately, the (no)autoescape block tags can be used in those cases)
Literal strings are safe.
Iterating over strings outputs safe characters.
If evaluations are shortcircuited where possible
The is_bad() function should not be evaluated. If it is, an exception is raised.
Non-existent args
62.5 should round to 63 on Python 2 and 62 on Python 3 See http://docs.python.org/py3k/whatsnew/3.0.html
simple translation of a string delimited by '
simple translation of a string delimited by "
simple translation of a variable
simple translation of a variable and filter
simple translation of a variable and filter
simple translation of a string with interpolation
simple translation of a string to german
translation of singular form
translation of singular form
translation of plural form
translation of plural form
simple non-translation (only marking) of a string to german
translation of a variable with a translated filter
translation of a variable with a non-translated filter
usage of the get_available_languages tag
translation of constant strings
translation of constant strings
translation of constant strings
translation of constant strings
Escaping inside blocktrans and trans works as if it was directly in the template.
5972 - Use filters with the {% trans %} tag
translation of plural form with extra field in singular form (13568)
translation of singular form in russian (14126)
simple translation of multiple variables
Basic filter usage
Chained filters
Allow spaces before the filter pipe
Allow spaces after the filter pipe
Raise TemplateSyntaxError for a nonexistent filter
Raise TemplateSyntaxError when trying to access a filter containing an illegal character
Raise TemplateSyntaxError for invalid block tags
Raise TemplateSyntaxError for empty block tags
Raise TemplateSyntaxError for empty block tags in templates with multiple lines.
Chained filters, with an argument to the first one
Literal string as argument is always "safe" from auto-escaping.
Variable as argument
Default argument testing
Fail silently for methods that raise an exception with a `silent_variable_failure` attribute
In methods that raise an exception without a `silent_variable_attribute` set to True, the exception propagates
Escaped backslash in argument
Escaped backslash using known escape char
Empty strings can be passed as arguments to filters
Make sure that any unicode strings are converted to bytestrings in the final output.
Numbers as filter arguments should work
Filters should accept empty string constants
Fail silently for non-callable attribute and dict lookups which raise an exception with a "silent_variable_failure" attribute
Fail silently for non-callable attribute and dict lookups which raise an exception with a `silent_variable_failure` attribute
In attribute and dict lookups that raise an unexpected exception without a `silent_variable_attribute` set to True, the exception propagates
In attribute and dict lookups that raise an unexpected exception without a `silent_variable_attribute` set to True, the exception propagates
15092 - Also accept simple quotes
Raise exception for invalid template name
Raise exception for invalid variable template name
Raise exception for extra {% extends %} tags
Raise exception for custom tags used in child with {% load %} tag in parent, not in child
Test for silent failure when target variable isn't found
Regression tests for 17675 The date template filter has expects_localtime = True
Standard template with no inheritance
Standard two-level inheritance
Three-level with no redefinitions on third level
Two-level with no redefinitions on second level
Two-level with double quotes instead of single quotes
Three-level with variable parent-template name
Two-level with one block defined, one block not defined
Three-level with one block defined on this level, two blocks defined next level
Three-level with second and third levels blank
Three-level with space NOT in a block -- should be ignored
Three-level with both blocks defined on this level, but none on second level
Three-level with this level providing one and second level providing the other
Three-level with this level overriding second level
A block defined only in a child template shouldn't be displayed
A block within another block
A block within another block (level 2)
{% load %} tag (parent -- setup for exception04)
{% load %} tag (standard usage, without inheritance)
{% load %} tag (within a child template)
Two-level inheritance with {{ block.super }}
Three-level inheritance with {{ block.super }} from parent
Three-level inheritance with {{ block.super }} from grandparent
Three-level inheritance with {{ block.super }} from parent and grandparent
Inheritance from local context without use of template loader
Inheritance from local context with variable parent template
Set up a base template to extend
Inheritance from a template that doesn't have any blocks
Set up a base template with a space in it.
Inheritance from a template with a space in its name should work.
Base template, putting block in a conditional {% if %} tag
Inherit from a template with block wrapped in an {% if %} tag (in parent), still gets overridden
Inherit from a template with block wrapped in an {% if %} tag (in parent), still gets overridden
Base template, putting block in a {% for %} tag
Inherit from a template with block wrapped in an {% for %} tag (in parent), still gets overridden
Numpy's array-index syntax allows a template to access a certain item of a subscriptable object.
Hello, boys. How are you gentlemen.
List-index syntax allows a template to access a certain item of a subscriptable object.
Fail silently when the list index is out of range.
Fail silently when the list index is out of range.
Fail silently when variable is a dict without the specified key.
Dictionary lookup wins out when dict's key is a string.
But list-index lookup wins out when dict's key is an int, which behind the scenes is really a dictionary lookup (for a dict) after converting the key to an int.
Let's just make sure setup runs cases in the right order.
Ensure that a single loopvar doesn't truncate the list in val.
19882
Plain text should go through the template parser untouched.
Variables should be replaced with their value in the current context
More than one replacement variable is allowed in a template
Fail silently when a variable is not found in the current context
A variable may not contain more than one word
Raise TemplateSyntaxError for empty variable tags.
Raise TemplateSyntaxError for empty variable tags.
Attribute syntax allows a template to call an object's attribute
Multiple levels of attribute access are allowed.
Fail silently when a variable's attribute isn't found.
Attribute syntax allows a template to call a dictionary key's value.
Fail silently when a variable's dictionary key isn't found.
Fail silently when accessing a non-simple method
Treat "moo } {{ cow" as the variable. Not ideal, but costly to work around, so this triggers an error.
Call methods in the top level of the context.
Call methods returned from dictionary lookups.
Regression test for 7460.
Regression test for 11270.
Test whitespace in filter arguments
When a cache called "template_fragments" is present, the cache tag will use it in preference to 'default'
Tests that the correct template is identified as not existing when {% include %} specifies a template that does not exist.
12787 -- Tests that the correct template is identified as not existing when {% extends %} specifies a template that does exist, but that template has an {% include %} of something that does not exist.
Support any render() supporting object
16417 -- Include tags pointing to missing templates should not raise an error at parsing time.
A noop filter that always return its first argument and does nothing withits second (optional) one. Useful for testing out whitespace in filter arguments (see 19882).
Expected no_params __doc__
Expected one_param __doc__
Expected explicit_no_context __doc__
Expected no_params_with_context __doc__
Expected params_and_context __doc__
Expected simple_two_params __doc__
Expected simple_one_default __doc__
Expected simple_unlimited_args __doc__
Expected simple_only_unlimited_args __doc__
Expected simple_tag_without_context_parameter __doc__
A tag that doesn't even think about escaping issues
A tag that uses escape explicitly
A tag that uses format_html
Expected assignment_no_params __doc__
Expected inclusion_no_params __doc__
Expected inclusion_no_params_from_template __doc__
Expected inclusion_one_param __doc__
Expected inclusion_one_param_from_template __doc__
Expected inclusion_explicit_no_context __doc__
Expected inclusion_explicit_no_context_from_template __doc__
Expected inclusion_no_params_with_context __doc__
Expected inclusion_no_params_with_context_from_template __doc__
Expected inclusion_params_and_context __doc__
Expected inclusion_params_and_context_from_template __doc__
Expected inclusion_two_params __doc__
Expected inclusion_two_params_from_template __doc__
Expected inclusion_one_default __doc__
Expected inclusion_one_default_from_template __doc__
Expected inclusion_unlimited_args __doc__
Expected inclusion_unlimited_args_from_template __doc__
Expected inclusion_only_unlimited_args __doc__
Expected inclusion_only_unlimited_args_from_template __doc__
Expected inclusion_tag_use_l10n __doc__
Expected inclusion_tag_without_context_parameter __doc__
7027 -- _() syntax should work with spaces
Catch if a template extends itself and no other matching templates are found.
Extending should continue even if two loaders return the same name for a template.
With template debugging disabled, the raw TemplateDoesNotExist class should be cached when a template is missing. See ticket 26306 and docstrings in the cached loader for details.
With template debugging enabled, a TemplateDoesNotExist instance should be cached when a template is missing.
When a TemplateDoesNotExist instance is cached, the cached instance should not contain the __traceback__, __context__, or __cause__ attributes that Python sets when raising exceptions.
19949 -- TemplateDoesNotExist exceptions should be cached.
26536 -- A leading dash in a template name shouldn't be stripped from its cache key.
26603 -- A template name specified as a lazy string should be forced to text before computing its cache key.
Creates a mock egg with a list of resources. name: The name of the module. resources: A dictionary of template names mapped to file-like objects.
Template loading fails if the template is not in the egg.
Template loading fails if the egg is not in INSTALLED_APPS.
Invalid UTF-8 encoding in bytestrings should raise a useful error
The contents in "linebreaks" are escaped according to the current autoescape setting.
Tests for 11687 and 16676
Ensure iriencode keeps safe strings.
Filters decorated with stringfilter still respect is_safe.
The "upper" filter messes up entities (which are case-sensitive), so it's not safe for non-escaping purposes.
Without arg and when USE_L10N is True, the active language's DATE_FORMAT is used.
Notice that escaping is applied *after* any filters, so the string formatting here only needs to deal with pre-escaped characters.
Running slugify on a pre-escaped string leads to odd behavior, but the result is still safe.
Since dictsort uses template.Variable under the hood, it can sort on keys like 'foo.bar'.
The contents of "linenumbers" is escaped according to the current autoescape setting.
The make_list filter can destroy existing escaping, so the results are escaped.
9655 - Check urlize doesn't overquote already quoted urls. The teststring is the urlquoted version of 'http://hi.baidu.com/重新开始'
11911 - Check urlize keeps balanced parentheses
12183 - Check urlize adds nofollow properly - see 12183
13704 - Check urlize handles IDN correctly
16395 - Check urlize doesn't highlight malformed URIs
16656 - Check urlize accepts more TLDs
17592 - Check urlize don't crash on invalid email with dot-starting domain
18071 - Check urlize accepts uppercased URL schemes
18644 - Check urlize trims trailing period when followed by parenthesis
19070 - Check urlize handles brackets properly
20364 - Check urlize correctly include quotation marks in links
20364 - Check urlize copes with commas following URLs in quotes
23715 - Check urlize correctly handles exclamation marks after TLDs or query string
Check that we're not converting to scientific notation.
15789
20693: Timezone support for the time template filter
Literal string arguments to the default filter are always treated as safe strings, regardless of the auto-escaping state. Note: we have to use {"a": ""} here, otherwise the invalid template variable string interferes with the test result.
The contents in "linebreaksbr" are escaped according to the current autoescape setting.
17778 -- Variable shouldn't resolve RequestContext methods
Context.push() with a Context argument should work.
24273 -- Copy twice shouldn't raise an exception
15721 -- ``{% include %}`` and ``RequestContext`` should work together.
21460 -- Check that the order of template loader works.
To test postgres-specific general aggregation functions
To test postgres-specific aggregation functions for statistics
Check that unaccent can be used chained with a lookup (which should be the case since unaccent implements the Transform API)
Test PostgreSQL full text search. These tests use dialogue from the 1975 film Monty Python and the Holy Grail. All text copyright Python (Monty) Pictures. Thanks to sacred-texts.com for the transcript.
Indirection layer for PostgreSQL-specific fields, so the tests don't fail when run with a backend other than PostgreSQL.
Model.delete() should return the number of deleted rows and a dictionary with the number of deletions for each object type.
Tests for F() query expression syntax.
Make sure F objects can be deepcopied (23492)
Test that special characters (e.g. %, _ and \) stored in database are properly escaped when using a pattern lookup with an expression refs 16731
Test that special characters (e.g. %, _ and \) stored in database are properly escaped when using a case insensitive pattern lookup with an expression -- refs 16731
We can fill a value in all objects with an other value of the same object.
We can increment a value of all objects in a query set.
We can filter for objects, where a value is not equals the value of an other field.
Complex expressions of different connection types are possible.
Tests for bug #15672 ('request' referenced before assignment)
You can initialize a model instance using positional arguments, which should match the field order as defined in the model.
You can leave off the value for an AutoField when creating an object, because it'll get filled in automatically when you save().
as much precision in *seconds*
Field instances have a `__lt__` comparison function to define an ordering based on their creation. Prior to 17851 this ordering comparison relied on the now unsupported `__cmp__` and was assuming compared objects were both Field instances raising `AttributeError` when it should have returned `NotImplemented`.
This test ensures that the correct set of methods from `QuerySet` are copied onto `Manager`. It's particularly useful to prevent accidentally leaking new methods into `Manager`. New `QuerySet` methods that should also be copied onto `Manager` will need to be added to `ManagerTest.QUERYSET_PROXY_METHODS`.
Verify that ``get_wsgi_application`` returns a functioning WSGI callable.
Verify that FileResponse uses wsgi.file_wrapper.
If ``WSGI_APPLICATION`` is a dotted path, the referenced object is returned.
This lookup is used to test lookup registration.
Allow html in output on i18n strings
23340 -- Verify the documented behavior of humanize.naturaltime.
Check that :param message: has all :param headers: headers. :param message: can be an instance of an email.Message subclass or a string with the contents of an email message. :param headers: should be a set of (header-name, header-value) tuples.
Specifying dates or message-ids in the extra headers overrides the default values (9233)
Make sure we can manually set the From header (9214)
Specifying 'Reply-To' in headers should override reply_to.
Regression for 13259 - Make sure that headers are not changed when calling EmailMessage.message()
Regression for 11144 - When a to/from/cc header contains unicode, make sure the email addresses are parsed correctly (especially with regards to commas)
Make sure headers can be set with a different encoding than utf-8 in SafeMIMEMultipart as well
Regression test for #9367
Make sure that dummy backends returns correct number of sent messages
Make sure that get_connection() accepts arbitrary keyword that might be used with custom backends.
Test custom backend defined in this suite.
Test backend argument of mail.get_connection()
Tests for 12422 -- Django smarts (2472/11212) with charset of utf-8 text parts shouldn't pollute global email Python package charset registry when django.mail.message is imported.
Email line length is limited to 998 chars by the RFC: https://tools.ietf.org/html/rfc5322section-2.1.1 Message body containing longer lines are converted to Quoted-Printable to avoid having to insert newlines, which could be hairy to do properly.
Test send_mail without the html_message regression test for adding html_message parameter to send_mail()
Test html_message argument to send_mail
Test html_message argument to mail_managers
Test html_message argument to mail_admins
String prefix + lazy translated subject = bad output Regression for 13494
Test that mail_admins/mail_managers doesn't connect to the mail server if there are no recipients (9383)
Regression test for 7722
Regression test for 14301
Regression test for 15042
Email sending should support lazy email addresses (24416).
Test that connection can be closed (even when not explicitly opened)
Test that the connection can be used as a contextmanager.
Make sure that the locmen backend populates the outbox.
Make sure opening a connection creates a new file
Test that the console backend can be pointed at an arbitrary stream.
Test that opening the backend with non empty username/password tries to authenticate against the SMTP server.
Test that open() tells us whether it opened a connection.
Test that the connection's timeout value is None by default.
Test that the timeout parameter can be customized.
The mail is sent with the correct subject and recipient.
The mail may be sent with multiple recipients.
The mail should be sent to the email addresses specified in settings.MANAGERS.
The mail should be sent to the email addresses specified in settings.ADMIN.
Fake model not defining ``get_absolute_url`` for ContentTypesTests.test_shortcut_view_without_get_absolute_url()
Fake model defining ``get_absolute_url`` for ContentTypesTests.test_shortcut_view().
Fake model defining a ``get_absolute_url`` method containing an error
An ordered tag on an item.
Can view a shortcut when object's get_absolute_url returns a full URL the tested URLs are: "http://...", "https://..." and "//..."
The shortcut view works if a model's ForeignKey to site is None.
interactive mode of update_contenttypes() (the default) should delete stale contenttypes.
non-interactive mode of update_contenttypes() shouldn't delete stale content types.
24075 - A ContentType shouldn't be created or deleted if the model isn't available.
Test that, when using multiple databases, we use the db_for_read (see 20401).
Check that the shortcut view (used for the admin "view on site" functionality) returns a complete URL regardless of whether the sites framework is installed
Check that the shortcut view (used for the admin "view on site" functionality) returns 404 when get_absolute_url is not defined.
Check that the shortcut view does not catch an AttributeError raised by the model's get_absolute_url method. Refs 8997.
Get the full name of the person
23601 - Ensure the view exists in the URLconf.
``django.contrib.admindocs.utils.parse_rst`` should use ``cmsreference`` as the default role.
Django shouldn't break the default role for interpreted text when ``publish_parts`` is used directly, by setting it to ``cmsreference``. See 6681.
Methods that begin with strings defined in ``django.contrib.admindocs.views.MODEL_METHODS_EXCLUDE`` should not get displayed in the admin docs.
Methods that take arguments should also displayed.
Methods with arguments should have their arguments displayed.
Methods with keyword arguments should have their arguments displayed.
Methods with multiple arguments should have all their arguments displayed, but omitting 'self'.
We should be able to get a basic idea of the type returned by a method
A model with ``related_name`` of `+` should not show backward relationship links in admin docs
This __doc__ output is required for testing. I copied this example from `admindocs` documentation. (TITLE) Display an individual :model:`myapp.MyModel`. **Context** ``RequestContext`` ``mymodel`` An instance of :model:`myapp.MyModel`. **Template:** :template:`myapp/my_template.html` (DESCRIPTION) some_metadata: some data
OR lookups To perform an OR lookup, or a lookup that combines ANDs and ORs, combine ``QuerySet`` objects using ``&`` and ``|`` operators. Alternatively, use positional arguments, and pass one or more expressions of clauses using the variable ``django.db.models.Q`` (or any object with an ``add_to_query`` method).
Custom column/table names If your database column name is different than your model attribute, use the ``db_column`` parameter. Note that you'll use the field's name, not its column name, in API usage. If your database table name is different than your model name, use the ``db_table`` Meta attribute. This has no effect on the API used to query the database. If you need to use a table name for a many-to-many relationship that differs from the default generated name, use the ``db_table`` parameter on the ``ManyToManyField``. This has no effect on the API for querying the database.
Auto-created many-to-many through tables referencing a parent model are correctly found by the delete cascade when a child of that parent is deleted. Refs 14896.
Cascade deletion works with ForeignKey.to_field set to non-PK.
Test the {% timezone %} templatetag.
Test the {% get_current_timezone %} templatetag.
Test the {% get_current_timezone %} templatetag with pytz.
Test the django.template.context_processors.tz template context processor.
A property that simply returns the name. Used to test 24461
A simple article to test admin views. Test backwards compatibility.
A simple book that has chapters.
A simple persona associated with accounts, to test inlining of related accounts which inherit from a common accounts class.
A simple, generic account encapsulating the information shared by all types of accounts.
A service-specific account of type Foo.
A service-specific account of type Bar.
Secret! Not registered with the admin!
Secret! Not registered with the admin!
Model without any defined `Meta.ordering`. Refs 16819.
Model whose show_delete in admin change_view has been disabled Refs 10057.
Issue 20522 Model where the validation of child foreign-key relationships depends on validation of the parent
Issue 20522 Model that depends on validation of the parent class for one of its fields to validate during clean
Return a list of AdminFields for the AdminForm in the response.
Return the readonly fields for the response's AdminForm.
Return the readonly field for the given field_name.
Testing utility asserting that text1 appears before text2 in response content.
If you leave off the trailing slash, app should redirect and add it.
Test that admin_static.static is pointing to the collectstatic version (as django.contrib.collectstatic is in installed apps).
A smoke test to ensure GET on the add_view works.
A smoke test to ensure GET on the change_view works.
Ensure GET on the change_view works (returns an HTTP 404 error, see 11191) when passing a string as the PK argument for a model with an integer PK field.
The change URL changed in Django 1.9, but the old one still redirects.
Ensure GET on the change_view works on inherited models (returns an HTTP 404 error, see 19951) when passing a string as the PK argument for a model with an integer PK field.
Ensure http response from a popup is properly escaped.
Ensure we can sort on a list_display field that is a callable (column 2 is callable_year in ArticleAdmin)
Ensure we can sort on a list_display field that is a Model method (column 3 is 'model_year' in ArticleAdmin)
Ensure we can sort on a list_display field that is a ModelAdmin method (column 4 is 'modeladmin_year' in ArticleAdmin)
If no ordering is defined in `ModelAdmin.ordering` or in the query string, then the underlying order of the queryset should not be changed, even if it is defined in `Modeladmin.get_queryset()`. Refs 11868, 7309.
Ensure admin changelist filters do not contain objects excluded via limit_choices_to.This also tests relation-spanning filters (e.g. 'color__value').
Ensure is_null is handled correctly.
Ensures the admin changelist shows correct values in the relevant column for rows corresponding to instances of a model in which a named group has been used in the choices option of a field.
Ensures the filter UI shows correctly when at least one named group has been used in the choices option of a model field.
Check if the JavaScript i18n view returns an empty language catalog if the default language is non-English but the selected language is English. See 13388 and 3594 for more details.
Makes sure that the fallback language is still working properly in cases where the selected language cannot be found.
Check if L10N is deactivated, the JavaScript i18n view doesn't return localized date/time formats. Refs 14824.
Regression test for ticket 20664 - ensure the pk is properly quoted.
Tests if the "change password" link in the admin is hidden if the User does not have a usable password set. (against 9bea85795705d015cdadc82c68b99196a8554f5c)
Ensured that the 'show_delete' context variable in the admin's change view actually controls the display of the delete button. Refs 10057.
Ensure that AttributeErrors are allowed to bubble when raised inside a change list view. Requires a model to be created so there's something to be displayed Refs: 16655, 18593, and 18747
21056 -- URL reversing shouldn't work for nonexistent apps.
13749 - Admin should display link to front-end site 'View site'
Ensure that the admin/change_form.html template uses block.super in the bodyclass block.
Ensure that the auth/user/change_password.html template uses block super in the bodyclass block.
Ensure that the admin/index.html template uses block.super in the bodyclass block.
Ensure that the admin/change_list.html' template uses block.super in the bodyclass block.
Ensure that the admin/login.html template uses block.super in the bodyclass block.
Ensure that the admin/delete_confirmation.html template uses block.super in the bodyclass block.
Ensure that the admin/delete_selected_confirmation.html template uses block.super in bodyclass block.
Ensure that one can use a custom template to render an admin filter. Refs 17515.
Tests whether change_view has form_url in response.context
Ensure that the minified versions of the JS files are only used when DEBUG is False. Refs 17521.
Ensure save as actually creates a new person
Saving a new object using "Save as new" redirects to the changelist instead of the change view when ModelAdmin.save_as_continue=False.
Ensure that when you click "Save as new" and have a validation error, you only see the "Save as new" button and not the other save buttons, and that only the "Save as" button is visible.
Return the permission object, for the Model
Login redirect should be to the admin index page when going directly to /admin/login/.
Ensure that has_module_permission() returns True for all users who have any permission for that module (add, change, or delete), so that the module is displayed on the admin index page.
Ensure that overriding has_module_permission() has the desired effect. In this case, it always returns False, so the module should not be displayed on the admin index page for any users.
Objects should be nested to display the relationships that cause them to be scheduled for deletion.
Cyclic relationships should still cause each object to only be listed once.
A POST request to delete protected objects should display the page which says the deletion is prohibited.
should_contain = <li>Secret hideout: underground bunker
If a deleted object has two relationships from another model, both of those should be followed in looking for related objects to delete.
If a deleted object has two relationships pointing to it from another object, the other object should still only be listed once.
In the case of an inherited model, if either the child or parent-model instance is deleted, both instances are listed for deletion, as well as any relationships they have.
If a deleted object has GenericForeignKeys pointing to it, those objects should be listed for deletion.
If a deleted object has GenericForeignKey with GenericRelation(related_query_name='...') pointing to it, those objects should be listed for deletion.
-_.!~*'() ;/?:@&=+$, <>#%" {}|\^[]`
Retrieving the history for an object using urlencoded form of primary key should work. Refs 12349, 18550.
should_contain = <a href="%s">%s</a> % (escape(link), escape(self.pk))
should_contain = <h1>Change model with string primary key</h1>
should_contain = <h1>Change model with string primary key</h1>
should_contain = <h1>Change model with string primary key</h1>
Object history button link should work and contain the pk value quoted.
Ensure that we see the admin login form.
Ensure that staff_member_required decorator works with an argument (redirect_field_name).
Ensure that the delete_view handles non-ASCII characters
Ensure that pagination works for list_editable items. Refs 16819.
Fields should not be list-editable in popups.
Ensure that the to_field GET parameter is preserved when a search is performed. Refs 10918.
<span class="small quiet">1 result (<a href="?">3 total</a>)</span>,
<span class="small quiet">1 result (<a href="?">Show all</a>)</span>,
Test that actions which don't return an HttpResponse are redirected to the same page, retaining the querystring (which may contain changelist information).
Tests a custom action that returns a StreamingHttpResponse.
Tests a custom action that returns an HttpResponse with 403 code.
msg = Items must be selected in order to perform actions on them. No items have been changed.
msg = No action selected.
Check if the selection counter is there.
Actions should not be shown in popups.
Success on popups shall be rendered from template in order to allow easy customization.
InlineModelAdmin broken?
Ensure that custom querysets are considered for the admin history view. Refs 21013.
Test that inline file uploads correctly display prior data (10002).
Test that the 'collapse' class in fieldsets definition allows to show/hide the appropriate field section.
Regression test for 17911.
Regression test for 16433 - backwards references for related objects broke if the related field is read-only due to the help_text attribute
Regression test for 22087 - ModelForm Meta overrides are ignored by AdminReadonlyField
Make sure that non-field readonly elements are properly autoescaped (24461)
Should be able to use a ModelAdmin method in list_display that has the same name as a reverse model field ("sketch" in this case).
User addition through a FK popup should return the appropriate JavaScript response.
User change through a FK popup should return the appropriate JavaScript response.
User deletion through a FK popup should return the appropriate JavaScript response.
Ensure app and model tag are correctly read by change_form template
Ensure app and model tag are correctly read by change_list template
Ensure app and model tag are correctly read by delete_confirmation template
Ensure app and model tag are correctly read by app_index template
Ensure app and model tag are correctly read by delete_selected_confirmation template
Cells of the change list table should contain the field name in their class attribute Refs 11195.
Ensure that the year is not localized withUSE_THOUSAND_SEPARATOR. Refs 15234.
Ensure that no date hierarchy links display with empty changelist.
Ensure that single day-level date hierarchy appears for single object.
Ensure that day-level links appear for changelist within single month.
Ensure that one can easily customize the way related objects are saved. Refs 16115.
Assert that two URLs are equal despite the ordering of their querystring. Refs 22360.
#11277 -Labels of hidden fields in admin were not hidden.
23934 - When adding a new model instance in the admin, the 'obj' argument of get_formsets_with_inlines() should be None. When changing, it should be equal to the existing model instance. The GetFormsetsArgumentCheckingAdmin ModelAdmin throws an exception if obj is not None during add_view or obj is None during change_view.
Only allow changing objects with even id number
Tests various hooks for using custom templates and contexts.
A ModelAdmin with a custom get_queryset() method that uses defer(), to test verbose_name display in messages shown after adding/editing CoverLetter instances. Note that the CoverLetter model defines a __str__ method. For testing fix for ticket 14529.
A ModelAdmin with a custom get_queryset() method that uses only(), to test verbose_name display in messages shown after adding/editing Paper instances. For testing fix for ticket 14529.
A ModelAdmin with a custom get_queryset() method that uses defer(), to test verbose_name display in messages shown after adding/editing ShortMessage instances. For testing fix for ticket 14529.
A ModelAdmin with a custom get_queryset() method that uses only(), to test verbose_name display in messages shown after adding/editing Telegram instances. Note that the Telegram model defines a __str__ method. For testing fix for ticket 14529.
Issue 20522 Form to test child dependency on parent object's validation
A custom AdminSite for AdminViewPermissionsTest.test_login_has_permission().
Test if the 'isPermaLink' attribute of <guid> element of an item in the RSS feed is 'false'.
Test if the 'isPermaLink' attribute of <guid> element of an item in the RSS feed is 'true'.
Test the structure and content of feeds generated by Atom1Feed.
Test that the published and updated elements are not the same and now adhere to RFC 4287.
Tests that titles are escaped correctly in RSS feeds.
Test that datetimes with timezones don't get trodden on.
Tests the Last-Modified header with naive publication dates.
Test that the feed_url can be overridden.
Test URLs are prefixed with https:// when feed is requested over HTTPS.
Test that an ImproperlyConfigured is raised if no link could be found for the item(s).
Test that the item title and description can be overridden with templates.
Test that custom context data can be passed to templates for title and description.
Test add_domain() prefixes domains onto the correct URLs.
A feed where the latest entry date is an `updated` element.
A feed to test no link being defined. Articles have no get_absolute_url() method, and item_link() is not defined.
A feed to test that RSS feeds work with a single enclosure.
A feed to test that RSS feeds raise an exception with multiple enclosures.
A feed to test custom context data in templates for title or description.
A feed with naive (non-timezone-aware) dates.
Test of a custom feed generator class.
A feed to test that Atom feeds work with a single enclosure.
A feed to test that Atom feeds work with multiple enclosures.
Model without any defined `Meta.ordering`. Refs 17198.
Model with Manager that defines a default order. Refs 17198.
Regression test for 10348: ChangeList.get_queryset() shouldn't overwrite a custom select_related provided by ModelAdmin.get_queryset().
Regression test for 14982: EMPTY_CHANGELIST_VALUE should be honored for relationship fields
Test that empty value display can be set in ModelAdmin or individual fields.
Verifies that inclusion tag result_list generates a table when with default ModelAdmin settings.
Regression test for 13196: output of functions should be  localized in the changelist.
Regression tests for 16257: dynamic list_display_links support.
#15185 -- Allow no links from the 'change list' view grid.
The {% get_admin_log %} tag should work without specifying a user.
The Last-Modified header should be support dates (without time).
The Last-Modified header should be converted from timezone aware dates to GMT.
The Last-Modified header is omitted when lastmod isn't found in all sitemaps. Test sitemaps are sorted by lastmod in ascending order.
The Last-Modified header is omitted when lastmod isn't found in all sitemaps. Test sitemaps are sorted by lastmod in descending order.
The Last-Modified header is set to the most recent sitemap lastmod. Test sitemaps are sorted by lastmod in ascending order.
The Last-Modified header is set to the most recent sitemap lastmod. Test sitemaps are sorted by lastmod in descending order.
Check we get ImproperlyConfigured if we don't pass a site object to Sitemap.get_urls and no Site objects exist
Check we get ImproperlyConfigured when we don't pass a site object to Sitemap.get_urls if Site objects exists, but the sites framework is not actually installed.
Check to make sure that the raw item is included with each Sitemap.get_url() url result.
Tests that data loaded in migrations is available if we set serialized_rollback = True on TransactionTestCase
Tests that data loaded in migrations is available on TestCase
Ensures that only one master registry can exist.
Tests when INSTALLED_APPS contains an incorrect app config.
Tests when INSTALLED_APPS contains a class that isn't an app config.
Tests when INSTALLED_APPS contains an app that doesn't exist, either directly or via an app config.
Tests when INSTALLED_APPS contains an entry that doesn't exist.
Tests apps.get_app_configs().
Tests apps.get_app_config().
Tests apps.is_installed().
App discovery should preserve stack traces. Regression test for 22920.
Tests that the models in the models.py file were loaded correctly.
apps.get_containing_app_config() should raise an exception if apps.apps_ready isn't True.
If subclass sets path as class attr, no module attributes needed.
If path set as class attr, overrides __path__ and __file__.
If single element in __path__, use it (in preference to __file__).
If there is no __path__ attr, use __file__.
If the __path__ attr is empty, use __file__ if set.
If the __path__ attr is length>1, use __file__ if set.
If there is no __path__ or __file__, raise ImproperlyConfigured.
If the __path__ attr is empty and there is no __file__, raise.
If the __path__ attr is length>1 and there is no __file__, raise.
If the __path__ attr contains duplicate paths and there is no __file__, they duplicates should be deduplicated (25246).
A Py3.3+ namespace package can be an app if it has only one path.
Ticket 13839: select_related() should NOT cache None for missing objects on a reverse 1-1 relation.
Ticket 13839: select_related() should NOT cache None for missing objects on a reverse 0-1 relation.
Rverse related fields should be listed in the validation message when an invalid field is given in select_related().
Test that update queries do not generate non-necessary queries. Refs 18304.
A WSGI app that just reflects its HTTP environ.
Ensure that the LiveServerTestCase serves 404s. Refs 2879.
Ensure that the LiveServerTestCase serves views. Refs 2879.
Ensure that the LiveServerTestCase serves static files. Refs 2879.
Test that LiveServerTestCase reports a 404 status code when HTTP client tries to access a static file that isn't explicitly put under STATIC_ROOT.
Ensure that the LiveServerTestCase serves media files. Refs 2879.
Ensure that fixtures are properly loaded and visible to the live server thread. Refs 2879.
Ensure that data written to the database by a view can be read. Refs 2879.
Model with FKs to models with {Null,}BooleanField's, #15040
Model with FK to a model with a CharField primary key, #19299
Custom Field File class that records whether or not the underlying file was opened.
Model that defines an ImageField with no dimension fields.
Abstract model that defines an ImageField with only one dimension field to make sure the dimension update is correctly run on concrete subclass instance post-initialization.
Concrete model that subclass an abstract one with only on dimension field.
Model that defines height and width fields after the ImageField.
Model that defines height and width fields before the ImageField.
Model that: * Defines two ImageFields * Defines the height/width fields before the ImageFields * Has a nullable ImageField
Fields with choices respect show_hidden_initial as a kwarg to formfield().
__repr__() of a field displays its name.
A defined field name (name="fieldname") is used instead of the model model's attribute name (modelname).
Can supply a custom choices form class to Field.formfield()
SlugField honors max_length.
BooleanField with choices and defaults doesn't generate a formfield with the blank option (9640, 10549).
TextField passes its max_length attribute to form fields created using their formfield() method.
TextField.to_python() should return a string.
GenericIPAddressField with a specified protocol does not generate a formfield without a protocol.
Null values should be resolved to None.
CharField passes its max_length attribute to form fields created using the formfield() method.
A lazy callable may be used for ForeignKey.default.
Empty strings foreign key values don't get converted to None (19299).
Foreign key fields declared on abstract models should not add lazy relations to resolve relationship declared as string (24215).
Values within the documented safe range pass validation, and can be saved and retrieved without corruption.
Backend specific ranges can be saved without corruption.
Backend specific ranges are enforced at the model validation level (12030).
Creates a pristine temp directory (or deletes and recreates if it already exists) that the model uses as its storage directory. Sets up two ImageFile instances for use in tests.
Removes temp directory and all its contents.
If the underlying file is unavailable, still create instantiate the object without error.
Bug 8175: correctly delete an object where the file no longer exists on the file system.
Tests that ImageField can be pickled, unpickled, and that the image of the unpickled version is the same as the original.
Tests assigning an image field through the model's constructor.
Tests assigning an image in Manager.create().
Tests that the default value for an ImageField is an instance of the field's attr_class (TestImageFieldFile in this case) with no name (name set to None).
Tests behavior of an ImageField with no dimension fields.
Tests behavior of an ImageField with one dimensions field.
Tests behavior of an ImageField where the dimensions fields are defined before the ImageField.
Tests behavior of an ImageField when assigning it a File instance rather than an ImageFile instance.
FileField.save_form_data() will clear its instance attribute value if passed False.
FileField.save_form_data() considers None to mean "no change" rather than "clear".
FileField.save_form_data(), if passed a truthy value, updates its instance attribute.
Calling delete on an unset FileField should not call the file deletion process, but fail silently (20660).
Should be able to filter decimal fields using strings (8023).
Ensure decimals don't go through a corrupting float conversion during save (5079).
DateTimeField.to_python() supports microseconds.
TimeField.to_python() supports microseconds.
Many-to-many fields declared on abstract models should not add lazy relations to resolve relationship declared as string (24215).
Ensure a warning is raised upon class definition to suggest renaming the faulty method.
Ensure `old` complains and not `new` when only `new` is defined.
Ensure `old` complains when only `old` is defined.
Ensure the correct warnings are raised when a class that didn't rename `old` subclass one that did.
Ensure the correct warnings are raised when a class that renamed `old` subclass one that didn't.
Ensure the correct warnings are raised when a subclass inherit from a class that renamed `old` and mixins that may or may not have renamed `new`.
A tag on an item.
A model that tests having multiple GenericForeignKeys. One is defined through an inherited abstract model and one defined directly on this class.
Should be able to use update_or_create from the generic related manager to create a tag. Refs 23611.
Should be able to use update_or_create from the generic related manager to update a tag. Refs 23611.
Should be able to use get_or_create from the generic related manager to create a tag. Refs 23611.
Objects with declared GenericRelations can be tagged directly -- the API mimics the many-to-many API.
Test accessing the content object like a foreign key.
Test lookups through content type.
You can set a generic foreign key in the way you'd expect.
Queries across generic relations respect the content types. Even though there are two TaggedItems with a tag of "fatty", this query only pulls out the one with the content type related to Animals.
Create another fatty tagged instance with different PK to ensure there is a content type restriction in the generated queries below.
If you delete an object with an explicit Generic relation, the related objects are deleted when the source object is deleted.
If Generic Relation is not explicitly defined, any related objects remain after deletion of the source object.
If you delete a tag, the objects using the tag are unaffected (other than losing a tag).
Test that concrete model subclasses with generic relations work correctly (ticket 11263).
Generic relations on a base class (Vegetable) work correctly in subclasses (Carrot).
Test for 17927 Initial values support for BaseGenericInlineFormSet.
Assigning an unsaved object to GenericForeignKey should raise an exception on model.save().
Regression for 14572: Using base forms with widgets defined in Meta should not raise errors.
Regression for 16260: save_new should call form.save()
The default for for_concrete_model should be True
When for_concrete_model is False, we should still be able to get an instance of the concrete class.
Instances of the proxy should be returned when for_concrete_model is False.
Multiple many-to-many relationships between the same two tables In this example, an ``Article`` can have many "primary" ``Category`` objects and many "secondary" ``Category`` objects. Set ``related_name`` to designate what the reverse relationship is called.
Zero as id for AutoField should raise exception in MySQL, because MySQL does not allow zero for automatic primary key.
Test inserting a large batch with objects having primary key set mixed together with objects without PK set.
This test is related to the above one, testing that there aren't old JOINs in the query.
get() should clear ordering for optimization purposes.
Ensure that Meta.ordering=None works the same as Meta.ordering=[]
Tests for the Queryset.ordered attribute.
Slice a query that has a sliced subquery
Related objects constraints can safely contain sliced subqueries. refs 22434
Test that cloning a queryset does not get out of hand. While complete testing is impossible, this is a sanity check against invalid use of deepcopy. refs 16759.
Test that cloning a queryset does not get out of hand. While complete testing is impossible, this is a sanity check against invalid use of deepcopy. refs 16759.
hint: inverting your ordering might do what you need
hint: inverting your ordering might do what you need
Tests for the union of two querysets. Bug 12252.
This should exclude Orders which have some items with status 1
This should exclude Orders which have some items with status 1
This should exclude Orders which have some items with status 1
This should exclude Orders which have some items with status 1
Using exclude(condition) and exclude(Q(condition)) should yield the same QuerySet
Using exclude(condition) and exclude(Q(condition)) should yield the same QuerySet
This should only return orders having ALL items set to status 1, or those items not having any orders at all. The correct way to write this query in SQL seems to be using two nested subqueries.
Regression test for #15786
The following case is not handled properly because SQL's COL NOT IN (list containing null) handling is too weird to abstract away.
Test that filtering on non-null character fields works as expected. The reason for these tests is that Oracle treats '' as NULL, and this can cause problems in query construction. Refs 17957.
Test that generating the query string doesn't alter the query's state in irreversible ways. Refs 18248.
Test that the queries reuse joins sensibly (for example, direct joins are always reused).
Tests QuerySet ORed combining in exclude subquery case.
A ValueError is raised when the incorrect object type is passed to a query lookup for backward relations.
Subquery table names should be quoted.
Execute the passed query against the passed model and check the output
Check that the results of a raw query contain no annotations
Check that the passed raw query results contain the expected annotations
Basic test of raw query with a simple database query
Raw queries are lazy: they aren't actually executed until they're iterated over.
Test of a simple raw query against a model containing a foreign key
Test of a simple raw query against a model containing a field with db_column defined.
Test of raw raw query's tolerance for columns being returned in any order
Test of raw query's optional ability to translate unexpected result column names to specific model fields
Test passing optional query parameters
Test passing optional query parameters
Test representation of raw query with parameters
Test of a simple raw query against a model containing a m2m field
Test to insure that extra translations are ignored.
Regression test that ensures the `column` attribute on the field is used to generate the list of fields included in the query, as opposed to the `attname`. This is important when the primary key is a ForeignKey field because `attname` and `column` are not necessarily the same.
Emulates the case when some internal code raises an unexpected IndexError.
Makes sure Article has a get_latest_by
Test that the wsgi.file_wrapper works for the builting server. Tests for 9659: wsgi.file_wrapper in the builtin server. We need to mock a couple of handlers and keep track of what gets called when using a couple kinds of WSGI apps.
Server handler that counts the number of chunks written after headers were sent. Used to make sure large response body chunking works properly.
Test that the ServerHandler chunks data properly. Tests for 18972: The logic that performs the math to break data into 32MB (MAX_SOCKET_CHUNK_SIZE) chunks was flawed, BUT it didn't actually cause any problems.
Dummy class for testing max recursion error in child class call to super().  Refs 17011.
Test that settings are overridden within setUpClass -- refs #21281
Overriding a method on a super class and then calling that method on the super class should not trigger infinite recursion. See 17011.
The empty string is accepted, even though it doesn't end in a slash.
It works if the value ends in a slash.
An ImproperlyConfigured exception is raised if the value doesn't end in a slash.
If the value ends in more than one slash, presume they know what they're doing.
Make sure settings that should be lists or tuples throw ImproperlyConfigured if they are set to a string instead of a list or tuple.
Serializing control characters with XML should fail as those characters are not supported in the XML 1.0 standard (except HT, LF, CR).
The XML deserializer shouldn't allow a DTD. This is the most straightforward way to prevent all entity definitions and avoid both external entities and entity-expansion attacks.
Tests that basic serialization works.
Tests that serialized content can be deserialized.
Tests that if you use your own primary key field (such as a OneToOneField), it doesn't appear in the serialized field list - it replaces the pk identifier.
Tests that output can be restricted to a subset of fields
Tests that unicode makes the roundtrip intact
Ensure no superfluous queries are made when serializing ForeignKeys17602
Tests that serialized data with no primary key results in a model instance with no id
Tests that float values serialize and deserialize intact
Tests that custom fields serialize and deserialize intact
Tests that serialized strings without PKs can be turned into models
Mapping such as fields should be deterministically ordered. (#24558)
Tests that deserialized content can be saved with force_insert as a parameter.
Models for test_natural.py
A tag on an item.
Serialization ``django.core.serializers`` provides interfaces to converting Django ``QuerySet`` objects to and from "flat" data (i.e. strings).
Provides a wrapped import_module function to simulate yaml ImportErrorIn order to run tests that verify the behavior of the YAML serializer when run on a system that has yaml installed (like the django CI server), mock import_module, so that it raises an ImportError when the yaml serializer is being imported.  The importlib.import_module() call is being made in the serializers.register_serializer(). Refs: 12756
Using yaml serializer without pyyaml raises ImportError
Using yaml deserializer without pyyaml raises ImportError
Calling dumpdata produces an error when yaml package missing
for obj in serializers.deserialize("json", [{"pk":1}):
These should both work without a problem.
Child has two ForeignKeys to Parent, so if we don't specify which one to use for the inline formset, we should get an exception.
If we specify fk_name, but it isn't a ForeignKey from the child model to the parent model, we should get an exception.
If the field specified in fk_name is not a ForeignKey, we should get an exception.
A foreign key name isn't duplicated in form._meta fields (21332).
URLs with slashes should go unmolested.
Matches to explicit slashless URLs should go unmolested.
APPEND_SLASH should not redirect to unknown resources.
APPEND_SLASH should redirect slashless URLs to a valid pattern.
APPEND_SLASH should preserve querystrings when redirecting.
Tests that while in debug mode, an exception is raised with a warning when a failed attempt is made to POST, PUT, or PATCH to an URL which would normally be redirected to a slashed version.
Disabling append slash functionality should leave slashless URLs alone.
URLs which require quoting should be redirected to their slash version.
URLs with slashes should go unmolested.
Matches to explicit slashless URLs should go unmolested.
APPEND_SLASH should not redirect to unknown resources.
APPEND_SLASH should redirect slashless URLs to a valid pattern.
Tests that while in debug mode, an exception is raised with a warning when a failed attempt is made to POST to an URL which would normally be redirected to a slashed version.
Disabling append slash functionality should leave slashless URLs alone.
URLs which require quoting should be redirected to their slash version.
Regression test for #15152
The X_FRAME_OPTIONS setting can be set to SAMEORIGIN to have the middleware use that value for the HTTP header.
The X_FRAME_OPTIONS setting can be set to DENY to have the middleware use that value for the HTTP header.
If the X-Frame-Options header is already set then the middleware does not attempt to override it.
If the response has an xframe_options_exempt attribute set to False then it still sets the header, but if it's set to True then it doesn't.
Compression is performed on responses with compressible content.
Compression is performed on responses with streaming content.
Compression is performed on responses with streaming Unicode content.
Compression is performed on FileResponse.
Compression is performed on responses with a status other than 200 (10762).
Compression isn't performed on responses with short content.
Compression isn't performed on responses that are already compressed.
Compression isn't performed on responses with incompressible content.
ETag is changed after gzip compression is performed.
With HSTS_SECONDS=3600, the middleware adds "strict-transport-security: max-age=3600" to the response.
The middleware will not override a "strict-transport-security" header already present in the response.
The "strict-transport-security" header is not added to responses going over an insecure connection.
With HSTS_SECONDS of 0, the middleware does not add a "strict-transport-security" header to the response.
With HSTS_SECONDS non-zero and HSTS_INCLUDE_SUBDOMAINS True, the middleware adds a "strict-transport-security" header with the "includeSubDomains" tag to the response.
With HSTS_SECONDS non-zero and HSTS_INCLUDE_SUBDOMAINS False, the middleware adds a "strict-transport-security" header without the "includeSubDomains" tag to the response.
With CONTENT_TYPE_NOSNIFF set to True, the middleware adds "x-content-type-options: nosniff" header to the response.
The middleware will not override an "x-content-type-options" header already present in the response.
With CONTENT_TYPE_NOSNIFF False, the middleware does not add an "x-content-type-options" header to the response.
With BROWSER_XSS_FILTER set to True, the middleware adds "s-xss-protection: 1; mode=block" header to the response.
The middleware will not override an "x-xss-protection" header already present in the response.
With BROWSER_XSS_FILTER set to False, the middleware does not add an "x-xss-protection" header to the response.
With SSL_REDIRECT True, the middleware redirects any non-secure requests to the https:// version of the same URL.
The middleware does not redirect secure requests.
The middleware does not redirect requests with URL path matching an exempt pattern.
The middleware redirects to SSL_HOST if given.
Using a custom primary key By default, Django adds an ``"id"`` field to each model. But you can override this behavior by explicitly adding ``primary_key=True`` to a field.
Both pk and custom attribute_name can be used in filter and friends
Custom pk doesn't affect related_name based lookups
Queries across tables, involving primary key
Custom pks work with in_bulk, both for integer and non-integer types
custom pks do not affect save
New objects can be created both with pk and the custom name
Get the indexes on the table using a new cursor.
Get the constraints on a table using a new cursor.
Changing a field type shouldn't affect the not null status.
Changing a field type shouldn't affect the not null status.
Should be able to rename an IntegerField(primary_key=True) to AutoField(primary_key=True).
Renaming a field shouldn't affect the not null status.
Ensures transaction is correctly closed when an error occurs inside a SchemaEditor context.
23065 - Constraint names must be quoted if they contain capital letters.
Changing the primary key field name of a model with a self-referential foreign key (26384).
A MySQL BinaryField that uses a different blob size.
Tests that validators have valid equality operators (21638)

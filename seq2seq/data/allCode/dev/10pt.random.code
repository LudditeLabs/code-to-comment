#----------------------------------------------------------------------------- COMMENT, INPUT, OUTPUT = range(3)
x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1 y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
xx = np.linspace(-22, 22, 10) yy = np.linspace(-22, 22, 10) xx, yy = np.meshgrid(xx, yy) n_centres = np.hstack((np.ravel(xx)[:, np.newaxis], np.ravel(yy)[:, np.newaxis]))
failed_commands.discard('lspci') failed_commands.discard('dmidecode')
return list(self._params)
labels = [x.reshape(n_shuffle, -1).ravel(order='F') for x in labels] names = self.names return MultiIndex(levels=levels, labels=labels, names=names)
return self.q(css=self.ADD_MISSING_GROUPS_SELECTOR).present
s = Series(['a3', 'b3', 'c2'], name='bob') r = s.str.extract(r'(?P<sue>[a-z])', expand=False) e = Series(['a', 'b', 'c'], name='sue') tm.assert_series_equal(r, e) self.assertEqual(r.name, e.name)
for i in range(6): pids = _chroot_pids(root) if not pids: break for pid in pids: sig = 15 if i < 3 else 9 os.kill(pid, sig)
return bool(STUDIO_EDIT_CONTENT & get_user_permissions(user, course_key))
from __future__ import unicode_literals
self.assertEqual(list(summarize(ip1, ip2))[0], ipaddress.ip_network('1::/16')) ip2 = ipaddress.ip_address('2::') self.assertEqual(list(summarize(ip1, ip2)), [ipaddress.ip_network('1::/16'), ipaddress.ip_network('2::/128')])
if driver.name != 'MEM' and 'name' not in ds_input: raise GDALException('Specify name for creation of raster with driver "{}".'.format(driver.name))
compressed = zlib.compress(data) if len(compressed) < (len(data) - 1): data = compressed is_compressed = True
param_obj = getattr(self._obj, param, None) if not (callable(param_obj) or isinstance(param_obj, property) or inspect.isgetsetdescriptor(param_obj)): param_obj = None
self.course_db = {}
from __future__ import absolute_import import os
if not value.strip(): raise ValidationError("This field may not be blank.")
for i, layer in enumerate(self.layers): if self.routing_needed and i in self.layers_to_inputs: cur_state_below = [state_below[j] for j in self.layers_to_inputs[i]] if len(cur_state_below) == 1: cur_state_below, = cur_state_below else: cur_state_below = state_below
post_data = {"family_name": "Test2", "dependentchild_set-TOTAL_FORMS": "1", "dependentchild_set-INITIAL_FORMS": "0", "dependentchild_set-MAX_NUM_FORMS": "1", "dependentchild_set-0-id": "", "dependentchild_set-0-parent": str(pwdc.id), "dependentchild_set-0-family_name": "Test1"} response = self.client.post( reverse('admin:admin_views_parentwithdependentchildren_change', args=(pwdc.id,)), post_data )
self.auth_page = AutoAuthPage( self.browser, staff=is_staff, username=user.get('username'), email=user.get('email'), password=user.get('password') ) self.auth_page.visit()
if self._mean is None: raise ValueError("can't convert %s to block without fitting" % self.__class__.__name__) return ExamplewiseAddScaleTransform(add=-self._mean)
riak.__salt__ = {}
LogoutPage(self.browser).visit() AutoAuthPage(self.browser, course_id=self.course_id, staff=False).visit()
TESTABLE_BLOCK_TYPES = set(DIRECT_ONLY_CATEGORIES) TESTABLE_BLOCK_TYPES.discard('course')
try: from pandas import to_datetime return to_datetime(v) except: pass
if style is not None: args = (x, y, style) else: args = (x, y) return ax.plot(*args, **kwds)
if len(centers) != n_centers: raise ValueError('The shape of the initial centers (%s) ' 'does not match the number of clusters %i' % (centers.shape, n_centers)) if centers.shape[1] != X.shape[1]: raise ValueError( "The number of features of the initial centers %s " "does not match the number of features of the data %s." % (centers.shape[1], X.shape[1]))
gated_content = gating_api.get_gated_content(course, student)
class BadUser(AbstractBaseUser): username = models.CharField(max_length=30, unique=True) USERNAME_FIELD = 'username' def is_anonymous(self): return True def is_authenticated(self): return True errors = checks.run_checks(app_configs=self.apps.get_app_configs()) self.assertEqual(errors, [ checks.Critical( '%s.is_anonymous must be an attribute or property rather than ' 'a method. Ignoring this is a security issue as anonymous ' 'users will be treated as authenticated!' % BadUser, obj=BadUser, id='auth.C009', ), checks.Critical( '%s.is_authenticated must be an attribute or property rather ' 'than a method. Ignoring this is a security issue as anonymous ' 'users will be treated as authenticated!' % BadUser, obj=BadUser, id='auth.C010', ), ])
X, y = np.ones(4), [1, 1, 0, 0] splits = StratifiedKFold(2).split(X, y) train, test = next(splits) assert_array_equal(test, [0, 2]) assert_array_equal(train, [1, 3])
result = f.clean('21.12.10') self.assertEqual(result, date(2010, 12, 21))
hashed_name = self.hashed_name(name, original_file)
self.assertFalse(os.path.exists(path_test + '.bak'))
for average in ['macro', 'weighted', 'micro']: assert_not_equal(recall_13(average=average), recall_all(average=average))
request.COOKIES[settings.SESSION_COOKIE_NAME] = 'abc'
sparse_results = sparse_classifier.staged_decision_function( X_test_sparse) dense_results = dense_classifier.staged_decision_function(X_test) for sprase_res, dense_res in zip(sparse_results, dense_results): assert_array_equal(sprase_res, dense_res)
tensor_10D_idx = numpy.ndindex(*([2] * block_bits)) for i, j in enumerate(tensor_10D_idx): logz_data_c[i, -block_bits:] = j try: logz_data = numpy.array(logz_data_c, order='F', dtype=config.floatX) except MemoryError: reraise_as(MemoryError("failed to allocate (%d, %d) matrix of " "type %s in compute_log_z; try a smaller " "value of max_bits" % (block_size, width, str(config.floatX))))
self.assertFalse(module_has_submodule(egg_module, 'no_such_module')) with self.assertRaises(ImportError): import_module('egg_module.sub1.sub2.no_such_module')
assert_frame_equal(-self.frame, -1 * self.frame)
self.check_apps_ready() return self.app_configs.values()
from __future__ import print_function __authors__ = "Vincent Dumoulin" __copyright__ = "Copyright 2014, Universite de Montreal" __credits__ = ["Vincent Dumoulin"] __license__ = "3-clause BSD" __maintainer__ = "LISA Lab" __email__ = "pylearn-dev@googlegroups" import os import urllib import numpy assert 'PYLEARN2_DATA_PATH' in os.environ, "PYLEARN2_DATA_PATH not defined" mnist_path = os.path.join(os.environ['PYLEARN2_DATA_PATH'], "binarized_mnist") if not os.path.isdir(mnist_path): print("creating path: " + mnist_path) os.makedirs(mnist_path) in_dir = os.listdir(mnist_path) mnist_files = ["binarized_mnist_train", "binarized_mnist_valid", "binarized_mnist_test"] base_url = "http://www.cs.toronto.edu/~larocheh/public/datasets/" + \ "binarized_mnist/" if not all([f + ".npy" in in_dir for f in mnist_files]) or in_dir == []: print("Downloading MNIST data...") npy_out = [os.path.join(mnist_path, f + ".npy") for f in mnist_files] mnist_url = ["".join([base_url, f, ".amat"]) for f in mnist_files] for n_out, m_url in zip(npy_out, mnist_url): print("Downloading " + m_url + "...", end='') numpy.save(n_out, numpy.loadtxt(urllib.urlretrieve(m_url)[0])) print(" Done") print("Done downloading MNIST") else: print("MNIST files already in PYLEARN2_DATA_PATH")
if self.op in ['==', '!=']:
return self._data.get(key, default)
self.wait_for_ajax()
tab_id = self._active_sequence_tab.attrs('id')[0] return int(tab_id.split('_')[1])
clf_cyclic = ElasticNet(selection='cyclic', tol=1e-8) clf_cyclic.fit(sparse.csr_matrix(X), y) clf_random = ElasticNet(selection='random', tol=1e-8, random_state=42) clf_random.fit(sparse.csr_matrix(X), y) assert_array_almost_equal(clf_cyclic.coef_, clf_random.coef_) assert_almost_equal(clf_cyclic.intercept_, clf_random.intercept_)
if ret.get(host_name) is None: msg = 'Could not find service \'{0}\' for host \'{1}\'.'.format(service_name, host_name) log.debug(msg) ret.update({host_name: {'Error': msg}})
class Meta(object): model = CourseCohort
response_data = self.get_response_data() uid = strategy.request.backend.get_user_id(response_data, response_data) user = social_utils.Storage.user.create_user(email=email, password=password, username=username) profile = student_models.UserProfile(user=user) profile.save() registration = student_models.Registration() registration.register(user) registration.save() if not skip_social_auth: social_utils.Storage.user.create_social_auth(user, uid, self.provider.backend_name) return user
gentoo_service.__grains__ = {} gentoo_service.__salt__ = {} gentoo_service.__context__ = {} gentoo_service.__opts__ = {}
parent_ids = [ valid_parent for valid_parent in all_parent_ids if self.has_path_to_root(valid_parent, course) ]
problem_url_name = 'H1P1' self.define_option_problem(problem_url_name) self.submit_student_answer('u1', problem_url_name, [OPTION_1, OPTION_1]) expected_message = "bad things happened" with patch('capa.capa_problem.LoncapaProblem.rescore_existing_answers') as mock_rescore: mock_rescore.side_effect = ZeroDivisionError(expected_message) instructor_task = self.submit_rescore_all_student_answers('instructor', problem_url_name) self._assert_task_failure(instructor_task.id, 'rescore_problem', problem_url_name, expected_message)
DEPRECATION_VSCOMPAT_EVENT = 'deprecation.vscompat'
idx = info.get(self.name) if idx is not None: self.__dict__.update(idx)
self.infer_axes() dc = ",dc->[%s]" % ','.join( self.data_columns) if len(self.data_columns) else '' ver = '' if self.is_old_version: ver = "[%s]" % '.'.join([str(x) for x in self.version]) return "%-12.12s%s (typ->%s,nrows->%s,ncols->%s,indexers->[%s]%s)" % ( self.pandas_type, ver, self.table_type_short, self.nrows, self.ncols, ','.join([a.name for a in self.index_axes]), dc )
teams = self.create_teams(self.topic, self.TEAMS_PAGE_SIZE + 10, time_between_creation=1) self.browse_teams_page.visit() self.verify_page_header() self.verify_on_page(self.browse_teams_page, 1, teams, 'Showing 1-10 out of 20 total', True) self.browse_teams_page.go_to_page(2) self.verify_on_page(self.browse_teams_page, 2, teams, 'Showing 11-20 out of 20 total', True) self.browse_teams_page.go_to_page(1) self.verify_on_page(self.browse_teams_page, 1, teams, 'Showing 1-10 out of 20 total', True)
result = df.copy() result.iloc[1, 0] = np.nan result = result.replace( {'A': pd.NaT}, Timestamp('20130104', tz='US/Pacific')) expected = DataFrame({'A': [Timestamp('20130101', tz='US/Eastern'), Timestamp('20130104', tz='US/Pacific'), Timestamp('20130103', tz='US/Eastern')], 'B': [0, np.nan, 2]}) assert_frame_equal(result, expected)
_ZFILE_PREFIX = asbytes('ZF') _MAX_LEN = len(hex_str(2 ** 64))
preview_url = reverse_usage_url("xblock_view_handler", usage_key, {'view_name': 'container_preview'}) data = data if data else {} resp = self.client.get(preview_url, data, HTTP_ACCEPT='application/json') return resp
covariance_, precision_ = graph_lasso( emp_cov, alpha=alpha, cov_init=covariance_, mode=mode, tol=tol, enet_tol=enet_tol, max_iter=max_iter, verbose=inner_verbose) covariances_.append(covariance_) precisions_.append(precision_) if X_test is not None: this_score = log_likelihood(test_emp_cov, precision_)
ax = fig.add_subplot(211, projection='3d') ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
'unique_for_date': _("%(field_label)s must be unique for " "%(date_field_label)s %(lookup_type)s."),
if sample_weight is None: base_estimator.fit(X_subset, y_subset) else: base_estimator.fit(X_subset, y_subset, sample_weight=sample_weight[subset_idxs])
raise SkipTest
if image_file is not None: self.save_image(image_file)
return self
class Managed1(models.Model): mm = models.ManyToManyField(Unmanaged1)
d = Document() d.myfile.delete()
score_bucket = "incorrect" if grade > 0 and grade < max_grade: score_bucket = "partial" elif grade == max_grade: score_bucket = "correct" return score_bucket
tooltip = { 'type': 'problem', 'label': label, 'problem_name': problem_name, 'count_grade': count_grade, 'percent': percent, 'grade': grade, 'max_grade': max_grade, 'student_count_percent': student_count_percent, }
reparse_data = _get_reparse_data(path)
elif opts['master_type'] == 'failover': if isinstance(opts['master'], list): log.info('Got list of available master addresses:' ' {0}'.format(opts['master'])) if opts['master_shuffle']: if opts['master_failback']: secondary_masters = opts['master'][1:] shuffle(secondary_masters) opts['master'][1:] = secondary_masters else: shuffle(opts['master']) opts['auth_tries'] = 0 if opts['master_failback'] and opts['master_failback_interval'] == 0: opts['master_failback_interval'] = opts['master_alive_interval'] elif isinstance(opts['master'], str) and ('master_list' not in opts): opts['master'] = [opts['master']] elif opts['__role'] == 'syndic': log.info('Syndic setting master_syndic to \'{0}\''.format(opts['master']))
if len(xmltree.getchildren()) < 1: name = xmltree.tag if '}' in name: comps = name.split('}') name = comps[1] return {name: xmltree.text}
courses_list, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(len(courses_list), 1) self.assertNotIn( ccx_course_key, [course.id for course in courses_list] )
assert_almost_equal(label_ranking_loss([[0, 0]], [[0.75, 0.25]]), 0) assert_almost_equal(label_ranking_loss([[1, 1]], [[0.75, 0.25]]), 0) assert_almost_equal(label_ranking_loss([[0, 0]], [[0.5, 0.5]]), 0) assert_almost_equal(label_ranking_loss([[1, 1]], [[0.5, 0.5]]), 0)
import salt.utils
chain = [] engines = _engine_list(using) for engine in engines: try: return engine.get_template(template_name) except TemplateDoesNotExist as e: chain.append(e) raise TemplateDoesNotExist(template_name, chain=chain)
self.kernel = kernel self.gamma = gamma self.n_neighbors = n_neighbors
Ensure myasg is deleted: boto_asg.absent: - name: myasg - force: True
with ensure_clean_store(self.path) as store:
for base in self.bases: if isinstance(base, six.string_types): strings_to_check.append(base.split(".")[-1]) for fname, field in self.fields: if field.remote_field: if isinstance(field.remote_field.model, six.string_types): strings_to_check.append(field.remote_field.model.split(".")[-1]) for string in strings_to_check: if string.lower() == name.lower(): return True return False
if self.y is None: return self.X else: return (self.X, self.y)
win32api.SetFileAttributes(path, file_attributes)
tasks = json.loads(response.content)['tasks'] self.assertEqual(len(tasks), 0)
super(EnrollmentDataTest, self).setUp() self.course = CourseFactory.create() self.user = UserFactory.create(username=self.USERNAME, email=self.EMAIL, password=self.PASSWORD) self.client.login(username=self.USERNAME, password=self.PASSWORD)
from contextlib import contextmanager import sys import time import logging
connection.timezone del connection.timezone connection.timezone_name del connection.timezone_name
executor.loader.build_graph()
match = time_re.match(value) if match: kw = match.groupdict() if kw['microsecond']: kw['microsecond'] = kw['microsecond'].ljust(6, '0') kw = {k: int(v) for k, v in six.iteritems(kw) if v is not None} return datetime.time(**kw)
self.assertTrue(can_execute_unsafe_code(SlashSeparatedCourseKey('edX', 'full', '2012_Fall'))) self.assertTrue(can_execute_unsafe_code(SlashSeparatedCourseKey('edX', 'full', '2013_Spring'))) self.assertFalse(can_execute_unsafe_code(LibraryLocator('edX', 'test_bank')))
cursor.execute("ALTER SESSION SET NLS_TERRITORY = 'AMERICA'") cursor.execute( "ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD HH24:MI:SS'" " NLS_TIMESTAMP_FORMAT = 'YYYY-MM-DD HH24:MI:SS.FF'" + (" TIME_ZONE = 'UTC'" if settings.USE_TZ else '') ) cursor.close() if 'operators' not in self.__dict__: cursor = self.create_cursor() try: cursor.execute("SELECT 1 FROM DUAL WHERE DUMMY %s" % self._standard_operators['contains'], ['X']) except DatabaseError: self.operators = self._likec_operators self.pattern_ops = self._likec_pattern_ops else: self.operators = self._standard_operators self.pattern_ops = self._standard_pattern_ops cursor.close()
assert_not_in(attribute, seq.xml_attributes)
assert_series_equal(ts.reindex(i), ts.iloc[j], check_index_type=False)
y_i = np.ones(y.shape, dtype=np.float64, order="C") y_i[y != est.classes_[i]] = -1.0 average_intercept = 0 average_coef = None if len(est.classes_) == 2: if not est.average: coef = est.coef_.ravel() intercept = est.intercept_[0] else: coef = est.standard_coef_.ravel() intercept = est.standard_intercept_[0] average_coef = est.average_coef_.ravel() average_intercept = est.average_intercept_[0] else: if not est.average: coef = est.coef_[i] intercept = est.intercept_[i] else: coef = est.standard_coef_[i] intercept = est.standard_intercept_[i] average_coef = est.average_coef_[i] average_intercept = est.average_intercept_[i] return y_i, coef, intercept, average_coef, average_intercept
if amount == 0: return array new_shape = [] slices = [] for i, s in enumerate(array.shape): if i in axes: new_shape.append(s + 2 * amount) slices.append(slice(amount, -amount)) else: new_shape.append(s) slices.append(slice(None)) new_shape = tuple(new_shape) slices = tuple(slices) new_array = numpy.zeros(new_shape, dtype=array.dtype) new_array[slices] = array return new_array
nowait = self.query.select_for_update_nowait if nowait and not self.connection.features.has_select_for_update_nowait: raise DatabaseError('NOWAIT is not supported on this database backend.') result.append(self.connection.ops.for_update_sql(nowait=nowait))
with self.assertRaises(forms.ValidationError): f.clean('2010-12-21')
self.r.article_set.set([new_article]) self.assertQuerysetEqual( self.r.article_set.all(), ["<Article: John's second story>", "<Article: This is a test>"] ) self.assertQuerysetEqual(self.r2.article_set.all(), ["<Article: Paul's story>"])
ridge.fit(X, y, sample_weight=np.ones(n_samples)) assert_greater(ridge.score(X, y), 0.47)
log.error( 'An extra return was detected from minion {0}, please verify ' 'the minion, this could be a replay attack'.format( load['id'] ) ) return False
def _install_signal_handlers(self): signal.signal(signal.SIGTERM, self._handle_signals) signal.signal(signal.SIGINT, self._handle_signals)
supports_column_check_constraints = True
frame = DataFrame(mat) self.assert_index_equal(frame.index, pd.Index(lrange(2))) self.assert_index_equal(frame.columns, pd.Index(lrange(3)))
response = self.client.get(show_url) for msg in data['messages']: self.assertNotContains(response, msg)
is_entrance_exam = Boolean( display_name=_("Is Entrance Exam"), help=_( "Tag this course module as an Entrance Exam. " "Note, you must enable Entrance Exams for this course setting to take effect." ), default=False, scope=Scope.settings, )
self.assertDictContainsSubset( result.pop("encoded_videos")[self.TEST_PROFILE], self.TEST_ENCODED_VIDEO, ) self.assertDictEqual( result, { "only_on_web": False, "duration": self.TEST_DURATION, "transcripts": {self.TEST_LANGUAGE: self.transcript_url}, } )
out = six.StringIO() apps.register_model('migrations', UnicodeModel) with self.temporary_migration_module() as migration_dir: call_command("makemigrations", "migrations", stdout=out) self.assertIn(os.path.join(migration_dir, '0001_initial.py'), out.getvalue())
self._verify_masquerade_for_all_groups()
newly_created_cohort = get_cohort_by_name(self.course.id, cohort_name) cohort_name = 'I AM AN UPDATED RANDOM COHORT' data = {'name': cohort_name, 'assignment_type': CourseCohort.RANDOM} response_dict = self.put_handler(self.course, newly_created_cohort, data=data)
warnings.warn( 'Storage.accessed_time() is deprecated. ' 'Storage backends should implement get_accessed_time().', RemovedInDjango20Warning, stacklevel=2, ) dt = self.accessed_time(name) return _possibly_make_aware(dt)
_deprecated_valids = _attributes + ['_ipython_display_', '__doc__', '_cache', '_attributes', 'binner', 'grouper', 'groupby', 'keys', 'sort', 'kind', 'squeeze', 'group_keys', 'as_index', 'exclusions', '_groupby']
rng = np.random.RandomState(0) n, p = 100, 3 X = rng.randn(n, p) * .1 X[:10] += np.array([3, 4, 5]) Xt = 0.1 * rng.randn(1, p) + np.array([3, 4, 5])
) use_latex_compiler = Boolean( help=_("Enable LaTeX templates?"), default=False, scope=Scope.settings ) matlab_api_key = String( display_name=_("Matlab API key"), help=_("Enter the API key provided by MathWorks for accessing the MATLAB Hosted Service. " "This key is granted for exclusive use by this course for the specified duration. " "Please do not share the API key with other courses and notify MathWorks immediately " "if you believe the key is exposed or compromised. To obtain a key for your course, " "or to report an issue, please contact moocsupport@mathworks.com"), scope=Scope.settings )
if isinstance(index_names[0], compat.string_types)\ and 'Unnamed' in index_names[0]: index_names[0] = None
from defusedxml import defuse_stdlib defuse_stdlib() import lxml import lxml.etree from . import etree as safe_etree lxml.etree = safe_etree
_data_api().add_or_update_enrollment_attr(user_id, course_id, attributes)
try: ManualPrimaryKeyTest.objects.update_or_create(id=1, data="Different") except IntegrityError: formatted_traceback = traceback.format_exc() self.assertIn('obj.save', formatted_traceback)
def setUp(self): context.clear() conn_parameters['key'] = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(50))
#from IPython.utils.io import Tee
DEBUG = True
class FavoriteForm(Form): color = CharField(label='Favorite color?') animal = CharField(label='Favorite animal') answer = CharField(label='Secret answer', label_suffix=' =')
student = self.create_student(u'username', u'student@example.com') mock_iterate_grades_for.return_value = [ (student, {}, error_message) ] result = upload_problem_grade_report(None, None, self.course.id, None, 'graded') self.assertDictContainsSubset({'attempted': 1, 'succeeded': 0, 'failed': 1}, result)
from salt.states import mac_defaults as macdefaults
def __init__(self, seed): self.seed = seed def __enter__(self): self.start_state = np.random.get_state() np.random.seed(self.seed) def __exit__(self, exc_type, exc_value, traceback): np.random.set_state(self.start_state)
self.assertRaises(TypeError, self.ipv4_network.compare_networks, self.ipv6_network) ipv6 = ipaddress.IPv6Interface('::/0') ipv4 = ipaddress.IPv4Interface('0.0.0.0/0') self.assertRaises(TypeError, ipv4.__lt__, ipv6) self.assertRaises(TypeError, ipv4.__gt__, ipv6) self.assertRaises(TypeError, ipv6.__lt__, ipv4) self.assertRaises(TypeError, ipv6.__gt__, ipv4)
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit()
for mode in unexpired_modes[course_key]: if mode.min_price > 0 and not CourseMode.is_credit_mode(mode): return mode
self.wait_for( lambda: self.q(css='.team-count')[0].text == "0 Teams" if expected_count == 0 else "1 Team", description="Team count text on topic is wrong" )
for kind in ['integer', 'block']: values = np.array([np.nan, 1, 2, 0, np.nan, 0, 1, 2, 1, np.nan]) rvalues = np.array([np.nan, 2, 3, 4, np.nan, 0, 1, 3, 2, np.nan])
if weak is not None: warnings.warn("Passing `weak` to disconnect has no effect.", RemovedInDjango20Warning, stacklevel=2) if dispatch_uid: lookup_key = (dispatch_uid, _make_id(sender)) else: lookup_key = (_make_id(receiver), _make_id(sender)) disconnected = False with self.lock: self._clear_dead_receivers() for index in range(len(self.receivers)): (r_key, _) = self.receivers[index] if r_key == lookup_key: disconnected = True del self.receivers[index] break self.sender_receivers_cache.clear() return disconnected
with MongoContentstoreBuilder().build() as contentstore: with self.build_with_contentstore(contentstore) as modulestore: yield contentstore, modulestore
affinity[0, n_sample + 1] = 1 affinity[n_sample + 1, 0] = 1 affinity.flat[::2 * n_sample + 1] = 0 affinity = 0.5 * (affinity + affinity.T)
from __future__ import unicode_literals
import salt.ext.six as six
X = theano._asarray(X, theano.config.floatX)
assert_equal(_dynamic_max_trials(100, 100, 2, 0.99), 1)
result = CategoricalIndex(categories=categories) self.assert_index_equal(result.categories, Index(categories)) tm.assert_numpy_array_equal(result.codes, np.array([], dtype='int8')) self.assertFalse(result.ordered)
for oper in memory_oper: for scope in memory_scope: key = 'Operation: {0} Scope: {1}'.format(oper, scope) run_command = test_command.format(oper, scope) result = __salt__['cmd.run'](run_command) ret_val[key] = _parser(result)
self.assertNotIn(REG_STR, resp.content)
db_name = connection.creation.create_test_db(verbosity=verbosity, autoclobber=not interactive, serialize=False)
result = df1.loc[(slice(None), slice('B1', 'B2'), slice('2013-08-06') ), :] expected = df1.iloc[[2, 3, 4, 7, 8, 9, 12, 13]] assert_frame_equal(result, expected)
try: val = self[key] except KeyError: return default if val == []: return default return val
TRACKING_IGNORE_URL_PATTERNS = [r'^/event', r'^/login', r'^/heartbeat', r'^/segmentio/event', r'^/performance']
self.assertGreaterEqual(iteration_over_courses_time_1.elapsed, iteration_over_groups_time_1.elapsed) self.assertGreaterEqual(iteration_over_courses_time_2.elapsed, iteration_over_groups_time_2.elapsed)
cache[cache.keys()[0]] = (None, {'a': 17})
with override_settings(FORCE_SCRIPT_NAME='/FORCED_PREFIX/'): request = WSGIRequest({'PATH_INFO': '/somepath/', 'REQUEST_METHOD': 'get', 'wsgi.input': BytesIO(b'')}) self.assertEqual(request.path, '/FORCED_PREFIX/somepath/') with override_settings(FORCE_SCRIPT_NAME='/FORCED_PREFIX'): request = WSGIRequest({'PATH_INFO': '/somepath/', 'REQUEST_METHOD': 'get', 'wsgi.input': BytesIO(b'')}) self.assertEqual(request.path, '/FORCED_PREFIX/somepath/')
rng = np.random.RandomState(0) X = generate_clustered_data(n_clusters=3, n_features=3, n_samples_per_cluster=10)
try: cmd_log(['git', 'fetch', ], rdir) except subprocess.CalledProcessError as ex: log.exception('Unable to fetch remote: %r', ex.output) raise GitImportErrorCannotBranch()
self.assertEqual(self.loader[self.module_key](), (self.count, self.lib_count)) self.loader.clear() self.assertNotIn(self.module_key, self.loader)
self.publish(location.version_agnostic(), user_id, blacklist=EXCLUDE_ALL, **kwargs)
if isinstance(err, (DataFrame, dict)): if label is not None and label in err.keys(): err = err[label] else: err = None elif index is not None and err is not None: err = err[index]
if freqstr in _rule_aliases: new = _rule_aliases[freqstr] warnings.warn(_LEGACY_FREQ_WARNING.format(freqstr, new), FutureWarning, stacklevel=3) freqstr = new freqstr = _lite_rule_alias.get(freqstr, freqstr)
if predicate: self.assertIn(member, container) else: self.assertNotIn(member, container)
video_block = XBlockFixtureDesc('video', "Test Video") vertical = XBlockFixtureDesc('vertical', "Test Vertical") vertical.add_children(video_block) sequential = XBlockFixtureDesc('sequential', "Test Section") sequential.add_children(vertical) chapter = XBlockFixtureDesc('chapter', "Test Chapter") chapter.add_children(sequential) self.course_fixture.add_children(chapter)
check_is_fitted(self, 'x_mean_') X = check_array(X, dtype=np.float64) Xr = (X - self.x_mean_) / self.x_std_ x_scores = np.dot(Xr, self.x_weights_) if Y is not None: if Y.ndim == 1: Y = Y.reshape(-1, 1) Yr = (Y - self.y_mean_) / self.y_std_ y_scores = np.dot(Yr, self.y_weights_) return x_scores, y_scores return x_scores
for solver in ['lbfgs', 'newton-cg', 'sag']: max_iter = 100 if solver == 'sag' else 15 clf_multi = LogisticRegressionCV( solver=solver, multi_class='multinomial', max_iter=max_iter, random_state=42, tol=1e-2, cv=2) clf_multi.fit(train, target) multi_score = clf_multi.score(train, target) ovr_score = clf.score(train, target) assert_greater(multi_score, ovr_score)
pass
mark.save(using='default')
if left.nlevels != right.nlevels: raise_assert_detail(obj, '{0} levels are different'.format(obj), '{0}, {1}'.format(left.nlevels, left), '{0}, {1}'.format(right.nlevels, right))
pmap[alias][driver] = []
self.assertFalse(user_has_role(self.creator, CourseInstructorRole(self.course_key))) add_users(self.global_admin, CourseInstructorRole(self.course_key), self.creator) add_users(self.global_admin, CourseStaffRole(self.course_key), self.creator) self.assertTrue(user_has_role(self.creator, CourseInstructorRole(self.course_key)))
ave_total = kl(Y=Y, Y_hat=Y_hat, batch_axis=batch_axis) ave = ave_total.mean() return ave
for block in value._data.blocks: self.assertTrue(block.values.flags.writeable) block.values[0] += rhs[block.dtype]
jid_dir, jid_file = self._make_tmp_jid_dirs()
s = Series([1, 22, 3333, 44444], index=[1, 'AB', pd.Timestamp('2011-01-01'), u'あああ']) expected = (u"1 1\nAB 22\n" u"2011-01-01 00:00:00 3333\nあああ 44444\ndtype: int64" ) self.assertEqual(_rep(s), expected)
self._enroll_students_in_course(self.course.id, extra_count) return {}
return redirect_to_custom_form(strategy.request, auth_entry, kwargs)
return etree.Element("annotationresponse")
ind2 = self.index.copy() inplace_return = ind2.set_levels(new_levels, inplace=True) self.assertIsNone(inplace_return) assert_matching(ind2.levels, new_levels)
class ReplaceField(forms.ModelForm): url = forms.BooleanField()
df = DataFrame(dict(A=np.random.rand(20), B=np.random.rand(20))) store.append('df', df)
url_re = '(https?)://.*@' redacted = r'\1://<redacted>@' if sys.version_info >= (2, 7): return re.sub(url_re, redacted, output, flags=re.IGNORECASE) else: if re.search(url_re, output.lower()): return re.sub(url_re, redacted, output.lower()) return output
for yerr in ['yerr', u('誤差')]: s_df = df.copy() s_df[yerr] = np.ones(12) * 0.2 ax = _check_plot_works(s_df.plot, yerr=yerr) self._check_has_errorbars(ax, xerr=0, yerr=2) ax = _check_plot_works(s_df.plot, y='y', x='x', yerr=yerr) self._check_has_errorbars(ax, xerr=0, yerr=1)
test3 = value 3B
origin = bool(re.search('/', name))
if features is not None: valid_set = valid_set[:, features] test_set = test_set[:, features]
from __future__ import unicode_literals
def setUp(self): super(CourseViewTest, self).setUp() self.url = reverse("discussion_course", kwargs={"course_id": unicode(self.course.id)}) def test_404(self): response = self.client.get( reverse("course_topics", kwargs={"course_id": "non/existent/course"}) ) self.assert_response_correct( response, 404, {"developer_message": "Course not found."} ) def test_basic(self): response = self.client.get(self.url) self.assert_response_correct( response, 200, { "id": unicode(self.course.id), "blackouts": [], "thread_list_url": "http://testserver/api/discussion/v1/threads/?course_id=x%2Fy%2Fz", "following_thread_list_url": ( "http://testserver/api/discussion/v1/threads/?course_id=x%2Fy%2Fz&following=True" ), "topics_url": "http://testserver/api/discussion/v1/course_topics/x/y/z", } )
start = datetime(2011, 3, 12, tzinfo=pytz.utc) dr = bdate_range(start, periods=50, freq=datetools.Hour()) self.assertIs(dr.tz, pytz.utc)
nodedata = copy.deepcopy(profile_data) for setting in ('grains', 'master', 'minion', 'volumes', 'requires'): deprecated = 'map_{0}'.format(setting) if deprecated in overrides: log.warning( 'The use of \'{0}\' on the \'{1}\' mapping has ' 'been deprecated. The preferred way now is to ' 'just define \'{2}\'. For now, salt-cloud will do ' 'the proper thing and convert the deprecated ' 'mapping into the preferred one.'.format( deprecated, nodename, setting ) ) overrides[setting] = overrides.pop(deprecated)
if wrap_xmodule_display is True: block_wrappers.append(partial( wrap_xblock, 'LmsRuntime', extra_data={'course-id': course_id.to_deprecated_string()}, usage_id_serializer=lambda usage_id: quote_slashes(usage_id.to_deprecated_string()), request_token=request_token, ))
self.r = Reporter(first_name='John', last_name='Smith', email='john@example.com') self.r.save() self.r2 = Reporter(first_name='Paul', last_name='Jones', email='paul@example.com') self.r2.save() self.a = Article(headline="This is a test", pub_date=datetime.date(2005, 7, 27), reporter=self.r) self.a.save()
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
raise NotImplementedError()
@override_settings(LOGGING_CONFIG='logging_tests.tests.dictConfig', LOGGING=OLD_LOGGING) def test_configure_initializes_logging(self): from django import setup setup() self.assertTrue(dictConfig.called)
raise VersionConflictError(course_key, version_guid)
if login_when_done: user = authenticate(username=username, password=password) login(request, user)
cmd = ( 'salt -t 600 {target} state.sls {cloud_prep_sls} pillar="{pillar}" ' '--no-color'.format( target=build_minion_target(opts, vm_name), cloud_prep_sls='cloud-test-configs', pillar=build_pillar_data(opts), ) ) print('Running CMD: {0}'.format(cmd)) sys.stdout.flush()
def __contains__(self, item): return True def __getitem__(self, item): return "notmigrations"
parent_locator = unicode(self.course.location) created_block = create_xblock( parent_locator=parent_locator, user=self.user, category='chapter', display_name=('Entrance Exam'), is_entrance_exam=True ) add_entrance_exam_milestone(self.course.id, created_block) content_milestones = milestones_helpers.get_course_content_milestones( unicode(self.course.id), unicode(created_block.location), self.milestone_relationship_types['FULFILLS'] ) self.assertTrue(len(content_milestones)) self.assertEqual(len(milestones_helpers.get_course_milestones(self.course.id)), 1)
opts = self.remote_field.model._meta from_opts = self.model._meta return [PathInfo(from_opts, opts, self.foreign_related_fields, self, False, True)]
self.course.advanced_modules = ["foo", "imageannotation", "boo"] self.assertFalse(helpers.is_feature_enabled(self.course)) self.course.advanced_modules = ["foo", "boo", "videoannotation"] self.assertFalse(helpers.is_feature_enabled(self.course)) self.course.advanced_modules = ["textannotation", "foo", "boo"] self.assertFalse(helpers.is_feature_enabled(self.course)) self.course.advanced_modules = ["textannotation", "videoannotation", "imageannotation"] self.assertFalse(helpers.is_feature_enabled(self.course))
if not permissions.can_access_all_blocks(requesting_user, course_key): raise PermissionDenied( "'{requesting_username}' does not have permission to access all blocks in '{course_key}'." .format(requesting_username=requesting_user.username, course_key=unicode(course_key)) )
orphan_locations = [ course_id.make_usage_key('chapter', 'OrphanChapter'), course_id.make_usage_key('vertical', 'OrphanVertical'), course_id.make_usage_key('problem', 'OrphanProblem'), course_id.make_usage_key('html', 'OrphanHTML'), ]
import salt.ext.six as six import salt.utils
idx = PeriodIndex(['2011-01', 'NaT', '2011-03', '2011-04'], freq='M', name='idx')
if n_holes == 1 and isinstance(init_holes[0], (tuple, list)): if len(init_holes[0]) == 0: init_holes = () n_holes = 0 elif isinstance(init_holes[0][0], LinearRing): init_holes = init_holes[0] n_holes = len(init_holes)
thread = self.make_minimal_cs_thread({ "thread_type": thread_type, response_field: [make_minimal_cs_comment()], response_total_field: 5, })
return result
with check_mongo_calls(4, 2):
X2 = rng.randint(5, size=(6, 100)) y2 = np.array([1, 1, 2, 2, 3, 3])
CROSS_DOMAIN_CSRF_COOKIE_DOMAIN = ENV_TOKENS.get('CROSS_DOMAIN_CSRF_COOKIE_DOMAIN')
.... return theta_opt, func_min
q = pd.Series([1, 3, 4]).quantile(0.5, interpolation='lower') self.assertEqual(q, percentile(np.array([1, 3, 4]), 50)) self.assertTrue(com.is_integer(q))
if np.abs(np_var(A) - two_pass_var(A)).max() < 1e-6: stable_var = np_var else: stable_var = two_pass_var
url = reverse( 'get_registration_codes', kwargs={'course_id': self.course.id.to_deprecated_string()} ) data = {'download_company_name': ''} response = self.client.post(url, data) self.assertEqual(response.status_code, 200, response.content) self.assertEqual(response['Content-Type'], 'text/csv') body = response.content.replace('\r', '') self.assertTrue(body.startswith(EXPECTED_CSV_HEADER)) self.assertEqual(len(body.split('\n')), 14) generate_code_url = reverse( 'generate_registration_codes', kwargs={'course_id': self.course.id.to_deprecated_string()} ) data = { 'total_registration_codes': 9, 'company_name': 'Group Alpha', 'company_contact_name': 'Test@company.com', 'company_contact_email': 'Test@company.com', 'unit_price': 122.45, 'recipient_name': 'Test123', 'recipient_email': 'test@123.com', 'address_line_1': 'Portland Street', 'address_line_2': '', 'address_line_3': '', 'city': '', 'state': '', 'zip': '', 'country': '', 'customer_reference_number': '123A23F', 'internal_reference': '', 'invoice': '' } response = self.client.post(generate_code_url, data, **{'HTTP_HOST': 'localhost'}) self.assertEqual(response.status_code, 200, response.content) data = {'download_company_name': 'Group Alpha'} response = self.client.post(url, data) self.assertEqual(response.status_code, 200, response.content) self.assertEqual(response['Content-Type'], 'text/csv') body = response.content.replace('\r', '') self.assertTrue(body.startswith(EXPECTED_CSV_HEADER)) self.assertEqual(len(body.split('\n')), 11)
gis = 'django.contrib.gis' if connection.features.gis_enabled and gis not in settings.INSTALLED_APPS: if verbosity >= 2: print("Importing application %s" % gis) settings.INSTALLED_APPS.append(gis)
def __init__(self, rng): self.rng = rng
if __grains__['os_family'] == 'RedHat': output = _cmd('at', '-l') else: output = _cmd('atq')
this_path = Parallel( n_jobs=self.n_jobs, verbose=self.verbose )(delayed(graph_lasso_path)(X[train], alphas=alphas, X_test=X[test], mode=self.mode, tol=self.tol, enet_tol=self.enet_tol, max_iter=int(.1 * self.max_iter), verbose=inner_verbose) for train, test in cv.split(X, y))
df = DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six' ], 'c': Series( range(7), dtype='int64')}) self.assertIsNone(df.is_copy) expected = DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six'], 'c': [42, 42, 2, 3, 4, 42, 6]})
('body_stats', Pipeline([ ('selector', ItemSelector(key='body')),
docstring = trim_docstring(docstring) parts = re.split(r'\n{2,}', docstring) title = parts[0] if len(parts) == 1: body = '' metadata = {} else: parser = HeaderParser() try: metadata = parser.parsestr(parts[-1]) except HeaderParseError: metadata = {} body = "\n\n".join(parts[1:]) else: metadata = dict(metadata.items()) if metadata: body = "\n\n".join(parts[1:-1]) else: body = "\n\n".join(parts[1:]) return title, body, metadata
tokens = token.contents.split() if len(tokens) > 1 and tokens[1] != 'as': raise template.TemplateSyntaxError( "First argument in '%s' must be 'as'" % tokens[0]) if len(tokens) > 1: varname = tokens[2] else: varname = None return cls(varname, name)
return self._upsample('backfill', limit=limit)
mail_admins('Subject', 'Content', html_message='HTML Content') message = self.get_the_message() self.assertEqual(message.get('subject'), '[Django] Subject') self.assertEqual(message.get_all('to'), ['nobody@example.com']) self.assertTrue(message.is_multipart()) self.assertEqual(len(message.get_payload()), 2) self.assertEqual(message.get_payload(0).get_payload(), 'Content') self.assertEqual(message.get_payload(0).get_content_type(), 'text/plain') self.assertEqual(message.get_payload(1).get_payload(), 'HTML Content') self.assertEqual(message.get_payload(1).get_content_type(), 'text/html')
root = etree.Element('error') root.text = self.contents err_node = etree.SubElement(root, 'error_msg') err_node.text = self.error_msg return etree.tostring(root, encoding='unicode')
_translator = FakeTranslations.translator( { 'es': {'Hello': 'es-hello-world'}, 'fr': {'Hello': 'fr-hello-world'}, }, ) localedir = '/translations' translation.activate("es") with mock.patch('gettext.translation', return_value=_translator(domain='text', localedir=localedir, languages=[get_language()])): i18n_service = self.get_module_i18n_service(self.descriptor) self.assertEqual(i18n_service.ugettext('Hello'), 'es-hello-world') translation.activate("ar") with mock.patch('gettext.translation', return_value=_translator(domain='text', localedir=localedir, languages=[get_language()])): i18n_service = self.get_module_i18n_service(self.descriptor) self.assertEqual(i18n_service.ugettext('Hello'), 'Hello') self.assertNotEqual(i18n_service.ugettext('Hello'), 'fr-hello-world') self.assertNotEqual(i18n_service.ugettext('Hello'), 'es-hello-world') translation.activate("fr") with mock.patch('gettext.translation', return_value=_translator(domain='text', localedir=localedir, languages=[get_language()])): i18n_service = self.get_module_i18n_service(self.descriptor) self.assertEqual(i18n_service.ugettext('Hello'), 'fr-hello-world')
('completed', 'completed'),
kf = KFold(3) kf2 = KFold(3, shuffle=True, random_state=0) kf3 = KFold(3, shuffle=True, random_state=1)
library_container.edit() edit_modal = StudioLibraryContentEditor(self.browser, library_container.locator) self.assertEqual(edit_modal.library_name, library_name) self.assertEqual(edit_modal.count, max_count) self.assertEqual(edit_modal.scored, scored)
return x
oh = OneHotEncoder(handle_unknown='error') oh.fit(X) assert_raises(ValueError, oh.transform, y)
if (isinstance(labels, MultiIndex) and len(keyarr) and not isinstance(keyarr[0], tuple)): level = 0 else: level = None
attempts = random.randint(1, 10) module = CapaFactory.create(attempts=attempts, max_attempts=attempts, force_save_button="true", done=True) self.assertFalse(module.should_show_save_button())
for cache in settings.CACHES: caches[cache].clear()
if self.config['transport'].lower() in ('zeromq', 'tcp'): import salt.minion self.daemonize_if_required() self.set_pidfile() if self.config.get('master_type') == 'func': salt.minion.eval_master_func(self.config) if isinstance(self.config.get('master'), list): if self.config.get('master_type') == 'failover': self.minion = salt.minion.Minion(self.config) else: self.minion = salt.minion.MultiMinion(self.config) else: self.minion = salt.minion.Minion(self.config) else: import salt.daemons.flo self.daemonize_if_required() self.set_pidfile() self.minion = salt.daemons.flo.IofloMinion(self.config)
Options.FORWARD_PROPERTIES = {'fields', 'many_to_many', 'concrete_fields', 'local_concrete_fields', '_forward_fields_map'}
response = self.api_response() self.assertIn("/courses/{}/".format(self.course.id), response.data['handouts_html'])
import salt.crypt import salt.payload import salt.master import salt.transport.frame import salt.utils.event import salt.ext.six as six from salt.utils.cache import CacheCli
if self.server.delete_note(note_id): self.respond(204, "No Content") else: self.respond(404, "404 Not Found")
import tornado import tornado.gen import tornado.concurrent
config.toggle() self.assertFalse(config.details_warning_icon_is_present) self.assertFalse(config.details_message_is_present)
def __eq__(self, other): raise NotImplementedError def __ne__(self, other): equal = self.__eq__(other) if equal is NotImplemented: return NotImplemented return not equal def __lt__(self, other): raise NotImplementedError def __le__(self, other): less = self.__lt__(other) if less is NotImplemented or not less: return self.__eq__(other) return less def __gt__(self, other): less = self.__lt__(other) if less is NotImplemented: return NotImplemented equal = self.__eq__(other) if equal is NotImplemented: return NotImplemented return not (less or equal) def __ge__(self, other): less = self.__lt__(other) if less is NotImplemented: return NotImplemented return not less
outside_tar = self.unsafe_common_dir / "unsafe_file.tar.gz" with tarfile.open(outside_tar, "w:gz") as tar: tar.addfile(tarfile.TarInfo(str(self.content_dir / "a_file"))) return outside_tar
match = datetime_re.match(value) if match: kw = match.groupdict() if kw['microsecond']: kw['microsecond'] = kw['microsecond'].ljust(6, '0') tzinfo = kw.pop('tzinfo') if tzinfo == 'Z': tzinfo = utc elif tzinfo is not None: offset_mins = int(tzinfo[-2:]) if len(tzinfo) > 3 else 0 offset = 60 * int(tzinfo[1:3]) + offset_mins if tzinfo[0] == '-': offset = -offset tzinfo = get_fixed_timezone(offset) kw = {k: int(v) for k, v in six.iteritems(kw) if v is not None} kw['tzinfo'] = tzinfo return datetime.datetime(**kw)
creq_in = context.socket(zmq.REP) creq_in.setsockopt(zmq.LINGER, 100) creq_in.bind('ipc://' + self.cache_sock)
continue
TEST_DIR = path(__file__).dirname() TEST_DATA_DIR = 'common/test/data/' PLATFORM_ROOT = TEST_DIR.parent.parent.parent.parent TEST_DATA_ROOT = PLATFORM_ROOT / TEST_DATA_DIR
random_state = check_random_state(random_state) X, y = make_sparse_uncorrelated(random_state=random_state) X = sparse.coo_matrix(X) Y = np.vstack((y, y)).T n_features = X.shape[1]
return "{store}[{collection}] already has {element_id} ({exception})".format( store=self.store, collection=self.collection, element_id=self.element_id, exception=Exception.__str__(self, *args, **kwargs), )
if 'user_id' in context and 'course_id' in context: message_body = substitute_keywords_with_data(message_body, context)
def equal(value, max_d, places, result): self.assertEqual(format_number(Decimal(value), max_d, places), result) equal('0', 12, 3, '0.000') equal('0', 12, 8, '0.00000000') equal('1', 12, 9, '1.000000000') equal('0.00000000', 12, 8, '0.00000000') equal('0.000000004', 12, 8, '0.00000000') equal('0.000000008', 12, 8, '0.00000001') equal('0.000000000000000000999', 10, 8, '0.00000000') equal('0.1234567890', 12, 10, '0.1234567890') equal('0.1234567890', 12, 9, '0.123456789') equal('0.1234567890', 12, 8, '0.12345679') equal('0.1234567890', 12, 5, '0.12346') equal('0.1234567890', 12, 3, '0.123') equal('0.1234567890', 12, 1, '0.1') equal('0.1234567890', 12, 0, '0') equal('0.1234567890', None, 0, '0') equal('1234567890.1234567890', None, 0, '1234567890') equal('1234567890.1234567890', None, 2, '1234567890.12') equal('0.1234', 5, None, '0.1234') equal('123.12', 5, None, '123.12') with self.assertRaises(Rounded): equal('0.1234567890', 5, None, '0.12346') with self.assertRaises(Rounded): equal('1234567890.1234', 5, None, '1234600000')
import bz2 import copy
tmp_start = self._start + (other._start - self._start) * \ self._step // gcd * s new_step = self._step * other._step // gcd new_index = RangeIndex(tmp_start, int_high, new_step, fastpath=True)
HAS_LIBS = False try: import azure import azure.storage import azure.servicemanagement from azure.common import (AzureConflictHttpError, AzureMissingResourceHttpError, AzureException) import salt.utils.msazure from salt.utils.msazure import object_to_dict HAS_LIBS = True except ImportError: pass
import difflib import salt.utils import salt.utils.network import salt.loader
Site.objects.all().delete()
is_copy = axis != 0 or result._is_view result._set_is_copy(self, copy=is_copy) return result
cat = Categorical([0, 1, 2]) self.assertFalse(cat.ordered) cat = Categorical([0, 1, 2], ordered=False) self.assertFalse(cat.ordered) cat = Categorical([0, 1, 2], ordered=True) self.assertTrue(cat.ordered)
nanosecond = getattr(other, 'nanosecond', 0) other = datetime(other.year, other.month, other.day, other.hour, other.minute, other.second, other.microsecond) n = self.n if n >= 0: if (other.time() == self.end or not self._onOffset(other, businesshours)): other = self._next_opening_time(other) else: if other.time() == self.start: other = other - timedelta(seconds=1) if not self._onOffset(other, businesshours): other = self._next_opening_time(other) other = other + bhdelta
make_asset_xml(num_assets, ASSET_XML_PATH) validate_xml(ASSET_XSD_PATH, ASSET_XML_PATH)
import salt.utils
import salt.utils.jid import salt.returners
import os
return CourseLocator( org=course_info['org'], course=course_info['course'], run=course_info['run'], branch=branch, )
return alias
topics = self.discussion_topics return [d["id"] for d in topics.values()]
logadm.__salt__ = {}
start = datetime(1999, 3, 1, 5) end = datetime(2012, 7, 31, 4) bad_ind = date_range(start, end, freq="30min") df = DataFrame({'close': 1}, index=bad_ind)
module.system.DEBUG = False
random_state = check_random_state(0) y_true = random_state.randint(0, 5, size=(n_samples, )) y_pred = random_state.randint(0, 5, size=(n_samples, )) y_score = random_state.random_sample(size=(n_samples, 5)) for name in ALL_METRICS: if (name in METRICS_WITHOUT_SAMPLE_WEIGHT or name in METRIC_UNDEFINED_BINARY_MULTICLASS): continue metric = ALL_METRICS[name] if name in THRESHOLDED_METRICS: yield check_sample_weight_invariance, name, metric, y_true, y_score else: yield check_sample_weight_invariance, name, metric, y_true, y_pred
if not set_permissions(username, permissions, uid): log.warning('unable to set user permissions') delete_user(username, uid) return False
if ( (obj["thread_type"] == "question" and endorsed is None) or (obj["thread_type"] == "discussion" and endorsed is not None) ): return None path = reverse("comment-list") query_dict = {"thread_id": obj["id"]} if endorsed is not None: query_dict["endorsed"] = endorsed return self.context["request"].build_absolute_uri( urlunparse(("", "", path, "", urlencode(query_dict), "")) )
if left.columns.nlevels != right.columns.nlevels: msg = ('merging between different levels can give an unintended ' 'result ({0} levels on the left, {1} on the right)') msg = msg.format(left.columns.nlevels, right.columns.nlevels) warnings.warn(msg, UserWarning)
class CustomWidget(TextInput): def render(self, name, value, attrs=None): return format_html(str('<input{} required />'), ' id=custom') class SampleForm(Form): name = CharField(widget=CustomWidget) f = SampleForm(data={'name': 'bar'}) self.assertIsInstance(force_text(f['name']), SafeData)
self.assertIs(ix[:, :], f)
return (name, kwargs)
with patch('student.models.cc.User.save'): uname = 'student' email = 'student@edx.org'
f = GenericIPAddressField() self.assertEqual(f.clean(' ::ffff:0a0a:0a0a '), '::ffff:10.10.10.10') self.assertEqual(f.clean(' ::ffff:10.10.10.10 '), '::ffff:10.10.10.10') self.assertEqual(f.clean(' 2001:000:a:0000:0:fe:fe:beef '), '2001:0:a::fe:fe:beef') self.assertEqual(f.clean(' 2001::a:0000:0:fe:fe:beef '), '2001:0:a::fe:fe:beef')
return value.center(int(arg))
self.assertEqual(self.func(None), [])
class WriterForm(forms.Form): persons = forms.ModelMultipleChoiceField(show_hidden_initial=True, queryset=Writer.objects.all()) person1 = Writer.objects.create(name="Person 1") person2 = Writer.objects.create(name="Person 2") form = WriterForm(initial={'persons': [person1, person2]}, data={'initial-persons': [str(person1.pk), str(person2.pk)], 'persons': [str(person1.pk), str(person2.pk)]}) self.assertTrue(form.is_valid()) self.assertFalse(form.has_changed()) form = WriterForm(initial={'persons': [person1, person2]}, data={'initial-persons': [str(person1.pk), str(person2.pk)], 'persons': [str(person2.pk)]}) self.assertTrue(form.is_valid()) self.assertTrue(form.has_changed())
exc_info_on_loglevel=logging.DEBUG
def __init__(self, opts, grains, minion_id, saltenv, ext=None, functions=None, pillar=None, pillarenv=None):
@functools.wraps(get_target_space) def outer(self): if (not self.rnn_friendly and self._requires_reshape and (not isinstance(get_target_space(self), SequenceSpace) and not isinstance(get_target_space(self), SequenceDataSpace))): if isinstance(self.mlp.input_space, SequenceSpace): return SequenceSpace(get_target_space(self)) elif isinstance(self.mlp.input_space, SequenceDataSpace): return SequenceDataSpace(get_target_space(self)) else: return get_target_space(self) return outer
enrollment.update(**enrollment.pop('course_details')) course_key = CourseKey.from_string(enrollment['course_id']) self.include_verified_mode_info(enrollment, course_key) enrollment['manual_enrollment'] = self.manual_enrollment_data(enrollment, course_key)
])
if token != '': new_headers['X-Amz-Security-Token'] = token
name_instructions = _(u"Your legal name, used for any certificates you earn.")
dimensiondata.__active_provider_name__ = '' dimensiondata.__opts__ = { 'providers': { 'my-dimensiondata-cloud': { 'dimensiondata': { 'driver': 'dimensiondata', 'region': 'dd-au', 'user_id': 'jon_snow', 'key': 'IKnowNothing' } } } } VM_NAME = 'winterfell'
self.assertEqual(ugettext("Image"), "Bild") with translation.override('en'): self.assertEqual(ugettext("Image"), "Image") with translation.override('en-us'): self.assertEqual(ugettext("Image"), "Image") with translation.override('en-ca'): self.assertEqual(ugettext("Image"), "Image")
user = User.objects.get(email=self.enrolled_student.email) self.assertFalse(CourseEnrollment.is_enrolled(user, self.course.id))
self.assertTrue(acid_block.init_fn_passed) self.assertTrue(acid_block.resource_url_passed) self.assertTrue(acid_block.scope_passed('user_state')) self.assertTrue(acid_block.scope_passed('user_state_summary')) self.assertTrue(acid_block.scope_passed('preferences')) self.assertTrue(acid_block.scope_passed('user_info'))
try: return cls(unit=string) except ValueError: raise TypeError("could not construct DatetimeTZDtype")
@patch('os.access') def test_existing_binary_in_windows_pathext(self, osaccess): osaccess.side_effect = [ False, False, False, False, False, True ] with patch.dict(os.environ, {'PATH': '/bin', 'PATHEXT': '.COM;.EXE;.BAT;.CMD;.VBS;' '.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.PY'}): with patch('salt.utils.is_windows', lambda: True): with patch('os.path.isfile', lambda x: True): self.assertEqual( salt.utils.which('this-binary-exists-under-windows'), '/bin/this-binary-exists-under-windows.CMD' )
for prefix, root in self.locations: storage = self.storages[root] for path in utils.get_files(storage, ignore_patterns): yield path, storage
if LANGUAGE_SESSION_KEY in request.session: del request.session[LANGUAGE_SESSION_KEY]
s = Series(list('abcdef'))
try: data_root = path(settings.GITHUB_REPO_ROOT) subdir = base64.urlsafe_b64encode(repr(courselike_key)) course_dir = data_root / subdir filename = request.FILES['course-data'].name
self.assertRaises(ValueError, lambda: ci.set_categories( list('cab'), inplace=True))
self.submit_question_answer('p1', {'2_1': 'Incorrect'})
if value is not None:
return mixed_setting["default"]["OPTIONS"]["stores"]
with restrict_course(self.course.id) as redirect_url: url = reverse( 'register_code_redemption', kwargs={'registration_code': 'abcd1234'} ) response = getattr(self.client, method)(url) self.assertRedirects(response, redirect_url)
import salt.states.gem as gem gem.__salt__ = {} gem.__opts__ = {'test': False}
thisMonthEnd = MonthEnd(0) thisBMonthEnd = BMonthEnd(0) thisYearEnd = YearEnd(0) thisYearBegin = YearBegin(0) thisBQuarterEnd = BQuarterEnd(0) thisQuarterEnd = QuarterEnd(0)
laplacian, dd = graph_laplacian(sims, normed=False, return_diag=True) _, diffusion_map = eigh(laplacian) embedding_2 = diffusion_map.T[:n_components] * dd embedding_2 = _deterministic_vector_sign_flip(embedding_2).T
coords = x
if __opts__['test']: ret['result'] = None ret['comment'] = 'Service {0} set to be disabled'.format(name) return ret
if validator: validator(defval)
pass
for create_data, additional_data_to_expect in data: xblock_cache = XBlockCache.create(create_data) create_data.update(additional_data_to_expect) self.assert_xblock_cache_data(xblock_cache, create_data)
ret = grains.present( name='foo:is:nested', value={'k1': 'v1'}) self.assertEqual(ret['result'], True) self.assertEqual(ret['comment'], 'Set grain foo:is:nested to {\'k1\': \'v1\'}') self.assertEqual(ret['changes'], {'foo': ['one', {'is': {'nested': {'k1': 'v1'}}}, 'correct']}) self.assertEqual( grains.__grains__, {'a': 'aval', 'foo': ['one', {'is': {'nested': {'k1': 'v1'}}}, 'correct']}) self.assertGrainFileContent("a: aval\n" + "foo:\n" + "- one\n" + "- is:\n" + " nested:\n" + " k1: v1\n" + "- correct\n" )
world.wait_for_visible('.mce-window')
X = np.ones((10, 2)) X[:5, :] = 0
s = pd.Series([1., 2., 3.]) result = s.interpolate(limit=1) expected = s assert_series_equal(result, expected)
_change_access(course, user, level, 'revoke', send_email)
self.assertEqual(pivoted.index.name, 'index') self.assertEqual(pivoted.columns.name, 'columns')
template = lookup_template(namespace, template_name) return template.render_unicode(**context_dictionary)
with assertRaisesRegexp(TypeError, '^Level type mismatch'): idx.slice_locs(df.index[1], (16, "a"))
self.assertEqual(Tag.objects.filter(name__in=()).update(name="foo"), 0)
if hasattr(self, 'mlp'): return self.mlp return None
DATADOG = AUTH_TOKENS.get("DATADOG", {}) DATADOG.update(ENV_TOKENS.get("DATADOG", {}))
rast = GDALRaster({ 'datatype': 1, 'width': 16, 'height': 16, 'srid': 4326, 'bands': [{ 'data': range(256), 'nodata_value': 255, }], })
get_user_credentials(self.user)
if (start != int(start) or stop != int(stop) or step != int(step)): return super_getitem(key)
os.remove(filename)
from __future__ import absolute_import
s = pd.SparseSeries([1, np.nan, 2, 0, np.nan]) tm.assert_sp_series_equal(s[...], s)
self.q(css='input.calibration-feedback-button').first.click()
post_data = { '_popup': '1', 'title': 'title with a new\nline', 'content': 'some content', 'date_0': '2010-09-10', 'date_1': '14:55:39', } response = self.client.post(reverse('admin:admin_views_article_add'), post_data) self.assertContains(response, 'title with a new\\nline')
if input_include_probs is None: input_include_probs = {} if input_scales is None: input_scales = {} if not hasattr(rng, 'uniform'): rng = np.random.RandomState(rng) mlp._validate_layer_names(list(input_include_probs.keys())) mlp._validate_layer_names(list(input_scales.keys())) if per_example: outputs = [mlp.dropout_fprop(inputs, default_input_include_prob, input_include_probs, default_input_scale, input_scales) for _ in xrange(num_masks)] else: masks = [generate_dropout_mask(mlp, default_input_include_prob, input_include_probs, rng) for _ in xrange(num_masks)] outputs = [mlp.masked_fprop(inputs, mask, None, default_input_scale, input_scales) for mask in masks] return geometric_mean_prediction(outputs)
self.client = Client() cache.clear()
idxh = date_range('2014-07-01 09:00', freq='S', periods=50) idxl = date_range('2014-07-01 09:00', freq='100L', periods=500) high = Series(np.random.randn(len(idxh)), idxh) low = Series(np.random.randn(len(idxl)), idxl) high.plot() ax = low.plot() self.assertEqual(len(ax.get_lines()), 2) for l in ax.get_lines(): self.assertEqual(PeriodIndex(data=l.get_xdata()).freq, 'L') tm.close()
if not exists(zpool): ret[zpool] = 'storage pool does not exist' return ret
if hasattr(original_file, 'seek'): original_file.seek(0)
SETTINGS_MODULE = None
view_html = None
tab_id_locator = request.json['tab_id_locator']
self.assertTrue(filesystem.exists('policy.json'))
admin = AdminFactory.create()
from reclass.adapters.salt import top as reclass_top from reclass.errors import ReclassException
AUTH_BASE_URL = os.environ.get('test_url', 'http://localhost:8031')
if parts[1] == 'job': if parts[3] == 'new': self.process_new_job_event(salt_data) if salt_data['data']['fun'] == 'grains.items': self.minions = {} elif parts[3] == 'ret': self.process_ret_job_event(salt_data) if salt_data['data']['fun'] == 'grains.items': self.process_minion_update(salt_data) if parts[1] == 'key': self.process_key_event(salt_data) if parts[1] == 'presence': self.process_presence_events(salt_data, token, opts)
cmd = 'dockerng.ps' docker_hosts = get('*', cmd)
if 'enabled' in kwargs: return _enabled_used_error(ret)
rng = np.random.RandomState(42) all_data, _ = make_blobs(n_samples=n_samples_max + n_queries, n_features=n_features, centers=n_centers, shuffle=True, random_state=0) queries = all_data[:n_queries] index_data = all_data[n_queries:]
setattr(self._modulestore, name, value)
for key, func in six.iteritems(self.loader): break self.assertNotEqual(self.loader._dict, {})
from salttesting import TestCase, expectedFailure from salttesting.helpers import ensure_in_syspath
for url in self.non_existing_urls: response = self.client.get(url) self.assertNotEqual(response.content, 'NOTPROVIDED') self.assertNotEqual(response.content, '')
val = Decimal("{0:.2f}".format(Decimal(self.percentage_discount / 100.00) * cost)) return cost - val
raise NotImplementedError( 'subclasses of Loader must provide a get_template_sources() method' )
path = os.path.join('legacy_hdf', 'legacy_table_0.11.h5') with ensure_clean_store(tm.get_data_path(path), mode='r') as store: str(store) assert 'df' in store assert 'df1' in store assert 'mi' in store df = store.select('df') df1 = store.select('df1') mi = store.select('mi') assert isinstance(df, DataFrame) assert isinstance(df1, DataFrame) assert isinstance(mi, DataFrame)
try:
qs = ItalianRestaurant.objects.all() self.assertEqual(str(qs.query).count('JOIN'), 2) qs = ItalianRestaurant.objects.filter(name='foo') self.assertEqual(str(qs.query).count('JOIN'), 2)
y = np.asarray([[1, 0], [1, 0], [1, 0], [2, 1], [2, 1], [2, 1]]) sample_weight = compute_sample_weight([{1: 2, 2: 1}, {0: 1, 1: 2}], y) assert_array_almost_equal(sample_weight, [2., 2., 2., 2., 2., 2.])
linter = MakoTemplateLinter() expressions = linter._find_mako_expressions(data['template']) self.assertTrue(2 <= len(expressions)) self.assertEqual(expressions[0].start_index, data['start_index']) self.assertIsNone(expressions[0].expression)
auth.add_users(request.user, role, user) role_added = True
s = Series(np.random.randint(0, 100, 1000)) grouper = s.apply(lambda x: np.round(x, -1)) grouped = s.groupby(grouper) f = lambda x: x.mean() > 10 old_way = s[grouped.transform(f).astype('bool')] new_way = grouped.filter(f) assert_series_equal(new_way.sort_values(), old_way.sort_values())
self._process_map[pid]['Process'].join(1)
self._enable_cohorting() self._create_verified_cohort() self.assertFalse(VerifiedTrackCohortedCourse.is_verified_track_cohort_enabled(self.course.id)) self._verify_no_automatic_cohorting() self.assertFalse(error_logger.called)
if item['author_name'] is not None: handler.startElement("author", {}) handler.addQuickElement("name", item['author_name']) if item['author_email'] is not None: handler.addQuickElement("email", item['author_email']) if item['author_link'] is not None: handler.addQuickElement("uri", item['author_link']) handler.endElement("author")
log_ais_w = numpy.zeros(batch_size, dtype=floatX)
for Vv, Hv in get_debug_values(V, H_hat): if Vv.shape != (self.model._test_batch_size,self.model.nvis): raise Exception('Well this is awkward. We require visible input test tags to be of shape '+str((self.model._test_batch_size,self.model.nvis))+' but the monitor gave us something of shape '+str(Vv.shape)+". The batch index part is probably only important if recycle_q is enabled. It's also probably not all that realistic to plan on telling the monitor what size of batch we need for test tags. the best thing to do is probably change self.model._test_batch_size to match what the monitor does") assert Vv.shape[0] == Hv.shape[0] if not (Hv.shape[1] == self.model.nhid): raise AssertionError("Hv.shape[1] is %d, does not match self.model.nhid, %d" \ % ( Hv.shape[1], self.model.nhid) ) mu = self.model.mu alpha = self.model.alpha W = self.model.W B = self.model.B w = self.model.w BW = B.dimshuffle(0,'x') * W BW.name = 'infer_S_hat:BW' HS = H_hat * S_hat HS.name = 'infer_S_hat:HS' mean_term = mu * alpha mean_term.name = 'infer_S_hat:mean_term' assert V.dtype == config.floatX assert BW.dtype == config.floatX, \ "Expected %s, got %s" % (config.floatX, BW.dtype) data_term = T.dot(V, BW) data_term.name = 'infer_S_hat:data_term' iterm_part_1 = - T.dot(T.dot(HS, W.T), BW) iterm_part_1.name = 'infer_S_hat:iterm_part_1' assert w.name is not None iterm_part_2 = w * HS iterm_part_2.name = 'infer_S_hat:iterm_part_2' interaction_term = iterm_part_1 + iterm_part_2 interaction_term.name = 'infer_S_hat:interaction_term' for i1v, Vv in get_debug_values(iterm_part_1, V): assert i1v.shape[0] == Vv.shape[0] assert mean_term.dtype == config.floatX assert data_term.dtype == config.floatX assert interaction_term.dtype == config.floatX debug_interm = mean_term + data_term debug_interm.name = 'infer_S_hat:debug_interm' numer = debug_interm + interaction_term numer.name = 'infer_S_hat:numer' assert numer.dtype == config.floatX alpha = self.model.alpha w = self.model.w denom = alpha + w assert denom.dtype == config.floatX denom.name = 'infer_S_hat:denom' S_hat = numer / denom return S_hat
session_status = request.session.get('import_status') if session_status is None: session_status = request.session.setdefault("import_status", {}) session_status[key] = status request.session.save()
return []
s = Series(np.random.randn(21), index=date_range(start='1/1/2012 9:30', freq='1min', periods=21)) s[0] = np.nan
from __future__ import absolute_import import os import logging
if pkg_ver: if installed_pkgs[pkg_name].get('version') != pkg_ver: pkgs_to_install.append(pkg) else: pkgs_satisfied.append(installed_name_ver)
self.assertEqual( self.course.clean_id(), "course_ORSXG5C7N5ZGOL3UMVZXIX3DN52XE43FF52GK43UL5ZHK3Q=" ) self.assertEqual( self.course.clean_id(padding_char='$'), "course_ORSXG5C7N5ZGOL3UMVZXIX3DN52XE43FF52GK43UL5ZHK3Q$" )
course_keys = CourseOverview.get_all_course_keys() for expected_course_key in courses: self.assertNotIn(expected_course_key, course_keys)
assert self.b.ndim == 1 shuffle = [ 'x' ] * 4 shuffle[self.output_axes.index('c')] = 0 return self.b.dimshuffle(*shuffle)
assert_equal(len(folds), len(labels)) for i in np.unique(folds): assert_greater_equal(tolerance, abs(sum(folds == i) - ideal_n_labels_per_fold))
top_dirs = [] dir_init = os.path.join(dirname, "__init__.py") if os.path.exists(dir_init): top_dirs.append(dirname) for directory in ['djangoapps', 'lib']: subdir = os.path.join(dirname, directory) subdir_init = os.path.join(subdir, "__init__.py") if os.path.exists(subdir) and not os.path.exists(subdir_init): dirs = os.listdir(subdir) top_dirs.extend(d for d in dirs if os.path.isdir(os.path.join(subdir, d))) return top_dirs
- file: {good_file}
overrides = salt.utils.clean_kwargs(**copy.deepcopy(kwargs)) profile_match = salt.utils.dictupdate.update( copy.deepcopy(profile_match), overrides ) return profile_match
try: self.clean() except ValidationError as e: errors = e.update_error_dict(errors)
#pylint: disable=E0602
from salt.states import win_path
dict(safe_zip(range(0, 36, 2), numpy.arange(0, 360, 20))),
with self.assertRaises(AttributeError): Book.objects
ver = openpyxl.__version__ if ver >= LooseVersion('2.0.0') and ver < LooseVersion('2.1.0'): number_format = styles.NumberFormat(format_code='0.00') else:
for element in self.q(css=self._bounded_selector(selector)): note = EdxNoteHighlight(self.browser, element, self.item_id) note.select_and_click_adder() yield note note.save()
resp = self.client.get_json(self.url + '/0') self.assertEqual(resp.status_code, 200) obj = json.loads(resp.content) self.assertEqual(self.starting_graders[0], obj)
from __future__ import absolute_import
if out['result'] is None: ret['result'] = False ret['comment'] = out['comment'] return ret
def test_empty(self): resp = JsonResponseBadRequest() self.assertIsInstance(resp, HttpResponseBadRequest) self.assertEqual(resp.content, "") self.assertEqual(resp.status_code, 400) self.assertEqual(resp["content-type"], "application/json") def test_empty_string(self): resp = JsonResponseBadRequest("") self.assertIsInstance(resp, HttpResponse) self.assertEqual(resp.content, "") self.assertEqual(resp.status_code, 400) self.assertEqual(resp["content-type"], "application/json") def test_dict(self): obj = {"foo": "bar"} resp = JsonResponseBadRequest(obj) compare = json.loads(resp.content) self.assertEqual(obj, compare) self.assertEqual(resp.status_code, 400) self.assertEqual(resp["content-type"], "application/json") def test_set_status_kwarg(self): obj = {"error": "resource not found"} resp = JsonResponseBadRequest(obj, status=404) compare = json.loads(resp.content) self.assertEqual(obj, compare) self.assertEqual(resp.status_code, 404) self.assertEqual(resp["content-type"], "application/json") def test_set_status_arg(self): obj = {"error": "resource not found"} resp = JsonResponseBadRequest(obj, 404) compare = json.loads(resp.content) self.assertEqual(obj, compare) self.assertEqual(resp.status_code, 404) self.assertEqual(resp["content-type"], "application/json") def test_encoder(self): obj = [1, 2, 3] encoder = object() with mock.patch.object(json, "dumps", return_value="[1,2,3]") as dumps: resp = JsonResponseBadRequest(obj, encoder=encoder) self.assertEqual(resp.status_code, 400) compare = json.loads(resp.content) self.assertEqual(obj, compare) kwargs = dumps.call_args[1] self.assertIs(kwargs["cls"], encoder)
if __opts__['test']: ret['result'] = None ret['comment'] = 'Group {0} is set for removal'.format(name) return ret ret['result'] = __salt__['group.delete'](name) if ret['result']: ret['changes'] = {name: ''} ret['comment'] = 'Removed group {0}'.format(name) return ret else: ret['comment'] = 'Failed to remove group {0}'.format(name) return ret
epoch_num = 6 termination_criterion = EpochCounter(epoch_num)
if isinstance(value, bytes): return bytes(value) if isinstance(value, six.text_type): return bytes(value.encode(self.charset))
csv_content = "test_student.example.com,dummy_notes" data = self.upload_file(csv_content=csv_content) self.assertEquals(len(data['row_errors']['user_not_exist']), 1) self.assertEquals(len(data['success']), 0) self.assertEquals(len(CertificateWhitelist.objects.all()), 0)
try: migrations_module = import_module(migrations_package_name) except ImportError: pass else: try: return upath(module_dir(migrations_module)) except ValueError: pass
if len(inputfiles) > settings.MAX_FILEUPLOADS_PER_INPUT: msg = 'Submission aborted! Maximum %d files may be submitted at once' % \ settings.MAX_FILEUPLOADS_PER_INPUT return msg
return value.isoformat()
import salt.spm import salt.utils.parsers as parsers from salt.utils.verify import verify_log
solution_element = rendered_html.find("div") self.assertEqual(solution_element.text, 'Input Template Render')
if hasattr(client, 'create_user'): client.create_user(name, passwd) return True
log.debug( 'ext_pillar.mongo: no document found in collection {0}'.format( collection ) ) return {}
try: chunk_ret = yield getattr(self, '_disbatch_{0}'.format(low['client']))(low) ret.append(chunk_ret) except EauthAuthenticationError as exc: ret.append('Failed to authenticate') break except Exception as ex: ret.append('Unexpected exception while handling request: {0}'.format(ex)) logger.error('Unexpected exception while handling request:', exc_info=True)
return (unpickle_inner_exception, (attached_to, name), self.args)
self.register_flag_response("thread", thread_id)
DATE_INPUT_FORMATS = [
if not hasattr(res, 'dtype') or res.dtype.kind not in ['c', 'O']: raise if res.dtype.kind == 'O': if targ.dtype.kind != 'O': res = res.astype(targ.dtype) else: try: res = res.astype('c16') except: res = res.astype('f8') try: targ = targ.astype('c16') except: targ = targ.astype('f8') elif targ.dtype.kind == 'O': raise tm.assert_almost_equal(targ.real, res.real, check_dtype=check_dtype) tm.assert_almost_equal(targ.imag, res.imag, check_dtype=check_dtype)
self.assertEqual(new_draft.edited_by, 'test@edx.org') self.assertEqual(new_draft_locator.version_guid, original_index['versions'][BRANCH_NAME_DRAFT]) new_index = modulestore().get_course_index_info(new_draft_locator.course_key) self.assertEqual(new_index['edited_by'], 'leech_master')
payload = FakePayload('name=value') request = WSGIRequest({'REQUEST_METHOD': 'POST', 'CONTENT_TYPE': 'application/x-www-form-urlencoded', 'CONTENT_LENGTH': len(payload), 'wsgi.input': payload}) self.assertEqual(request.POST, {'name': ['value']}) self.assertEqual(request.body, b'name=value') self.assertEqual(request.read(), b'name=value')
available_version = salt.utils.alias_function(latest_version, 'available_version')
assert_raises(ValueError, compute_sample_weight, {1: 2, 2: 1}, y, range(4))
f = PhoneField() with self.assertRaisesMessage(ValidationError, "'This field is required.'"): f.clean('') with self.assertRaisesMessage(ValidationError, "'This field is required.'"): f.clean(None) with self.assertRaisesMessage(ValidationError, "'This field is required.'"): f.clean([]) with self.assertRaisesMessage(ValidationError, "'This field is required.'"): f.clean(['+61']) with self.assertRaisesMessage(ValidationError, "'This field is required.'"): f.clean(['+61', '287654321', '123']) self.assertEqual('+61.287654321 ext. 123 (label: Home)', f.clean(['+61', '287654321', '123', 'Home'])) with self.assertRaisesMessage(ValidationError, "'Enter a valid country code.'"): f.clean(['61', '287654321', '123', 'Home'])
self._setup_test_topfile_mocks(Matcher, get_file_client, 1, 2) pillar = salt.pillar.Pillar(opts, grains, 'mocked-minion', 'base') self.assertEqual(pillar.compile_pillar()['ssh'], 'bar') self._setup_test_topfile_mocks(Matcher, get_file_client, 2, 1) pillar = salt.pillar.Pillar(opts, grains, 'mocked-minion', 'base') self.assertEqual(pillar.compile_pillar()['ssh'], 'foo')
if com.is_numeric_v_string_like(arr, x): mask |= False else: mask |= arr == x
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.deny("Not valid!")
return hash(str(self))
for item in list(required_keys): if item in provider_key: required_keys.remove(item)
fire_event( 'event', '{0} has been deployed at {1}'.format(name, host), 'salt/cloud/{0}/deploy_windows'.format(name), {'name': name}, transport=opts.get('transport', 'zeromq') )
try:
win_ntp.__salt__ = {}
log = logging.getLogger(__name__)
if request is not None and hasattr(request, 'META'): return get_ip(request) else: return default
self.assertEqual(self.team_page.team_name, name) self.assertEqual(self.team_page.team_description, description) self.assertEqual(self.team_page.team_location, location) self.assertEqual(self.team_page.team_language, language)
if len(verstr) > 0 and verstr[0] != ':' and verstr[0] != '[': prefix = prefix or '=' target = '{0}{1}-{2}'.format(prefix, param, verstr) else: target = '{0}{1}'.format(param, verstr)
self.assertEqual(2, Country.objects.count()) self.assertEqual(8, City.objects.count()) self.assertEqual(2, State.objects.count())
cart = Order.get_cart_for_user(user=self.user) CertificateItem.add_to_order(cart, self.course_key, self.cost, 'honor', currency='usd') cart.start_purchase() self.assertEqual(cart.status, 'paying') for item in cart.orderitem_set.all(): self.assertEqual(item.status, 'paying')
state = dict(self.__dict__) state.pop('f_', None) return state
lazymod_py_file = lazymod.__file__ if lazymod_py_file.endswith("c"): lazymod_py_file = lazymod_py_file[:-1]
self.q(css=self.search_bar_selector + ' [type="submit"]').click() self.wait_for_ajax() self.wait_for_element_visibility(self.search_results_selector, 'Search results are visible')
_solver_type_dict = { 'logistic_regression': { 'l1': {False: 6}, 'l2': {False: 0, True: 7}}, 'hinge': { 'l2': {True: 3}}, 'squared_hinge': { 'l1': {False: 5}, 'l2': {False: 2, True: 1}}, 'epsilon_insensitive': { 'l2': {True: 13}}, 'squared_epsilon_insensitive': { 'l2': {False: 11, True: 12}}, 'crammer_singer': 4 }
for alg in ['SAMME', 'SAMME.R']: obj = AdaBoostClassifier(algorithm=alg) obj.fit(iris.data, iris.target) score = obj.score(iris.data, iris.target) s = pickle.dumps(obj)
from __future__ import absolute_import import functools import logging import os.path import os import re import time
return super(Ridge, self).fit(X, y, sample_weight=sample_weight)
verify_name(self.seq_usage_key, self.chapter_usage_key, "Duplicate of sequential")
new_minor = list(self.panel4d.minor_axis[:2]) result = self.panel4d.reindex(minor=new_minor) assert_frame_equal( result['l2']['ItemB'], ref['ItemB'].reindex(columns=new_minor))
pandas_sql = pandasSQL_builder(con) if isinstance(pandas_sql, SQLiteDatabase): return pandas_sql.read_query( sql, index_col=index_col, params=params, coerce_float=coerce_float, parse_dates=parse_dates, chunksize=chunksize) try: _is_table_name = pandas_sql.has_table(sql) except: _is_table_name = False if _is_table_name: pandas_sql.meta.reflect(only=[sql]) return pandas_sql.read_table( sql, index_col=index_col, coerce_float=coerce_float, parse_dates=parse_dates, columns=columns, chunksize=chunksize) else: return pandas_sql.read_query( sql, index_col=index_col, params=params, coerce_float=coerce_float, parse_dates=parse_dates, chunksize=chunksize)
df = DataFrame(dict(A=np.arange(5, dtype='int64'), B=np.arange( 5, 10, dtype='int64'))) df.iloc[2:4] = [[10, 11], [12, 13]] expected = DataFrame(dict(A=[0, 1, 10, 12, 4], B=[5, 6, 11, 13, 9])) assert_frame_equal(df, expected)
task_type = 'rescore_problem' task_class = rescore_problem task_input, task_key = encode_entrance_exam_and_student_input(usage_key, student) return submit_task(request, task_type, task_class, usage_key.course_key, task_input, task_key)
from __future__ import unicode_literals
get_conn.return_value.send_messages.side_effect = cycle([SMTPDataError(554, "Email address is blacklisted"), None, None, None]) students = [UserFactory() for _ in xrange(settings.BULK_EMAIL_EMAILS_PER_TASK - 1)] for student in students: CourseEnrollmentFactory.create(user=student, course_id=self.course.id)
if isinstance(metric, partial): metric.__module__ = 'tmp' metric.__name__ = name
results = (course for course in results if course.scope_ids.block_type == 'course')
if '_dataset' in d: d['_datasets'] = [d['_dataset']] del d['_dataset']
super(OAuth2ProviderConfig, self).clean() self.other_settings = clean_json(self.other_settings, dict)
if len(sindexers) == 1 and sindexers[0] != 0: df = df.T
spaces = map(lambda c: get_space(c[0]), self.costs)
altered_grader = CourseGradingModel.fetch(self.course.id) self.assertDictEqual(test_grader.grade_cutoffs, altered_grader.grade_cutoffs, "Noop update")
if not self._rot_set: self.rot = 30 format_date_labels(ax, rot=self.rot)
try: conn.enable_load_extension(True) except AttributeError: raise ImproperlyConfigured( 'The pysqlite library does not support C extension loading. ' 'Both SQLite and pysqlite must be configured to allow ' 'the loading of extensions to use SpatiaLite.') cur = conn.cursor(factory=SQLiteCursorWrapper) try: cur.execute("SELECT load_extension(%s)", (self.spatialite_lib,)) except Exception as msg: new_msg = ( 'Unable to load the SpatiaLite library extension ' '"%s" because: %s') % (self.spatialite_lib, msg) six.reraise(ImproperlyConfigured, ImproperlyConfigured(new_msg), sys.exc_info()[2]) cur.close() return conn
df = DataFrame({'A1': np.random.randn(20), 'A2': np.random.randn(20), 'B': 'foo', 'C': 'bar'}, index=np.arange(20))
s = pd.Series([True, False, True]) result = s.replace(True, '2u') expected = pd.Series(['2u', False, '2u']) tm.assert_series_equal(expected, result)
with connection.schema_editor() as editor: editor.create_model(BookWithoutAuthor) self.assertEqual( self.get_constraints_for_column(BookWithoutAuthor, 'title'), ['schema_book_d5d3db17', 'schema_book_title_2dfb2dff_like'] ) old_field = BookWithoutAuthor._meta.get_field('title') new_field = CharField(max_length=100, db_index=True, unique=True) new_field.set_attributes_from_name('title') with connection.schema_editor() as editor: editor.alter_field(BookWithoutAuthor, old_field, new_field, strict=True) self.assertEqual( self.get_constraints_for_column(BookWithoutAuthor, 'title'), ['schema_book_d5d3db17', 'schema_book_title_2dfb2dff_like', 'schema_book_title_2dfb2dff_uniq'] ) old_field = BookWithoutAuthor._meta.get_field('title') new_field = CharField(max_length=100, db_index=True) new_field.set_attributes_from_name('title') with connection.schema_editor() as editor: editor.alter_field(BookWithoutAuthor, old_field, new_field, strict=True) self.assertEqual( self.get_constraints_for_column(BookWithoutAuthor, 'title'), ['schema_book_d5d3db17', 'schema_book_title_2dfb2dff_like', 'schema_book_title_2dfb2dff_uniq'] )
from xmodule.modulestore.inheritance import InheritanceMixin from xmodule.modulestore import prefer_xmodules from xmodule.x_module import XModuleMixin
GeoIPRecord_delete = lgeoip.GeoIPRecord_delete GeoIPRecord_delete.argtypes = [RECTYPE] GeoIPRecord_delete.restype = None
if self.isalive(): self.wait()
course_modes = CourseMode.modes_for_course(CourseKey.from_string(course_id)) available_modes = [m.slug for m in course_modes] if CourseMode.DEFAULT_MODE_SLUG in available_modes: return CourseMode.DEFAULT_MODE_SLUG elif 'audit' in available_modes: return 'audit' elif 'honor' in available_modes: return 'honor' return CourseMode.DEFAULT_MODE_SLUG
r = conn.describe_stacks(name) if r: stack = r[0] log.debug('Found VPC: {0}'.format(stack.stack_id)) keys = ('stack_id', 'description', 'stack_status', 'stack_status_reason')
self.update_structure(usage_locator.course_key, new_structure)
index = v.index.copy() if index.name is None: names = set(v.name for v in values) if len(names) == 1: index.name = list(names)[0]
mask = isnull(values) if mask.any(): result = np.array(result) result[mask] = na_rep result = result.tolist()
self._job_queue = queue.Queue(queue_size)
def __init__(self, class_instance): self.class_instance = class_instance def __str__(self): return ("This method must be defined in the concrete class of %s" % self.class_instance.__class__.__name__)
if raise_error is not None: raise raise_error
return id_
return child_class( self.browser, self.q(css=child_class.BODY_SELECTOR).filter( lambda el: title in [inner.text for inner in el.find_elements_by_css_selector(child_class.NAME_SELECTOR)] ).attrs('data-locator')[0] )
self.check_migrations() now = datetime.now().strftime('%B %d, %Y - %X') if six.PY2: now = now.decode(get_system_encoding()) self.stdout.write(now) self.stdout.write(( "Django version %(version)s, using settings %(settings)r\n" "Starting development server at http://%(addr)s:%(port)s/\n" "Quit the server with %(quit_command)s.\n" ) % { "version": self.get_version(), "settings": settings.SETTINGS_MODULE, "addr": '[%s]' % self.addr if self._raw_ipv6 else self.addr, "port": self.port, "quit_command": quit_command, })
subdomain = microsite.get_value('subdomain', 'default') if hasattr(settings, 'COURSE_LISTINGS') and subdomain in settings.COURSE_LISTINGS and not settings.DEBUG: filtered_visible_ids = frozenset( [SlashSeparatedCourseKey.from_deprecated_string(c) for c in settings.COURSE_LISTINGS[subdomain]] )
if mode is None and not salt.utils.is_windows(): mask = os.umask(0) os.umask(mask) mode = oct((0o777 ^ mask) & 0o666) ret, _ = check_perms(name, ret, user, group, mode)
AUTH_BASE_URL = os.environ.get('test_url', 'http://localhost:8031')
self.assertTrue(act.called) expected_tasks = [ftask.to_dict() for ftask in self.tasks] actual_tasks = json.loads(response.content)['tasks'] for exp_task, act_task in zip(expected_tasks, actual_tasks): self.assertDictEqual(exp_task, act_task) self.assertEqual(actual_tasks, expected_tasks)
if not is_platform_windows(): for i in [tm.makeStringIndex(10), tm.makeUnicodeIndex(10)]: self.assertRaises(ValueError, lambda: frequencies.infer_freq(i))
self.assertRaises( ValidationError, BadgeClass( slug='test', issuing_component='test2', criteria='test3', description='test4', image=get_image('unbalanced') ).full_clean )
ref = fromstr( 'MULTIPOLYGON(((12.4 43.87,12.45 43.87,12.45 44.1,12.5 44.1,12.5 43.87,12.45 43.87,12.4 43.87)))' ) self.assertTrue( ref.equals_exact( Country.objects.annotate( snap=functions.SnapToGrid('mpoly', 0.05, 0.23, 0.5, 0.17) ).get(name='San Marino').snap, tol ) )
render_args, _ = module.system.render_template.call_args self.assertEqual(len(render_args), 2)
task_progress.succeeded += 1 if not header: header = [section['label'] for section in gradeset[u'section_breakdown']] rows.append( ["id", "email", "username", "grade"] + header + cohorts_header + group_configs_header + teams_header + ['Enrollment Track', 'Verification Status'] + certificate_info_header )
self.store.convert_to_draft(self.problem.location, self.user.id)
expc = Categorical.from_codes(np.arange(4).repeat(8), levels, ordered=True) exp = CategoricalIndex(expc) self.assert_index_equal(desc_result.index.get_level_values(0), exp) exp = Index(['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'] * 4) self.assert_index_equal(desc_result.index.get_level_values(1), exp)
def __init__(self, offset=None, name=None): if offset is not None: self.__offset = timedelta(minutes=offset) if name is not None: self.__name = name def utcoffset(self, dt): return self.__offset def tzname(self, dt): return self.__name def dst(self, dt): return ZERO
def wrapper(cls): orig_vars = cls.__dict__.copy() orig_vars.pop('__dict__', None) orig_vars.pop('__weakref__', None) for slots_var in orig_vars.get('__slots__', ()): orig_vars.pop(slots_var) return metaclass(cls.__name__, cls.__bases__, orig_vars) return wrapper
if obj.min_num is None: return [] elif not isinstance(obj.min_num, int): return must_be('an integer', option='min_num', obj=obj, id='admin.E205') else: return []
operation = migrations.CreateModel( "Food", fields=[ ("id", models.AutoField(primary_key=True)), ], managers=[ ("food_qs", FoodQuerySet.as_manager()), ("food_mgr", FoodManager("a", "b")), ("food_mgr_kwargs", FoodManager("x", "y", 3, 4)), ] ) self.assertEqual(operation.describe(), "Create model Food") new_state = project_state.clone() operation.state_forwards("test_cmoma", new_state) self.assertIn(("test_cmoma", "food"), new_state.models) managers = new_state.models["test_cmoma", "food"].managers self.assertEqual(managers[0][0], "food_qs") self.assertIsInstance(managers[0][1], models.Manager) self.assertEqual(managers[1][0], "food_mgr") self.assertIsInstance(managers[1][1], FoodManager) self.assertEqual(managers[1][1].args, ("a", "b", 1, 2)) self.assertEqual(managers[2][0], "food_mgr_kwargs") self.assertIsInstance(managers[2][1], FoodManager) self.assertEqual(managers[2][1].args, ("x", "y", 3, 4))
white_label_org = microsite.get_value('course_org_filter') if white_label_org: zendesk_tags = zendesk_tags + ["whitelabel_{org}".format(org=white_label_org)]
import salt.utils.boto3 import salt.utils.compat import salt.utils
try:
assert_equals(attempt_2, SoftwareSecurePhotoVerification.active_for_user(user))
'username': example_cert.uuid,
func_globals.update(overridden_func_globals)
self.q(css=self.PAGE_NUMBER_INPUT_CSS).results[0].send_keys(unicode(page_number), Keys.ENTER) self.wait_for_ajax()
ret['result'] = None return ret
info = np.zeros(span, dtype=[('val', np.int64), ('maj', bool), ('min', bool), ('fmt', '|S20')]) info['val'][:] = dates_.values info['fmt'][:] = '' info['maj'][[0, -1]] = True info_maj = info['maj'] info_min = info['min'] info_fmt = info['fmt']
if self._convert_dates: for c, o in zip(columns, original_columns): if c != o: self._convert_dates[c] = self._convert_dates[o] del self._convert_dates[o]
result = sparse.loc[orig % 2 == 1] exp = orig.loc[orig % 2 == 1].to_sparse() tm.assert_sp_series_equal(result, exp)
output = StringIO.StringIO() test_log_handler = logging.StreamHandler(output) test_log_handler.setLevel(logging.DEBUG) glog = git_import.log glog.addHandler(test_log_handler)
class CommonFeature(Article):
field_dictionary, filter_dictionary, _ = LmsSearchFilterGenerator.generate_field_filters( user=self.user, course_id=unicode(self.courses[0].id) ) self.assertTrue('start_date' in filter_dictionary) self.assertEqual(unicode(self.courses[0].id), field_dictionary['course'])
svms = [ svm.SVC(kernel='linear').fit(iris.data, iris.target), svm.NuSVC(kernel='linear').fit(iris.data, iris.target), svm.SVR(kernel='linear').fit(iris.data, iris.target), svm.NuSVR(kernel='linear').fit(iris.data, iris.target), svm.OneClassSVM(kernel='linear').fit(iris.data), ] for clf in svms: assert_raises(AttributeError, clf.__setattr__, 'coef_', np.arange(3)) assert_raises((RuntimeError, ValueError), clf.coef_.__setitem__, (0, 0), 0)
}
try: original_pip_version = pip.__version__ pip.__version__ = MagicMock( side_effect=AttributeError( 'Faked missing __version__ attribute' ) ) except AttributeError: pass
if 'cmap' in kwds and colormap: raise TypeError("Only specify one of `cmap` and `colormap`.") elif 'cmap' in kwds: self.colormap = kwds.pop('cmap') else: self.colormap = colormap
split = cval.train_test_split(X, y, test_size=None, train_size=.5) X_train, X_test, y_train, y_test = split assert_equal(len(y_test), len(y_train)) assert_array_equal(X_train[:, 0], y_train * 10) assert_array_equal(X_test[:, 0], y_test * 10)
if names is None: names = ['time', 'panel'] time, panels = _ensure_like_indices(time, panels) time_factor = Categorical.from_array(time, ordered=True) panel_factor = Categorical.from_array(panels, ordered=True) labels = [time_factor.codes, panel_factor.codes] levels = [time_factor.categories, panel_factor.categories] return MultiIndex(levels, labels, sortorder=None, names=names, verify_integrity=False)
self.na_op = lambda x, y: getattr(x, self.name)(y) return lvalues, rvalues
pieces = modulename.split('.') str_e = str(e) found = True in [piece.find(str(e)) != -1 for piece in pieces]
l = [1, 2, 3]
td = Series(date_range('2012-1-1', periods=3, freq='D')) - \ Timestamp('20120101')
potential_clashes = rel_opts.fields + rel_opts.many_to_many for clash_field in potential_clashes:
return self._time_has_obs.astype(int)
raise TypeError('incompatible index of inserted column ' 'with frame index')
if tag.startswith('__master_disconnected') and data['master'] != self.opts['master']: raise SaltException('Bad master disconnected \'{0}\' when mine one is \'{1}\''.format( data['master'], self.opts['master'])) if tag.startswith('__master_failback'): if data['master'] != self.opts['master_list'][0]: raise SaltException('Bad master \'{0}\' when mine failback is \'{1}\''.format( data['master'], self.opts['master'])) elif data['master'] == self.opts['master'][0]: raise SaltException('Already connected to \'{0}\''.format(data['master']))
df0 = DataFrame({"a": [datetime(2000, 1, 1), datetime(2000, 1, 2), datetime(2000, 1, 3)]}) df1 = DataFrame({"a": [None, None, None]}) df2 = df1.combine_first(df0) assert_frame_equal(df2, df0)
with connection.schema_editor() as editor: editor.create_model(Author) editor.create_model(TagM2MTest) editor.create_model(LocalBookWithM2M) columns = self.column_classes(LocalBookWithM2M._meta.get_field("tags").remote_field.through) self.assertEqual(columns['tagm2mtest_id'][0], "IntegerField")
import salt.modules.cmdmod import salt.modules.smbios
return list(self._params)
prefix = 'django_%s_template_' % self.app_or_project tempdir = tempfile.mkdtemp(prefix=prefix, suffix='_extract') self.paths_to_remove.append(tempdir) if self.verbosity >= 2: self.stdout.write("Extracting %s\n" % filename) try: archive.extract(filename, tempdir) return tempdir except (archive.ArchiveException, IOError) as e: raise CommandError("couldn't extract file %s to %s: %s" % (filename, tempdir, e))
self.assertEqual(resp_obj.status_code, http_code) self.assertIn('error_code', resp_obj.data) self.assertEqual(resp_obj.data['error_code'], error_code_str)
for section in self.chapter.get_children(): section.visible_to_staff_only = True self.store.update_item(section, ModuleStoreEnum.UserID.test)
is_safe = isinstance(value, SafeData) value = value.replace('%%', '%') value = mark_safe(value) if is_safe else value if self.asvar: context[self.asvar] = value return '' else: return value
for panel in row['panels']: _ids.append(_ids[-1] + 1) panel['id'] = _ids[-1] title = row['title'] if title not in _data: update_rows.append(title) _dashboard['rows'].append(row) continue _n = _data[title] if _rows_differ(row, _dashboard['rows'][_n]): _dashboard['rows'][_n] = row update_rows.append(title)
self.assertEqual(i, 100) self.assertEqual(s, 1)
ts.resample('d').mean()
for course_id, store in self.mappings.iteritems(): candidate_key = store.make_course_key(org, course, run) if candidate_key == course_id: return candidate_key
nsqrt = sqrt(n_samples) llconst = n_features * log(2. * np.pi) + n_components var = np.var(X, axis=0)
import xml.etree.cElementTree as ElementTree
filelist = random.sample(filelist, int(num_entries / self._cull_frequency)) for fname in filelist: self._delete(fname)
OrgInstructorRole(self.course_key.org).add_users(self.student) CourseInstructorRole(self.course_key).remove_users(self.student) self.assertTrue( OrgInstructorRole(self.course_key.org).has_user(self.student), "Student lost has access to {}".format(self.course_key.org) ) self.assertFalse( CourseInstructorRole(self.course_key).has_user(self.student), "Student doesn't have access to {}".format(unicode(self.course_key)) )
matching_coupons = Coupon.objects.filter(code=code, is_active=True) if matching_coupons: return save_registration_code( user, course_id, mode_slug, invoice=invoice, order=order, invoice_item=invoice_item )
_decorate_axes(ax, freq, kwds) if hasattr(ax, 'left_ax'): _decorate_axes(ax.left_ax, freq, kwds) if hasattr(ax, 'right_ax'): _decorate_axes(ax.right_ax, freq, kwds) ax._plot_data.append((data, cls._kind, kwds))
if self.flag == 1: raise Unauthorized return True
f = ignore_warnings scoring = make_scorer(mean_squared_error, greater_is_better=False) ridge_gcv2 = RidgeCV(fit_intercept=False, scoring=scoring) f(ridge_gcv2.fit)(filter_(X_diabetes), y_diabetes) assert_equal(ridge_gcv2.alpha_, alpha_)
if not isinstance(expr, list): return expr for row in expr: if not isinstance(row, list): return expr rdim = len(expr[0]) for row in expr: if not len(row) == rdim: return expr return sympy.Matrix(expr)
if process_all: locales = all_locales else: locales = locale or all_locales locales = set(locales) - set(exclude)
CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] ).install()
from salt.states import mysql_grants
HAS_MARKUPSAFE = False
CourseModeFactory.create( course_id=self.course_key, mode_display_name="Honor Not Expired", mode_slug="honor_not_expired", expiration_datetime=future )
transcripts_utils.download_youtube_subs(good_youtube_sub, self.course, settings)
raise NotImplementedError
if n_iter > 0: ss = ((prev_alpha[0] - alpha_min) / (prev_alpha[0] - alpha[0])) coef[:] = prev_coef + ss * (coef - prev_coef) alpha[0] = alpha_min
service = _get_service(name) path = service['file_path']
if connection.features.allows_auto_pk_0: test_data.extend([ (data_obj, 0, Anchor, "Anchor 0"), (fk_obj, 465, FKData, 0), ])
used_all_attempts = CapaFactory.create(showanswer='finished', max_attempts="1", attempts="1", due=self.tomorrow_str) self.assertTrue(used_all_attempts.answer_available())
if item['unique_id'] is not None: unique_id = item['unique_id'] else: unique_id = get_tag_uri(item['link'], item['pubdate']) handler.addQuickElement("id", unique_id)
with connection.schema_editor() as editor: operation.database_backwards("test_runsql", editor, new_state, project_state) self.assertTableNotExists("i_love_ponies")
import salt.config import salt.wheel import salt.auth
k_dims = self.k1.n_dims for i, kernel in enumerate(self.kernels): kernel.theta = theta[i * k_dims:(i + 1) * k_dims]
self.assertEqual(Player.objects.filter(games__season__year=2009).distinct().count(), 2) self.assertEqual(Player.objects.filter(games__season__year__exact=2009).distinct().count(), 2) self.assertEqual(Player.objects.filter(games__season__gt=111).distinct().count(), 2) self.assertEqual(Player.objects.filter(games__season__gt__exact=111).distinct().count(), 2)
def get_field(*args, **kwargs): kwargs['db_column'] = "CamelCase" field = kwargs.pop('field_class', IntegerField)(*args, **kwargs) field.set_attributes_from_name("CamelCase") return field model = Author field = get_field() table = model._meta.db_table column = field.column with connection.schema_editor() as editor: editor.create_model(model) editor.add_field(model, field) editor.execute( editor.sql_create_index % { "table": editor.quote_name(table), "name": editor.quote_name("CamelCaseIndex"), "columns": editor.quote_name(column), "extra": "", } ) editor.alter_field(model, get_field(db_index=True), field) editor.execute( editor.sql_create_unique % { "table": editor.quote_name(table), "name": editor.quote_name("CamelCaseUniqConstraint"), "columns": editor.quote_name(field.column), } ) editor.alter_field(model, get_field(unique=True), field) editor.execute( editor.sql_create_fk % { "table": editor.quote_name(table), "name": editor.quote_name("CamelCaseFKConstraint"), "column": editor.quote_name(column), "to_table": editor.quote_name(table), "to_column": editor.quote_name(model._meta.auto_field.column), "deferrable": connection.ops.deferrable_sql(), } ) editor.alter_field(model, get_field(Author, CASCADE, field_class=ForeignKey), field)
cls.start_cache_isolation() override = override_settings( MODULESTORE=cls.MODULESTORE(), CONTENTSTORE=cls.CONTENTSTORE(), ) cls.__old_modulestores.append(copy.deepcopy(settings.MODULESTORE)) cls.__old_contentstores.append(copy.deepcopy(settings.CONTENTSTORE)) override.__enter__() cls.__settings_overrides.append(override) XMODULE_FACTORY_LOCK.enable() clear_existing_modulestores() cls.store = modulestore()
expected = Int64Index([5, 0]) for cls in [Index, Int64Index]: for idx in [cls([5, 0], dtype='int64'), cls(np.array([5, 0]), dtype='int64'), cls(Series([5, 0]), dtype='int64')]: tm.assert_index_equal(idx, expected)
df.plot(subplots=True, ax=axes)
res = __salt__['boto_apigateway.create_api_deployment'](restApiId=self.restApiId, stageName=self._stage_name, stageDescription=stage_desc_json, description=self.deployment_label_json, variables=stage_variables, **self._common_aws_args) if not res.get('created'): ret['abort'] = True ret['result'] = False ret['comment'] = res.get('error') else: ret = _log_changes(ret, 'publish_api (new deployment)', res.get('deployment'))
if (isinstance(data, Node) and not data.negated and (data.connector == conn_type or len(data) == 1)): self.children.extend(data.children) return self else: self.children.append(data) return data
cmd = 'emerge-webrsync -q' if salt.utils.which('emerge-delta-webrsync'): cmd = 'emerge-delta-webrsync -q' return __salt__['cmd.retcode'](cmd, python_shell=False) == 0
_CreateTransaction = ctypes.windll.ktmw32.CreateTransaction _CommitTransaction = ctypes.windll.ktmw32.CommitTransaction _MoveFileTransacted = ctypes.windll.kernel32.MoveFileTransactedW _CloseHandle = ctypes.windll.kernel32.CloseHandle CAN_RENAME_OPEN_FILE = True
if recurse_set: if 'user' in recurse_set: if user: uid = __salt__['file.user_to_uid'](user) if isinstance(uid, six.string_types): ret['result'] = False ret['comment'] = 'Failed to enforce ownership for ' \ 'user {0} (user does not ' \ 'exist)'.format(user) else: ret['result'] = False ret['comment'] = 'user not specified, but configured as ' \ 'a target for recursive ownership ' \ 'management' else: user = None if 'group' in recurse_set: if group: gid = __salt__['file.group_to_gid'](group) if isinstance(gid, six.string_types): ret['result'] = False ret['comment'] = 'Failed to enforce group ownership ' \ 'for group {0}'.format(group) else: ret['result'] = False ret['comment'] = 'group not specified, but configured ' \ 'as a target for recursive ownership ' \ 'management' else: group = None
raise NotImplementedError(str(self.__class__) + " does not implement " "log_p_z.")
self.assertEqual(message.text, expected_message) self.assertEqual(message.type, expected_message_type) if expected_action_class: self.assertEqual(message.action_class, expected_action_class) else: self.assertFalse(hasattr(message, "action_class")) if expected_action_label: self.assertEqual(message.action_label, expected_action_label) else: self.assertFalse(hasattr(message, "action_label")) if expected_action_runtime_event: self.assertEqual(message.action_runtime_event, expected_action_runtime_event) else: self.assertFalse(hasattr(message, "action_runtime_event"))
for supported_field in SUPPORTED_FIELDS: if supported_field.requested_field_name in self.context['requested_fields']: field_value = self._get_field( block_key, supported_field.transformer, supported_field.block_field_name, supported_field.default_value, ) if field_value is not None: data[supported_field.serializer_field_name] = field_value
uname = self.__random_string() if self.run_function('user.add', [uname]) is not True: self.run_function('user.delete', [uname, True, True]) self.skipTest('Failed to create user')
y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred]) y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
giptm = GenericIPAddrUnpackUniqueTest(generic_v4unpack_ip="::ffff:18.52.18.52") giptm.save() giptm = GenericIPAddrUnpackUniqueTest(generic_v4unpack_ip="18.52.18.52") self.assertFailsValidation(giptm.full_clean, ['generic_v4unpack_ip'])
with patch('contentstore.views.item.get_preview_fragment') as patched_get_preview_fragment: retval = Mock() type(retval).content = PropertyMock(return_value="Some content") type(retval).resources = PropertyMock(return_value=[]) patched_get_preview_fragment.return_value = retval root_usage_key = self._create_vertical() _, _ = self._get_container_preview( root_usage_key, {'enable_paging': 'true', 'page_number': 0, 'page_size': 2} ) call_args = patched_get_preview_fragment.call_args[0] _, _, context = call_args self.assertIn('paging', context) self.assertEqual({'page_number': 0, 'page_size': 2}, context['paging'])
loc = Location.objects.annotate(num_cities=Count('city')).get(id=dallas.location.id) self.assertEqual(2, loc.num_cities)
def test_force_header(self): request = HttpRequest() wrapper = force_header('Vary', 'Origin') wrapped_view = wrapper(fake_view) response = wrapped_view(request) self.assertEqual(len(response.force_headers), 1) self.assertEqual(response.force_headers['Vary'], 'Origin')
return self._engine.is_unique
if not history_entries: raise self.DoesNotExist()
self.attrs.pandas_type = str(self.pandas_kind) self.attrs.pandas_version = str(_version) self.set_version()
course_key = course.id self.assertEqual(data['id'], unicode(course_key)) self.assertEqual(data['name'], course.display_name) self.assertEqual(data['course'], course_key.course) self.assertEqual(data['org'], course_key.org) self.assertEqual(data['run'], course_key.run) uri = self.build_absolute_url( reverse('course_structure_api:v0:detail', kwargs={'course_id': unicode(course_key)})) self.assertEqual(data['uri'], uri)
class UserProfile(models.Model): user = models.OneToOneField(User, models.CASCADE, primary_key=True)
self.clear_user_module_score(real_user) return Response(status=200)
from __future__ import absolute_import import os
split_test = self._update_partition_id(0) self.assertEqual(2, len(split_test.children)) initial_vertical_0_location = split_test.children[0] initial_vertical_1_location = split_test.children[1]
#_connarg('connection_use_unicode', 'use_unicode') connargs['use_unicode'] = False _connarg('connection_charset', 'charset') MySQLdb.paramstyle = 'pyformat'
executable_name = None
s = pd.Series([1, 2, 3]) w = s.where(s > 1, 'X')
import salt.utils import integration
kinds = tuple(list(compat.string_types) + [ABCSeries, np.ndarray, Index, list]) if isinstance(slice_, kinds): slice_ = IndexSlice[:, slice_]
self.assertTrue(self.run_function('xattr.clear', [TEST_FILE]))
self.assertIn(self.SUCCESSFUL_RESPONSE, response.content) self.assertEqual(response.status_code, 200)
course_key = SlashSeparatedCourseKey('org', 'course', 'run') with self.assertRaisesRegexp(GitExportError, unicode(GitExportError.URL_BAD)): git_export_utils.export_to_git(course_key, 'Sillyness') with self.assertRaisesRegexp(GitExportError, unicode(GitExportError.URL_BAD)): git_export_utils.export_to_git(course_key, 'example.com:edx/notreal') with self.assertRaisesRegexp(GitExportError, unicode(GitExportError.URL_NO_AUTH)): git_export_utils.export_to_git(course_key, 'http://blah')
n_samples = X.shape[0]
if not alias.strip(): line_to_add = ''
if name in all_virt and name not in pkgs: candidate = '1' else: candidate = ''
log = logging.getLogger(__name__)
if not hasattr(self, 'needs_reformat'): self.needs_reformat = self.needs_reshape del self.needs_reshape
self.assertEqual(idx.reindex(dt_idx.values)[0].name, 'foobar') self.assertEqual(idx.reindex(dt_idx.tolist())[0].name, 'foobar')
with tm.assert_produces_warning(RuntimeWarning): c_old = Categorical([0, 1, 2, 0, 1, 2],
self.course = modulestore().get_course(self.course.id)
actual = grouped_ser.filter(lambda x: len(x) > 1) expected = ser.take(expected_indexes) assert_series_equal(actual, expected)
class MyWidget4(TextInput): def _media(self): return Media(css={'all': ('/some/path',)}, js=('/some/js',)) media = property(_media)
from sklearn.utils.testing import install_mldata_mock install_mldata_mock({ 'iris': { 'data': np.empty((150, 4)), 'label': np.empty(150), }, 'datasets-uci-iris': { 'double0': np.empty((150, 4)), 'class': np.empty((150,)), }, 'leukemia': { 'data': np.empty((72, 7129)), }, })
mock_info = MagicMock(return_value={'Newvolume1': {'status': '0'}}) with patch.object(glusterfs, 'info', mock_info): mock_run = MagicMock(return_value=xml_command_success) with patch.dict(glusterfs.__salt__, {'cmd.run': mock_run}): self.assertEqual(glusterfs.start_volume('Newvolume1'), True) self.assertEqual(glusterfs.start_volume('nonExisting'), False) mock_run = MagicMock(return_value=xml_command_fail) with patch.dict(glusterfs.__salt__, {'cmd.run': mock_run}): self.assertEqual(glusterfs.start_volume('Newvolume1'), False)
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
expected = np.array([[1., 2., 4.], [5., np.nan, 10.]]) df = self.read_csv(StringIO(data), comment='#', header=1) tm.assert_numpy_array_equal(df.values, expected)
np.testing.assert_allclose(f([[1]], [[0]]), [20, 20 / np.sqrt(2)])
from __future__ import absolute_import
BULK_EMAIL_DEFAULT_FROM_EMAIL = ENV_TOKENS.get('BULK_EMAIL_DEFAULT_FROM_EMAIL', BULK_EMAIL_DEFAULT_FROM_EMAIL) BULK_EMAIL_EMAILS_PER_TASK = ENV_TOKENS.get('BULK_EMAIL_EMAILS_PER_TASK', BULK_EMAIL_EMAILS_PER_TASK) BULK_EMAIL_DEFAULT_RETRY_DELAY = ENV_TOKENS.get('BULK_EMAIL_DEFAULT_RETRY_DELAY', BULK_EMAIL_DEFAULT_RETRY_DELAY) BULK_EMAIL_MAX_RETRIES = ENV_TOKENS.get('BULK_EMAIL_MAX_RETRIES', BULK_EMAIL_MAX_RETRIES) BULK_EMAIL_INFINITE_RETRY_CAP = ENV_TOKENS.get('BULK_EMAIL_INFINITE_RETRY_CAP', BULK_EMAIL_INFINITE_RETRY_CAP) BULK_EMAIL_LOG_SENT_EMAILS = ENV_TOKENS.get('BULK_EMAIL_LOG_SENT_EMAILS', BULK_EMAIL_LOG_SENT_EMAILS) BULK_EMAIL_RETRY_DELAY_BETWEEN_SENDS = ENV_TOKENS.get('BULK_EMAIL_RETRY_DELAY_BETWEEN_SENDS', BULK_EMAIL_RETRY_DELAY_BETWEEN_SENDS) BULK_EMAIL_ROUTING_KEY = HIGH_PRIORITY_QUEUE
self.check_html(widget, 'beatle', 'J', html=html)
cur.executemany(cmd, newitems)
nvis = 5 num_classes = 10 MLP(layers=[Softmax(num_classes, 's', irange=0.1)], nvis=nvis) MLP(layers=[Softmax(num_classes, 's', istdev=0.1)], nvis=nvis) MLP(layers=[Softmax(num_classes, 's', sparse_init=2)], nvis=nvis)
Ensure mysecgroup exists: boto_secgroup.present: - name: mysecgroup - description: My security group - profile: myprofile
url(r'^search/', include('search.urls')),
if format is None: try: fname = filepath_or_buffer.lower() if fname.endswith(".xpt"): format = "xport" elif fname.endswith(".sas7bdat"): format = "sas7bdat" else: raise ValueError("unable to infer format of SAS file") except: pass if format.lower() == 'xport': from pandas.io.sas.sas_xport import XportReader reader = XportReader(filepath_or_buffer, index=index, encoding=encoding, chunksize=chunksize) elif format.lower() == 'sas7bdat': from pandas.io.sas.sas7bdat import SAS7BDATReader reader = SAS7BDATReader(filepath_or_buffer, index=index, encoding=encoding, chunksize=chunksize) else: raise ValueError('unknown SAS format') if iterator or chunksize: return reader return reader.read()
return_data = mysql.MySQLExtPillar() args, kwargs = [], { '1': 'SELECT blah', '2': '', '3': 'SELECT blah2' } qbuffer = return_data.extract_queries(args, kwargs) self.assertEqual([ ['1', {'query': 'SELECT blah', 'depth': 0, 'as_list': False, 'with_lists': None, 'ignore_null': False}], ['3', {'query': 'SELECT blah2', 'depth': 0, 'as_list': False, 'with_lists': None, 'ignore_null': False}] ], qbuffer)
assert_true(hasattr(InheritanceMixin, attribute))
axis = self.view_converter.axes.index('b') return axis
connection_persists_old_columns = False
self.q(css='.nav-item .new-button').click()
return self.energy_function.score(V)
return self.build_membership_data_raw(self.users[username].username, team.team_id)
if ssh_interface == 'private_ips' and private_ip_assignment is False: create_private_ip(node_id) private_ip_assignment = True
invalidate_generated_certificates(course_id, students_to_generate_certs_for, statuses_to_regenerate)
with modulestore().default_store(store): response = self.client.ajax_post(self.course_create_rerun_url, { 'org': 'orgX', 'number': 'CS101', 'display_name': 'Course with web certs enabled', 'run': '2015_T2' }) self.assertEqual(response.status_code, 400) data = parse_json(response) self.assertIn(u'Organization you selected does not exist in the system', data['error'])
self.assertAlmostEqual(1.41421356237, pnt.distance(Point(1, 1)), 11)
from __future__ import absolute_import import os import threading
if parent_id: url = "http://localhost:4567/api/v1/comments/{}".format(parent_id) else: url = "http://localhost:4567/api/v1/threads/{}/comments".format(thread_id) httpretty.register_uri( httpretty.POST, url, body=_get_comment_callback(comment_data, thread_id, parent_id) )
('US', None, [], [], True), ('IR', None, ['IR', 'CU'], [], False), ('US', 'IR', ['IR', 'CU'], [], False), ('IR', 'IR', ['IR', 'CU'], [], False), ('US', None, [], ['US'], True), ('IR', None, [], ['US'], False), ('US', 'IR', [], ['US'], False),
g = GEOSGeometry('POINT (-104.609 38.255)', 4326) with self.assertRaises(GEOSException): g.transform(2774) g = GEOSGeometry('POINT (-104.609 38.255)', 4326) with self.assertRaises(GEOSException): g.transform(2774, clone=True)
if not HAS_BOTO: return (False, 'The boto_apigateway module could not be loaded: ' 'boto libraries not found') elif _LooseVersion(boto.__version__) < _LooseVersion(required_boto_version): return (False, 'The boto_apigateway module could not be loaded: ' 'boto version {0} or later must be installed.'.format(required_boto_version)) elif _LooseVersion(boto3.__version__) < _LooseVersion(required_boto3_version): return (False, 'The boto_apigateway module could not be loaded: ' 'boto3 version {0} or later must be installed.'.format(required_boto3_version)) else: return True
output = self.engine.render_to_string('basic-syntax04') if self.engine.string_if_invalid: self.assertEqual(output, 'asINVALIDdf') else: self.assertEqual(output, 'asdf')
ret = self.run_run_plus(fun='fileserver.dir_list', args=['backend="roots"']) self.assertIsInstance(ret['fun'], list)
return y
field, list_filter_class = item if not issubclass(list_filter_class, FieldListFilter): return must_inherit_from(parent='FieldListFilter', option='%s[1]' % label, obj=obj, id='admin.E115') else: return []
opts = copy.deepcopy(minion_opts) opts['file_client'] = 'local' opts['file_roots'] = master_opts['master_roots'] opts['renderer'] = master_opts['renderer'] opts['state_top'] = master_opts['state_top'] opts['id'] = id_ opts['grains'] = grains HighState.__init__(self, opts)
socket.setdefaulttimeout(10)
service = super(ModuleSystem, self).service(block=block, service_name=service_name) if callable(service): return service(block) return service
self.assertEqual(Person.custom_queryset_custom_manager.init_arg, 'hello')
if (not s.is_table or (s.is_table and format == 'fixed' and s.is_exists)): raise ValueError('Can only append to Tables') if not s.is_exists: s.set_object_info()
dm = DataFrame(index=self.frame.index) dm['A'] = 'foo' dm['B'] = 'bar' self.assertEqual(len(dm.columns), 2) self.assertEqual(dm.values.dtype, np.object_)
response = self.client.get(self.index_url) self.assertEqual(response.status_code, 302) login = self.client.post(login_url, self.joepublic_login) self.assertEqual(login.status_code, 200) self.assertContains(login, ERROR_MESSAGE)
HTTPServer.shutdown(self)
p = self.PersonModel(name="Fred") p.mugshot.save("shot", self.file1) os.remove(p.mugshot.path) p.delete()
frame = DataFrame({'A': {'1': 1, '2': 2}}) self.assert_index_equal(frame.index, pd.Index(['1', '2']))
self._base_test_extauth_auto_activate_user_with_flag(log_user_string="user.id: 1")
safe_course_key = loc.course_key if safe_course_key.run is None: safe_course_key = safe_course_key.replace(run='only')
try: minions_remaining.remove(event['data']['id']) except ValueError: pass if len(minions_remaining) == 0: raise tornado.gen.Return(chunk_ret)
try: course = CourseOverview.get_from_id(course_key) except CourseOverview.DoesNotExist: if check_access: log.warning(u"User %s failed to enroll in non-existent course %s", user.username, unicode(course_key)) raise NonExistentCourseError
state = __salt__['{0}.state'.format(container_type)]
urls = [reverse('instructor_dashboard', kwargs={'course_id': self.course.id.to_deprecated_string()}), reverse('instructor_dashboard', kwargs={'course_id': self.test_course.id.to_deprecated_string()})]
self.browser.delete_cookie('hide_captions') self.edit_component() self.open_advanced_tab() self.video.set_field_value('Show Transcript', 'False', 'select') self.save_unit_settings() self.assertFalse(self.video.is_captions_visible())
data = self.queue.getvalue() data = data.decode("utf-8") data = self.encoder.encode(data) self.stream.write(data) self.queue.truncate(0)
def __init__(self, layer_name, layer_content, freeze_params=False): super(PretrainedLayer, self).__init__() self.__dict__.update(locals()) del self.self @wraps(Layer.set_input_space) def set_input_space(self, space): assert self.get_input_space() == space @wraps(Layer.get_params) def get_params(self): if self.freeze_params: return [] return self.layer_content.get_params() @wraps(Layer.get_input_space) def get_input_space(self): return self.layer_content.get_input_space() @wraps(Layer.get_output_space) def get_output_space(self): return self.layer_content.get_output_space() @wraps(Layer.get_layer_monitoring_channels) def get_layer_monitoring_channels(self, state_below=None, state=None, targets=None): return OrderedDict([]) @wraps(Layer.fprop) def fprop(self, state_below): return self.layer_content.upward_pass(state_below)
contingency_nm = contingency[nnz] log_contingency_nm = np.log(contingency_nm) contingency_nm /= contingency_sum log_outer = -np.log(outer[nnz]) + log(pi.sum()) + log(pj.sum()) mi = (contingency_nm * (log_contingency_nm - log(contingency_sum)) + contingency_nm * log_outer) return mi.sum()
provider_url = reverse('openid-provider-login') factory = RequestFactory() request = factory.request() abs_provider_url = request.build_absolute_uri(location=provider_url)
from __future__ import absolute_import
upload_file = request.FILES['file'] filename = upload_file.name mime_type = upload_file.content_type size = get_file_size(upload_file)
internal_use_only = False progress_class = ProgressBar
recommended_datastores = si.content.storageResourceManager.RecommendDatastores(storageSpec=storage_spec)
tm.assert_index_equal(df.set_index(df.index).index, mi)
sls_file = '{0}.sls'.format(os.sep.join(repo)) if not os.path.exists(sls_file):
badge = badges[0] tracker.emit( 'edx.badge.assertion.evidence_visited', { 'badge_name': badge.badge_class.display_name, 'badge_slug': badge.badge_class.slug, 'badge_generator': badge.backend, 'issuing_component': badge.badge_class.issuing_component, 'user_id': user.id, 'course_id': unicode(course_key), 'enrollment_mode': badge.badge_class.mode, 'assertion_id': badge.id, 'assertion_image_url': badge.image_url, 'assertion_json_url': badge.assertion_url, 'issuer': badge.data.get('issuer'), } )
option1 = main1
is_active = models.BooleanField(default=True)
assert_raises(ValueError, sel.transform, np.array([[1], [2]]))
return len(value.split())
clf = self.factory(alpha=0.1, n_iter=2, fit_intercept=False) clf.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2]) assert_equal(clf.coef_[0], clf.coef_[1])
site_url = '/'
proba = self.predict_proba(X) if self.n_outputs_ == 1: return np.log(proba) else: return [np.log(p) for p in proba]
freq, data = _maybe_resample(data, ax, kwds)
if 'start_date' in course_details: course_details['start_date'] = course_details['start_date'].isoformat() if 'end_date' in course_details: course_details['end_date'] = course_details['end_date'].isoformat() self._course_details.update(course_details)
from __future__ import absolute_import
def get_indexers_list():
self.assert_index_equal(c.categories, Index([4, 3, 2, 1]))
all_asset_md = [] for __ in xrange(amount): all_asset_md.append(generate_random_asset_md()) return all_asset_md
ps_diff_cache = self._create_service(username, {})
verify_name(self.html_usage_key, self.seq_usage_key, "Duplicate of 'Text'")
theta = np.asarray(theta, dtype=np.float64) d = np.asarray(d, dtype=np.float64) if d.ndim > 1: n_features = d.shape[1] else: n_features = 1 lth = theta.size if lth == 1: td = np.abs(d) * theta elif lth != n_features: raise Exception("Length of theta must be 1 or " + str(n_features)) else: td = np.abs(d) * theta.reshape(1, n_features) td[td > 1.] = 1. ss = 1. - td ** 2. * (3. - 2. * td) r = np.prod(ss, 1) return r
df = DataFrame({'A': [0, 1, 2], 'B': ['a', 'b', np.nan]}) df.to_sql('test_nan', self.conn, index=False)
index = MultiIndex.from_arrays([np.arange(4000)] * 3) df = DataFrame(np.random.randn(4000), index=index, dtype=np.int32)
item.transcripts.pop(lang) reraised_message += ' ' + ex.message
num_emails = settings.BULK_EMAIL_EMAILS_PER_TASK self._create_students(num_emails - 1) with patch('bulk_email.tasks.get_connection', autospec=True) as get_conn: get_conn.return_value.send_messages.side_effect = cycle([None]) self._test_run_with_task(send_bulk_course_email, 'emailed', num_emails, num_emails)
ccx = models.ForeignKey(CustomCourseForEdX, db_index=True) location = LocationKeyField(max_length=255, db_index=True) field = models.CharField(max_length=255) class Meta(object): app_label = 'ccx' unique_together = (('ccx', 'location', 'field'),) value = models.TextField(default='null')
df = DataFrame({'date': [pd.Timestamp('20130101').tz_localize('UTC'), pd.NaT]}) expected = df.dtypes
self.assertTrue( self.run_function('assistive.installed', [OSA_SCRIPT]) ) self.run_function('assistive.remove', [OSA_SCRIPT]) self.assertFalse( self.run_function('assistive.installed', [OSA_SCRIPT]) )
ret = self.run_state( 'pip.installed', name='pip==6.0.7', upgrade=True, bin_env=venv_dir ) try: self.assertSaltTrueReturn(ret) self.assertInSaltReturn( 'Installed', ret, ['changes', 'pip==6.0.7'] ) except AssertionError: import pprint pprint.pprint(ret) raise
res_df = df.ix["j":"k", :] tm.assert_frame_equal(res_df, exp_df) self.assertTrue(com.is_categorical_dtype(res_df["cats"]))
set_user_preference(self.user, "dict_pref", {"int_key": 10}) set_user_preference(self.user, "string_pref", "value")
mgmt_command = -1
entry = InstructorTask.objects.get(id=task_entry.id) self.assertEquals(entry.task_state, FAILURE) self.assertGreater(1023, len(entry.task_output)) output = json.loads(entry.task_output) self.assertEquals(output['exception'], 'TestTaskFailure') self.assertEquals(output['message'], expected_message[:len(output['message']) - 3] + "...") self.assertTrue('traceback' not in output)
left = self.rng[:5] right = self.rng[5:10]
make_asset_xml(num_assets, ASSET_XML_PATH) validate_xml(ASSET_XSD_PATH, ASSET_XML_PATH)
if self.value.__name__ == '<lambda>': raise ValueError("Cannot serialize function: lambda") if self.value.__module__ is None: raise ValueError("Cannot serialize function %r: No module" % self.value) if getattr(self.value, "__qualname__", None) and getattr(self.value, "__module__", None):
PER_STUDENT_ANONYMIZED_DESCRIPTORS = set( class_ for (name, class_) in XModuleDescriptor.load_classes() if not issubclass(class_, PER_COURSE_ANONYMIZED_DESCRIPTORS) )
__virtualname__ = 'docker_events'
try: if os.fork() > 0: reinit_crypto() sys.exit(0) else: reinit_crypto() except OSError as ex: sys.exit(1)
unknowns = ["unknown_user{}".format(i) for i in range(3)] response_dict = self.request_add_users_to_cohort( ",".join( unknowns + [ user.username for user in self.cohortless_users + self.cohort1_users + self.cohort2_users + self.cohort3_users ] ), self.cohort1, self.course ) self.verify_added_users_to_cohort( response_dict, self.cohort1, self.course, expected_added=self.cohortless_users, expected_changed=( [(user, self.cohort2.name) for user in self.cohort2_users] + [(user, self.cohort3.name) for user in self.cohort3_users] ), expected_present=[(user, user.username) for user in self.cohort1_users], expected_unknown=unknowns )
elif needs_i8_conversion(left) and needs_i8_conversion(right): if not is_dtype_equal(left.dtype, right.dtype): return False
bwait = self.opts.get('batch_wait', 0) wait = []
clf_invalid_method = CalibratedClassifierCV(clf, method="foo") assert_raises(ValueError, clf_invalid_method.fit, X_train, y_train)
with option_context('mode.chained_assignment', None): for key, group in grouped: res = f(group) assert_frame_equal(res, result.ix[key])
element = etree.fromstring(xml_str) renderer = lookup_tag('math')(test_capa_system(), element) self.assertEqual(renderer.mathstr, mathjax_out)
if isinstance(data, basestring): data = {'data': data}
pass
self.x_sum = numpy.zeros([self.n_dim])
df = DataFrame({'time': to_datetime(['201412120154', '201412110254'], utc=True)}) db = sql.SQLiteDatabase(self.conn, self.flavor) table = sql.SQLiteTable("test_type", db, frame=df) schema = table.sql_schema() self.assertEqual(self._get_sqlite_column_type(schema, 'time'), "TIMESTAMP")
self.selenium.find_element_by_id('change_id_form-0-section').click() self.wait_for_popup() self.selenium.switch_to.window(self.selenium.window_handles[-1]) self.wait_for_text('#content h1', 'Change section') name_input = self.selenium.find_element_by_id('id_name') name_input.clear() name_input.send_keys('edited section') self.selenium.find_element_by_xpath('//input[@value="Save"]').click() self.selenium.switch_to.window(self.selenium.window_handles[0]) select = Select(self.selenium.find_element_by_id('id_form-0-section')) self.assertEqual(select.first_selected_option.text, 'edited section')
gc = AgglomerativeClustering(n_clusters=10) brc2 = Birch(n_clusters=gc) brc2.fit(X) assert_array_equal(brc1.subcluster_labels_, brc2.subcluster_labels_) assert_array_equal(brc1.labels_, brc2.labels_)
self.key = key return key
return JsonResponse({"error": unicode(err)}, 400)
management.call_command( 'loaddata', 'pretty.xml', verbosity=0, ) self.assertEqual(Stuff.objects.all()[0].name, '') self.assertEqual(Stuff.objects.all()[0].owner, None)
rng = np.random.RandomState(0) n_topics, X = _build_sparse_mtx() lda = LatentDirichletAllocation(n_topics=n_topics, evaluate_every=1, learning_method='batch', random_state=rng) lda.fit(X)
staff_user_ids = { user.id for role in Role.objects.filter( name__in=[FORUM_ROLE_ADMINISTRATOR, FORUM_ROLE_MODERATOR], course_id=course.id ) for user in role.users.all() } ta_user_ids = { user.id for role in Role.objects.filter(name=FORUM_ROLE_COMMUNITY_TA, course_id=course.id) for user in role.users.all() } requester = request.user cc_requester = CommentClientUser.from_django_user(requester).retrieve() cc_requester["course_id"] = course.id return { "course": course, "request": request, "thread": thread, "group_ids_to_names": get_cohort_names(course), "is_requester_privileged": requester.id in staff_user_ids or requester.id in ta_user_ids, "staff_user_ids": staff_user_ids, "ta_user_ids": ta_user_ids, "cc_requester": cc_requester, }
items = sorted(self.param_distributions.items()) for _ in six.moves.range(self.n_iter): params = dict() for k, v in items: if hasattr(v, "rvs"): params[k] = v.rvs() else: params[k] = v[rnd.randint(len(v))] yield params
msg = _( "A refund request has been initiated for {username} ({email}). " "To process this request, please visit the link(s) below." ).format(username=student.username, email=student.email) ecommerce_url_root = get_value('ECOMMERCE_PUBLIC_URL_ROOT', settings.ECOMMERCE_PUBLIC_URL_ROOT) refund_urls = [urljoin(ecommerce_url_root, '/dashboard/refunds/{}/'.format(refund_id)) for refund_id in refund_ids] return '{msg}\n\n{urls}'.format(msg=msg, urls='\n'.join(refund_urls))
for saltenv, buckets in six.iteritems(_get_buckets()): bucket_files = {} for bucket_name in buckets: s3_meta = __get_s3_meta(bucket_name)
config.add_subpackage('cluster') config.add_subpackage('datasets') config.add_subpackage('decomposition') config.add_subpackage('ensemble') config.add_subpackage('externals') config.add_subpackage('feature_extraction') config.add_subpackage('manifold') config.add_subpackage('metrics') config.add_subpackage('metrics/cluster') config.add_subpackage('neighbors') config.add_subpackage('tree') config.add_subpackage('svm')
data = models.OneToOneField(Anchor, models.CASCADE, primary_key=True)
cmd = '{vmadm} delete {uuid}'.format( vmadm=vmadm, uuid=vm ) res = __salt__['cmd.run_all'](cmd) retcode = res['retcode'] if retcode != 0: ret['Error'] = res['stderr'] if 'stderr' in res else _exit_status(retcode) return ret return True
'privext': __within([0, 1, 2], dtype=int), 'dhcp': __within([0, 1], dtype=int), 'media': __anything, 'accept_ra': __within([0, 1], dtype=int), 'autoconf': __within([0, 1], dtype=int), 'preferred-lifetime': __int,
self.assertEqual(len(video_outline), 1) self.assertIn(u"video in vertical", video_outline[0]["summary"]["name"]) a_or_b = video_outline[0]["summary"]["name"][-1:] self._verify_paths( video_outline, [ self.section.display_name, self.sub_section.display_name, self.unit.display_name, self.split_test.display_name, u"split test block " + a_or_b ], )
return 0.5 - scores
df.to_sql('test_schema_other', self.conn, schema='other', index=False) df.to_sql('test_schema_other', self.conn, schema='other', index=False, if_exists='replace') df.to_sql('test_schema_other', self.conn, schema='other', index=False, if_exists='append') res = sql.read_sql_table( 'test_schema_other', self.conn, schema='other') tm.assert_frame_equal(concat([df, df], ignore_index=True), res)
for est in estimators: random_search = RandomizedSearchCV(est, est_parameters, cv=cv, n_iter=3) random_search.fit(X, y) for parameters, _, cv_validation_scores in random_search.grid_scores_: est.set_params(**parameters)
response = self.client.ajax_post(grade_type_url, {'graderType': u'notgraded'}) self.assertEqual(200, response.status_code) response = self.client.get_json(grade_type_url + '?fields=graderType') self.assertEqual(json.loads(response.content).get('graderType'), u'notgraded')
if isinstance(ind, MultiIndex): continue
if sparse_input: inputs = SparseType('csr', dtype=theano.config.floatX)() else: inputs = tensor.matrix() return theano.function( [inputs], outputs=self(inputs)[repr_index], name=name)
return "assignment_no_params - Expected result"
tag = self.runtime.service(self.mock_block, 'user_tags').get_tag(self.scope, self.key) self.assertIsNone(tag)
log = logging.getLogger(__name__)
rnn = RNN(input_space=SequenceSpace(VectorSpace(dim=1)), layers=[Recurrent(dim=1, layer_name='recurrent', irange=0.1, indices=[-1], nonlinearity=lambda x: x)]) W, U, b = rnn.layers[0].get_params() W.set_value([[1]]) U.set_value([[2]]) X_data, X_mask = rnn.get_input_space().make_theano_batch() y_hat = rnn.fprop((X_data, X_mask)) seq_len = 20 X_data_vals = np.ones((seq_len, seq_len, 1)) X_mask_vals = np.triu(np.ones((seq_len, seq_len))) f = function([X_data, X_mask], y_hat, allow_input_downcast=True) np.testing.assert_allclose(2 ** np.arange(1, seq_len + 1) - 1, f(X_data_vals, X_mask_vals).flatten())
if item_shape is None: item_shape = (N, ) offset = 0 mgr_items = [] block_placements = OrderedDict() for d in descr.split(';'): d = d.strip() if not len(d): continue names, blockstr = d.partition(':')[::2] blockstr = blockstr.strip() names = names.strip().split(',') mgr_items.extend(names) placement = list(np.arange(len(names)) + offset) try: block_placements[blockstr].extend(placement) except KeyError: block_placements[blockstr] = placement offset += len(names) mgr_items = Index(mgr_items) blocks = [] num_offset = 0 for blockstr, placement in block_placements.items(): typestr = blockstr.split('-')[0] blocks.append(create_block(typestr, placement, item_shape=item_shape, num_offset=num_offset, )) num_offset += len(placement) return BlockManager(sorted(blocks, key=lambda b: b.mgr_locs[0]), [mgr_items] + [np.arange(n) for n in item_shape])
(["foo", "bar", "baz"], [None, "bar", "baz"]),
df1 = DataFrame(np.random.randn(5, 2), columns=['bar', 'foo']) df2 = DataFrame() result = df1.append(df2) expected = df1.copy() assert_frame_equal(result, expected)
if pkginfo[version_num].get('use_scheduler', False):
self.prepare_new_page() totals_table.drawOn(self.pdf, self.margin + left_padding, self.second_page_start_y_pos - rendered_height) return self.second_page_start_y_pos - rendered_height - self.min_clearance
ret['comment'] = 'User {0} is not present, so it cannot be removed'\ .format(name) return ret
from __future__ import unicode_literals from django.db import models from django.utils.encoding import python_2_unicode_compatible @python_2_unicode_compatible class Reporter(models.Model): first_name = models.CharField(max_length=30) last_name = models.CharField(max_length=30) def __str__(self): return "%s %s" % (self.first_name, self.last_name) @python_2_unicode_compatible class Article(models.Model): headline = models.CharField(max_length=100) pub_date = models.DateField() def __str__(self): return self.headline @python_2_unicode_compatible class Writer(models.Model): reporter = models.ForeignKey(Reporter, models.CASCADE) article = models.ForeignKey(Article, models.CASCADE) position = models.CharField(max_length=100) def __str__(self): return '%s (%s)' % (self.reporter, self.position)
models = [model] + [resolve_relation(model, rel) for rel in related_models] model_keys = (make_model_tuple(m) for m in models) apps = model._meta.apps return apps.lazy_model_operation(partial(function, **kwargs), *model_keys)
import yaml
from __future__ import absolute_import, print_function import datetime import copy import textwrap import difflib import logging import tempfile import os import pipes import time import shutil import re import random
output = _cmd('at', '-d', ' '.join(opts)) if output is None: return '\'at.atrm\' is not available.'
connection.ops.check_expression_support(self) self.function = connection.ops.spatial_aggregate_name(self.name) return super(GeoAggregate, self).as_sql(compiler, connection)
self.plural = other.plural self._info = other._info.copy() self._catalog = other._catalog.copy()
response = ( _visible_to_nonstaff_users(descriptor) and _has_group_access(descriptor, user, course_key) and ( _has_detached_class_tag(descriptor) or _can_access_descriptor_with_start_date(user, descriptor, course_key) ) ) return ( ACCESS_GRANTED if (response or _has_staff_access_to_descriptor(user, descriptor, course_key)) else response )
resp = self.client.ajax_post( self.url, data={'tabs': [{'tab_id': tab_id} for tab_id in tab_ids]}, ) self.assertEqual(resp.status_code, 204)
series = self.frame['A'][::2] self.frame['col5'] = series self.assertIn('col5', self.frame)
install_mldata_mock({ 'mnist-original': { 'data': np.empty((70000, 784)), 'label': np.repeat(np.arange(10, dtype='d'), 7000), }, 'iris': { 'data': np.empty((150, 4)), }, 'datasets-uci-iris': { 'double0': np.empty((150, 4)), 'class': np.empty((150,)), }, })
ins_row = self.fmt.tr_row_num for lnum, records in enumerate(level_lengths): rec_new = {} for tag, span in list(records.items()): if tag >= ins_row: rec_new[tag + 1] = span elif tag + span > ins_row: rec_new[tag] = span + 1 dot_row = list(idx_values[ins_row - 1]) dot_row[-1] = u('...') idx_values.insert(ins_row, tuple(dot_row)) else: rec_new[tag] = span if tag + span == ins_row: rec_new[ins_row] = 1 if lnum == 0: idx_values.insert(ins_row, tuple( [u('...')] * len(level_lengths))) level_lengths[lnum] = rec_new
self.refresh_course()
result = read_json(json, date_unit=None) assert_frame_equal(result, df)
metadata['xml_attributes'][attr] = value
compute_inherited_metadata(course_descriptor)
if hasattr(self, 'oob_improvement_'): self.oob_improvement_.resize(total_n_estimators) else: self.oob_improvement_ = np.zeros((total_n_estimators,), dtype=np.float64)
current_users_state = self._get_users_state() self._check_response(users_state_before_fail, current_users_state)
obj1 = Storage() self.assertEqual(obj1.normal.name, "") with self.assertRaises(ValueError): obj1.normal.size
with self.assertRaises(ImproperlyConfigured): self.client.post( '/edit/author/%d/update/naive/' % a.pk, {'name': 'Randall Munroe (author of xkcd)', 'slug': 'randall-munroe'} )
def test_sparse_random_projection_transformer_invalid_density(): for RandomProjection in all_SparseRandomProjection: assert_raises(ValueError, RandomProjection(density=1.1).fit, data)
with self.assertRaisesMessage(AssertionError, 'Cannot combine queries on two different base models.'): Author.objects.all() & Tag.objects.all() with self.assertRaisesMessage(AssertionError, 'Cannot combine queries on two different base models.'): Author.objects.all() | Tag.objects.all()
expected = np.array([31200, 45678, 10000], dtype='m8[ns]')
expected_1_nofreq = DatetimeIndex(['2000-01-31', '2000-01-31', '2000-02-29', '2000-03-31'], name='idx', freq=None) expected_3_nofreq = DatetimeIndex(['2000-01-31', '2000-02-29', '2000-03-31', '2000-01-02'], name='idx', freq=None)
confirmation_message = self.cohort_management_page.get_cohort_discussions_message(key=key) self.assertEqual("Your changes have been saved.", confirmation_message)
self.dummy_int = dummy_int self.dummy_str = dummy_str self.dummy_obj = dummy_obj if callback is not None: callback(self) if self.allow_nd: X = X.reshape(len(X), -1) if X.ndim >= 3 and not self.allow_nd: raise ValueError('X cannot be d') if sample_weight is not None: assert_true(sample_weight.shape[0] == X.shape[0], 'MockClassifier extra fit_param sample_weight.shape[0]' ' is {0}, should be {1}'.format(sample_weight.shape[0], X.shape[0])) if class_prior is not None: assert_true(class_prior.shape[0] == len(np.unique(y)), 'MockClassifier extra fit_param class_prior.shape[0]' ' is {0}, should be {1}'.format(class_prior.shape[0], len(np.unique(y)))) if sparse_sample_weight is not None: fmt = ('MockClassifier extra fit_param sparse_sample_weight' '.shape[0] is {0}, should be {1}') assert_true(sparse_sample_weight.shape[0] == X.shape[0], fmt.format(sparse_sample_weight.shape[0], X.shape[0])) if sparse_param is not None: fmt = ('MockClassifier extra fit_param sparse_param.shape ' 'is ({0}, {1}), should be ({2}, {3})') assert_true(sparse_param.shape == P_sparse.shape, fmt.format(sparse_param.shape[0], sparse_param.shape[1], P_sparse.shape[0], P_sparse.shape[1])) return self
attrs['freq'] = 'infer'
exclude = self._get_validation_exclusions() try: self.instance.validate_unique(exclude=exclude) except ValidationError as e: self._update_errors(e)
def __init__(self, opts, **kwargs): pass
ind = date_range(start="2000", freq="D", periods=1000) df = DataFrame( np.random.randn( len(ind), 5), index=ind, columns=list('ABCDE')) panel = Panel(dict([('frame_' + c, df) for c in list('ABC')]))
def one_case(dataset_type, model_type): cost, model, dataset, sgd, state = prepare_adagrad_test(dataset_type, model_type) sgd.train(dataset=dataset) for param in model.get_params(): assert not np.any(np.isnan(param.get_value())) one_case('zeros', 'zeros') one_case('arange', 'zeros') one_case('zeros', 'random') one_case('arange', 'random')
if len(figure_list) == 1: figure_name = figure_list[0] image_list = SINGLE_IMAGE % figure_name.lstrip('/') else: image_list = HLIST_HEADER for figure_name in figure_list: image_list += HLIST_IMAGE_TEMPLATE % figure_name.lstrip('/')
recorder = MigrationRecorder(connection) recorder.record_applied('migrations', '0002_second') msg = "Migration migrations.0002_second is applied before its dependency migrations.0001_initial" with self.temporary_migration_module(module="migrations.test_migrations"): with self.assertRaisesMessage(InconsistentMigrationHistory, msg): call_command("makemigrations")
from django.db import models from django.utils.encoding import python_2_unicode_compatible class Author(models.Model): class Meta: ordering = ('-pk',) @python_2_unicode_compatible class Article(models.Model): author = models.ForeignKey(Author, models.SET_NULL, null=True) second_author = models.ForeignKey(Author, models.SET_NULL, null=True) headline = models.CharField(max_length=100) pub_date = models.DateTimeField() class Meta: ordering = ('-pub_date', 'headline') def __str__(self): return self.headline class OrderedByAuthorArticle(Article): class Meta: proxy = True ordering = ('author', 'second_author') class Reference(models.Model): article = models.ForeignKey(OrderedByAuthorArticle, models.CASCADE) class Meta: ordering = ('article',)
ext = kwargs.pop('ext', 'shp') self.ds = get_ds_file(name, ext) super(TestDS, self).__init__(**kwargs)
res = self.index[1] expected = 2 self.assertEqual(res, expected)
language = UserPreference.get_value(data['user'], LANGUAGE_KEY)
axes = [_ensure_index(columns), _ensure_index(index)]
step_size = x if self.verbose: logger.info('best objective: {0}'.format(mn)) assert not np.isnan(mn)
self.assertEqual(list(User.objects.using('other').values_list('username', flat=True)), ['bob']) self.assertEqual(list(UserProfile.objects.using('other').values_list('flavor', flat=True)), ['crunchy frog'])
self._extra_fields_setting = copy.deepcopy(get_themed_value('REGISTRATION_EXTRA_FIELDS')) if not self._extra_fields_setting: self._extra_fields_setting = copy.deepcopy(settings.REGISTRATION_EXTRA_FIELDS) self._extra_fields_setting["honor_code"] = self._extra_fields_setting.get("honor_code", "required")
output_buffer = StringIO() gzip_file = GzipFile(fileobj=output_buffer, mode="wb") csvwriter = csv.writer(gzip_file) csvwriter.writerows(self._get_utf8_encoded_rows(rows)) gzip_file.close() self.store(course_id, filename, output_buffer)
if isinstance(data_source, six.string_types): data_source = DataSource(data_source) elif isinstance(data_source, DataSource): pass else: raise TypeError('Data source parameter must be a string or a DataSource object.')
ones = np.ones(len(boston.target)) last_y_pred = None for sample_weight in None, ones, 2 * ones: clf = GradientBoostingRegressor(n_estimators=100, loss=loss, max_depth=4, subsample=subsample, min_samples_split=2, random_state=1, presort=presort)
self.assertEqual(len(res.context['object_list']), 7)
import logging
return self.test_passed('.local-resource-test')
dtype = np.int if all(isinstance(c, int) for c in tmp) else object class_mapping = np.empty(len(tmp), dtype=dtype) class_mapping[:] = tmp self.classes_, inverse = np.unique(class_mapping, return_inverse=True) yt.indices = np.take(inverse, yt.indices)
for name, clf in zip(names, classifiers): ax = plt.subplot(len(datasets), len(classifiers) + 1, i) clf.fit(X_train, y_train) score = clf.score(X_test, y_test)
if isdense(A): if (np.issubdtype(A.dtype, np.complexfloating) or np.imag(sigma) == 0): A = np.copy(A) else: A = A + 0j A.flat[::A.shape[1] + 1] -= sigma return LuInv(A).matvec elif isspmatrix(A): A = A - sigma * identity(A.shape[0]) if symmetric and isspmatrix_csr(A): A = A.T return SpLuInv(A.tocsc()).matvec else: return IterOpInv(_aslinearoperator_with_dtype(A), M, sigma, tol=tol).matvec
pass
self.assertNotEqual(new_module.location.version_guid, premod_course.location.version_guid) parent = modulestore().get_item(locator) self.assertIn(new_module.location.version_agnostic(), version_agnostic(parent.children)) self.assertEqual(new_module.definition_locator.definition_id, original.definition_locator.definition_id)
mlp = MLP(nvis=2, layers=[Linear(2, 'h0', irange=0), Linear(2, 'h1', irange=0)]) s = str(mlp) assert isinstance(s, six.string_types)
message = ( "Calling modelform_factory without defining 'fields' or 'exclude' " "explicitly is prohibited." ) with self.assertRaisesMessage(ImproperlyConfigured, message): modelform_factory(Person)
if value is None: return None return six.text_type(value)
if 'changes' in ret: chg = ret['changes'] if not chg: if ret['comment']: msg = ret['comment'] else: msg = 'No changes made for {0[name]}'.format(ret) elif isinstance(chg, dict): if 'diff' in chg: if isinstance(chg['diff'], six.string_types): msg = 'File changed:\n{0}'.format(chg['diff']) if all([isinstance(x, dict) for x in six.itervalues(chg)]): if all([('old' in x and 'new' in x) for x in six.itervalues(chg)]): msg = 'Made the following changes:\n' for pkg in chg: old = chg[pkg]['old'] if not old and old not in (False, None): old = 'absent' new = chg[pkg]['new'] if not new and new not in (False, None): new = 'absent' msg += '\'{0}\' changed from \'{1}\' to ' \ '\'{2}\'\n'.format(pkg, old, new) if not msg: msg = str(ret['changes']) if ret['result'] is True or ret['result'] is None: log.info(msg) else: log.error(msg)
if microsite.get_value( 'ALWAYS_REDIRECT_HOMEPAGE_TO_DASHBOARD_FOR_AUTHENTICATED_USER', settings.FEATURES.get('ALWAYS_REDIRECT_HOMEPAGE_TO_DASHBOARD_FOR_AUTHENTICATED_USER', True)): return redirect(reverse('dashboard'))
ret = generate_user_certificates( student, course_key, course=course, insecure=options['insecure'] )
other = other + relativedelta(months=3 * n - monthsSince) wkday, _ = tslib.monthrange(other.year, other.month) first = _get_firstbday(wkday) result = datetime(other.year, other.month, first, other.hour, other.minute, other.second, other.microsecond) return result
s = pd.Series([1, 2, 3, 4, np.nan]) result = s.quantile(0.5) expected = 2.5 self.assertEqual(result, expected)
self.assert_enrollment_status()
data = 'a,b,c\n4,5,6\n ' result = self.read_csv(StringIO(data)) tm.assert_frame_equal(result, expected)
data['poem_set-0-DELETE'] = 'on' formset = PoemFormSet(data, instance=poet) self.assertEqual(formset.is_valid(), True) formset.save() self.assertEqual(Poem.objects.count(), 0)
pass
if not set(addusers).isdisjoint(set(delusers)): ret['result'] = None ret['comment'] = ( 'Error. Same user(s) can not be added and deleted' ' simultaneously') return ret
return np.memmap(filename, dtype=dtype, shape=shape, mode=mode, offset=offset, order=order)
self.verify_server_task("lms", options)
resources_dir = None data = String(default='', scope=Scope.content) @classmethod def definition_from_xml(cls, xml_object, system): if len(xml_object) == 0 and len(xml_object.items()) == 0: return {'data': ''}, [] return {'data': etree.tostring(xml_object, pretty_print=True, encoding='unicode')}, [] def definition_to_xml(self, resource_fs): if self.data: return etree.fromstring(self.data) return etree.Element(self.category)
value = default
self.grid_scores_ = scores[::-1] / cv.get_n_splits(X, y) return self
self.assertEqual( views.add_domain('example.com', '/foo/?arg=value'), 'http://example.com/foo/?arg=value' ) self.assertEqual( views.add_domain('example.com', '/foo/?arg=value', True), 'https://example.com/foo/?arg=value' ) self.assertEqual( views.add_domain('example.com', 'http://djangoproject.com/doc/'), 'http://djangoproject.com/doc/' ) self.assertEqual( views.add_domain('example.com', 'https://djangoproject.com/doc/'), 'https://djangoproject.com/doc/' ) self.assertEqual( views.add_domain('example.com', 'mailto:uhoh@djangoproject.com'), 'mailto:uhoh@djangoproject.com' ) self.assertEqual( views.add_domain('example.com', '//example.com/foo/?arg=value'), 'http://example.com/foo/?arg=value' )
def __grant_normalize(grant): if grant == 'ALL': grant = 'ALL PRIVILEGES'
df = DataFrame({'x': [], 'y': []}) q = df.quantile(0.1, axis=0, interpolation='higher') assert(np.isnan(q['x']) and np.isnan(q['y']))
enrollment_report_headers = { 'User ID': _('User ID'), 'Username': _('Username'), 'Full Name': _('Full Name'), 'First Name': _('First Name'), 'Last Name': _('Last Name'), 'Company Name': _('Company Name'), 'Title': _('Title'), 'Language': _('Language'), 'Year of Birth': _('Year of Birth'), 'Gender': _('Gender'), 'Level of Education': _('Level of Education'), 'Mailing Address': _('Mailing Address'), 'Goals': _('Goals'), 'City': _('City'), 'Country': _('Country'), 'Enrollment Date': _('Enrollment Date'), 'Currently Enrolled': _('Currently Enrolled'), 'Enrollment Source': _('Enrollment Source'), 'Manual (Un)Enrollment Reason': _('Manual (Un)Enrollment Reason'), 'Enrollment Role': _('Enrollment Role'), 'List Price': _('List Price'), 'Payment Amount': _('Payment Amount'), 'Coupon Codes Used': _('Coupon Codes Used'), 'Registration Code Used': _('Registration Code Used'), 'Payment Status': _('Payment Status'), 'Transaction Reference Number': _('Transaction Reference Number') }
values = Series([u('a'), u('b'), NA, u('c'), NA, u('eeeeee')])
new_id = max(Vegetable.objects.order_by('-id')[0].id, Mineral.objects.order_by('-id')[0].id) + 1 broccoli = Vegetable.objects.create(id=new_id, name="Broccoli") diamond = Mineral.objects.create(id=new_id, name="Diamond", hardness=7) tag = TaggedItem.objects.create(content_object=broccoli, tag="yummy") tag.content_type = ContentType.objects.get_for_model(diamond) self.assertEqual(tag.content_object, diamond)
if re_pattern: minion_id = re.sub(re_pattern, re_replace, minion_id)
coupon_redemption = cls.objects.get( user=user, coupon__course_id=order_item_course_id if order_item_course_id else CourseKeyField.Empty, order=item.order_id ) coupon_redemption.delete() log.info( u'Coupon "%s" redemption entry removed for user "%s" for order item "%s"', coupon_redemption.coupon.code, user, str(item.id), )
def f(x): x[:] = 10
self.__dict__['_deleted'] = set() self.default_settings = default_settings
DEFAULTS = {'mongo.db': 'salt', 'mongo.host': 'salt', 'mongo.password': '', 'mongo.port': 27017, 'mongo.user': '', 'redis.db': '0', 'redis.host': 'salt', 'redis.port': 6379, 'test.foo': 'unconfigured', 'ca.cert_base_path': '/etc/pki', 'solr.cores': [], 'solr.host': 'localhost', 'solr.port': '8983', 'solr.baseurl': '/solr', 'solr.type': 'master', 'solr.request_timeout': None, 'solr.init_script': '/etc/rc.d/solr', 'solr.dih.import_options': {'clean': False, 'optimize': True, 'commit': True, 'verbose': False}, 'solr.backup_path': None, 'solr.num_backups': 1, 'poudriere.config': '/usr/local/etc/poudriere.conf', 'poudriere.config_dir': '/usr/local/etc/poudriere.d', 'ldap.server': 'localhost', 'ldap.port': '389', 'ldap.tls': False, 'ldap.scope': 2, 'ldap.attrs': None, 'ldap.binddn': '', 'ldap.bindpw': '', 'hosts.file': '/etc/hosts', 'aliases.file': '/etc/aliases', 'virt.images': os.path.join(syspaths.SRV_ROOT_DIR, 'salt-images'), 'virt.tunnel': False, }
self.assertFalse(self.creator_admin.has_delete_permission(self.request))
scal_points = new_W / norms.dimshuffle('x',0)
course_policy_dir_name = courselike.url_name
d_tries += 1 hsum = salt.utils.get_hash(dest, salt.utils.to_str(data.get('hash_type', b'md5'))) if hsum != data['hsum']: log.warning('Bad download of file {0}, attempt {1} ' 'of 3'.format(path, d_tries)) continue
% {'blue_pk': colour.pk})
mlp = MLP(layers=[Linear(layer_name='h', dim=5, irange=0.01)]) conditional = DummyConditional(mlp=mlp, name='conditional') vae = DummyVAE() conditional.set_vae(vae) testing.assert_same_object(conditional.vae, vae) testing.assert_same_object(conditional.rng, vae.rng) testing.assert_equal(conditional.batch_size, vae.batch_size)
PIPELINE_ENABLED = False STATICFILES_STORAGE = 'openedx.core.storage.DevelopmentStorage'
if kwargs.get('current_zone_only') == 'True': cmd_prefix += '-G '
from __future__ import absolute_import import os import re import fnmatch import json import subprocess
supports_foreign_keys = True
for nvitem in thelist: if isinstance(nvitem, dict): name, value = next(six.iteritems(nvitem)) if names is None or name in names: yield nvitem, name, value
APP_UPGRADE_CACHE_TIMEOUT = 3600
user2 = UserFactory.create() user2.save() assert_is_none(SoftwareSecurePhotoVerification.active_for_user(user2))
log.debug('Master _pillar using ext: {0}'.format(load.get('ext'))) pillar = salt.pillar.get_pillar( self.opts, load['grains'], load['id'], load.get('saltenv', load.get('env')), load.get('ext'), self.mminion.functions, pillar=load.get('pillar_override', {})) pillar_dirs = {} data = pillar.compile_pillar(pillar_dirs=pillar_dirs) if self.opts.get('minion_data_cache', False): cdir = os.path.join(self.opts['cachedir'], 'minions', load['id']) if not os.path.isdir(cdir): os.makedirs(cdir) datap = os.path.join(cdir, 'data.p') tmpfh, tmpfname = tempfile.mkstemp(dir=cdir) os.close(tmpfh) with salt.utils.fopen(tmpfname, 'w+b') as fp_: fp_.write( self.serial.dumps( {'grains': load['grains'], 'pillar': data}) ) salt.utils.atomicfile.atomic_rename(tmpfname, datap) return data
if publish == 'republish' and xblock.category not in DIRECT_ONLY_CATEGORIES: if modulestore().has_published_version(xblock): publish = 'make_public'
HAS_GLANCE = False try: from glanceclient import client from glanceclient import exc HAS_GLANCE = True except ImportError: pass
self._assert_output(output)
scaler_batch = StandardScaler(with_std=False).fit(X)
ordering.extend(queryset.query.order_by)
result = self.index.append([]) self.assertTrue(result.equals(self.index))
import hashlib import json import logging import requests import dogstats_wrapper as dog_stats_api
mdict = copy.deepcopy(self.dict1) mdict['Z'] = 'Y' res = dictupdate.update(copy.deepcopy(self.dict1), {'Z': 'Y'}) self.assertEqual(res, mdict)
from __future__ import absolute_import import logging import os import datetime
s1 = Series([1, 2, 3], index=['a', 'b', 'c'], name='x')
if value: wait_for(lambda _: css_value(css_selector, index=index))
md4 = hashlib.new("md4") md4.update(string) return md4.hexdigest()
chris = Person(name="Chris Mills") html5 = Book(title="Dive into HTML5", published=datetime.date(2010, 3, 15)) self.assertEqual(chris._state.db, None) self.assertEqual(html5._state.db, None)
return super(NullBackend, self).get_dict(dict_name, default, **kwargs)
random_state = np.random.RandomState(36) data = random_state.randn(10, 30) sims = rbf_kernel(data) n_components = 8 embedding_1 = spectral_embedding(sims, norm_laplacian=False, n_components=n_components, drop_first=False)
num_classes = 10 batch_size = 20 mlp_bin = MLP( layers=[Softmax(num_classes, 's1', irange=0.1, binary_target_dim=1)], nvis=100 ) mlp_vec = MLP( layers=[Softmax(num_classes, 's1', irange=0.1)], nvis=100 ) X = mlp_bin.get_input_space().make_theano_batch() y_bin = mlp_bin.get_target_space().make_theano_batch() y_vec = mlp_vec.get_target_space().make_theano_batch() y_hat_bin = mlp_bin.fprop(X) y_hat_vec = mlp_vec.fprop(X) cost_bin = theano.function([X, y_bin], mlp_bin.cost(y_bin, y_hat_bin), allow_input_downcast=True) cost_vec = theano.function([X, y_vec], mlp_vec.cost(y_vec, y_hat_vec), allow_input_downcast=True) X_data = np.random.random(size=(batch_size, 100)) y_bin_data = np.random.randint(low=0, high=10, size=(batch_size, 1)) y_vec_data = np.zeros((batch_size, num_classes)) y_vec_data[np.arange(batch_size), y_bin_data.flatten()] = 1 np.testing.assert_allclose(cost_bin(X_data, y_bin_data), cost_vec(X_data, y_vec_data))
if mode == 1: if matvec is None: raise ValueError("matvec must be specified for mode=1") if M_matvec is not None: raise ValueError("M_matvec cannot be specified for mode=1") if Minv_matvec is not None: raise ValueError("Minv_matvec cannot be specified for mode=1")
try: SimpleView.as_view('value') self.fail('Should not be able to use non-keyword arguments instantiating a view') except TypeError: pass
monitoring_dataset = DenseDesignMatrix(X=X, y=Y)
self.assert_middleware_usage(pre_middleware, True, False, False, True, False) self.assert_middleware_usage(bad_middleware, True, False, False, True, False) self.assert_middleware_usage(post_middleware, False, False, False, True, False)
DEFAULT_DATETIME_STR = datetime.datetime(year=2014, month=12, day=1).isoformat(' ')
self.results = batch()
if settings.LMS_SEGMENT_KEY: event_name = 'edx.bi.user.certificate.generate' tracking_context = tracker.get_tracker().resolve_context() analytics.track( user_id, event_name, { 'category': 'certificates', 'label': unicode(course_id) }, context={ 'ip': tracking_context.get('ip'), 'Google Analytics': { 'clientId': tracking_context.get('client_id') } } )
for letter in range(ord('A'), ord('Z')): self.frame[chr(letter)] = chr(letter)
SITE_ID = 1 SITE_NAME = "example.com" HTTPS = 'on' ROOT_URLCONF = 'lms.urls'
libraries = [LibraryFactory.create() for _ in range(3)] lib_dict = dict([(lib.location.library_key, lib) for lib in libraries])
return cls.verification_valid_or_pending(user, earliest_allowed_date, queryset).exists()
self.assertRaises(TypeError, self.read_csv, path, dtype={'A': 'datetime64', 'B': 'float64'}, index_col=0) self.assertRaises(TypeError, self.read_csv, path, dtype={'A': 'datetime64', 'B': 'float64'}, index_col=0, parse_dates=['B'])
return True
field_column = self.quote_name(field.column) if field.geom_type == 'RASTER': field_column = self.rast_index_wrapper % field_column index_ops = '' elif field.geography: index_ops = '' else: if field.dim > 2: index_ops = self.geom_index_ops_nd else: index_ops = '' self.geometry_sql.append( self.sql_add_spatial_index % { "index": self.quote_name('%s_%s_id' % (model._meta.db_table, field.column)), "table": self.quote_name(model._meta.db_table), "column": field_column, "index_type": self.geom_index_type, "ops": index_ops, } )
os.rmdir(path)
func = with_comprehensive_theme(EDX_THEME_DIR)(func)
self.name = tab_dict.get('name', self.title) self.tab_id = tab_dict.get('tab_id', getattr(self, 'tab_id', self.type)) self.link_func = tab_dict.get('link_func', link_reverse_func(self.view_name)) self.is_hidden = tab_dict.get('is_hidden', False)
self.prev_floatX = config.floatX config.floatX = 'float64'
for old_alias, new_alias in six.iteritems(change_map): if old_alias not in self.alias_map: continue alias_data = self.alias_map[old_alias].relabeled_clone(change_map) self.alias_map[new_alias] = alias_data self.alias_refcount[new_alias] = self.alias_refcount[old_alias] del self.alias_refcount[old_alias] del self.alias_map[old_alias]
thumbnail_content, thumbnail_location = static_content_store.generate_thumbnail(content)
self.assert_enrollment_status() resp = self.client.get( reverse('courseenrollment', kwargs={"course_id": unicode(self.course.id)}) ) self.assertEqual(resp.status_code, status.HTTP_200_OK) data = json.loads(resp.content) self.assertEqual(unicode(self.course.id), data['course_details']['course_id']) self.assertEqual(CourseMode.DEFAULT_MODE_SLUG, data['mode']) self.assertTrue(data['is_active'])
from salt.exceptions import CommandExecutionError, SaltInvocationError
swagger = _Swagger(api_name, stage_name, lambda_funcname_format, swagger_file, common_args)
return self.q(css=self._bounded_selector('.add-admin-role')).present
clf = svm.LinearSVC().fit(iris.data, iris.target) values = clf.decision_function(iris.data) clf.coef_ = clf.coef_.copy() clf.intercept_ = clf.intercept_.copy() values2 = clf.decision_function(iris.data) assert_array_almost_equal(values, values2)
queryset = _get_queryset(klass) try: return queryset.get(*args, **kwargs) except AttributeError: klass__name = klass.__name__ if isinstance(klass, type) else klass.__class__.__name__ raise ValueError( "First argument to get_object_or_404() must be a Model, Manager, " "or QuerySet, not '%s'." % klass__name ) except queryset.model.DoesNotExist: raise Http404('No %s matches the given query.' % queryset.model._meta.object_name)
raw_path = script.get('system_path', '').split(":") + DEFAULT_PATH
blocks_stack.extend(children)
actual_url = staticfiles_storage.url(path_overrides[module])
for field_object in self._read_objects(fields, xblocks, aside_types): self._cache[self._cache_key_for_field_object(field_object)] = field_object
patch_response_headers(response, cache_timeout=-1) patch_cache_control(response, no_cache=True, no_store=True, must_revalidate=True)
if self.teams_configuration: return len(self.teams_configuration.get('topics', [])) > 0 return False
class_=class_, selector=selector
return "django_date_extract('%s', %s)" % (lookup_type.lower(), field_name)
self.store.delete_item( self.icrv.location, ModuleStoreEnum.UserID.test, revision=ModuleStoreEnum.RevisionOption.published_only ) self._update_partitions()
return {}
from salt.states import schedule
def f(): s.iloc[3.0] = 0 self.assertRaises(TypeError, f)
import logging
if [ -z "$debian_chroot" ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi
include, exclude = map( lambda x: frozenset(map(com._get_dtype_from_object, x)), selection) for dtypes in (include, exclude): com._invalidate_string_dtypes(dtypes)
apps = Apps() verbose_name = 'úñí©óðé µóðéø' verbose_name_plural = 'úñí©óðé µóðéøß'
return _slugify(value)
def test_notifications_defaults(self): self.assertFalse(PushNotificationConfig.is_enabled()) def test_notifications_enabled(self): PushNotificationConfig(enabled=True).save() self.assertTrue(PushNotificationConfig.is_enabled())
def test_rolling_cov(self): A = self.series B = A + randn(len(A))
arr = np.arange(5, dtype='int64') * 3.2 expected = Float64Index(arr) fidx = idx * 3.2 tm.assert_index_equal(fidx, expected) fidx = 3.2 * idx tm.assert_index_equal(fidx, expected)
server.ping()
if vm.config.template: status = 'VMware tools cannot be updated on a template' return status
test1=value 1
if ( tag == 'msup' and len(k) == 2 and gettag(k[1]) == 'mrow' and
trunc_kl = - model.entropy_hs(H_hat = H_var, var_s0_hat = sigma0, var_s1_hat = Sigma1) + \ model.expected_energy_vhs(V = X, H_hat = H_var, S_hat = Mu1_var, var_s0_hat = sigma0, var_s1_hat = Sigma1)
if value.ndim == 0: value = tslib.iNaT
photo_id_key = models.TextField(max_length=1024)
if self.xml.get('inline', ''): tree.set('class', 'inline')
return self.__class__(which_set='test', split=self.split)
anova_filter = SelectKBest(f_regression, k=3) clf = svm.SVC(kernel='linear')
result = concat([df, df2, df3], axis=1, copy=True) for b in result._data.blocks: self.assertIsNone(b.values.base)
def dec(func): return self.tag(name, func) return dec
self.assertContentBefore(response, link2, link1)
log.error('Received local command remotely! Ignoring: {0}'.format(msg)) return
return settings.FEATURES.get('ENABLE_COURSEWARE_INDEX', False)
executor.migrate([ ("author_app", "0001_initial"), ("book_app", "0001_initial"), ]) self.assertTableExists("author_app_author") self.assertTableExists("book_app_book") executor.loader.build_graph()
spca = SparsePCA(n_components=3, n_jobs=2, method='lars', alpha=alpha, random_state=0).fit(Y) U2 = spca.transform(Y) assert_true(not np.all(spca_lars.components_ == 0)) assert_array_almost_equal(U1, U2)
if not is_safe_url(url=next_page, host=request.get_host()): next_page = request.path
link = reverse('upload_transcripts') filename = os.path.splitext(os.path.basename(self.good_srt_file.name))[0] resp = self.client.post(link, { 'locator': '{0}_{1}'.format(self.video_usage_key, 'BAD_LOCATOR'), 'transcript-file': self.good_srt_file, 'video_list': json.dumps([{ 'type': 'html5', 'video': filename, 'mode': 'mp4', }]) }) self.assertEqual(resp.status_code, 400) self.assertEqual(json.loads(resp.content).get('status'), "Can't find item by locator.")
available_version = salt.utils.alias_function(latest_version, 'available_version')
if isinstance(pkg_verify, list) \ and any(x.get('verify_options') is not None for x in pkg_verify if isinstance(x, _OrderedDict) and 'verify_options' in x): verify_options = next(x.get('verify_options') for x in pkg_verify if 'verify_options' in x) else: verify_options = []
URLCONF_MODULES = None
self.assertOptimizesTo( [ migrations.AddField("Foo", "name", models.CharField(max_length=255)), migrations.RenameField("Foo", "name", "title"), ], [ migrations.AddField("Foo", "title", models.CharField(max_length=255)), ], )
pip install https://pysphere.googlecode.com/files/pysphere-0.1.8.zip
SEARCH_SKIP_ENROLLMENT_START_DATE_FILTERING = True
lhs = pd.eval(value, local_dict=self.env, engine=self.engine, parser=self.parser) v = lhs[result]
face = face[::2, ::2] + face[1::2, ::2] + face[::2, 1::2] + face[1::2, 1::2] face /= 4.0 height, width = face.shape
request = self.rf.get('/slash/') self.assertEqual(CommonMiddleware().process_request(request), None) response = HttpResponseNotFound() self.assertEqual(CommonMiddleware().process_response(request, response), response)
import salt.utils import salt.utils.decorators as decorators
for axes_row in all_axes: for axes in axes_row: axes.get_xaxis().set_visible(False) axes.get_yaxis().set_visible(False)
all_checks_points = cls.objects.filter( user_id=user_id, checkpoint__course_id=course_key ) check_points = {} for check in all_checks_points: check_points[check.checkpoint.checkpoint_location] = check.status return check_points
from salt.exceptions import ( CommandExecutionError, MinionError ) import salt.client import salt.crypt import salt.loader import salt.payload import salt.transport import salt.fileserver import salt.utils import salt.utils.files import salt.utils.templates import salt.utils.url import salt.utils.gzip_util import salt.utils.http import salt.utils.s3 import salt.ext.six as six from salt.utils.locales import sdecode from salt.utils.openstack.swift import SaltSwift
return self.view_converter.view_shape()
n_features = 501 n_relevant_features = 3 noise_level = .2 coef_min = .2 n_samples = 25 block_size = n_relevant_features
LIBRARY_LABEL = "Library" COUNT_LABEL = "Count" SCORED_LABEL = "Scored" PROBLEM_TYPE_LABEL = "Problem Type"
df = tm.makeDataFrame() store.append('df', df)
if 'provider' in details: details['driver'] = details.pop('provider')
self._state = threading.local() self._state.data = None self.global_data = {}
for state in READY_STATES: instructor_tasks = instructor_tasks.exclude(task_state=state) return instructor_tasks.order_by('-id')
text = "J'ai mang\xe9 du kangourou ce midi, c'\xe9tait pas tr\xeas bon." text_bytes = text.encode('utf-8')
from __future__ import unicode_literals from django.db import models from django.utils import six from django.utils.encoding import python_2_unicode_compatible class User(models.Model): username = models.CharField(max_length=20) @python_2_unicode_compatible class Issue(models.Model): num = models.IntegerField() cc = models.ManyToManyField(User, blank=True, related_name='test_issue_cc') client = models.ForeignKey(User, models.CASCADE, related_name='test_issue_client') def __str__(self): return six.text_type(self.num) class Meta: ordering = ('num',) class UnicodeReferenceModel(models.Model): others = models.ManyToManyField("UnicodeReferenceModel")
result = concat([t1, t2], axis=1, keys=['t1', 't2']) self.assertEqual(list(result.columns), [('t1', 'value'), ('t2', 'value')])
res = linkage_tree(X, affinity=manhattan_distances) assert_array_equal(res[0], linkage_tree(X, affinity="manhattan")[0])
W_irange = 2 / numpy.sqrt(nvis * nhid)
sh('xmodule_assets common/static/xmodule') print("\t\tFinished processing xmodule assets.")
for alphas_min in alphas_min: alphas, _, lasso_path = linear_model.lars_path(X, y, method='lasso', alpha_min=0.9) lasso_cd = linear_model.Lasso(fit_intercept=True, normalize=True, tol=1e-8) lasso_cd.alpha = alphas[-1] lasso_cd.fit(X, y) error = linalg.norm(lasso_path[:, -1] - lasso_cd.coef_) assert_less(error, 0.01)
cp = self.ts.copy() cp.name = 'changed' result = getattr(s, op)(cp) self.assertIsNone(result.name)
yield check_estimators_empty_data_messages
dirty = git_describe.endswith("-dirty") pieces["dirty"] = dirty if dirty: git_describe = git_describe[:git_describe.rindex("-dirty")]
if column_num == 0: cls._initialize_stacker(ax, stacking_id, len(y)) y_values = cls._get_stacked_values(ax, stacking_id, y, kwds['label']) lines = MPLPlot._plot(ax, x, y_values, style=style, **kwds) cls._update_stacker(ax, stacking_id, y) return lines
images = as_cuda_ndarray_variable(images) assert images.ndim == 4 channels_broadcastable = images.type.broadcastable[0] batch_broadcastable = images.type.broadcastable[3] rows_broadcastable = False cols_broadcastable = False targets_broadcastable = (channels_broadcastable, rows_broadcastable, cols_broadcastable, batch_broadcastable) targets_type = CudaNdarrayType(broadcastable=targets_broadcastable) targets = targets_type() return Apply(self, [images], [targets])
layer = Softmax(num_classes, 's', irange=.1) softmax_mlp = MLP(layers=[layer], input_space=Conv2DSpace(shape=(conv_dim[0], conv_dim[1]), num_channels=conv_dim[2])) conv_weights = np.random.randn(conv_dim[0], conv_dim[1], conv_dim[2], num_classes).astype(config.floatX) layer.set_weights(conv_weights.reshape(np.prod(conv_dim), num_classes)) assert np.allclose(layer.W.get_value(), conv_weights.reshape(np.prod(conv_dim), num_classes)) layer.W.set_value(conv_weights.reshape(np.prod(conv_dim), num_classes)) assert np.allclose(layer.get_weights_topo(), np.transpose(conv_weights, axes=(3, 0, 1, 2)))
assert isinstance(course_key, CourseKey) course_key = self.fill_in_run(course_key) location = course_key.make_usage_key('course', course_key.run) try: return self.get_item(location, depth=depth) except ItemNotFoundError: return None
client = self.login_client(api_client, user) new_value = "new value" self.send_put(client, new_value, expected_status=403 if user == "staff_user" else 404)
knn.fit(X, y_str) y_pred = knn.predict(X[:n_test_pts] + epsilon) assert_array_equal(y_pred, y_str[:n_test_pts])
self.addAvailable('alpha') self.addAvailable('beta') self.addPresenceInfo('aliveds', 'alpha', '1.1.1.1', '1234') self.addPresenceInfo('aliveds', 'beta', '1.2.3.4', '1234') testStack = self.store.fetch('.salt.test.lane.stack').value presenceReq = self.store.fetch('.salt.presence.event_req').value ryn = 'manor' presenceReq.append({'route': {'dst': (None, ryn, 'presence_req'), 'src': (None, testStack.local.name, None)}, 'data': {'state': 'available'}}) presenceReq.append({'route': {'dst': (None, ryn, 'presence_req'), 'src': (None, testStack.local.name, None)}}) presenceReq.append({'route': {'dst': (None, ryn, 'presence_req'), 'src': (None, testStack.local.name, None)}, 'data': {}}) presenceReq.append({'route': {'dst': (None, ryn, 'presence_req'), 'src': (None, testStack.local.name, None)}, 'data': {'state': None}}) presenceReq.append({'route': {'dst': (None, ryn, 'presence_req'), 'src': (None, testStack.local.name, None)}, 'data': {'state': 'present'}})
self._test_setup(num_chapters=11) self._bookmark_units(num_units=11) self.bookmarks_page.click_bookmarks_button() self.assertTrue(self.bookmarks_page.results_present()) self.assertEqual(self.bookmarks_page.get_total_pages, 2) self.bookmarks_page.go_to_page(3) self._verify_pagination_info( bookmark_count_on_current_page=10, header_text='Showing 1-10 out of 11 total', previous_button_enabled=False, next_button_enabled=True, current_page_number=1, total_pages=2 )
try: state_action = matching_states[action] except KeyError: log.error( 'The use of \'{0}\' as an action is not supported in this context. ' 'Only \'start\', \'stop\', and \'reboot\' are supported options.'.format(action) ) raise SaltCloudException() if vm_details != 'Absent' and vm_details['state'].lower() in state_action: vm_names.append(vm_name)
assert_array_almost_equal((tfidf ** 2).sum(axis=1), [1., 1., 1.])
logging_config_func = import_string(logging_config)
return len(tup) <= self.lexsort_depth
from __future__ import absolute_import
usage_key = UsageKey.from_string(usage_key_string) course_key = CourseKey.from_string(course_id) usage_key = usage_key.map_into_course(course_key) user = User.objects.get(id=user_id) field_data_cache = FieldDataCache.cache_for_descriptor_descendents( course_key, user, modulestore().get_item(usage_key), depth=0, ) instance = get_module(user, request, usage_key, field_data_cache, grade_bucket_type='xqueue', course=course) if instance is None: msg = "No module {0} for user {1}--access denied?".format(usage_key_string, user) log.debug(msg) raise Http404 return instance
bob = User.objects.db_manager('other').create_user('bob', 'bob@example.com')
self.assertNumContains(sql, 'tbl_tbsp', 3)
course = self.store.create_course('org_x', 'course_y', 'run_z', self.user_id) course_key = course.id
not_enough_centroids = False if isinstance(clusterer, int): clusterer = AgglomerativeClustering( n_clusters=self.n_clusters) if len(centroids) < self.n_clusters: not_enough_centroids = True elif (clusterer is not None and not hasattr(clusterer, 'fit_predict')): raise ValueError("n_clusters should be an instance of " "ClusterMixin or an int")
request_context = get_template_request_context() if request_context: for item in request_context: context_dictionary.update(item) for item in context_instance: context_dictionary.update(item) if context: context_dictionary.update(context)
self.frame.columns.name = 'columns' resetted = self.frame.reset_index() self.assertEqual(resetted.columns.name, 'columns')
self.assertEqual(self.selenium.find_element_by_id('id_main_band').get_attribute('value'), '')
SplitMongoKVSid = namedtuple('SplitMongoKVSid', 'id, def_id') new_contract('BlockUsageLocator', BlockUsageLocator)
return _write_js(output_root, _list_descriptors())
if configuration_id: self.configuration['id'] = int(configuration_id) else: self.configuration['id'] = generate_int_id( MINIMUM_GROUP_ID, MYSQL_MAX_INT, GroupConfiguration.get_used_ids(self.course) )
if i + 1 == len(course_keys): self.assertTrue(badge_class.get_for_user(user)) else: self.assertFalse(badge_class.get_for_user(user))
url( r'^courses/{}/syllabus$'.format( settings.COURSE_ID_PATTERN, ), 'courseware.views.views.syllabus', name='syllabus', ),
import logging
from __future__ import unicode_literals
return self.P_H_given_V(v)
u_based = (np.abs(u).max(axis=0) == u.max(axis=0)).all() v_based = (np.abs(v).max(axis=1) == v.max(axis=1)).all() return u_based, v_based
self._export_import_course_round_trip( self.store, contentstore, source_course_key, self.export_dir )
c = c.set_categories([4, 3, 2, 1])
template = engine.get_template('other-recursive.html') output = template.render(Context({})) self.assertEqual(output.strip(), 'fs3/recursive fs2/recursive fs/recursive')
try: self.client.get('/broken_view/') self.fail('Should raise an error') except KeyError: pass
self.deferred_loading = (set(), True)
if len(parents_index) > 1: for index in range(1, len(parents_index)): parent_index = parents_index[index] parent_block = self.get_block(parent_index) parent_block.children.append(self.xblock_keys[i]) update_block(parent_block)
__ = CourseOverview.get_from_id(course.id)
res = df.loc[lambda x: 1, lambda x: 'A'] self.assertEqual(res, df.loc[1, 'A'])
response = None log.exception("Outcome Service: Error when sending result.")
rng = np.random.RandomState(0) X = rng.normal(size=(10, 4)) y = multioutput_estimator_convert_y_2d(name, X[:, 0]) regressor = Regressor()
class MyWidget4(TextInput): class Media: css = {'all': ('/path/to/css1', '/path/to/css1')} js = ('/path/to/js1', '/path/to/js1')
if not hasattr(self, 'inference_procedure') or \ self.inference_procedure is None: self.inference_procedure = WeightDoubling() self.inference_procedure.set_dbm(self)
rpm_tags = __salt__['cmd.run_stdout']( ['rpm', '--querytags'], python_shell=False).splitlines() if 'LONGSIZE' in rpm_tags: size_tag = '%{LONGSIZE}' else: size_tag = '%{SIZE}'
for freq in ['D', '2D', '3D']: p = Period('NaT', freq=freq) for o in [offsets.Day(5), offsets.Hour(24), np.timedelta64(2, 'D'), np.timedelta64(3600 * 24, 's'), timedelta(-2), timedelta(hours=48)]: self.assertEqual((p + o).ordinal, tslib.iNaT)
problem_name, correctness = scenarios[name] problem = problems[problem_name]
return redirect(reverse('dashboard'))
return y - expit(pred.ravel())
import salt.returners import salt.utils.jid import salt.exceptions from salt.exceptions import CommandExecutionError
grouper, _, _ = _get_grouper(dropped, key=self.keys, axis=self.axis, level=self.level, sort=self.sort, mutated=self.mutated)
if mask.any():
'service_status',
repo = kwargs.get('repo', '') if not fromrepo and repo: fromrepo = repo
try: ExternalAuthMap.objects.get(external_id=self.USER_EMAIL) except ExternalAuthMap.DoesNotExist, ex: self.fail('User did not get properly added to external auth map, exception was {0}'.format(str(ex))) try: User.objects.get(email=self.USER_EMAIL) except ExternalAuthMap.DoesNotExist, ex: self.fail('User did not get properly added to internal users, exception was {0}'.format(str(ex)))
if self.fmt.index: if isinstance(self.frame.index, MultiIndex): self._write_hierarchical_rows(fmt_values, indent) else: self._write_regular_rows(fmt_values, indent) else: for i in range(len(self.frame)): row = [fmt_values[j][i] for j in range(len(self.columns))] self.write_tr(row, indent, self.indent_delta, tags=None)
assert_raises(ValueError, k_means, X, n_clusters=X.shape[0] + 1)
abstract = True
end = info[1].index(' ', column+1)
self.file_name = 'subs_3_yD_cEKoCk.srt.sjson' self.test_dir = path(__file__).abspath().dirname().dirname().dirname().dirname().dirname() self.file_path = self.test_dir + '/common/test/data/uploads/' + self.file_name
self.client.ajax_post( self.seq1_url, data={'isPrereq': True} ) mock_add_prereq.assert_called_with(self.course.id, self.seq1.location)
self.assertRaises(TypeError, self.read_csv, path, dtype={'A': 'foo', 'B': 'float64'}, index_col=0)
return check_sum_of_calls(object_with_method, [method_name], maximum_calls, minimum_calls)
setattr(instance, self.cache_name, value)
if (n_inliers_subset == n_inliers_best and score_subset < score_best): continue
self.check_result('slice', 'iloc', slice(1, 3), 'ix', {0: [2, 4], 1: [3, 6], 2: [4, 8]}, typs=['ints']) self.check_result('slice', 'iloc', slice(1, 3), 'indexer', slice( 1, 3), typs=['labels', 'mixed', 'ts', 'floats', 'empty'], fails=IndexError)
task_id = str(uuid4()) progress_json = json.dumps(task_output) if task_output is not None else None task_input, task_key = encode_problem_and_student_input(self.problem_url, student) instructor_task = InstructorTaskFactory.create(course_id=TEST_COURSE_KEY, requester=self.instructor, task_input=json.dumps(task_input), task_key=task_key, task_id=task_id, task_state=task_state, task_output=progress_json) return instructor_task
clear_load.pop('password', None) jid = salt.utils.jid.gen_jid() fun = clear_load.pop('fun') tag = tagify(jid, prefix='wheel') data = {'fun': "wheel.{0}".format(fun), 'jid': jid, 'tag': tag, 'user': clear_load.pop('username', 'UNKNOWN')} try: self.event.fire_event(data, tagify([jid, 'new'], 'wheel')) ret = self.wheel_.call_func(fun, **clear_load) data['return'] = ret data['success'] = True self.event.fire_event(data, tagify([jid, 'ret'], 'wheel')) return {'tag': tag, 'data': data} except Exception as exc: log.error('Exception occurred while ' 'introspecting {0}: {1}'.format(fun, exc)) data['return'] = 'Exception occurred in wheel {0}: {1}: {2}'.format( fun, exc.__class__.__name__, exc, ) self.event.fire_event(data, tagify([jid, 'ret'], 'wheel')) return {'tag': tag, 'data': data}
return "start_date"
tarball_re = re.compile(r'^salt-([^-]+)(?:-(\d+)-(g[0-9a-f]+))?\.tar\.gz$') try: base, offset, oid = tarball_re.match(os.path.basename(sdist)).groups() except AttributeError: _abort('Unable to extract version info from sdist filename \'{0}\'' .format(sdist))
with mock_create_refund(status=403): refund_seat(self.course_enrollment, UserFactory()) self.assertTrue(mock_log_warning.called)
return { "id": i, "type": grader["type"], "min_count": grader.get('min_count', 0), "drop_count": grader.get('drop_count', 0), "short_label": grader.get('short_label', ""), "weight": grader.get('weight', 0) * 100, }
opts = parse_bokchoy_opts(options) opts['report_dir'] = Env.BOK_CHOY_A11Y_REPORT_DIR opts['coveragerc'] = Env.BOK_CHOY_A11Y_COVERAGERC opts['extra_args'] = opts['extra_args'] + ' -a "a11y" ' run_bokchoy(**opts)
self.test_tab = ItemFactory.create( parent_location=self.course.location, category="static_tab", display_name="Static_1" ) self.reload_course()
break
courses_list_by_groups, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(len(courses_list_by_groups), 1)
import salt.ext.six as six from salt.ext.six.moves import input try: import msgpack except ImportError: pass
if hasattr(course_key, 'ccx'): ccx_id = course_key.ccx role = CourseCcxCoachRole(course_key) if role.has_user(user): list_ccx = CustomCourseForEdX.objects.filter( course_id=course_key.to_course_locator(), coach=user ) if list_ccx.exists(): coach_ccx = list_ccx[0] return str(coach_ccx.id) == ccx_id else: raise CCXLocatorValidationException("Invalid CCX key. To verify that " "user is a coach on CCX, you must provide key to CCX") return False
msg = r'Shape of passed values is \(3, 2\), indices imply \(3, 1\)' with tm.assertRaisesRegexp(ValueError, msg): DataFrame(mat, columns=['A', 'B', 'C'], index=[1]) msg = r'Shape of passed values is \(3, 2\), indices imply \(2, 2\)' with tm.assertRaisesRegexp(ValueError, msg): DataFrame(mat, columns=['A', 'B'], index=[1, 2])
if np.fabs(result) > _int64_max: raise ValueError("overflow in timedelta operation")
import inspect import logging import sys
mgr = create_mgr('f: i8; g: f8') new_mgr = mgr.convert() _compare(mgr, new_mgr)
_skip_if_has_locale()
annotation_span_selector = '.annotatable-span[data-problem-id="{}"]'.format(problem) self.mouse_hover(self.browser.find_element_by_css_selector(annotation_span_selector)) self.wait_for_element_visibility(annotation_span_selector, "Reply to Annotation link is visible") annotation_reply_selector = '.annotatable-reply[data-problem-id="{}"]'.format(problem) self.q(css=annotation_reply_selector).click() self.active_problem = problem
pts = tuple([p[i] for p in points]) indexer.put(indlist, slice_indexer)
rnd = np.random.RandomState(0) X_train_finite = rnd.uniform(size=(10, 3)) X_train_nan = rnd.uniform(size=(10, 3)) X_train_nan[0, 0] = np.nan X_train_inf = rnd.uniform(size=(10, 3)) X_train_inf[0, 0] = np.inf y = np.ones(10) y[:5] = 0 y = multioutput_estimator_convert_y_2d(name, y) error_string_fit = "Estimator doesn't check for NaN and inf in fit." error_string_predict = ("Estimator doesn't check for NaN and inf in" " predict.") error_string_transform = ("Estimator doesn't check for NaN and inf in" " transform.") for X_train in [X_train_nan, X_train_inf]: with warnings.catch_warnings(record=True): estimator = Estimator() set_testing_parameters(estimator) set_random_state(estimator, 1) try: estimator.fit(X_train, y) except ValueError as e: if 'inf' not in repr(e) and 'NaN' not in repr(e): print(error_string_fit, Estimator, e) traceback.print_exc(file=sys.stdout) raise e except Exception as exc: print(error_string_fit, Estimator, exc) traceback.print_exc(file=sys.stdout) raise exc else: raise AssertionError(error_string_fit, Estimator) estimator.fit(X_train_finite, y)
try: course_code_number = int(request.POST['total_registration_codes']) except ValueError: course_code_number = int(float(request.POST['total_registration_codes']))
self.assertEqual(len(httpretty.httpretty.latest_requests), 3)
from __future__ import absolute_import
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
if course_html_parsed.tag == 'ol': for index, update in enumerate(course_html_parsed): if len(update) > 0: content = _course_info_content(update) computed_id = len(course_html_parsed) - index payload = { "id": computed_id, "date": update.findtext("h2"), "content": content } if provided_index == 0: course_update_items.append(payload) elif provided_index == computed_id: return payload
if 'file_client' not in __opts__ or not __opts__['file_client']: __opts__['file_client'] = 'local'
return iri_to_uri(get_script_prefix().rstrip('/') + self.url)
if __opts__['test']: ret['result'] = None if name not in dbs: ret['comment'] = 'Database {0} is set to be created'.format(name) else: ret['comment'] = 'Database {0} exists, but parameters ' \ 'need to be changed'.format(name) return ret if ( name not in dbs and __salt__['postgres.db_create']( name, tablespace=tablespace, encoding=encoding, lc_collate=lc_collate, lc_ctype=lc_ctype, owner=owner, template=template, **db_args) ): ret['comment'] = 'The database {0} has been created'.format(name) ret['changes'][name] = 'Present' elif ( name in dbs and __salt__['postgres.db_alter']( name, tablespace=tablespace, owner=owner, owner_recurse=owner_recurse, **db_args) ): ret['comment'] = ('Parameters for database {0} have been changed' ).format(name) ret['changes'][name] = 'Parameters changed' elif name in dbs: ret['comment'] = ('Failed to change parameters for database {0}' ).format(name) ret['result'] = False else: ret['comment'] = 'Failed to create database {0}'.format(name) ret['result'] = False
if nd_state[2] == 'M8[us]': new_state = np.ndarray.__reduce__(data.astype('M8[ns]')) np.ndarray.__setstate__(data, new_state[2])
vbox = vb_get_box() machine = vbox.findMachine(name) log.info("Stopping machine %s", name) session = _virtualboxManager.openMachineSession(machine) try: console = session.console progress = console.powerDown() progress.waitForCompletion(timeout) finally: _virtualboxManager.closeMachineSession(session) vb_wait_for_session_state(session) log.info("Stopped machine %s is now %s", name, vb_machinestate_to_str(machine.state)) return vb_xpcom_to_attribute_dict(machine, "IMachine")
ROUTING_KEY = getattr(settings, 'CREDENTIALS_GENERATION_ROUTING_KEY', None)
bnorm = beta1 n_params = len(bs)
resolvers = kwargs.pop('resolvers', None) kwargs['level'] = kwargs.pop('level', 0) + 1 if resolvers is None: index_resolvers = self._get_index_resolvers() resolvers = dict(self.iteritems()), index_resolvers if 'target' not in kwargs: kwargs['target'] = self kwargs['resolvers'] = kwargs.get('resolvers', ()) + resolvers return _eval(expr, inplace=inplace, **kwargs)
CourseGradingModel.delete_grace_period(self.course.id, self.user) altered_grader = CourseGradingModel.fetch(self.course.id) self.assertEqual(None, altered_grader.grace_period, "Delete grace period")
except ValueError as detail: raise except Exception as detail: result = handle_error()
df = read_csv('s3a://pandas-test/tips.csv', nrows=10) self.assertTrue(isinstance(df, DataFrame)) self.assertFalse(df.empty) tm.assert_frame_equal(read_csv( tm.get_data_path('tips.csv')).iloc[:10], df)
selector = '{} .acid-block {} .pass'.format(self.context_selector, test_selector) return bool(self.q(css=selector).results)
children = self.get_children() progresses = [child.get_progress() for child in children] progress = reduce(Progress.add_counts, progresses, None) return progress
return tile_slices_to_image( self._filters.get_value()[:,:,::-1,::-1].transpose(0,2,3,1), scale_each=scale_each, **kwargs)
action_name = ugettext_noop('reset') update_fcn = partial(reset_attempts_module_state, xmodule_instance_args) visit_fcn = partial(perform_module_state_update, update_fcn, None) return run_main_task(entry_id, visit_fcn, action_name)
kwargs = {} if cls.grading_policy is not None: kwargs['grading_policy'] = cls.grading_policy cls.course = CourseFactory.create(**kwargs)
problem = self.build_problem( answer=4, tolerance=0.2, credit_type='close' ) correct_responses = ["4", "4.1", "3.9"] incorrect_responses = ["1", "3", "4.5", "0", "-2"] partial_responses = ["4.3", "3.7"] self.assert_multiple_partial(problem, correct_responses, incorrect_responses, partial_responses)
import salt.ext.six as six
elif existing['code'] == 404:
self.select = []
self._collection.clear() self._uri_cache.clear()
coach_user = User.objects.create_user('test_coach_user', 'test_coach_user@openedx.org', 'test') CourseCcxCoachRole(self.master_course_key).add_users(coach_user)
return any( access_role.role == role and access_role.course_id == course_id and access_role.org == org for access_role in self._roles )
self.update_structure(destination_course, destination_structure) self._update_head(destination_course, index_entry, destination_course.branch, destination_structure['_id'])
self.setup_cohorts(self.course)
return result
post_data['username'] = 'dummy3' post_data['_addanother'] = 1 response = self.client.post(self.get_add_url(), data=post_data) self.assertEqual(response.status_code, 302) self.assertURLEqual( response.url, self.get_add_url() ) post_data.pop('_addanother')
DATE_INPUT_FORMATS = [
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
try: with transaction.atomic(using=self.db): obj = self.create(**params) return obj, True except IntegrityError: exc_info = sys.exc_info() try: return self.get(**lookup), False except self.model.DoesNotExist: pass six.reraise(*exc_info)
result = self.panel.swapaxes('items', 'items') assert_panel_equal(self.panel, result) self.assertNotEqual(id(self.panel), id(result))
obj = WithCustomPK(name=1, value=1) with self.assertRaises(DatabaseError): with transaction.atomic(): obj.save(force_update=True)
if include_parents is False: include_parents = PROXY_PARENTS return self._get_fields(include_parents=include_parents, include_hidden=include_hidden)
for newuser_id in range(len(self.student_list), user_id): username = "user_{i}".format(i=newuser_id) email = "user_{i}@example.edx.org".format(i=newuser_id) password = "12345" self._enroll_user(username, email, password) self.student_list.append({'email': email, 'password': password})
stub = get_request_or_stub() expected_url = "http://{site_name}/foobar".format(site_name=settings.SITE_NAME) self.assertEqual(stub.build_absolute_uri("foobar"), expected_url)
values = Series(['a_b_c', 'c_d_e', NA, 'f_g_h']) result = values.str.rsplit('_', n=1) exp = Series([['a_b', 'c'], ['c_d', 'e'], NA, ['f_g', 'h']]) tm.assert_series_equal(result, exp)
return self.score_samples(X).mean()
name_placeholder = _(u"Jane Doe")
resp = self.client.get(reverse('courseenrollments')) self.assertEqual(resp.status_code, status.HTTP_200_OK)
dev['hash'] = all_devices['hash'] log.info('Found device %s in Zenoss', device) return dev
short_description = "Not just anybody" CourseDetails.update_about_item( self.course, "short_description", short_description, ModuleStoreEnum.UserID.test, store ) self.reindex_course(store) response = self.searcher.search( doc_type=CourseAboutSearchIndexer.DISCOVERY_DOCUMENT_TYPE, field_dictionary={"course": unicode(self.course.id)} ) self.assertEqual(response["total"], 1) self.assertEqual(response["results"][0]["data"]["content"]["short_description"], short_description)
if self._is_single_block:
cs = GEOSCoordSeq(capi.create_cs(length, ndim), z=hasz) for i, c in enumerate(items): cs[i] = c
qda = QuadraticDiscriminantAnalysis(store_covariances=True) y_pred = qda.fit(X, y).predict(X) splot = plot_data(qda, X, y, y_pred, fig_index=2 * i + 2) plot_qda_cov(qda, splot) plt.axis('tight')
bad_messages = { 'Not Provided', '[{"IdReasons": ["Not provided"]}]', '{"IdReasons": ["Not provided"]}', u'[{"ïḋṚëäṡöṅṡ": ["Ⓝⓞⓣ ⓟⓡⓞⓥⓘⓓⓔⓓ "]}]', } for msg in bad_messages: attempt.error_msg = msg parsed_error_msg = attempt.parsed_error_msg() self.assertEquals(parsed_error_msg, "There was an error verifying your ID photos.")
self.Xt = numpy.zeros([self.n_eigen + self.minibatch_size, self.n_dim])
s = {'desc': 'KML', 'procedure_fmt': '%(geo_col)s,%(precision)s', 'procedure_args': {'precision': kwargs.pop('precision', 8)}, } return self._spatial_attribute('kml', s, **kwargs)
return - self.free_energy(X) - self.logZ
test_combined_expression = unittest.expectedFailure(test_combined_expression)
self.login_staff() response = self.get_course_info_page() self.assertEqual(response.status_code, 200) content = response.content self.assertIn("OOGIE BLOOGIE", content)
is_stanford_theming_enabled = settings.FEATURES.get("USE_CUSTOM_THEME", False) is_microsite = microsite.is_request_in_microsite() if is_stanford_theming_enabled and not is_microsite: return relative_path return microsite.get_template_path(default_path, **kwargs)
from salt.modules import nagios import os
import lettuce.django
self.q(css='input#allowance_value').fill(allowed_minutes) self.q(css='input#user_info').fill(username) self.q(css="input#addNewAllowance").click() self.wait_for_element_absence("div.modal div.modal-header", "Popup should be hidden") self.wait_for_ajax()
if getattr(xblock, 'is_draft', False): published_xblock_location = as_published(xblock.location) try: xblock.runtime.lookup_item(published_xblock_location) except ItemNotFoundError: return False return True
for kernel in kernels: gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y) assert_equal(gpr.log_marginal_likelihood(gpr.kernel_.theta), gpr.log_marginal_likelihood())
num_certs = GeneratedCertificate.eligible_certificates.filter(user=self.student).count() self.assertEqual(num_certs, 1)
from salttesting import skipIf from salttesting.helpers import destructiveTest, ensure_in_syspath ensure_in_syspath('../../')
ret['size'] = size.splitlines()[-1].split()[1]
import ioflo.base.deeding from ioflo.aid.odicting import odict
ticks = ax.get_xticks() labels = ax.get_xticklabels() for t, l in zip(ticks, labels): m, s = divmod(int(t), 60) h, m = divmod(m, 60) xp = l.get_text() if len(xp) > 0: rs = time(h, m, s).strftime('%H:%M:%S') self.assertEqual(xp, rs)
url = reverse('create_mode', args=[unicode(self.course.id)]) response = self.client.get(url)
rs = df.iloc[[0, 1]] xp = df.xs(4, drop_level=False) assert_frame_equal(rs, xp)
status_by_req = defaultdict(lambda: False) for status in CreditRequirementStatus.get_statuses(requirements, username): status_by_req[status.requirement.id] = status.status
y_upper = clf.predict(xx)
self.assertEqual(list(settings.STATICFILES_FINDERS), before_finders) self.assertEqual(settings.STATICFILES_DIRS[0], settings.REPO_ROOT / 'themes/red-theme/lms/static') self.assertEqual(settings.STATICFILES_DIRS[1:], before_dirs)
rdp.__salt__ = {}
try: res = FuncNode(node.func.id) except ValueError: raise
self.youtube_configuration.update({ 'youtube_api_blocked': True, })
pass
iris_multi = np.vstack((iris.target, iris.target, iris.target)).T clf3 = ForestClassifier(class_weight=[{0: 2., 1: 2., 2: 1.}, {0: 2., 1: 1., 2: 2.}, {0: 1., 1: 2., 2: 2.}], random_state=0) clf3.fit(iris.data, iris_multi) assert_almost_equal(clf2.feature_importances_, clf3.feature_importances_) clf4 = ForestClassifier(class_weight='balanced', random_state=0) clf4.fit(iris.data, iris_multi) assert_almost_equal(clf3.feature_importances_, clf4.feature_importances_)
df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': list('aabb'), 'C': [1, 2, 3, 4]})
assert_array_almost_equal(coef_dense, coef_sorted.toarray())
axes = df.plot.bar(linewidth=2, subplots=True) self._check_axes_shape(axes, axes_num=5, layout=(5, 1)) for ax in axes: for r in ax.patches: self.assertEqual(r.get_linewidth(), 2)
from __future__ import absolute_import
response = self.send_post( client=self.client, url=reverse('bookmarks'), data={'usage_id': 'invalid'}, expected_status=400 ) self.assertEqual(response.data['user_message'], u'An error has occurred. Please try again.')
response = self.assert_request_status_code(400, url, method="POST", data=data) self.assertIn("The sale associated with this invoice has already been invalidated.", response.content)
self._enable_cohorting() self._create_verified_cohort() self._enable_verified_track_cohorting() self._enroll_in_course() self._upgrade_to_verified() self.assertEqual(DEFAULT_VERIFIED_COHORT_NAME, get_cohort(self.user, self.course.id, assign=False).name)
pass
error_msg = _(u"You must agree to the {platform_name} {terms_of_service}.").format( platform_name=get_themed_value("PLATFORM_NAME", settings.PLATFORM_NAME), terms_of_service=terms_link )
self.assertEqual(len(res.context['object_list']), 7)
memory = Memory(os.path.join(get_data_home(), 'mnist_benchmark_data'), mmap_mode='r')
from __future__ import unicode_literals
response = self.client.post(self.url, { "email": self.EMAIL, "name": self.NAME, "username": self.USERNAME, "password": self.PASSWORD, "honor_code": "true", }) self.assertHttpOK(response)
is_movable = True
return staticfiles_storage.url(microsite.get_value('favicon_path', default))
if settings.ROOT_URLCONF == 'lms.urls': url_pattern = '/info' else: url_pattern = '/course/{}'.format(unicode(course_key))
expected_index = pd.Index(o[::-1]) expected_index.name = None
if not isinstance(self.storage, (Storage, LazyObject)): self.storage = self.storage() super(BaseStorageFinder, self).__init__(*args, **kwargs)
labels, uniques = algos.factorize(inds, sort=True)
xx, yy = np.meshgrid(np.linspace(-7, 7, 500), np.linspace(-7, 7, 500)) n_inliers = int((1. - outliers_fraction) * n_samples) n_outliers = int(outliers_fraction * n_samples) ground_truth = np.ones(n_samples, dtype=int) ground_truth[-n_outliers:] = -1
super(EditMembershipPage, self).__init__(browser, course_id) self.team = team self.url_path = "teams/#teams/{topic_id}/{team_id}/edit-team/manage-members".format( topic_id=self.team['topic_id'], team_id=self.team['id'] )
response = self.client.trace('/trace_view/') self.assertEqual(response.status_code, 200) self.assertEqual(response.context['method'], 'TRACE') self.assertEqual(response.templates[0].name, 'TRACE Template')
self._pydsl_all_decls = {}
(index < 2 or tokens[index - 2][1] != 'class') and not keyword.iskeyword(prev_text)): yield prev_end, "E211 whitespace before '%s'" % text
executor.migrate([ ("lookuperror_b", None), ("lookuperror_c", None) ]) self.assertTableNotExists("lookuperror_a_a1") self.assertTableNotExists("lookuperror_b_b1") self.assertTableNotExists("lookuperror_c_c1")
self._test_dir = tempfile.mkdtemp(prefix='salt-testdaemon-')
if os.path.isdir(tmp_dir): shutil.rmtree(tmp_dir) elif os.path.isfile(tmp_dir): os.remove(tmp_dir)
ans = commentable_id in course_cohort_settings.cohorted_discussions
mappings = mappings or {} self.store = MixedModuleStore( contentstore, create_modulestore_instance=create_modulestore_instance, mappings=mappings, **self.options ) self.addCleanup(self.store.close_all_connections)
if self.orig is not None: result = take_1d(result, self.orig.cat.codes)
X = np.array([[0, 0, 5], [0, 5, 0], [3, 0, 0], [0, 0, 6], [6, 0, 0]]) y = ["eggs", "spam", "ham", "eggs", "ham"] Y = np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0]])
idx = self.create_index()
return zlib.compress(pickle.dumps(data, pickle.HIGHEST_PROTOCOL))
api_rst_members = set() file_name = '../doc/source/api.rst' with open(file_name, 'r') as f: pattern = re.compile('({})\.(\w+)'.format('|'.join([cls.__name__ for cls in classes]))) for line in f: match = pattern.search(line) if match: api_rst_members.add(match.group(0))
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
return len(self.q(css='.wrapper-translations-settings .list-settings-item').results)
test_model = CourseMetadata.update_from_json( self.course, { "giturl": {"value": "http://example.com"}, }, user=self.user ) self.assertNotIn('giturl', test_model)
for case in [[2, 1, 2, 0], [2, 2, 1, 0], [0, 1, 2, 1], [-2, 0, 2], [2, 0, -2]]: indices = np.array(case, dtype=np.int64) maybe_slice = lib.maybe_indices_to_slice(indices, len(target)) self.assertFalse(isinstance(maybe_slice, slice)) self.assert_numpy_array_equal(maybe_slice, indices) self.assert_numpy_array_equal(target[indices], target[maybe_slice])
bob = User.objects.using('default').get(username='bob')
print "Couldn't uninstall unwanted Python packages!" return
if not isinstance(model_att, six.string_types): model_att = att
form = self._admin_form(course_mode)
feed_elem = doc.getElementsByTagName('rss') self.assertEqual(len(feed_elem), 1) feed = feed_elem[0] self.assertEqual(feed.getAttribute('version'), '2.0')
return None
library = LibraryFactory.create(modulestore=self.store) lib_key = library.location.library_key test_block = ItemFactory.create( category="vertical", parent_location=library.location, user_id=self.user.id, publish_item=False, ) test_block2 = ItemFactory.create( category="vertical", parent_location=library.location, user_id=self.user.id, publish_item=False ) unchanged_lib = LibraryFactory.create() unchanged_key = unchanged_lib.location.library_key test_block3 = ItemFactory.create( category="vertical", parent_location=unchanged_lib.location, user_id=self.user.id, publish_item=False ) test_block4 = ItemFactory.create( category="vertical", parent_location=unchanged_lib.location, user_id=self.user.id, publish_item=False ) library = self.store.get_library(lib_key) children = [self.store.get_item(child).url_name for child in library.children] self.assertEqual(len(children), 2) self.assertIn(test_block.url_name, children) self.assertIn(test_block2.url_name, children)
class Category(models.Model): name = models.CharField(max_length=50)
mongo_course1_id = self.import_and_populate_course()
cv = check_cv(self.cv, classifier=False)
if not is_cross_domain_request_allowed(request): log.debug("Could not disable CSRF middleware referer check for cross-domain request.") return with skip_cross_domain_referer_check(request): return super(CorsCSRFMiddleware, self).process_view(request, callback, callback_args, callback_kwargs)
c1 = Categorical(["a", "b", "c", "a"]) c2 = Categorical(Series(["a", "b", "c", "a"])) tm.assert_categorical_equal(c1, c2)
X = [[1, 0], [1, 0], [0, 1], [0, 1]] y = [0, 0, 1, 1] clf = self.factory(alpha=0.1, n_iter=1000, class_weight=None) clf.fit(X, y)
desc_parts = row[2].split(':') if desc_parts[0] != 'FindAssetTest': continue modulestore, amount_md = desc_parts[1:3] self.all_modulestores.add(modulestore) test_phase = 'all' sort = None if len(desc_parts) >= 4: test_phase = desc_parts[3] if len(desc_parts) >= 5: sort = desc_parts[4]
selector = self.get_element_selector(CSS_CLASS_NAMES['error_message']) return self.q(css=selector).visible
self.client.get('/fr/simple/') self.assertNotIn(LANGUAGE_SESSION_KEY, self.client.session)
for blkno, indexer in lib.get_blkno_indexers(blknos, group): yield blkno, BlockPlacement(indexer)
max_squared_sum = row_norms(X, squared=True).max()
try: from salt._compat import ElementTree as ET HAS_ELEMENT_TREE = True except ImportError: HAS_ELEMENT_TREE = False
indices = array.array('i') indptr = array.array('i', [0]) for labels in y: indices.extend(set(class_mapping[label] for label in labels)) indptr.append(len(indices)) data = np.ones(len(indices), dtype=int) return sp.csr_matrix((data, indices, indptr), shape=(len(indptr) - 1, len(class_mapping)))
raise NotImplementedError
return total_state
max_scores_cache.set(self.locations[0], 1) self.assertEqual(max_scores_cache.num_cached_updates(), 1)
import collections import json import logging from pkg_resources import resource_string
for n, f in l: df = tm.makeDataFrame() df[n] = f self.assertRaises( TypeError, store.append, 'df1_%s' % n, df)
if not request.data: return self.error_response(ugettext_noop(u'No data provided.'), DEFAULT_USER_MESSAGE) usage_id = request.data.get('usage_id', None) if not usage_id: return self.error_response(ugettext_noop(u'Parameter usage_id not provided.'), DEFAULT_USER_MESSAGE) try: usage_key = UsageKey.from_string(unquote_slashes(usage_id)) except InvalidKeyError: error_message = ugettext_noop(u'Invalid usage_id: {usage_id}.').format(usage_id=usage_id) log.error(error_message) return self.error_response(error_message, DEFAULT_USER_MESSAGE) try: bookmark = api.create_bookmark(user=self.request.user, usage_key=usage_key) except ItemNotFoundError: error_message = ugettext_noop(u'Block with usage_id: {usage_id} not found.').format(usage_id=usage_id) log.error(error_message) return self.error_response(error_message, DEFAULT_USER_MESSAGE) except BookmarksLimitReachedError: error_message = ugettext_noop( u'You can create up to {max_num_bookmarks_per_course} bookmarks.' u' You must remove some bookmarks before you can add new ones.' ).format(max_num_bookmarks_per_course=settings.MAX_BOOKMARKS_PER_COURSE) log.info( u'Attempted to create more than %s bookmarks', settings.MAX_BOOKMARKS_PER_COURSE ) return self.error_response(error_message) return Response(bookmark, status=status.HTTP_201_CREATED)
student_module = StudentModule.objects.get( course_id=self.course.id, student=self.student_user ) student_module.module_state_key = student_module.module_state_key.replace( name=student_module.module_state_key.name + "_fake" ) student_module.save()
import salt.utils
raise DeprecationWarning( '`salt.utils.cloud.deploy_script now only accepts ' 'dictionaries for it\'s `master_conf` parameter. ' 'Loading from YAML ...' )
self.make_course(pdf_textbooks=[HTML_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('html_book', book_index=0, chapter='xyzzy')
def _test_seq(df, idx_ser, col_ser): idx_eq = df.eq(idx_ser, axis=0) col_eq = df.eq(col_ser) idx_ne = df.ne(idx_ser, axis=0) col_ne = df.ne(col_ser) assert_frame_equal(col_eq, df == Series(col_ser)) assert_frame_equal(col_eq, -col_ne) assert_frame_equal(idx_eq, -idx_ne) assert_frame_equal(idx_eq, df.T.eq(idx_ser).T) assert_frame_equal(col_eq, df.eq(list(col_ser))) assert_frame_equal(idx_eq, df.eq(Series(idx_ser), axis=0)) assert_frame_equal(idx_eq, df.eq(list(idx_ser), axis=0))
for processor in get_template_context_processors(): context.update(processor(request))
target = capi.auto_create_warped_vrt( self._ptr, self.srs.wkt.encode(), target_srs.wkt.encode(), algorithm, max_error, c_void_p() ) target = GDALRaster(target)
if not data: data = json.dumps({})
result = Index([Timestamp('2011-01-01 10:00', tz='US/Eastern'), Timestamp('2011-08-01 10:00', tz='US/Eastern')], name='idx') exp = DatetimeIndex([Timestamp('2011-01-01 10:00'), Timestamp('2011-08-01 10:00')], tz='US/Eastern', name='idx') self.assert_index_equal(result, exp, exact=True) self.assertTrue(isinstance(result, DatetimeIndex)) self.assertIsNotNone(result.tz) self.assertEqual(result.tz, exp.tz)
from salt.utils import etcd_util try: from urllib3.exceptions import ReadTimeoutError, MaxRetryError HAS_URLLIB3 = True except ImportError: HAS_URLLIB3 = False
self.request.user = AnonymousUser() context = user_has_cart_context_processor(self.request) self.assertFalse(context['should_display_shopping_cart_func']())
assets, count = content_store.get_all_content_for_course(course.id) self.assertEqual(count, 2)
getkeys = self.ts.keys self.assertIs(getkeys(), self.ts.index)
'auth_tries': int,
if 'master_list' not in opts: opts['master_list'] = local_masters
self.assertEqual( self.run_function('xattr.delete', [NO_FILE, 'spongebob']), 'ERROR: File not found: {0}'.format(NO_FILE))
return False
_UPLOADING = ugettext_noop("Uploading") _IN_PROGRESS = ugettext_noop("In Progress") _COMPLETE = ugettext_noop("Ready") _FAILED = ugettext_noop("Failed") _INVALID_TOKEN = ugettext_noop("Invalid Token") _IMPORTED = ugettext_noop("Imported") _UNKNOWN = ugettext_noop("Unknown")
self.assertFalse(auth.user_has_role(self.user, CourseCreatorRole()))
s = Series([1, 3, 4, 1, 3, 4], index=MultiIndex.from_product([list( 'AB'), list(date_range('20130903', periods=3))])) result = s.xs('20130903', level=1) expected = Series([1, 1], index=list('AB')) assert_series_equal(result, expected)
if dtype == _NS_DTYPE or dtype == _TD_DTYPE: new_values = np.vstack([x.view('i8') for x in to_stack]) return new_values.view(dtype)
import datetime as dt from pandas import NaT
A = rng.random_sample([2, 2]) for dtype in ['f8', 'f4']: A = A.astype(dtype) B = B.astype(dtype)
raise ValueError("cannot set Categorical codes directly")
p.book_set.add(b) self.assertEqual(receiver._database, DEFAULT_DB_ALIAS) with self.override_router(): p.book_set.add(b) self.assertEqual(receiver._database, "other")
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
chapter = self._create_child(self.course, 'chapter', "Test Chapter") sequential = self._create_child(chapter, 'sequential', "Test Sequential") self._create_child(sequential, 'vertical', "Unit") self._create_child(sequential, 'vertical', "Locked Unit", staff_only=True) xblock_info = self._get_xblock_info(chapter.location) self._verify_visibility_state(xblock_info, VisibilityState.staff_only, self.FIRST_SUBSECTION_PATH, should_equal=False) self._verify_visibility_state(xblock_info, VisibilityState.staff_only, self.FIRST_UNIT_PATH, should_equal=False) self._verify_visibility_state(xblock_info, VisibilityState.staff_only, self.SECOND_UNIT_PATH)
def __init__(self, message='Test'): super(Unauthorized, self).__init__(message) self.msg = message
if IdentityPoolName is not None and IdentityPoolName != request_params.get('IdentityPoolName'): request_params['IdentityPoolName'] = IdentityPoolName
if '/' in name: cmd = [_pkg(jail, chroot, root), 'search'] else: cmd = [_pkg(jail, chroot, root), 'search', '-S', 'name', '-Q', 'version', '-e'] if quiet: cmd.append('-q') cmd.append(name)
for kernel in kernels: gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True) gpr.fit(X, y)
b.authors.clear() self.assertEqual(receiver._database, DEFAULT_DB_ALIAS) with self.override_router(): b.authors.clear() self.assertEqual(receiver._database, "other")
if cfg.versionfile_build: target_versionfile = os.path.join(self.build_lib, cfg.versionfile_build) print("UPDATING %s" % target_versionfile) write_to_version_file(target_versionfile, versions)
U, V = svd_flip(U, V)
result = g.agg(np.sum) assert_series_equal(result, expect)
DATE_FORMAT = 'j F Y' TIME_FORMAT = 'h:ia' DATETIME_FORMAT = 'j F Y h:ia' MONTH_DAY_FORMAT = 'j F' SHORT_DATE_FORMAT = 'j M Y' SHORT_DATETIME_FORMAT = 'j M Y h:ia'
remote_files = set(srv.listdir(path='.'))
X, y = load_svmlight_file(datafile) X = X.toarray() query_id = np.arange(X.shape[0]) // 2 f = BytesIO() dump_svmlight_file(X, y, f, query_id=query_id, zero_based=True)
pass
self.current_block.add(string, start, end, line)
expected = { "action": "unenroll", "auto_enroll": False, "results": [ { "identifier": self.notregistered_email, "before": { "enrollment": False, "auto_enroll": False, "user": False, "allowed": False, }, "after": { "enrollment": False, "auto_enroll": False, "user": False, "allowed": False, } } ] }
module_system = get_test_system()
if 'text/html' in accepts or '*/*' in accepts: cache_key = u"branding.footer.{params}.html".format( params=urllib.urlencode({ 'language': language, 'show_openedx_logo': show_openedx_logo, 'include_dependencies': include_dependencies, }) ) content = cache.get(cache_key) if content is None: with translation.override(language): content = _render_footer_html(request, show_openedx_logo, include_dependencies) cache.set(cache_key, content, settings.FOOTER_CACHE_TIMEOUT) return HttpResponse(content, status=200, content_type="text/html; charset=utf-8")
from django.db import models from django.utils.encoding import python_2_unicode_compatible @python_2_unicode_compatible class Donut(models.Model): name = models.CharField(max_length=100) is_frosted = models.BooleanField(default=False) has_sprinkles = models.NullBooleanField() baked_date = models.DateField(null=True) baked_time = models.TimeField(null=True) consumed_at = models.DateTimeField(null=True) review = models.TextField() class Meta: ordering = ('consumed_at',) def __str__(self): return self.name class RumBaba(models.Model): baked_date = models.DateField(auto_now_add=True) baked_timestamp = models.DateTimeField(auto_now_add=True)
self.client.login(username=self.staff.username, password='test') self.create_programs_config() response = self._assert_status(200) self.assertIn("Program Administration", response.content)
self._validate(is_numeric, batch)
Coupon.objects.create( code=code, description='testing code', course_id=course_key, percentage_discount=self.percentage_discount, created_by=self.user, is_active=is_active )
self.course = CourseFactory.create( user_partitions=[self.partition], grading_policy={ "GRADER": [{ "type": "Homework", "min_count": 1, "drop_count": 0, "short_label": "HW", "weight": 1.0 }] } ) chapter = ItemFactory.create(parent_location=self.course.location, display_name='Chapter')
self.q(css="input.no_special_exam").first.click()
result = func( Series([1.]), Series([1.]), 50, min_periods=min_periods) tm.assert_series_equal(result, Series([np.NaN]))
assert False
grp = Group(name=settings.PAYMENT_REPORT_GENERATOR_GROUP) grp.save() self.user.groups.add(grp) self.assertTrue(_can_download_report(self.user))
response = self.client.post(self.url, { "email": "invalid@example.com", "password": self.PASSWORD, }) self.assertHttpForbidden(response)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("SaltRaetRoadStackSetup") self.addEnterDeed("StatsMasterTestSetup") act = self.addRecurDeed("SaltRaetStatsEventerMaster")
return self.feature_names_
if estimator_error <= 0: return sample_weight, 1., 0.
random_state = check_random_state(0) X, _ = make_blobs(n_features=3, random_state=random_state) kl_divergences = [] for n_iter in [200, 250, 300]: tsne = TSNE(n_components=2, perplexity=10, learning_rate=100.0, n_iter=n_iter, random_state=0) tsne.fit_transform(X) kl_divergences.append(tsne.kl_divergence_) assert_less_equal(kl_divergences[1], kl_divergences[0]) assert_less_equal(kl_divergences[2], kl_divergences[1])
if img_provider: tgt = 'provider: {0}'.format(img_provider) if dalias == img_provider: data = get_provider(img_provider) matched = True if not data and 'profile' not in __opts__ and arg_providers: for name in arg_providers: tgt = 'provider: {0}'.format(name) if dalias == name: data = get_provider(name) if data: matched = True break elif 'profile' in __opts__: curprof = __opts__['profile'] profs = __opts__['profiles'] tgt = 'profile: {0}'.format(curprof) if ( curprof in profs and profs[curprof]['provider'] == __active_provider_name__ ): prov, cdriver = profs[curprof]['provider'].split(':') tgt += ' provider: {0}'.format(prov) data = get_provider(prov) matched = True if ( (__opts__.get('destroy', False) and not data) or ( not matched and __active_provider_name__ ) ): data = __opts__.get('providers', {}).get(dalias, {}).get(driver, {}) if data: ret = _salt('test.ping', salt_target=data['target']) if not ret: raise SaltCloudSystemExit( 'Configured provider {0} minion: {1} is unreachable'.format( __active_provider_name__, data['target'])) return data return False
ret_code = win_cmd( 'winexe {0} "sc query winexesvc"'.format(creds), logging_command=logging_creds ) if ret_code == 0: log.debug('winexe connected...') return True log.debug('Return code was {0}'.format(ret_code)) time.sleep(1)
grains['virtual_subtype'] = 'Xen PV DomU'
if include_self: query = X._fit_X else: query = None return query
self.frame['E'] = 7. self.frame.values[6] = 6 self.assertTrue((self.frame.values[6] == 6).all())
item = self.create_item(user_id, parent_usage_key.course_key, block_type, block_id=block_id, fields=fields, **kwargs) parent = self.get_item(parent_usage_key) parent.children.append(item.location) self.update_item(parent, user_id)
available_apps = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'view_tests', ]
self.assertEqual(len(site_configuration_history), 1)
from salt.ext.six import string_types import salt.utils import salt.ext.six as six
self.assertRaises(ValueError, store.put, 'b', df, format='fixed', complib='zlib')
skip_if_no_data() mode = get_default_mode() if hasattr(mode, 'check_py_code'): old_value = mode.check_py_code mode.check_py_code = False try: if config.mode == "DEBUG_MODE": yaml_file = 'mnist_fast' else: yaml_file = 'mnist' limited_epoch_train(os.path.join(yaml_file_path, '%s.yaml' % yaml_file)) try: os.remove(os.path.join(save_path, '%s.pkl' % yaml_file)) os.remove(os.path.join(save_path, '%s_best.pkl' % yaml_file)) except Exception: pass finally: if hasattr(mode, 'check_py_code'): mode.check_py_code = old_value
assert_index_equal(pd.Index([1, 2, 3], name=np.nan), pd.Index([1, 2, 3], name=np.nan)) assert_index_equal(pd.Index([1, 2, 3], name=pd.NaT), pd.Index([1, 2, 3], name=pd.NaT))
file.save(file.name, file, save=False)
newcmap = CorrectMap() for responder in self.responders.values(): if 'filesubmission' in responder.allowed_inputfields and student_answers is None: _ = self.capa_system.i18n.ugettext raise Exception(_(u"Cannot rescore problems with possible file submissions"))
xblock_info = create_xblock_info( xblock, data=data, metadata=own_metadata(xblock), include_ancestor_info=include_ancestor_info ) if include_publishing_info: add_container_page_publishing_info(xblock, xblock_info) return xblock_info
from __future__ import unicode_literals
try: import salt.utils.etcd_util HAS_LIBS = True except ImportError: HAS_LIBS = False
if pkgs: pkgs = [dict([(x, y)]) for x, y in six.iteritems(targets)] pkgs.extend([dict([(x, y)]) for x, y in six.iteritems(to_reinstall)]) elif sources: oldsources = sources sources = [x for x in oldsources if next(iter(list(x.keys()))) in targets] sources.extend([x for x in oldsources if next(iter(list(x.keys()))) in to_reinstall])
raise ValueError('cannot insert %s, already exists' % item)
self.assertIsNone(subq._result_cache)
clf.fit(docs_train, y_train)
return http.HttpResponseBadRequest(template.render())
self.course_outline_page.visit() self.course_outline_page.toggle_expand_collapse() self.assertEquals(self.course_outline_page.expand_collapse_link_state, ExpandCollapseLinkState.EXPAND) self.course_outline_page.section_at(0).expand_subsection() self.course_outline_page.toggle_expand_collapse() self.assertEquals(self.course_outline_page.expand_collapse_link_state, ExpandCollapseLinkState.COLLAPSE) self.verify_all_sections(collapsed=False)
mako_middleware_process_request(request)
'msg': the_input.submitted_msg, 'mode': self.mode, 'rows': self.rows, 'cols': self.cols, 'queue_msg': 'message', 'linenumbers': 'true', 'hidden': '', 'tabsize': int(self.tabsize), 'button_enabled': True, 'queue_len': '3', 'matlab_editor_js': '/dummy-static/js/vendor/CodeMirror/octave.js',
self.set_team_configuration({u"max_team_size": 10, u"topics": self.create_topics(13)}) self.topics_page.visit() self.topics_page.press_next_page_button() self.assertEqual(len(self.topics_page.topic_cards), 1) self.assertTrue(self.topics_page.get_pagination_header_text().startswith('Showing 13-13 out of 13 total')) self.topics_page.press_previous_page_button() self.assertEqual(len(self.topics_page.topic_cards), TOPICS_PER_PAGE) self.assertTrue(self.topics_page.get_pagination_header_text().startswith('Showing 1-12 out of 13 total'))
assert source == '' return None
int2byte = operator.methodcaller("to_bytes", 1, "big")
qs = self.get_queryset() if queryset is None else queryset
best_candidate = None best_pot = None best_dist_sq = None for trial in range(n_local_trials): new_dist_sq = np.minimum(closest_dist_sq, distance_to_candidates[trial]) new_pot = new_dist_sq.sum()
from lms.envs.dev import (WIKI_ENABLED)
try: os.mkdir(repo_dir) except OSError: pass self.addCleanup(shutil.rmtree, repo_dir) git_import.add_repo(self.TEST_REPO, repo_dir / 'edx4edx_lite', None) subprocess.check_output(['git', 'checkout', 'HEAD~2', ], stderr=subprocess.STDOUT, cwd=repo_dir / 'edx4edx_lite') with self.assertRaises(GitImportErrorCannotPull): git_import.add_repo(self.TEST_REPO, repo_dir / 'edx4edx_lite', None)
face = sp.misc.imresize(face, 0.10) / 255.
pipeline = preprocessing.Pipeline()
ed, created = book.authors.get_or_create(name="Ed") self.assertTrue(created) self.assertEqual(book.authors.count(), 1)
private_ip_assignment = get_private_ip(vm_) if private_ip_assignment: create_private_ip(node_id)
destroy(vm_['name'])
with self.branch_setting(ModuleStoreEnum.Branch.draft_preferred, item.id): super(SplitMongoModuleStore, self).create_course( org, course, run, user_id, runtime=item.runtime, **kwargs )
if isinstance(block, XModuleDescriptor):
for invalid in [lambda x: x.astype(pd.Categorical), lambda x: x.astype('object').astype(pd.Categorical)]: self.assertRaises(TypeError, lambda: invalid(s))
layer_trainers = [] layer_trainers.append(get_layer_trainer_sgd_rbm(layers[0], trainset[0])) layer_trainers.append(get_layer_trainer_sgd_autoencoder(layers[1], trainset[1])) layer_trainers.append(get_layer_trainer_sgd_autoencoder(layers[2], trainset[2])) layer_trainers.append(get_layer_trainer_logistic(layers[3], trainset[3]))
if self.algorithm not in ('SAMME', 'SAMME.R'): raise ValueError("algorithm %s is not supported" % self.algorithm)
obj_sm = win32com.client.Dispatch('Microsoft.Update.ServiceManager')
_ret = [] for row in _response['rows']: _ret.append(row['key']) return _ret
from salt.modules import debian_ip
sfn, source_sum, comments = get_managed( name, template, source, source_hash, user, group, mode, saltenv, context, defaults, skip_verify, **kwargs) if comments: __clean_tmp(sfn) return False, comments
if 'arg' not in locals(): arg = command ret['error'] = 'Invalid formatted command, ' \ 'see debug log for details: {0}'.format(arg) return ret
if y_test is None: score = scorer(estimator, X_test) else: score = scorer(estimator, X_test, y_test) if not isinstance(score, numbers.Number): raise ValueError("scoring must return a number, got %s (%s) instead." % (str(score), type(score))) return score
if isinstance(prefix_sep, compat.string_types): prefix_sep = cycle([prefix_sep]) elif isinstance(prefix_sep, dict): prefix_sep = [prefix_sep[col] for col in columns_to_encode]
message = 'foo: bar="baz", qux="quux"' self.assertTrue(mock_log.info.called_with(message))
s = Series([datetime(2010, 1, 1), datetime(2, 1, 1), np.nan]) self.assertTrue(s.dtype == 'object') self.assertTrue(s[2] is np.nan) self.assertTrue('NaN' in str(s))
index = period_range(start='1999-01', periods=5, freq='M') s1 = Series(np.random.rand(len(index)), index=index) s2 = Series(np.random.rand(len(index)), index=index) series = [('s1', s1), ('s2', s2)] df = DataFrame.from_items(series) grouped = df.groupby(df.index.month) list(grouped)
r = parse(self.io, parser=parser)
if self.fn is None: self.fn = self.function("perform") return self.fn(X)
continue
get_request_dict = {} result = module.reset_problem(get_request_dict)
if self._selection is None: slice_axis = self.obj.columns else: slice_axis = self._selection_list slicer = lambda x: self.obj[x]
top_words_content = sorted( content['top_words'], key=itemgetter('text') ) top_words_correct = sorted( correct_jsons[username]['top_words'], key=itemgetter('text') ) self.assertListEqual(top_words_content, top_words_correct)
if 0 in ndim and 1 not in ndim: values = np.array([b.values for b in blocks]) if len(values) == 1: return values.item() blocks = [make_block(values, ndim=1)] axes = Index([ax[0] for ax in axes])
assert np.all((ps == 0) + (ps == 1)) assert np.all((hs == 0) + (hs == 1))
self.assertEqual(self.func(None), [])
addr1 = ipaddress.IPv4Address('1.2.3.37') self.assertIn(addr1, self.ipv4_network) self.assertFalse(ipaddress.IPv4Network('1.1.0.0/16').__contains__( ipaddress.IPv4Network('1.0.0.0/15')))
raise NotImplementedError("__eq__ not implemented in class %s." % type(self))
mlb = MultiLabelBinarizer(classes=[1, 3, 2]) assert_array_equal(mlb.fit_transform(inp), indicator_mat) assert_array_equal(mlb.classes_, [1, 3, 2])
self.assertEqual(len(actual), 2) self.assertEqual(actual, expected)
([[10, 10, 8], [9, 8, 1], [9, 7, 4]], 18 ),
import salt.utils import salt.utils.fsutils from salt.exceptions import CommandExecutionError
super(Convex, self).__init__('convex', which_set, split)
ref_hash = '9vk1mfq8jx0c8e0386z6' h1 = City.objects.annotate(geohash=functions.GeoHash('point')).get(name='Houston') h2 = City.objects.annotate(geohash=functions.GeoHash('point', precision=5)).get(name='Houston') self.assertEqual(ref_hash, h1.geohash) self.assertEqual(ref_hash[:5], h2.geohash)
return ret
res = s.fillna(True) tm.assert_series_equal(res, pd.Series([1.1, 1.0, 3.3, 4.4])) self.assertEqual(res.dtype, np.float64)
self.sigma = 0
import salt.utils.cloud import salt.config as config from salt.exceptions import SaltCloudSystemExit
CELERY_ALWAYS_EAGER = True
empty = ParameterGrid({}) assert_equal(len(empty), 1) assert_equal(list(empty), [{}]) assert_grid_iter_equals_getitem(empty) assert_raises(IndexError, lambda: empty[1])
urlpatterns += ( url(r'^404$', handler404), url(r'^500$', handler500), )
with self.assertRaises(exceptions.TemplateRuntimeError): env.from_string('{{ document|load_json }}').render(document={"foo": "it works"})
tok = tok.replace("\\", "U")
levels[0].name = lev.name expected.index.set_levels(levels, inplace=True) expected['aux_date'] = to_datetime(expected['aux_date'], dayfirst=True) expected['aux_date'] = lmap(Timestamp, expected['aux_date']) tm.assertIsInstance(expected['aux_date'][0], datetime)
X, y = make_regression_with_outliers() huber = HuberRegressor(fit_intercept=True, alpha=0.01, max_iter=100) huber.fit(X, y) linear_loss = np.dot(X, huber.coef_) + huber.intercept_ - y mask = np.abs(linear_loss) < huber.epsilon * huber.scale_ huber_score = huber.score(X[mask], y[mask]) huber_outlier_score = huber.score(X[~mask], y[~mask])
return True
id_ = self.attrs.get('id') output = [] for i, choice in enumerate(self.choices): choice_value, choice_label = choice if isinstance(choice_label, (tuple, list)): attrs_plus = self.attrs.copy() if id_: attrs_plus['id'] += '_{}'.format(i) sub_ul_renderer = self.__class__( name=self.name, value=self.value, attrs=attrs_plus, choices=choice_label, ) sub_ul_renderer.choice_input_class = self.choice_input_class output.append(format_html( self.inner_html, choice_value=choice_value, sub_widgets=sub_ul_renderer.render(), )) else: w = self.choice_input_class(self.name, self.value, self.attrs.copy(), choice, i) output.append(format_html(self.inner_html, choice_value=force_text(w), sub_widgets='')) return format_html( self.outer_html, id_attr=format_html(' id="{}"', id_) if id_ else '', content=mark_safe('\n'.join(output)), )
result = TimedeltaIndex([np.timedelta64(0, 'ns'), np.timedelta64( 10, 's').astype('m8[ns]')]) expected = to_timedelta([0, 10], unit='s') tm.assert_index_equal(result, expected)
df['float_3'] = Series([1.] * len(df), dtype='float64') self.assertRaises(ValueError, store.append, 'df', df)
self.assertTrue(res_json['success'])
method = 'lasso'
response = other_view(request, '15') self.assertEqual(response.content, b'Hello World 15')
next_cart = Order.get_cart_for_user(user=self.user) self.assertNotEqual(cart, next_cart) self.assertEqual(next_cart.status, 'cart')
try: from salt._compat import ElementTree as ET HAS_ELEMENT_TREE = True except ImportError: HAS_ELEMENT_TREE = False
self.assert_grade(problem, 'choice_0', 'incorrect') self.assert_grade(problem, 'choice_1', 'correct') self.assert_grade(problem, 'choice_2', 'incorrect')
from __future__ import absolute_import import logging
if self.verbose > 0: print("Bound after updating %8s: %f" % (n, self.lower_bound(X, z))) if end: print("Cluster proportions:", self.gamma_.T[1]) print("covariance_type:", self.covariance_type)
tscopy = self.tsframe.copy() tscopy['weekday'] = [x.weekday() for x in tscopy.index] stragged = tscopy.groupby('weekday').aggregate(np.mean) assert_frame_equal(stragged, aggregated, check_names=False)
s = Series(index=np.array([None])) expected = Series(index=Index([None])) assert_series_equal(s, expected)
return self.children()
_expiration_datetime = models.DateTimeField( default=None, null=True, blank=True, verbose_name=_(u"Upgrade Deadline"), help_text=_( u"OPTIONAL: After this date/time, users will no longer be able to enroll in this mode. " u"Leave this blank if users can enroll in this mode until enrollment closes for the course." ), db_column='expiration_datetime', )
self.q(css='button.signatory-panel-save').click() self.mode = 'details' self.wait_for_ajax() self.wait_for_signatory_detail_view()
w = np.zeros(n_features + 1) loss_interp, grad_interp = _logistic_loss_and_grad( w, X, y, alpha=1. ) assert_array_almost_equal(loss, loss_interp)
Child.objects.create(name="c1", value="foo", related=self.s1) obj = Child.objects.only("name").get(name="c1") self.assert_delayed(obj, 3) self.assertEqual(obj.name, "c1") self.assertEqual(obj.value, "foo")
df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]}) expected = df.copy() expected = expected[expected['a'] == 2] df.query('a == 2', inplace=True) assert_frame_equal(expected, df)
html=True
pass
axes = [ax for row in axes for ax in row] min_x = min([ax.get_xlim()[0] for ax in axes]) max_x = max([ax.get_xlim()[1] for ax in axes]) min_y = min([ax.get_ylim()[0] for ax in axes]) max_y = max([ax.get_ylim()[1] for ax in axes]) [ax.set_xlim(min_x, max_x) for ax in axes] [ax.set_ylim(min_y, max_y) for ax in axes] for index, axis in enumerate(axes): if index % trellis.cols == 0: pass else: axis.get_yaxis().set_ticks([]) axis.set_ylabel('') if index / trellis.cols == trellis.rows - 1: pass else: axis.get_xaxis().set_ticks([]) axis.set_xlabel('') if trellis.by[0] == '.': label1 = "%s = %s" % (trellis.by[1], trellis.group_grid[ index // trellis.cols][index % trellis.cols]) label2 = None elif trellis.by[1] == '.': label1 = "%s = %s" % (trellis.by[0], trellis.group_grid[ index // trellis.cols][index % trellis.cols]) label2 = None else: label1 = "%s = %s" % ( trellis.by[0], trellis.group_grid[index // trellis.cols] [index % trellis.cols][0]) label2 = "%s = %s" % ( trellis.by[1], trellis.group_grid[index // trellis.cols] [index % trellis.cols][1]) if label2 is not None: axis.table(cellText=[[label1], [label2]], loc='top', cellLoc='center', cellColours=[['lightgrey'], ['lightgrey']]) else: axis.table(cellText=[[label1]], loc='top', cellLoc='center', cellColours=[['lightgrey']]) layers = [layer for row in layers for layer in row] legend = {} for layer in layers: legend = dictionary_union(legend, layer.legend) patches = [] labels = [] if len(list(legend.keys())) == 0: key_function = lambda tup: tup elif len(list(legend.keys())[0]) == 2: key_function = lambda tup: (tup[1]) else: key_function = lambda tup: (tup[1], tup[3]) for key in sorted(list(legend.keys()), key=key_function): value = legend[key] patches.append(value) if len(key) == 2: col, val = key labels.append("%s" % str(val)) elif len(key) == 4: col1, val1, col2, val2 = key labels.append("%s, %s" % (str(val1), str(val2))) else: raise ValueError( "Maximum 2 categorical attributes to display a lengend of") if len(legend): fig.legend(patches, labels, loc='upper right') fig.subplots_adjust(wspace=0.05, hspace=0.2)
errstring = "migrate_to_split requires at least two arguments" with self.assertRaisesRegexp(CommandError, errstring): self.command.handle()
pillar_mock = MagicMock(return_value=pillar_value) filestate.__salt__['pillar.get'] = pillar_mock
self.assertEqual(self.import_page.header_text, 'Course Import')
raise WorkerInterrupt()
user = User.objects.get(username='testclient') data = { 'old_password': 'password', 'new_password1': 'abc123', 'new_password2': 'abc123', } form = PasswordChangeForm(user, data) self.assertTrue(form.is_valid()) form.save(commit=False) self.assertEqual(password_changed.call_count, 0) form.save() self.assertEqual(password_changed.call_count, 1)
add_master_course_staff_to_ccx(self.course, self.ccx_locator, self.ccx.display_name) self.assertEqual(len(outbox), len(list_staff_master_course) + len(list_instructor_master_course)) with ccx_course(self.ccx_locator) as course_ccx: list_staff_ccx_course = list_with_level(course_ccx, 'staff') list_instructor_ccx_course = list_with_level(course_ccx, 'instructor') self.assertEqual(len(list_staff_master_course), len(list_staff_ccx_course)) for user in list_staff_master_course: self.assertIn(user, list_staff_ccx_course) self.assertEqual(len(list_instructor_master_course), len(list_instructor_ccx_course)) for user in list_instructor_master_course: self.assertIn(user, list_instructor_ccx_course)
from pandas.core.algorithms import factorize return factorize(self, sort=sort, na_sentinel=na_sentinel)
if hasattr(clf, "decision_function"): Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) else: Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
providers_string = _("{first_providers}, and {last_provider}").format( first_providers=u", ".join(providers[:-1]), last_provider=providers[-1] )
req_kwargs = {}
df.iloc[:10].to_hdf(path, 'df', append=False, format='table') df.iloc[10:].to_hdf(path, 'df', append=True) assert_frame_equal(read_hdf(path, 'df'), df)
formatted = [pprint_thing(na if isnull(x) else x, escape_chars=('\t', '\r', '\n')) for x in algos.take_1d(lev._values, lab)]
self.vert1 = ItemFactory.create( parent_location=self.seq1.location, category='vertical', display_name='untitled vertical 1' )
if content and content[-1] == '\n': content = content[:-1]
raise AbstractMethodError(self)
filters_values = numpy.ones( (32, 2, 2, 16), dtype=theano.config.floatX ) filters = sharedX(filters_values) image = numpy.random.rand(32, 3, 3, 1).astype(theano.config.floatX) conv2d = Conv2D(filters) f = theano.function([self.image_tensor], conv2d.lmul(self.image_tensor)) assert f(image).shape == (16, 2, 2, 1)
if self._version != other._version: raise TypeError('%s and %s are not of the same type' % ( self, other)) if self.network_address < other.network_address: return -1 if self.network_address > other.network_address: return 1 if self.netmask < other.netmask: return -1 if self.netmask > other.netmask: return 1 return 0
upload_csv_to_report_store(rows, 'proctored_exam_results_report', course_id, start_date)
os.write(fh_, str(os.getpid()))
idx = self.create_index() idx.name = 'foo' self.assertTrue("'foo'" in str(idx)) self.assertTrue(idx.__class__.__name__ in str(idx))
self._load_city_data() ref_ewkt = ( 'SRID=4326;MULTIPOINT(-123.305196 48.462611 15,-104.609252 38.255001 1433,' '-97.521157 34.464642 380,-96.801611 32.782057 147,-95.363151 29.763374 18,' '-95.23506 38.971823 251,-87.650175 41.850385 181,174.783117 -41.315268 14)' ) ref_union = GEOSGeometry(ref_ewkt) union = City3D.objects.aggregate(Union('point'))['point__union'] self.assertTrue(union.hasz) self.assertSetEqual({p.ewkt for p in ref_union}, {p.ewkt for p in union})
for extension in self.extensions: extension.on_save(self.model, self.dataset, self.algorithm) if self.save_path is not None: with log_timing(log, 'Saving to ' + self.save_path): if self.first_save and (not self.allow_overwrite) \ and os.path.exists(self.save_path): raise IOError("Trying to overwrite file when not allowed.") try: self.dataset._serialization_guard = SerializationGuard() serial.save(self.save_path, self.model, on_overwrite='backup') finally: self.dataset._serialization_guard = None self.first_save = False
return self._nsorted(columns, n, 'nlargest', keep)
writer = MigrationWriter(new_migration) with open(writer.path, "wb") as fh: fh.write(writer.as_string())
if not isinstance(config, dict): return False, ('Configuration for service beacon must be a dictionary.') return True, 'Valid beacon configuration'
err_string = "Invalid course_key: '{0}'".format(invalid_key) with self.assertRaisesRegexp(CommandError, err_string): call_command('reindex_course', invalid_key)
self.meta = 'category' self.set_metadata(block.values.categories)
with open(report_file) as f: violations_list = f.readlines() num_lines = len(violations_list) return num_lines, violations_list
res = df.query('index < b', engine=engine, parser=parser) expec = df[df.index < df.b] assert_frame_equal(res, expec)
elif existing['code'] == 404: ret['result'] = True ret['comment'] = 'This virtual already does not exist. No changes made.' ret['changes']['old'] = {} ret['changes']['new'] = {} else: ret = _load_result(existing, ret)
self._setup_gating_milestone(50) mock_module_score.return_value = module_score evaluate_prerequisite(self.course, self.prob1.location, self.user.id) self.assertEqual(milestones_api.user_has_milestone(self.user_dict, self.prereq_milestone), result)
if __rules != __saved_rules: out = __salt__['iptables.save'](filename, family=family) ret['comment'] += ('\nSaved iptables rule {0} for {1}\n' '{2}\n{3}').format(name, family, command.strip(), out)
username = email.split("@")[0] try: email = people[username]['email'] except KeyError: pass
todo = {node: (dependencies - current) for node, dependencies in todo.items() if node not in current}
if 'detached' not in published._class_tags and published.start is not None: return datetime.now(UTC) > published.start
self.assertEqual(Article.objects.get(id__exact=self.a.id), self.a) self.assertEqual(Article.objects.get(headline__startswith='Swallow'), self.a) self.assertEqual(Article.objects.get(pub_date__year=2005), self.a) self.assertEqual(Article.objects.get(pub_date__year=2005, pub_date__month=7), self.a) self.assertEqual(Article.objects.get(pub_date__year=2005, pub_date__month=7, pub_date__day=28), self.a) self.assertEqual(Article.objects.get(pub_date__week_day=5), self.a)
self.mock_programs_api() self.mock_credentials_api(self.user, reset_url=False) actual = get_programs_credentials(self.user) expected = self.expected_credentials_display_data()
from __future__ import absolute_import
self.assertTrue( re.match( 'ALTER ROLE "test_username" WITH INHERIT NOCREATEDB ' 'CREATEROLE NOREPLICATION LOGIN NOPASSWORD;' ' GRANT "test_groups" TO "test_username"', postgres._run_psql.call_args[0][0][13] ) )
exp = pd.Series(self.rep[to_key], index=index, name='yyy', dtype=from_key)
if (value is not None and value == value and method is None and limit is None): self._default_fill_value = value
assert_array_almost_equal(clf_lda_svd.explained_variance_ratio_, clf_lda_eigen.explained_variance_ratio_[:tested_length])
with warnings.catch_warnings(record=True) as recorded: warnings.simplefilter('ignore') class Deprecated(six.with_metaclass(RenameManagerMethods)): def old(self): pass class Renamed(Deprecated): def new(self): super(Renamed, self).new() warnings.simplefilter('always') renamed = Renamed() renamed.new() self.assertEqual(len(recorded), 0) renamed.old() self.assertEqual(len(recorded), 1) msg = str(recorded.pop().message) self.assertEqual(msg, '`Renamed.old` is deprecated, use `new` instead.')
from salt.cloud.clouds import vmware from salt.exceptions import SaltCloudSystemExit
FILE_UPLOAD_TEMP_DIR = None
df = DataFrame(np.arange(25.).reshape(5, 5), index=['a', 'b', 'c', 'd', 'e'], columns=['A', 'B', 'C', 'D', 'E']) z = df[['A', 'C', 'A']].copy() expected = z.ix[['a', 'c', 'a']]
backend_op = connection.ops.gis_operators[self.lookup_name] if hasattr(backend_op, 'check_relate_argument'): backend_op.check_relate_argument(value[1]) else: pattern = value[1] if not isinstance(pattern, six.string_types) or not self.pattern_regex.match(pattern): raise ValueError('Invalid intersection matrix pattern "%s".' % pattern) return super(RelateLookup, self).get_db_prep_lookup(value, connection)
self.assertRaises(ValueError, index.truncate, 3, 1)
klist = list(kwargs.keys()) klist.sort() qbuffer.extend([[k, kwargs[k]] for k in klist])
return list(key)
if self.connection.settings_dict['TEST']['NAME']: return self.connection.settings_dict['TEST']['NAME'] return TEST_DATABASE_PREFIX + self.connection.settings_dict['NAME']
top_level_export_dir = 'exported_source_course' export_course_to_xml( modulestore, contentstore, source_course_key, export_dir, top_level_export_dir, ) import_course_from_xml( modulestore, 'test_user', export_dir, source_dirs=[top_level_export_dir], static_content_store=contentstore, target_id=source_course_key, create_if_not_present=True, raise_on_failure=True, )
store = self._get_modulestore_for_courselike(asset_key.course_key) return store.find_asset_metadata(asset_key, **kwargs)
_maybe_remove(store, 'df_tz') store.append('df_tz', df, data_columns=['A', 'B']) result = store['df_tz'] self._compare_with_tz(result, df) assert_frame_equal(result, df)
#epub_use_index = True
rtable = self.data.pivot_table(columns=['AA', 'BB'], margins=True, aggfunc=np.mean) tm.assertIsInstance(rtable, Series)
for f in [lambda x: x.expanding().cov(pairwise=False), lambda x: x.expanding().corr(pairwise=False), lambda x: x.rolling(window=3).cov(pairwise=False), lambda x: x.rolling(window=3).corr(pairwise=False), lambda x: x.ewm(com=3).cov(pairwise=False), lambda x: x.ewm(com=3).corr(pairwise=False), ]: results = [f(df) for df in df1s] for (df, result) in zip(df1s, results): tm.assert_index_equal(result.index, df.index) tm.assert_index_equal(result.columns, df.columns) for i, result in enumerate(results): if i > 0: self.assert_numpy_array_equal(result.values, results[0].values)
import dson import logging
api.add_enrollment(self.USERNAME, self.COURSE_ID, mode='audit')
parser = argparse.ArgumentParser() parser.add_argument("metric", help="the desired metric", choices=metrics.keys()) parser.add_argument("dataset", help="the dataset used for computing the " + "metric", choices=datasets.keys()) parser.add_argument("model_path", help="path to the pickled DBM model") args = parser.parse_args()
return self._asides
BULK_EMAIL_EMAILS_PER_TASK = 100
idx = mapping if isinstance(flat, (tuple, list)): assert 0 <= idx < len(flat) return flat[idx] else: assert idx == 0 return flat
try: return self.maps[0].pop(key, *args) except KeyError: raise KeyError('Key not found in the first mapping: {!r}' .format(key))
if not isinstance(config, list): return False, ('Configuration for load beacon must be a list.') else: for config_item in config: if not isinstance(config_item, dict): return False, ('Configuration for load beacon must ' 'be a list of dictionaries.') else: if not all(j in ['1m', '5m', '15m'] for j in config_item.keys()): return False, ('Configuration for load beacon must ' 'contain 1m, 5m or 15m items.')
self.base_loader = base_loader
x = np.array(x) y = np.array(y) plt.title('Classification accuracy as a function of %s' % x_legend) plt.xlabel('%s' % x_legend) plt.ylabel('Accuracy') plt.grid(True) plt.plot(x, y)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
blah = DataFrame(np.empty([0, 1]), columns=['A'], index=DatetimeIndex([]))
self.assertTrue(not "\t" in pp_t("a\tb", escape_chars=("\t", )))
if node is None: node = file_handle.getNode('/', 'Data') data_size = data_x.shape[0] last = np.floor(data_size / float(batch_size)) * batch_size for i in xrange(0, data_size, batch_size): stop = (i + np.mod(data_size, batch_size) if i >= last else i + batch_size) assert len(range(start + i, start + stop)) == len(range(i, stop)) assert (start + stop) <= (node.X.shape[0]) node.X[start + i: start + stop, :] = data_x[i:stop, :] if data_y is not None: node.y[start + i: start + stop, :] = data_y[i:stop, :] file_handle.flush()
Y = np.array([e.predict_proba(X)[:, 1] for e in self.estimators_]).T
arr = np.array( [[[1, 2, 3], [0, 0, 0]], [[0, 0, 0], [0, 0, 0]]], dtype=np.float64)
generate_cython()
if i == 0 and not is_writable(npath(base_path + '.mo')): self.stderr.write("The po files under %s are in a seemingly not writable location. " "mo files will not be updated/created." % dirpath) return
def setUp(self): super(TransformerRegistryTestMixin, self).setUp() self.patcher = patch( 'openedx.core.lib.block_structure.transformer_registry.TransformerRegistry.get_registered_transformers' ) mock_registry = self.patcher.start() mock_registry.return_value = {self.TRANSFORMER_CLASS_TO_TEST} self.transformers = BlockStructureTransformers([self.TRANSFORMER_CLASS_TO_TEST()]) def tearDown(self): self.patcher.stop()
attempt = 0 while attempt < max_attempts: try: return func() except ignored_exceptions: world.wait(1) attempt += 1 assert_true(attempt < max_attempts, 'Ran out of attempts to execute {}'.format(func))
if kw_overrides_match is _marker: return profile_match return kw_overrides_match
data = '{"test": "json"}' response = self.client.put('/request_methods/', data=data, content_type='application/json') self.assertEqual(response.status_code, 200) self.assertEqual(response.content, b'request method: PUT')
self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course.id)) course_mode, is_active = CourseEnrollment.enrollment_mode_for_user(self.user, self.course.id) self.assertTrue(is_active) self.assertEqual(course_mode, CourseMode.DEFAULT_MODE_SLUG)
plt.clf() plt.plot(recall[0], precision[0], lw=lw, color='navy', label='Precision-Recall curve') plt.xlabel('Recall') plt.ylabel('Precision') plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1.0]) plt.title('Precision-Recall example: AUC={0:0.2f}'.format(average_precision[0])) plt.legend(loc="lower left") plt.show()
_check_plot_works(df.hist, sharex=True, sharey=True)
if target: if grains.get('os_family', '') == 'Arch': for idx in range(13): if idx == 12: raise Exception('Package database locked after 60 seconds, ' 'bailing out') if not os.path.isfile('/var/lib/pacman/db.lck'): break time.sleep(5)
batch_size = 100 model.set_batch_size(batch_size) m = dataset.X.shape[0] extra = batch_size - m % batch_size assert (m + extra) % batch_size == 0 import numpy as np if extra > 0: dataset.X = np.concatenate((dataset.X, np.zeros((extra, dataset.X.shape[1]), dtype=dataset.X.dtype)), axis=0) assert dataset.X.shape[0] % batch_size == 0
first_checkpoint.add_verification_attempt(SoftwareSecurePhotoVerification.objects.create(user=self.user)) self.assertEqual(first_checkpoint.photo_verification.count(), 1)
if not isinstance(new_index, splib.SparseIndex): raise TypeError('new index must be a SparseIndex') block = self.block.sparse_reindex(new_index) new_data = SingleBlockManager(block, self.index) return self._constructor(new_data, index=self.index, sparse_index=new_index, fill_value=self.fill_value).__finalize__(self)
recipient_selector_css = "input[name='send_to'][value='{}']".format(recipient) self.q(css=self._bounded_selector(recipient_selector_css))[0].click()
if self.digest is not None: password = binascii.hexlify(self.digest(force_bytes(password)).digest()) else: password = force_bytes(password)
self.initiate_rerun()
linkedin_config = LinkedInAddToProfileConfiguration.current() linkedin_share_enabled = share_settings.get('CERTIFICATE_LINKEDIN', linkedin_config.enabled) if linkedin_share_enabled: context['linked_in_url'] = linkedin_config.add_to_profile_url( course.id, course.display_name, user_certificate.mode, smart_str(share_url) )
raise
defaults = {'max_length': self.max_length, 'widget': forms.Textarea} defaults.update(kwargs) return super(TextField, self).formfield(**defaults)
dates = date_range('01-Jan-2014', '05-Jan-2014', freq='D') series = Series(1, index=dates)
query = "SELECT * FROM raw_query_author WHERE first_name = %(first)s" author = Author.objects.all()[2] params = {'first': author.first_name} qset = Author.objects.raw(query, params=params) results = list(qset) self.assertProcessed(Author, results, [author]) self.assertNoAnnotations(results) self.assertEqual(len(results), 1) self.assertIsInstance(repr(qset), str)
response = super(Client, self).head(path, data=data, secure=secure, **extra) if follow: response = self._handle_redirects(response, **extra) return response
with self.assertRaises(ResponseError): problem.grade_answers({'1_2_1': '42'})
if candidates.shape[0] < n_neighbors: warnings.warn( "Number of candidates is not sufficient to retrieve" " %i neighbors with" " min_hash_match = %i. Candidates are filled up" " uniformly from unselected" " indices." % (n_neighbors, self.min_hash_match)) remaining = np.setdiff1d(np.arange(0, index_size), candidates) to_fill = n_neighbors - candidates.shape[0] candidates = np.concatenate((candidates, remaining[:to_fill]))
xblocks = page.xblocks blocks_checked = set() for expected_ordering in expected_orderings: for xblock in xblocks: parent = expected_ordering.keys()[0] if xblock.name == parent: blocks_checked.add(parent) children = xblock.children expected_length = len(expected_ordering.get(parent)) test_class.assertEqual( expected_length, len(children), "Number of children incorrect for group {0}. Expected {1} but got {2}.".format(parent, expected_length, len(children))) for idx, expected in enumerate(expected_ordering.get(parent)): test_class.assertEqual(expected, children[idx].name) blocks_checked.add(expected) break test_class.assertEqual(len(blocks_checked), len(xblocks))
num_choices = len(self.descriptor.get_children())
__virtualname__ = 'gnome'
if not file and not force: return
list_maintenance_windows = salt.utils.alias_function(list_windows, 'list_maintenance_windows')
sys.exit(3)
formset = self.NormalFormset(self.data) self.assertTrue(formset.is_valid()) self.assertEqual(len(formset.save()), 4)
result = sparse.loc[sparse % 2 == 1] exp = orig.loc[orig % 2 == 1].to_sparse() tm.assert_sp_series_equal(result, exp)
self.addPresenceInfo('alloweds', 'alpha', '1.1.1.1', '1234') self.addPresenceInfo('alloweds', 'beta', '1.2.3.4', '1234') testStack = self.store.fetch('.salt.test.lane.stack').value presenceReq = self.store.fetch('.salt.presence.event_req').value ryn = 'manor' msg = {'route': {'dst': (None, ryn, 'presence_req'), 'src': (None, testStack.local.name, None)}, 'data': {'state': 'joined'}} presenceReq.append(msg)
lines = [x for x in lines if not re.search("^\w+\s*\(.+\)\s*#",x)] hits = set(map(lambda x: x.split(" ")[0],lines)) cs.update(set([Hit(commit=c,path=f) for c in hits]))
grainsmod.__grains__ = {'a': 'aval', 'b': 'l1', 'c': 8} res = grainsmod.set('b,l3', 'val3', delimiter=',') self.assertFalse(res['result']) self.assertEqual(res['comment'], 'The key \'b\' value is \'l1\', which is ' + 'different from the provided key \'l3\'. ' + 'Use \'force=True\' to overwrite.') self.assertEqual(grainsmod.__grains__, {'a': 'aval', 'b': 'l1', 'c': 8})
self.input_space = VectorSpace(dim=self.nvis) self.input_source = 'features' self.latent_space = VectorSpace(dim=self.nhid)
sl = self.series[10:20] sl[:] = 0 self.assertTrue((self.series[10:20] == 0).all())
return np.sqrt(np.diag(self._var_beta_raw))
base_location = os.path.join(HERE, 'namespace_package_base') other_location = os.path.join(HERE, 'namespace_package_other_base') app_path = os.path.join(base_location, 'nsapp')
from __future__ import absolute_import
self.course_fixture._update_xblock(self.course_fixture._course_location, { "metadata": { u"user_partitions": [ create_user_partition_json(0, "Name", "Description.", groups), ], }, })
ext_geom = Geometry(clob.read(), srid) gtype = str(ext_geom.geom_type) if gtype == 'Polygon': shell = ext_geom.shell ll, ur = shell[0][:2], shell[2][:2] elif gtype == 'Point': ll = ext_geom.coords[:2] ur = ll else: raise Exception('Unexpected geometry type returned for extent: %s' % gtype) xmin, ymin = ll xmax, ymax = ur return (xmin, ymin, xmax, ymax)
if not _any_pandas_objects(terms): return _result_type_many(*term_values), None
has_real_datatype = False supports_subqueries_in_group_by = True supports_bitwise_or = True
try: import raven from raven.handlers.logging import SentryHandler HAS_RAVEN = True except ImportError: HAS_RAVEN = False
from pylearn2.datasets import vector_spaces_dataset
return '{}[data-locator="{}"] {}'.format( self.BODY_SELECTOR, self.locator, selector )
if self.per_second: seconds = channels['training_seconds_this_epoch'].val_record seconds = np.array(seconds) seconds = seconds.cumsum() x = seconds[x]
sql.to_sql(frame=df_if_exists_1, con=self.conn, name=table_name, flavor='sqlite', if_exists='replace', index=False) self.assertEqual(sql.tquery(sql_select, con=self.conn), [(1, 'A'), (2, 'B')]) sql.to_sql(frame=df_if_exists_2, con=self.conn, name=table_name, flavor='sqlite', if_exists='replace', index=False) self.assertEqual(sql.tquery(sql_select, con=self.conn), [(3, 'C'), (4, 'D'), (5, 'E')]) clean_up(table_name)
clf = MultinomialNB().fit(X_train, y_train, sample_weight=sw_train) prob_pos_clf = clf.predict_proba(X_test)[:, 1]
if not settings.FEATURES.get('ENABLE_TEAMS'): filtered_list.append('teams_configuration')
methods = ['sem', 'var', 'std'] df1 = DataFrame(np.random.randn(5, 3), columns=['foo', 'bar', 'baz']) df1.ix[0, 'foo'] = '100'
for pn_key, pn_val in run_out['virgo-dummy'].items(): if pn_key == 'source_rpm': continue self.assertEqual(installed['virgo-dummy'][pn_key], pn_val)
self.submit_student_answer('u1', problem_url_name, [OPTION_1, OPTION_1]) self.submit_student_answer('u2', problem_url_name, [OPTION_1, OPTION_2]) self.submit_student_answer('u3', problem_url_name, [OPTION_2, OPTION_1]) self.submit_student_answer('u4', problem_url_name, [OPTION_2, OPTION_2])
for shape_elem, axis in safe_zip(self.shape, (0, 1, 'c')): if topo_array.shape[self.axes.index(axis)] != shape_elem: raise ValueError( "topo_array's %s axis has a different size " "(%d) from the corresponding size (%d) in " "self.shape.\n" " self.shape: %s (uses standard axis order: 0, 1, " "'c')\n" " self.axes: %s\n" " topo_array.shape: %s (should be in self.axes' order)") topo_array_bc01 = topo_array.transpose([self.axes.index(ax) for ax in ('b', 'c', 0, 1)]) return topo_array_bc01.reshape((topo_array_bc01.shape[0], np.prod(topo_array_bc01.shape[1:])))
response = self._get_page(payment_flow, course.id) data = self._get_page_data(response) self.assertEqual(data['verification_deadline'], deadline.strftime("%b %d, %Y at %H:%M UTC"))
partitioned_fields = self.partition_xblock_fields_by_scope(xblock) new_def_data = self._serialize_fields(xblock.category, partitioned_fields[Scope.content]) is_updated = False if xblock.definition_locator is None or isinstance(xblock.definition_locator.definition_id, LocalId): xblock.definition_locator = self.create_definition_from_data( course_key, new_def_data, xblock.category, user_id ) is_updated = True elif new_def_data: xblock.definition_locator, is_updated = self.update_definition_from_data( course_key, xblock.definition_locator, new_def_data, user_id )
self.store_builders = store_builders self.mappings = mappings or {} self.mixed_modulestore = None
def test_explicit_ForeignKey(self): Package.objects.create() screening = Screening.objects.create(movie=self.movie) Package.objects.create(screening=screening)
from __future__ import absolute_import
if salt.utils.is_windows(): return __virtualname__ return (False, 'Module cyg: module only works on Windows systems.')
orig_key, orig_version = self.store.get_block_original_usage(usage_key) return { "usage_key": unicode(usage_key), "original_usage_key": unicode(orig_key) if orig_key else None, "original_usage_version": unicode(orig_version) if orig_version else None, }
dr = date_range('2014', '2015', freq='M') self.assertEqual(dr[0], datetime(2014, 1, 31)) self.assertEqual(dr[-1], datetime(2014, 12, 31))
'Content-Type': 'application/x-www-form-urlencoded',
x = sp.arange(6).reshape(2, 3) y = sp.array([1, -1]) z = sp.arange(12).reshape(4, 3)
if sp.issparse(coef): coef.data.flags.writeable = False else: coef.flags.writeable = False return coef
BaseLoader = getattr(yaml, 'CSafeLoader', yaml.SafeLoader) BaseDumper = yaml.SafeDumper if six.PY3 else getattr(yaml, 'CSafeDumper', yaml.SafeDumper)
return self.q(css='.components-list li>a').text
cursor = self.connection._cursor() try: cursor.execute('SELECT %s' % func) row = cursor.fetchone() finally: cursor.close() return row[0]
with self.settings(PIPELINE_ENABLED=True): js_include = compressed_js('base_application') self.assertIn(u'lms-base-application.js', js_include)
t = Series(date_range('20130101', periods=1000, tz='US/Eastern')) self.assertTrue('datetime64[ns, US/Eastern]' in str(t))
store.put('c', df[:10], format='table', append=False) tm.assert_frame_equal(df[:10], store['c'])
course = _get_and_validate_course(course_key_string, request.user) if not course: return HttpResponseNotFound() if request.method == "GET": if "application/json" in request.META.get("HTTP_ACCEPT", ""): return videos_index_json(course) else: return videos_index_html(course) else: return videos_post(course, request)
if send_email: response = self._change_password(email=self.OLD_EMAIL) self.assertEqual(response.status_code, 200) else: response = self._change_password() self.assertEqual(response.status_code, 400)
if not self.args: self.args.insert(0, '*') if len(self.args) < 2: self.args.insert(1, 'sys.doc') if self.args[1] != 'sys.doc': self.args.insert(1, 'sys.doc') if len(self.args) > 3: self.error('You can only get documentation for one method at one time.')
if not name: raise ValueError(u"{key} must contain at least one underscore".format(key=key))
return self._folds[0][-1].get_output_space()
return self.q(css='button.start-timed-exam[data-start-immediately="false"]').is_present()
class ChildModel1Inline(admin.TabularInline): model = ChildModel1
if not s: return
key = Timestamp(key, tz=tz)
return self.backend_name == pipeline['backend']
f = DecimalField(localize=True) self.assertWidgetRendersTo(f, '<input id="id_f" name="f" type="text" required />')
self.page.add_user_to_course(self.other_user.get('email')) self._assert_user_present(self.other_user, present=True) current = self.page.get_user(self.user.get('email')) self.assertFalse(current.can_demote) self.assertFalse(current.can_delete) self.assertIn("Promote another member to Admin to remove your admin rights", current.no_change_warning_text) other = self.page.get_user(self.other_user.get('email')) other.click_promote() self._refresh_page() other = self.page.get_user(self.other_user.get('email')) self._assert_is_admin(other) current = self.page.get_user(self.user.get('email')) self.assertTrue(current.can_demote) self.assertTrue(current.can_delete) current.click_delete() self.log_in(self.user) self._assert_current_course(visible=False)
dynamic_string_fields = ( 'accept_language', 'agent', 'host', 'ip', 'event', 'session' ) for field in dynamic_string_fields: self.assert_field_type(load_video_event, field, basestring) self.assertIn(field, load_video_event, '{0} not found in the root of the event'.format(field)) del load_video_event[field]
cache.clear()
return get_instructions(xmltree)
safe_exec("a = int(math.pi)", g, cache=DictCache(cache)) self.assertEqual(g['a'], 3) self.assertEqual(cache.values()[0], (None, {'a': 3}))
DIRECT_ONLY_CATEGORIES = ['course', 'chapter', 'sequential', 'about', 'static_tab', 'course_info']
self.make_course(pdf_textbooks=[PDF_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('pdf_book', book_index=0, page='xyzzy')
VerificationCheckpoint.objects.create(course_id=self.course.id, checkpoint_location=self.checkpoint_midterm)
self.oauth_page.confirm() self.oauth_page.wait_for_element_absence( 'input[name=authorize]', 'Authorization button is not present' )
pipe = Pipeline([('cls', LinearRegression())])
from django.db import models class UnimportantThing(models.Model): importance = models.IntegerField() def get_absolute_url(self): return '/importance/%d/' % (self.importance,)
request.grant_type = grant_type request.user = user
return self.q(css="div.problem div.problem-hint").text[0]
U = rng.randn(self.dim, self.dim) U, _ = scipy.linalg.qr(U)
if cword == 1: print(' '.join(sorted(filter(lambda x: x.startswith(curr), subcommands)))) elif cwords[0] in subcommands and cwords[0] != 'help': subcommand_cls = self.fetch_command(cwords[0]) if cwords[0] in ('dumpdata', 'sqlmigrate', 'sqlsequencereset', 'test'): try: app_configs = apps.get_app_configs() options.extend((app_config.label, 0) for app_config in app_configs) except ImportError: pass parser = subcommand_cls.create_parser('', cwords[0]) options.extend( (sorted(s_opt.option_strings)[0], s_opt.nargs != 0) for s_opt in parser._actions if s_opt.option_strings ) prev_opts = [x.split('=')[0] for x in cwords[1:cword - 1]] options = [opt for opt in options if opt[0] not in prev_opts]
data = io.read() self.book = xlrd.open_workbook(file_contents=data)
try: conn = self.databases[alias] except KeyError: raise ConnectionDoesNotExist("The connection %s doesn't exist" % alias) test_settings = conn.setdefault('TEST', {}) for key in ['CHARSET', 'COLLATION', 'NAME', 'MIRROR']: test_settings.setdefault(key, None)
if isinstance(library, (basestring, LibraryLocator)): lib_key = library else: lib_key = library.location.library_key response = self.client.get(reverse_library_url('library_handler', unicode(lib_key))) self.assertIn(response.status_code, (200, 302, 403)) return response.status_code == 200
css_selectors = [ '.outline-subsection .expand-collapse', '.outline-subsection .button-new' ] for selector in css_selectors: world.css_click(selector) world.wait_for_mathjax() world.wait_for_xmodule() world.wait_for_loading() assert world.is_css_present('ul.new-component-type')
def __init__( self ): super(StringEnd,self).__init__() self.errmsg = "Expected end of text" #self.myException.msg = self.errmsg def parseImpl( self, instring, loc, doActions=True ): if loc < len(instring): #~ raise ParseException( instring, loc, "Expected end of text" ) exc = self.myException exc.loc = loc exc.pstr = instring raise exc elif loc == len(instring): return loc+1, [] elif loc > len(instring): return loc, [] else: exc = self.myException exc.loc = loc exc.pstr = instring raise exc
p = Person.objects.get(pk=self.person.pk) self.assertEqual('Reinhardt', p.name)
ishfts = 1 self.mode = mode self.iparam[0] = ishfts self.iparam[2] = maxiter self.iparam[3] = 1 self.iparam[6] = mode
return [this_dir, config.pthreads.inc_dir] if config.pthreads.inc_dir else [this_dir]
return_data = sqlcipher.SQLCipherExtPillar() args, kwargs = [], { '1': 'SELECT blah', '2': '', '3': 'SELECT blah2' } qbuffer = return_data.extract_queries(args, kwargs) self.assertEqual([ ['1', {'query': 'SELECT blah', 'depth': 0, 'as_list': False, 'with_lists': None, 'ignore_null': False}], ['3', {'query': 'SELECT blah2', 'depth': 0, 'as_list': False, 'with_lists': None, 'ignore_null': False}] ], qbuffer)
if self.advertised_start: return self.advertised_start elif self.start != DEFAULT_START_DATE: return defaultfilters.date(self.start, "DATE_FORMAT") else: return None
above_cutoff = (abs(s) > cond * np.max(abs(s))) psigma_diag = np.zeros_like(s) psigma_diag[above_cutoff] = 1.0 / s[above_cutoff]
return library.location.library_key
#print 'actual x',actual_x #print 'A:' #print A #print 'b:' #print b #print 'c:' #print c x.set_value(actual_x) minimizer._compute_grad(A,b,c) x_grad = minimizer.param_to_grad_shared[x] actual_grad = x_grad.get_value() correct_grad = 0.5 * np.dot(A,x.get_value())+ 0.5 * \ np.dot(A.T, x.get_value()) +b if not np.allclose(actual_grad, correct_grad): print('gradient was wrong at convergence point') print('actual grad: ') print(actual_grad) print('correct grad: ') print(correct_grad) print('max difference: ', end='') np.abs(actual_grad-correct_grad).max() assert False minimizer._normalize_grad() d = minimizer.param_to_grad_shared[x].get_value() step_len = ( np.dot(b,d) + 0.5 * np.dot(d,np.dot(A,actual_x)) \ + 0.5 * np.dot(actual_x,np.dot(A,d)) ) \ / np.dot(d, np.dot(A,d)) g = np.dot(A,actual_x)+b deriv = np.dot(g,d) print('directional deriv at actual', deriv) print('optimal step_len', step_len) optimal_x = actual_x - d * step_len g = np.dot(A,optimal_x) + b deriv = np.dot(g,d) print('directional deriv at optimal: ',deriv) x.set_value(optimal_x) print('obj at optimal: ',minimizer.obj(A,b,c)) print('eigenvalue range:') val, vec = np.linalg.eig(A) print((val.min(),val.max())) print('condition number: ',(val.max()/val.min())) assert False
pass
pass
optima = [self._constrained_optimization(obj_func, self.kernel_.theta, self.kernel_.bounds)]
for do in ['Hour', 'Minute', 'Second', 'Day', 'Micro', 'Milli', 'Nano']: op = getattr(pd.offsets, do) s + op(5) op(5) + s
response = salt.utils.vmware.esxcli(host, username, password, cmd, protocol=protocol, port=port) if response['retcode'] != 0: ret.update({host: {'Error': response.get('stdout')}}) else: stdout = _format_coredump_stdout(response) ret.update({host: {'Coredump Config': stdout}})
assert_greater(ridge_outlier_score, huber_outlier_score)
if not settings.FEATURES.get('ENABLE_EXPORT_GIT'): filtered_list.append('giturl')
headers = {"date": "Fri, 09 Nov 2001 01:08:47 -0000", "Message-ID": "foo"} email = EmailMessage('subject', 'content', 'from@example.com', ['to@example.com'], headers=headers) self.assertMessageHasHeaders(email.message(), { ('Content-Transfer-Encoding', '7bit'), ('Content-Type', 'text/plain; charset="utf-8"'), ('From', 'from@example.com'), ('MIME-Version', '1.0'), ('Message-ID', 'foo'), ('Subject', 'subject'), ('To', 'to@example.com'), ('date', 'Fri, 09 Nov 2001 01:08:47 -0000'), })
partitions = [] for partition_id, group_ids in merged_access.items(): try:
ci = CategoricalIndex(np.random.randint(0, 5, size=100)) if PY3: str(ci) else: compat.text_type(ci)
daily = Series(1, index=date_range('2014-01-01', periods=3, freq='D')) check_format_of_first_point(daily.plot(), 't = 2014-01-01 y = 1.000000') tm.close()
runlevels = ['l1'] level_list_mock = MagicMock(return_value=self.__services({service_name: runlevels})) with patch.dict(gentoo_service.__salt__, {'cmd.run': level_list_mock}): with patch.dict(gentoo_service.__salt__, {'cmd.retcode': rc_update_mock}): self.assertFalse(gentoo_service.enable('name', runlevels='l2')) rc_update_mock.assert_called_once_with('rc-update delete name l1', python_shell=False) rc_update_mock.reset_mock()
keys = [randint(0, 11, m), choice( list('abcdefghijk'), m), choice( pd.date_range('20141009', periods=11).tolist(), m), choice( list('ZYXWVUTSRQP'), m)] keys = list(map(tuple, zip(*keys))) keys += list(map(lambda t: t[:-1], vals[::n // m]))
import_course_from_xml(self.store, self.user.id, TEST_DATA_DIR, ['toy'], target_id=target_id) course_module = self.store.get_course(target_id) self.assertEquals(course_module.wiki_slug, 'MITx.111.2013_Spring')
with tm.assertRaisesRegexp( IndexError, 'single positional indexer is out-of-bounds'): s.iloc[30] self.assertRaises(IndexError, lambda: s.iloc[-30])
s = Series([u'あ', u'いい', u'ううう', u'ええええ'], index=[u'ああ', u'いいいい', u'う', u'えええ'], name=u'おおおおおおお') expected = (u"ああ あ\nいいいい いい\nう ううう\n" u"えええ ええええ\nName: おおおおおおお, dtype: object") self.assertEqual(_rep(s), expected)
required_boto3_version = '1.2.1'
youtube_response = requests.get('http://' + youtube_text_api['url'], params=transcripts_param) if youtube_response.status_code == 200 and youtube_response.text: youtube_data = etree.fromstring(youtube_response.content, parser=utf8_parser) for element in youtube_data: if element.tag == 'track' and element.get('lang_code', '') == lang: return element.get('name') return None
if val == val and val != %(nan_val)s: nobs[lab, j] += 1 if nobs[lab, j] == rank: resx[lab, j] = val
course_mode, __ = self.create_mode('honor', 'honor') _listen_for_course_publish('store', self.course.id) course_mode.refresh_from_db() self.assertIsNone(course_mode.expiration_datetime)
pass
def f(): Categorical([1, 2], [1, 2, 2])
context = get_ccx_creation_dict(course) messages.error(request, context['use_ccx_con_error_message']) return render_to_response('ccx/coach_dashboard.html', context)
user = User.objects.db_manager('other').create_user(username='joe', password='qwerty') self.assertTrue(user.check_password('qwerty')) out = six.StringIO() call_command('changepassword', username='joe', database='other', stdout=out) command_output = out.getvalue().strip() self.assertEqual( command_output, "Changing password for user 'joe'\nPassword changed successfully for user 'joe'" ) self.assertTrue(User.objects.using('other').get(username="joe").check_password('not qwerty'))
delayed_grade_func = lambda: self._send_grade_response( callback_url, xqueue_header, self.post_dict['xqueue_body'] )
user = User.objects.get(username=username) world.scenario_dict['USER'] = user
all_classes = np.array([0, 1]) positive_class = 'acq'
context_fields_to_remove = set(CONTEXT_FIELDS_TO_INCLUDE) context_fields_to_remove.add('client_id') for field in context_fields_to_remove: if field in context: del context[field]
PIPELINE_UGLIFYJS_BINARY = 'node_modules/.bin/uglifyjs'
log.debug(' Response content: {0}'.format(response))
connectivity = grid_to_graph(n_x=size, n_y=size) ward = FeatureAgglomeration(n_clusters=10, connectivity=connectivity, memory=mem) clf = Pipeline([('ward', ward), ('ridge', ridge)]) clf = GridSearchCV(clf, {'ward__n_clusters': [10, 20, 30]}, n_jobs=1, cv=cv)
for n, b36 in [(0, '0'), (1, '1'), (42, '16'), (818469960, 'django')]: self.assertEqual(http.int_to_base36(n), b36) self.assertEqual(http.base36_to_int(b36), n)
containers = [__salt__['dockerng.inspect_container'](c)['Id'] for c in containers] networks = __salt__['dockerng.networks'](names=[name]) if networks:
idx = pd.timedelta_range('1 day', '31 day', freq='D', name='idx') result = idx.drop_duplicates() self.assert_index_equal(idx, result) self.assertEqual(idx.freq, result.freq)
return output_scores
new_dashboard = copy.deepcopy(_new_dashboard) old_dashboard = copy.deepcopy(_old_dashboard) dashboard_diff = DictDiffer(new_dashboard, old_dashboard) diff['dashboard'] = _stripped({ 'changed': list(dashboard_diff.changed()) or None, 'added': list(dashboard_diff.added()) or None, 'removed': list(dashboard_diff.removed()) or None, })
return self.cleaned_data.get("page") or 1
raise NotImplementedError
return self.test_func
return self.filter_by(data, "id", note_id)
widget = DateTimeInput( format='%d/%m/%Y %H:%M', attrs={'type': 'datetime'}, ) d = datetime(2007, 9, 17, 12, 51, 34, 482548) self.check_html(widget, 'date', d, html='<input type="datetime" name="date" value="17/09/2007 12:51" />')
pass
self.track_selection_page.enroll('verified')
mock_response = mock.Mock() mock_response.status_code = 500 mock_post.return_value = mock_response with self.assertRaises(ccxconapi.CCXConnServerError): ccxconapi.course_info_to_ccxcon(self.course_key)
indices, indptr, spmat_shape, sptype, outshp = \ convolution_indices.conv_eval(imgshp, maxpoolshp, maxpoolshp, mode='valid')
lhs_sql, params = self.process_lhs(compiler, connection, self.lhs.lhs) rhs_sql, rhs_params = self.process_rhs(compiler, connection) rhs_sql = self.get_rhs_op(connection, rhs_sql) start, finish = self.year_lookup_bounds(connection, rhs_params[0]) params.append(self.get_bound(start, finish)) return '%s %s' % (lhs_sql, rhs_sql), params
name = models.CharField(max_length=50) mugshot = TestImageField(storage=temp_storage, upload_to='tests', height_field='mugshot_height', width_field='mugshot_width') mugshot_height = models.PositiveSmallIntegerField() mugshot_width = models.PositiveSmallIntegerField()
ABOUT_INFORMATION_TO_INCLUDE = [ AboutInfo("advertised_start", AboutInfo.PROPERTY, AboutInfo.FROM_COURSE_PROPERTY), AboutInfo("announcement", AboutInfo.PROPERTY, AboutInfo.FROM_ABOUT_INFO), AboutInfo("start", AboutInfo.PROPERTY, AboutInfo.FROM_COURSE_PROPERTY), AboutInfo("end", AboutInfo.PROPERTY, AboutInfo.FROM_COURSE_PROPERTY), AboutInfo("effort", AboutInfo.PROPERTY, AboutInfo.FROM_ABOUT_INFO), AboutInfo("display_name", AboutInfo.ANALYSE, AboutInfo.FROM_COURSE_PROPERTY), AboutInfo("overview", AboutInfo.ANALYSE, AboutInfo.FROM_ABOUT_INFO), AboutInfo("title", AboutInfo.ANALYSE | AboutInfo.PROPERTY, AboutInfo.FROM_ABOUT_INFO), AboutInfo("university", AboutInfo.ANALYSE | AboutInfo.PROPERTY, AboutInfo.FROM_ABOUT_INFO), AboutInfo("number", AboutInfo.ANALYSE | AboutInfo.PROPERTY, AboutInfo.FROM_COURSE_PROPERTY), AboutInfo("short_description", AboutInfo.ANALYSE, AboutInfo.FROM_ABOUT_INFO), AboutInfo("description", AboutInfo.ANALYSE, AboutInfo.FROM_ABOUT_INFO), AboutInfo("key_dates", AboutInfo.ANALYSE, AboutInfo.FROM_ABOUT_INFO), AboutInfo("video", AboutInfo.ANALYSE, AboutInfo.FROM_ABOUT_INFO), AboutInfo("course_staff_short", AboutInfo.ANALYSE, AboutInfo.FROM_ABOUT_INFO), AboutInfo("course_staff_extended", AboutInfo.ANALYSE, AboutInfo.FROM_ABOUT_INFO), AboutInfo("requirements", AboutInfo.ANALYSE, AboutInfo.FROM_ABOUT_INFO), AboutInfo("syllabus", AboutInfo.ANALYSE, AboutInfo.FROM_ABOUT_INFO), AboutInfo("textbook", AboutInfo.ANALYSE, AboutInfo.FROM_ABOUT_INFO), AboutInfo("faq", AboutInfo.ANALYSE, AboutInfo.FROM_ABOUT_INFO), AboutInfo("more_info", AboutInfo.ANALYSE, AboutInfo.FROM_ABOUT_INFO), AboutInfo("ocw_links", AboutInfo.ANALYSE, AboutInfo.FROM_ABOUT_INFO), AboutInfo("enrollment_start", AboutInfo.PROPERTY, AboutInfo.FROM_COURSE_PROPERTY), AboutInfo("enrollment_end", AboutInfo.PROPERTY, AboutInfo.FROM_COURSE_PROPERTY), AboutInfo("org", AboutInfo.PROPERTY, AboutInfo.FROM_COURSE_PROPERTY), AboutInfo("modes", AboutInfo.PROPERTY, AboutInfo.FROM_COURSE_MODE), AboutInfo("language", AboutInfo.PROPERTY, AboutInfo.FROM_COURSE_PROPERTY), ]
from __future__ import absolute_import import os
with self.assertNumQueries(2): avatar.delete()
__salt__['kmod.load']('bonding')
return reverse_url(handler_name, 'library_key_string', library_key, kwargs)
from pylearn2.costs.mlp import L1WeightDecay as _L1WD from pylearn2.costs.mlp import WeightDecay as _WD
sys.modules['libcloud'].__version__ = '0.0.0' sys.modules['pymongo'].version = '0.0.0' sys.modules['ntsecuritycon'].STANDARD_RIGHTS_REQUIRED = 0 sys.modules['ntsecuritycon'].SYNCHRONIZE = 0
return CourseLocator(org, course, run)
from django.db.migrations.loader import MigrationLoader loader = MigrationLoader(self.connection) app_list = [] for app_config in apps.get_app_configs(): if ( app_config.models_module is not None and app_config.label in loader.migrated_apps and app_config.name not in settings.TEST_NON_SERIALIZED_APPS ): app_list.append((app_config, None))
actual = grouped_ser.filter(lambda x: len(x) > 1) expected = ser.take(expected_indexes) assert_series_equal(actual, expected)
exp = Float64Index([1.0, 2.0, 3.0], name='x') self.assert_index_equal(idx.fillna(2), exp)
self.refresh_course() return problem
class UserRegistration(Form): username = CharField(max_length=10, label=None) password = CharField(widget=PasswordInput)
self.queue_len = 0 if self.status == 'incomplete': self.status = 'queued' self.queue_len = self.msg self.msg = bleach.clean(self.submitted_msg)
self.assertSessionLangEquals( 'rel', self.process_request(preview_lang='rel') )
module.system.DEBUG = True
from pandas.util.testing import makeCustomDataframe as mkdf
return self._fit(X, y, ParameterGrid(self.param_grid))
allow_empty = False @CourseViewMixin.course_check def get(self, request, **kwargs): return Response(api.course_grading_policy(self.course_key))
for name, penalty in (('unreg', 1), ('reg', 0.05)):
self.setGrains({'a': 'aval', 'foo': 'bar'}) ret = grains.absent( name='foo') self.assertEqual(ret['result'], True) self.assertEqual(ret['comment'], 'Value for grain foo was set to None') self.assertEqual(ret['changes'], {'grain': 'foo', 'value': None}) self.assertEqual( grains.__grains__, {'a': 'aval', 'foo': None}) self.assertGrainFileContent("a: aval\n" + "foo: null\n" )
def f(): return 42
return launchctl('unload', path, runas=runas)
LIMITED_RETRY_ERRORS = ( SMTPConnectError, SMTPServerDisconnected, AWSConnectionError, )
if PY2: ret = {'hive': _mbcs_to_unicode(hive), 'key': _mbcs_to_unicode(key), 'vname': _mbcs_to_unicode(vname), 'vdata': None, 'success': True} local_hive = _mbcs_to_unicode(hive) local_key = _unicode_to_mbcs(key) local_vname = _unicode_to_mbcs(vname)
TestModel.objects.create(name='Test Object') self.i18n_model = I18nTestModel.objects.create(name='Test Object')
self.course_fixture._update_xblock(component.locator, {'metadata': metadata}) self.browser.refresh() self.container_page.wait_for_page()
cls.create_image(prefix, (1, 1), 'black', 'weird {}_unlock.png') cls.create_image(prefix, (1, 1), 'black', 'special/weird {}_unlock.png')
choose_track_url = reverse('course_modes_choose', args=[unicode(self.course.id)]) self.client.post(choose_track_url, self.POST_PARAMS_FOR_COURSE_MODE['verified'])
] DATETIME_INPUT_FORMATS = [
user_input = '[{"1":"s_left"}, \ {"5":"s_right"},{"4":"s_sigma"},{"6":"s_sigma_star"},{"7":"p_left_1"}, \ {"8":"p_left_2"},{"17":"p_left_3"},{"10":"p_right_1"},{"9":"p_right_2"}, \ {"2":"p_pi_1"},{"3":"p_pi_2"},{"11":"s_sigma_name"}, \ {"13":"s_sigma_star_name"},{"15":"p_pi_name"},{"16":"p_pi_star_name"}, \ {"12":"p_sigma_name"},{"14":"p_sigma_star_name"}]' correct_answer = [{ 'draggables': ['1', '2', '3', '4', '5', '6'], 'targets': [ 's_left', 's_right', 's_sigma', 's_sigma_star', 'p_pi_1', 'p_pi_2' ], 'rule': 'anyof' }, { 'draggables': ['7', '8', '9', '10'], 'targets': ['p_left_1', 'p_left_2', 'p_right_1', 'p_right_2'], 'rule': 'anyof' }, { 'draggables': ['11', '12'], 'targets': ['s_sigma_name', 'p_sigma_name'], 'rule': 'anyof' }, { 'draggables': ['13', '14'], 'targets': ['s_sigma_star_name', 'p_sigma_star_name'], 'rule': 'anyof' }, { 'draggables': ['15'], 'targets': ['p_pi_name'], 'rule': 'anyof' }, { 'draggables': ['16'], 'targets': ['p_pi_star_name'], 'rule': 'anyof' }] self.assertFalse(draganddrop.grade(user_input, correct_answer))
super(CourseTestCase, self).setUp() self.client = AjaxEnabledTestClient() self.client.login(username=self.user.username, password=self.user_password) self.course = CourseFactory.create()
for freq in ['M', '2M', '3M']: base = Series([Period(x, freq=freq) for x in ['2011-01', '2011-02', '2011-03', '2011-04']])
try: dive.authors.add(marty) except ValueError: self.fail("Assignment across primary/replica databases with a common source should be ok")
new_module = modulestore().create_child( self.user_id, chapter1, "sequential", fields={'display_name': 'new sequential'}, ) expected.remove(BlockKey.from_usage_key(chapter1)) with self.assertRaises(ItemNotFoundError): modulestore().get_item(new_module.location.map_into_course(dest_course)) modulestore().copy(self.user_id, source_course, dest_course, [new_module.location], None) expected.append(BlockKey.from_usage_key(new_module.location)) pub_module = modulestore().get_item(new_module.location.map_into_course(dest_course)) self.assertEqual( modulestore().get_parent_location(pub_module.location).block_id, chapter1.block_id ) new_module = modulestore().create_item( self.user_id, source_course, "course_info", block_id="handouts" ) modulestore().copy(self.user_id, source_course, dest_course, [new_module.location], None) expected.append(BlockKey.from_usage_key(new_module.location)) pub_module = modulestore().get_item(new_module.location.map_into_course(dest_course)) self._check_course(source_course, dest_course, expected, unexpected)
if xblock_info['id'] == locator: return xblock_info children = xblock_info['child_info']['children'] if xblock_info.get('child_info', None) else None if children: for child_xblock_info in children: result = find_xblock_info(child_xblock_info, locator) if result: return result return None
check_is_fitted(self, "estimator_weights_") X = self._validate_X_predict(X) return self._get_median_predict(X, len(self.estimators_))
p = precision_.shape[0] cost = - 2. * log_likelihood(mle, precision_) + p * np.log(2 * np.pi) cost += alpha * (np.abs(precision_).sum() - np.abs(np.diag(precision_)).sum()) return cost
import urllib2 urlopen = urllib2.urlopen
result = df.drop_duplicates('A') expected = df.iloc[[0, 2, 3, 5, 7]] tm.assert_frame_equal(result, expected)
req = self._get_POST_csrf_cookie_request() req2 = CsrfViewMiddleware().process_view(req, csrf_exempt(post_form_view), (), {}) self.assertIsNone(req2)
biggie = DataFrame(np.zeros((200, 4)), columns=lrange(4), index=lrange(200)) repr(biggie)
dashboard_tracking_code = models.TextField(default="", blank=True)
cohort_course = CourseFactory.create(cohort_config={"cohorted": course_is_cohorted}) CourseEnrollmentFactory.create(user=self.user, course_id=cohort_course.id) cohort = CohortFactory.create(course_id=cohort_course.id, users=[self.user]) role = Role.objects.create(name=role_name, course_id=cohort_course.id) role.users = [self.user] self.register_comment_and_thread( overrides={"thread_id": "test_thread"}, thread_overrides={ "course_id": unicode(cohort_course.id), "group_id": ( None if thread_group_state == "no_group" else cohort.id if thread_group_state == "match_group" else cohort.id + 1 ), } ) expected_error = ( role_name == FORUM_ROLE_STUDENT and course_is_cohorted and thread_group_state == "different_group" ) try: delete_comment(self.request, self.comment_id) self.assertFalse(expected_error) except ThreadNotFoundError: self.assertTrue(expected_error)
self.indent(0) self.xml.endElement("django-objects") self.xml.endDocument()
index = pd.CategoricalIndex(['a', 'b', 'c']) df = DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, index=index)
sm = ShortMessage.objects.create(content="This is expensive") self.assertEqual(ShortMessage.objects.count(), 1) response = self.client.get(reverse('admin:admin_views_shortmessage_change', args=(sm.pk,))) self.assertEqual(response.status_code, 200) post_data = { "content": "Too expensive", "_save": "Save", } url = reverse('admin:admin_views_shortmessage_change', args=(sm.pk,)) response = self.client.post(url, post_data, follow=True) self.assertEqual(response.status_code, 200) self.assertEqual(ShortMessage.objects.count(), 1) self.assertContains( response, '<li class="success">The short message "<a href="%s">' 'ShortMessage object</a>" was changed successfully.</li>' % reverse('admin:admin_views_shortmessage_change', args=(sm.pk,)), html=True )
models.Article.objects.create(headline="Test article", pub_date=datetime.datetime(2010, 9, 4), reporter=self.r) a1 = models.Article.objects.get(headline="Test article") a1.reporter_id = 30 try: a1.save() except IntegrityError: pass else: self.skipTest("This backend does not support integrity checks.") r_proxy = models.ReporterProxy.objects.get(pk=self.r.pk) models.Article.objects.create(headline='Another article', pub_date=datetime.datetime(1988, 5, 15), reporter=self.r, reporter_proxy=r_proxy) a2 = models.Article.objects.get(headline='Another article') a2.reporter_proxy_id = 30 with self.assertRaises(IntegrityError): a2.save()
repr(df)
trivial_pred = np.zeros(y_true.shape) fpr, tpr, thresholds = roc_curve(y_true, trivial_pred) roc_auc = auc(fpr, tpr) assert_array_almost_equal(roc_auc, 0.50, decimal=2) assert_equal(fpr.shape, tpr.shape) assert_equal(fpr.shape, thresholds.shape)
varname = tokens[0][0] self.functions_used.add(varname)
if is_updated or asides_updated: new_structure = self.version_structure(course_key, original_structure, user_id) block_data = self._get_block_from_structure(new_structure, block_key)
if status not in [self.STATUS_SUCCESS, self.STATUS_ERROR]: msg = u"Invalid status: must be either '{success}' or '{error}'.".format( success=self.STATUS_SUCCESS, error=self.STATUS_ERROR ) raise ValueError(msg) self.status = status if status == self.STATUS_ERROR and error_reason: self.error_reason = error_reason if status == self.STATUS_SUCCESS and download_url: self.download_url = download_url self.save()
td = timedelta(days=1) resulta = df['A'] + td resultb = resulta - td assert_series_equal(resultb, df['A']) self.assertEqual(resultb.dtype, 'M8[ns]')
course_module = modulestore().get_course(course_key) return render_to_response('asset_index.html', { 'context_course': course_module, 'max_file_size_in_mbs': settings.MAX_ASSET_UPLOAD_FILE_SIZE_IN_MB, 'chunk_size_in_mbs': settings.UPLOAD_CHUNK_SIZE_IN_MB, 'max_file_size_redirect_url': settings.MAX_ASSET_UPLOAD_FILE_SIZE_URL, 'asset_callback_url': reverse_course_url('assets_handler', course_key) })
'transcript': 'http://video.google.com/timedtext?lang=en&v=3_yD_cEKoCk',
class MyWidget1(TextInput): class Media: css = { 'all': ('path/to/css1', '/path/to/css2') } js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')
r = self.rf.get('/') r.COOKIES = {} r.META = {'HTTP_ACCEPT_LANGUAGE': 'zh-my,en'} self.assertEqual(get_language_from_request(r), 'zh-hans')
return os.path.realpath(module.__path__[0])
ret = { "type": "comment", "id": "dummy", "commentable_id": "dummy", "thread_id": "dummy", "parent_id": None, "user_id": "0", "username": "dummy", "anonymous": False, "anonymous_to_peers": False, "created_at": "1970-01-01T00:00:00Z", "updated_at": "1970-01-01T00:00:00Z", "body": "dummy", "abuse_flaggers": [], "votes": {"up_count": 0}, "endorsed": False, "child_count": 0, "children": [], } ret.update(overrides or {}) return ret
if isinstance(key, (int, np.integer)): pass
course_location = locator.CourseLocator('Org1', 'Course1', 'Run1') self.course, self.enrollment = self._create_course_and_enrollment(course_location)
import salt.utils import salt.modules.cmdmod import salt.utils.systemd
f = lambda x: x.set_index('a', inplace=True) _check_f(data.copy(), f)
return dict(parse_qsl(urlparse(url).query))
if self._sample_switch: self._apply_corruption(activations, self._layer_samplers, idx_iter) return activations
self.errored_courses[course_dir] = errorlog
meta = sqlalchemy.schema.MetaData(bind=self.conn) meta.reflect() col_dict = meta.tables['test_dtypes'].columns self.assertEqual(str(col_dict['f32'].type), str(col_dict['f64_as_f32'].type)) self.assertTrue(isinstance(col_dict['f32'].type, sqltypes.Float)) self.assertTrue(isinstance(col_dict['f64'].type, sqltypes.Float)) self.assertTrue(isinstance(col_dict['i32'].type, sqltypes.Integer)) self.assertTrue(isinstance(col_dict['i64'].type, sqltypes.BigInteger))
attrs = [ ('levels', ibase.default_pprint(self._levels, max_seq_items=False)), ('labels', ibase.default_pprint(self._labels, max_seq_items=False))] if not all(name is None for name in self.names): attrs.append(('names', ibase.default_pprint(self.names))) if self.sortorder is not None: attrs.append(('sortorder', ibase.default_pprint(self.sortorder))) return attrs
self.course_key = self.store.make_course_key('myu', 'mydept.mycourse', 'myrun') course_url = reverse_url('course_handler') self.client.ajax_post( course_url, { 'org': self.course_key.org, 'number': self.course_key.course, 'display_name': 'My favorite course', 'run': self.course_key.run, }, )
self.assertEqual(len(self.lc_block.get_child_descriptors()), 1) self.assertEqual(len(self.lc_block.get_content_titles()), 1)
rcon = salt.utils.which('restorecon') if rcon: policy = False try: policy = salt.modules.selinux.getenforce() except (ImportError, CommandExecutionError): pass if policy == 'Enforcing': with salt.utils.fopen(os.devnull, 'w') as dev_null: cmd = [rcon, dest] subprocess.call(cmd, stdout=dev_null, stderr=dev_null) if os.path.isfile(tgt): try: os.remove(tgt) except Exception: pass
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
teams = self.create_teams(self.topic, self.TEAMS_PAGE_SIZE + 1, time_between_creation=1) self.browse_teams_page.visit() self.verify_page_header() self.verify_on_page(self.browse_teams_page, 1, teams, 'Showing 1-10 out of 11 total', True) self.browse_teams_page.press_next_page_button() self.verify_on_page(self.browse_teams_page, 2, teams, 'Showing 11-11 out of 11 total', True) self.browse_teams_page.press_previous_page_button() self.verify_on_page(self.browse_teams_page, 1, teams, 'Showing 1-10 out of 11 total', True)
if sys.version >= LooseVersion('2.7'): self.assertEqual(tup._fields, ('Index', 'a', 'b')) self.assertEqual((tup.Index, tup.a, tup.b), tup) self.assertEqual(type(tup).__name__, 'TestName')
try: return find_gating_milestones(course_key, content_key, relationship)[0] except IndexError: return None
get_connection(using).commit()
instructor_role = CourseInstructorRole(course_id)
from __future__ import unicode_literals
new_index = ser.index.union(Index([1.25, 1.5, 1.75, 2.25, 2.5, 2.75])) interp_s = ser.reindex(new_index).interpolate(method='akima') assert_series_equal(interp_s[1:3], expected)
bulk_ops_record.dirty = False
if test1_success: handle = _winreg.OpenKey( _winreg.HKEY_LOCAL_MACHINE, subkey, 0, _winreg.KEY_ALL_ACCESS ) (current_vdata, dummy_current_vtype) = _winreg.QueryValueEx(handle, vname) _winreg.CloseKey(handle) test2_success = (current_vdata == UNICODETEST_WITH_SIGNS) self.assertTrue(test1_success and test2_success)
if self.runtime.DEBUG: log.warning(msg) msg = u'<p>{msg}</p>'.format(msg=cgi.escape(msg)) msg += u'<p><pre>{tb}</pre></p>'.format( tb=cgi.escape( u''.join( ['Traceback (most recent call last):\n'] + traceback.format_tb(sys.exc_info()[2]) ) ) ) problem_text = (u'<problem><text><span class="inline-error">' u'Problem {url} has an error:</span>{msg}</text></problem>'.format( url=self.location.to_deprecated_string(), msg=msg) ) self.lcp = self.new_lcp(self.get_state_for_lcp(), text=problem_text) else: raise Exception(msg), None, sys.exc_info()[2]
from __future__ import absolute_import import logging
exp = np.percentile(np.array([[1, 2, 3], [2, 3, 4]]), .5, axis=0, interpolation='nearest') expected = Series(exp, index=[1, 2, 3], name=0.5, dtype='int64') assert_series_equal(result, expected)
return course_metadata_utils.may_certify_for_course( self.certificates_display_behavior, self.certificates_show_before_end, self.has_ended() )
locator = CourseLocator(org='testx', course='GreekHero', run="run", branch=BRANCH_NAME_DRAFT) course = modulestore().get_course(locator) versions = [course.location.version_guid, course.previous_version] locator = CourseLocator(version_guid=course.previous_version) course = modulestore().get_course(locator) versions.append(course.previous_version) locator = CourseLocator(version_guid=course.previous_version) result = modulestore().get_course_successors(locator) self.assertIsInstance(result, VersionTree) self.assertIsNone(result.locator.org) self.assertEqual(result.locator.version_guid, versions[-1]) self.assertEqual(len(result.children), 1) self.assertEqual(result.children[0].locator.version_guid, versions[-2]) self.assertEqual(len(result.children[0].children), 0, "descended more than one level") result = modulestore().get_course_successors(locator, version_history_depth=2) self.assertEqual(len(result.children), 1) self.assertEqual(result.children[0].locator.version_guid, versions[-2]) self.assertEqual(len(result.children[0].children), 1) result = modulestore().get_course_successors(locator, version_history_depth=99) self.assertEqual(len(result.children), 1) self.assertEqual(result.children[0].locator.version_guid, versions[-2]) self.assertEqual(len(result.children[0].children), 1) self.assertEqual(result.children[0].children[0].locator.version_guid, versions[0])
old_group = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, self.user_partition) self.assertIn(old_group.id, [0, 1])
CELERY_RESULT_BACKEND = 'djcelery.backends.cache:CacheBackend'
from salt.exceptions import SaltInvocationError
from .utils.validation import check_array data = check_array(data, accept_sparse='csr') row_ind, col_ind = self.get_indices(i) return data[row_ind[:, np.newaxis], col_ind]
block_structure.remove_block_if( check_child_removal )
courses = modulestore().get_courses(branch=BRANCH_NAME_DRAFT, org='test_org') self.assertEqual(len(courses), 1) self.assertEqual(courses[0].id.org, course_key.org) self.assertEqual(courses[0].id.course, course_key.course) self.assertEqual(courses[0].id.run, course_key.run)
iris = datasets.load_iris() rng = check_random_state(0) perm = rng.permutation(iris.target.size) iris.data = iris.data[perm] iris.target = iris.target[perm]
metadata = _refresh_buckets_cache_file(cache_file)
module.lcp.get_html = Mock(side_effect=Exception("Test"))
self.assertIn(k, body_plain) self.assertIn(v, body_plain) self.assertIn(k, body_html) self.assertIn(v, body_html)
termination_criterion = EpochCounter(5)
__opts__['test'] = orig_test
self.assertEqual(_mock_pep8_violations.call_count, 1) self.assertEqual(self._mock_paver_sh.call_count, 2)
@contextlib.contextmanager def altered_table_name(model, temporary_table_name): original_table_name = model._meta.db_table model._meta.db_table = temporary_table_name yield model._meta.db_table = original_table_name
if not CourseEnrollment.is_enrolled(params["user"], params["course_key"]): msg = _("User {username} is not enrolled in the course {course_key}").format( username=params["user"].username, course_key=params["course_key"] ) return HttpResponseBadRequest(msg)
self.course_nav.go_to_section('Test Section 1', 'Test Subsection 1,2') self.course_nav.go_to_section('Test Section 2', 'Test Subsection 2,1') filter_link_clicked = lambda event: event.get('name', '') == 'edx.ui.lms.link_clicked' link_clicked_events = self.wait_for_events(event_filter=filter_link_clicked, timeout=2) self.assertEqual(len(link_clicked_events), 2)
return self.model.current()
response_element = rendered_html.find("span") self.assertEqual(response_element.tag, "span")
df = DataFrame(dict((c, [1, 2, 3]) for c in ['a', 'b', 'c'])) df.set_index(['a', 'b', 'c'], inplace=True) s = Series([1], index=[(2, 2, 2)]) df['val'] = 0 df df['val'].update(s)
pipeline = Pipeline([ ('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SGDClassifier()), ])
from __future__ import absolute_import
_SEUPD_WHICH = ['LM', 'SM', 'LA', 'SA', 'BE']
with_header = df.to_string(col_space=20) with_header_row1 = with_header.splitlines()[1] no_header = df.to_string(col_space=20, header=False) self.assertEqual(len(with_header_row1), len(no_header))
vert_block = ItemFactory.create( category="vertical", parent_location=self.library.location, user_id=self.user.id, publish_item=False, ) child_block = ItemFactory.create( category="html", parent_location=vert_block.location, user_id=self.user.id, publish_item=False, display_name=name_value, data=data_value, ) self.assertEqual(child_block.data, data_value) self.assertEqual(child_block.display_name, name_value)
set_credit_requirements( self.course.id, [ { "namespace": "grade", "name": "grade", "display_name": "Grade", "criteria": { "min_grade": 0.8 }, }, ] )
assert_panel_equal(p.ix[:, dates, cols], p.reindex(major=dates, minor=cols))
for x in range(1, 3): self.update_lib() self.loader.clear() self.assertEqual(self.loader[self.module_key](), (self.count, self.lib_count))
driver = info().get('ExecutionDriver', 'lxc-') if driver.startswith('lxc-'): __context__[contextkey] = 'lxc-attach' elif driver.startswith('native-') and HAS_NSENTER: __context__[contextkey] = 'nsenter' else: raise NotImplementedError( 'Unknown docker ExecutionDriver \'{0}\', or didn\'t find ' 'command to attach to the container'.format(driver) )
for i, col in enumerate(arg1.columns): results[i] = f(arg1.iloc[:, i], arg2.iloc[:, i]) return dataframe_from_int_dict(results, arg1)
X = check_array(X, dtype=[np.float64, np.float32]) if n_components is not None and X.shape[0] < n_components: raise ValueError('Expected n_samples >= n_components ' 'but got n_components = %d, n_samples = %d' % (n_components, X.shape[0])) if n_features is not None and X.shape[1] != n_features: raise ValueError("Expected the input data X have %d features, " "but got %d features" % (n_features, X.shape[1])) return X
self.assertFalse(result['success']) self.assertIn(u"you have cancelled this transaction", result['error_html'])
response = self.client.post(url) self.assertEquals(response.status_code, 403)
module = module + '.' if not module.endswith('.') else module
xblocks = self._get_nested_xblocks(self) if category: xblocks = [x for x in xblocks if x.category == category] return xblocks
enrollment_mode = self.dashboard_page.get_enrollment_mode(self.course_info["display_name"]) self.assertEqual(enrollment_mode, 'honor')
execute_from_command_line(['django-admin', 'help', 'makemessages'])
self.assertGreater(len(locations), 0) for loc in locations: resp = self.client.get_html(get_url('container_handler', loc)) self.assertEqual(resp.status_code, 200)
response = self.client.get(reverse('admin:admin_views_villain_delete', args=(self.v1.pk,))) self.assertContains(response, should_contain, 1)
return policy.get(policy_key(usage_id), {})
if _NUMEXPR_INSTALLED and _USE_NUMEXPR: if n is None: n = ne.detect_number_of_cores() ne.set_num_threads(n)
response = self.client.get(reverse('admin:admin_views_subscriber_changelist')) self.assertNotEqual(response.context["action_form"], None) response = self.client.get( reverse('admin:admin_views_subscriber_changelist') + '?%s' % IS_POPUP_VAR) self.assertEqual(response.context["action_form"], None)
logging.getLogger().setLevel(logging.ERROR)
return self.sm_ols.resid
data = ('a:b:c\n' 'd:e:f\n' 'g:h:i\n' 'j:k:l:m\n' 'l:m:n\n' 'o:p:q:r')
elif line.startswith('backing file'): matches = re.match(r'.*\(actual path: (.*?)\)', line) if matches: output.append('backing file: {0}'.format(matches.group(1))) continue
certificates = [ { 'id': 1, 'name': 'Test Certificate Name', 'description': 'Test Certificate Description', 'course_title': 'tes_course_title', 'signatories': [], 'version': 1, 'is_active': True } ] self.course.certificates = {'certificates': certificates} self.course.cert_html_view_enabled = True self.course.save() self.store.update_item(self.course, self.user.id)
cache.delete(VerificationDeadline.ALL_DEADLINES_CACHE_KEY)
self.assertFileContains('test/file.txt', 'STATICFILES_DIRS')
self.assertEqual(source.ds, ds.name)
'transcript': 'http://video.google.com/timedtext?lang=en&v=OEoXaMPEzfM',
self.stub_api() self.auth() self.listing_page.visit() self.assertTrue(self.listing_page.is_sidebar_present) self.assertTrue(self.listing_page.are_cards_present)
from salttesting import skipIf from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch ensure_in_syspath('../../')
train_data, test_data, target_train, target_test = train_test_split( data, target, test_size=.1, random_state=0)
if not self.is_valid(): raise AttributeError("'%s' object has no attribute 'cleaned_data'" % self.__class__.__name__) return [form.cleaned_data for form in self.forms]
return values.view('i8')
'ENABLE_MAX_FAILED_LOGIN_ATTEMPTS': True,
self._create_video_component() self.edit_component() self.video.set_url_field('video_name_1.mp4', 1) self.assertEqual(self.video.message('status'), 'No Timed Transcript')
vect = CountVectorizer(analyzer='char', max_df=1.0, binary=True, dtype=np.float32) X_sparse = vect.fit_transform(test_data) assert_equal(X_sparse.dtype, np.float32)
new_minions = set(minions_detected) - set(curr_minions)
if re.match(GUID_REGEX, snap_name): return snap_name.strip('{}') else: return snapshot_name_to_id(name, snap_name, strict=True, runas=runas)
qs = RasterModel.objects.filter(rast__1__dwithin=(rast, 1, 40)) self.assertEqual(qs.count(), 1) qs = RasterModel.objects.filter(rast__1__dwithin=(rast, 40)) self.assertEqual(qs.count(), 1) qs = RasterModel.objects.filter(rast__dwithin=(rast, 1, 40)) self.assertEqual(qs.count(), 1)
'name': block.display_name_with_default_escaped, 'category': block.category, 'id': unicode(block.location)
with self.assertRaises(TemplateSyntaxError): self.engine.get_template('filter-syntax06')
raise NotImplementedError()
X, y = check_X_y(X, y, accept_sparse=['csr', 'csc']) self.classes_ = np.unique(y) n_classes = self.classes_.shape[0] self.estimators_ = Parallel(n_jobs=self.n_jobs)( delayed(_fit_ovo_binary)( self.estimator, X, y, self.classes_[i], self.classes_[j]) for i in range(n_classes) for j in range(i + 1, n_classes)) return self
def divide(x1, x2, out=None, dtype=None): out_orig = out if out is None: out = np.asarray(x1, dtype=dtype) if out is x1: out = x1.copy() else: if out is not x1: out[:] = x1 if dtype is not None and out.dtype != dtype: out = out.astype(dtype) out /= x2 if out_orig is None and np.isscalar(x1): out = np.asscalar(out) return out
self.courseware_page.visit() self.courseware_page.wait_for_page()
for layer in ds: for feat in layer: self.assertEqual(source.nfld, len(list(feat))) self.assertEqual(source.gtype, feat.geom_type)
inds = np.arange(mask.size) inds = inds[mask.ravel()] ind_mask = np.logical_and(np.in1d(edges[0], inds), np.in1d(edges[1], inds)) edges = edges[:, ind_mask] if weights is not None: weights = weights[ind_mask] if len(edges.ravel()): maxval = edges.max() else: maxval = 0 order = np.searchsorted(np.unique(edges.ravel()), np.arange(maxval + 1)) edges = order[edges] if weights is None: return edges else: return edges, weights
response = self.client.post(self.url, { "course_id": unicode(self.course.id), "email_opt_in": opt }) self.assertHttpOK(response) preference = UserOrgTag.objects.get( user=self.user, org=self.course.id.org, key="email-optin" ) self.assertEquals(preference.value, result)
self.login_page.login(email="nobody@nowhere.com", password="password")
return ipsecpolicy
self.assertIs(getattr(t, "__frozen"), True)
return install_updates([guid])
return OrderedDict()
import hashlib import re import salt.utils.dictupdate as dictupdate from salt.exceptions import SaltInvocationError import salt.ext.six as six
world.visit('/') course_link_css = 'a.course-link' world.css_click(course_link_css) course_title_css = 'span.course-title' assert_true(world.is_css_present(course_title_css))
wide_means = self.panel.to_panel().mean('major') assert_frame_equal(means, wide_means)
def test_optional_include_mako(self): out = render_to_string("test_optional_include_mako.html", {}) self.assertIn("Welcome to test_optional_include_mako.html", out) self.assertIn("This is test_exists.html", out)
plt.figure() plt.title(title) if ylim is not None: plt.ylim(*ylim) plt.xlabel("Training examples") plt.ylabel("Score") train_sizes, train_scores, test_scores = learning_curve( estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.grid() plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r") plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="g") plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score") plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score") plt.legend(loc="best") return plt
self.assertEqual(login_response.status_code, 302) self.assertTrue(login_response['Location'].endswith(reverse('signin_user'))) register_response = self.client.get(login_response['Location']) self.assertEqual(register_response.status_code, 200) self.assertIn( '"currentProvider": "Tool Consumer with Secret in Settings"', register_response.content ) self.assertIn('"errorMessage": null', register_response.content)
from __future__ import absolute_import import os
return self.min_num
return [('start', self._start), ('stop', self._stop), ('step', self._step)]
cmd = '{0}.{1}'.format(self.cmd_prefix, cmd)
post_data = { 'username': 'joepublic', 'last_login_0': '2007-05-30', 'last_login_1': '13:20:10', 'date_joined_0': '2007-05-30', 'date_joined_1': '13:20:10', }
response = self.send_get(client) self.assertEqual("m", response.data["gender"])
try: os.unlink(self.module_path + 'c') except OSError: pass
from __future__ import unicode_literals
idx.shift(1, freq='H')
_test_dtype(np.float64, True, writeable=writeable) _test_dtype(np.float32, True, writeable=writeable) _test_dtype(np.uint64, False, writeable=writeable) _test_dtype(np.uint32, False, writeable=writeable) _test_dtype(np.uint16, False, writeable=writeable) _test_dtype(np.uint8, False, writeable=writeable) _test_dtype(np.int64, False, writeable=writeable) _test_dtype(np.int32, False, writeable=writeable) _test_dtype(np.int16, False, writeable=writeable) _test_dtype(np.int8, False, writeable=writeable) _test_dtype(np.object_, True, writeable=writeable) _test_dtype(np.bool, False, writeable=writeable)
return self._block_relations.iterkeys()
self.assertRaises(ValueError, list, summarize(ipaddress.ip_address('1.1.1.0'), ipaddress.ip_address('1.1.0.0'))) self.assertRaises(TypeError, list, summarize(ipaddress.ip_network('1.1.1.0'), ipaddress.ip_network('1.1.0.0'))) self.assertRaises(TypeError, list, summarize(ipaddress.ip_network('1.1.1.0'), ipaddress.ip_network('1.1.0.0'))) self.assertRaises(TypeError, list, summarize(ipaddress.ip_address('::'), ipaddress.ip_network('1.1.0.0')))
if mysql: self.assertIsNone(city.num_geom) else: self.assertEqual(1, city.num_geom)
from lms.djangoapps.ccx.utils import get_ccx_from_ccx_locator return get_ccx_from_ccx_locator(course_id)
base_json_obj['resultScore'] = round(self.module_score, 2) base_json_obj['comment'] = self.score_comment return Response(json.dumps(base_json_obj), content_type=LTI_2_0_JSON_CONTENT_TYPE)
caches['v2'].set('answer4', 42, version=1) self.assertEqual(cache.get('answer4'), 42) self.assertEqual(cache.get('answer4', version=1), 42) self.assertIsNone(cache.get('answer4', version=2))
from __future__ import absolute_import, with_statement from distutils.version import LooseVersion import time import inspect import logging
return (os.access(exe, os.X_OK) and (os.path.isfile(exe) or os.path.islink(exe)))
plural = [el.strip() for el in plural.split(';') if el.strip().startswith('plural=')][0].split('=', 1)[1]
visibility_editor = self.edit_component_visibility(component) for label in labels: visibility_editor.select_option(label, save=False) visibility_editor.save()
if self.paginator.count == 0: return 0 return (self.paginator.per_page * (self.number - 1)) + 1
ext = "" if compression == "gzip": ext = ".gz" elif compression == "bz2": ext = ".bz2" pd.read_csv(self.big_fname + ext, nrows=10, compression=compression, engine=engine)
select = 'SDO_UTIL.TO_WKTGEOMETRY(%s)'
class Recipe(models.Model): rname = models.CharField(max_length=20, unique=True)
X = rng.random_sample((10, 3))
__virtualname__ = 'group'
assert_raises(ValueError, precision_recall_fscore_support, y_true, y_pred, pos_label=2, average='macro')
'1|no_arguments', '1|one_argument:"1"', '1|one_opt_argument', '1|one_opt_argument:"1"', '1|two_one_opt_arg:"1"',
a3 = Article(headline="Third article", pub_date=datetime.date(2005, 7, 27), reporter_id=self.r.id) a3.save() self.assertEqual(a3.reporter.id, self.r.id)
self.shell.exec_cmd('rm \'$HOME/{0}\''.format(target_shim_file))
error_msg = _("An error occurred while deleting the score.") return HttpResponse(error_msg, status=500)
return len(self._data)
request_uuid = self._create_credit_request_and_get_uuid() timestamp = str(to_timestamp(datetime.datetime.now(pytz.UTC))) response = self._credit_provider_callback(request_uuid, 'approved', timestamp=timestamp) self.assertEqual(response.status_code, 200)
token_privileges = dict(win32security.GetTokenInformation( hToken, win32security.TokenPrivileges)) if privilege not in token_privileges: if enable: raise SaltInvocationError( 'The requested privilege {0} is not available for this ' 'process (check Salt user privileges).'.format(privilege_name))
arr = self.view(np.ndarray).copy() return arr
assert all([len(arg) == len(args[0]) for arg in args]) return izip(*args)
method = getattr(self._values, name) if 'inplace' in kwargs: raise ValueError("cannot use inplace with CategoricalIndex") res = method(*args, **kwargs) if lib.isscalar(res): return res return CategoricalIndex(res, name=self.name)
import os import logging
mark.book_set.clear() self.assertEqual( list(Person.objects.using('other').filter(book__title='Dive into Python').values_list('name', flat=True)), [] ) self.assertEqual( list( Person.objects.using('other').filter(book__title='Greasemonkey Hacks').values_list('name', flat=True) ), [] )
self.assertEqual( self.run_function('timezone.set_zone', ['spongebob']), 'ERROR executing \'timezone.set_zone\': ' 'Invalid Timezone: spongebob')
if self.key_betas is not None and \ ki < len(self.key_betas) and \ bp1 == self.key_betas[ki]:
for entry in toc: if entry['url_name'] == url_name: return entry return None
modulestore().get_item(usage_key)
self._set_partitions([ UserPartition( id=0, name="Cohort user partition", scheme=UserPartition.get_scheme("cohort"), description="Cohorted user partition", groups=[ Group(id=0, name="Group A"), Group(id=1, name="Group B"), ], ), UserPartition( id=1, name="Verification user partition", scheme=UserPartition.get_scheme("verification"), description="Verification user partition", groups=[ Group(id=0, name="Group C"), ], active=False, ), ])
try: date_time = win32api.GetLocalTime() except win32api.error as exc: (number, context, message) = exc log.error('Failed to get local time') log.error('nbr: {0}'.format(number)) log.error('ctx: {0}'.format(context)) log.error('msg: {0}'.format(message)) return False
if len(test_path) > 1: test_module_name = '.'.join(test_path[:-1]) else: test_module_name = '.' test_module = __import__(test_module_name, {}, {}, force_str(test_path[-1])) test_runner = getattr(test_module, test_path[-1]) return test_runner
return lambda course, reverse_url_func: reverse_url_func(reverse_name, args=[course.id.to_deprecated_string()])
self.html_unit = ItemFactory.create( parent_location=self.vertical.location, category="html", display_name="Html Content", modulestore=store, publish_item=False, )
__opts__.get( 'log_level', 'error' )
salt_config_dir = os.environ.get('SALT_CONFIG_DIR', None) if salt_config_dir: env_config_file_path = os.path.join(salt_config_dir, 'minion') if salt_config_dir and os.path.isfile(env_config_file_path): os.environ[env_var] = env_config_file_path
df = read_csv('s3://cant_get_it/tips.csv') self.assertTrue(isinstance(df, DataFrame)) self.assertFalse(df.empty) tm.assert_frame_equal(read_csv(tm.get_data_path('tips.csv')), df)
if defaults: if option in defaults: log.info('Using default for %s %s', virtualname, option) yield option, defaults[option] continue
for plot in self.plots: if plot.freq is None: plot.freq = self.freq
outp = Index(ujson.decode(ujson.encode(i)), name='index') tm.assert_index_equal(i, outp)
continue_learning = (self.model.continue_learning() and extension_continue) assert continue_learning in [True, False, 0, 1] while continue_learning: if self.exceeded_time_budget(t0, time_budget): break
f, ax = plt.subplots() ax.plot(x, y) ax.set_title('Simple plot')
for child, parents in enumerate(self.get_parents_map(children_map)): self.assertSetEqual(set(block_structure.get_parents(child)), set(parents))
confusion = confusion_matrix(y1, y2, labels=labels) n_classes = confusion.shape[0] sum0 = np.sum(confusion, axis=0) sum1 = np.sum(confusion, axis=1) expected = np.outer(sum0, sum1) / np.sum(sum0) if weights is None: w_mat = np.ones([n_classes, n_classes], dtype=np.int) w_mat.flat[:: n_classes + 1] = 0 elif weights == "linear" or weights == "quadratic": w_mat = np.zeros([n_classes, n_classes], dtype=np.int) w_mat += np.arange(n_classes) if weights == "linear": w_mat = np.abs(w_mat - w_mat.T) else: w_mat = (w_mat - w_mat.T) ** 2 else: raise ValueError("Unknown kappa weighting type.") k = np.sum(w_mat * confusion) / np.sum(w_mat * expected) return 1 - k
compare_to = [var.resolve(context, True) for var in self._varlist]
'master_sign_key_name': str,
response = self.client.get_html('/course/edX/test') self.assertEquals(response.status_code, 404)
scsi_spec.device.sharedBus = vim.vm.device.VirtualSCSIController.Sharing.noSharing
pass
get_parents=None, get_children=get_children, filter_func=filter_func,
if not len(os.path.dirname(path)): path = create_tempfile(path)
@classmethod def setUpClass(cls): super(TestInstructorAccessForum, cls).setUpClass() cls.course = CourseFactory.create() def setUp(self): super(TestInstructorAccessForum, self).setUp() self.mod_role = Role.objects.create( course_id=self.course.id, name=FORUM_ROLE_MODERATOR ) self.moderators = [UserFactory.create() for _ in xrange(4)] for user in self.moderators: self.mod_role.users.add(user) def test_allow(self): user = UserFactory.create() update_forum_role(self.course.id, user, FORUM_ROLE_MODERATOR, 'allow') self.assertIn(user, self.mod_role.users.all()) def test_allow_twice(self): user = UserFactory.create() update_forum_role(self.course.id, user, FORUM_ROLE_MODERATOR, 'allow') self.assertIn(user, self.mod_role.users.all()) update_forum_role(self.course.id, user, FORUM_ROLE_MODERATOR, 'allow') self.assertIn(user, self.mod_role.users.all()) @raises(Role.DoesNotExist) def test_allow_badrole(self): user = UserFactory.create() update_forum_role(self.course.id, user, 'robot-not-a-real-role', 'allow') def test_revoke(self): user = self.moderators[0] update_forum_role(self.course.id, user, FORUM_ROLE_MODERATOR, 'revoke') self.assertNotIn(user, self.mod_role.users.all()) def test_revoke_twice(self): user = self.moderators[0] update_forum_role(self.course.id, user, FORUM_ROLE_MODERATOR, 'revoke') self.assertNotIn(user, self.mod_role.users.all()) update_forum_role(self.course.id, user, FORUM_ROLE_MODERATOR, 'revoke') self.assertNotIn(user, self.mod_role.users.all()) def test_revoke_notallowed(self): user = UserFactory() update_forum_role(self.course.id, user, FORUM_ROLE_MODERATOR, 'revoke') self.assertNotIn(user, self.mod_role.users.all()) @raises(Role.DoesNotExist) def test_revoke_badrole(self): user = self.moderators[0] update_forum_role(self.course.id, user, 'robot-not-a-real-role', 'allow') @raises(ValueError) def test_bad_mode(self): user = UserFactory() update_forum_role(self.course.id, user, FORUM_ROLE_MODERATOR, 'robot-not-a-mode')
process_count = 3 expected_verbosity_string = ( "--with-xunitmp --xunitmp-file={repo_dir}/reports/bok_choy{shard_str}/xunit.xml" " --processes={procs} --no-color --process-timeout=1200".format( repo_dir=REPO_DIR, shard_str='/shard_' + self.shard if self.shard else '', procs=process_count ) ) suite = BokChoyTestSuite('', num_processes=process_count) self.assertEqual(BokChoyTestSuite.verbosity_processes_string(suite), expected_verbosity_string)
return self.event[u'current_tab'] == 1
return _click_edit(self, '.edit-button', '.xblock-studio_view')
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError
if optname == 'proto' and valuestr == 'none': valuestr = 'static'
good_mode_kwargs_testfile = os.path.join( testcase_temp_dir, 'good_mode_kwargs', 'testappend' ) good_template = [ '{0}:'.format(good_mode_kwargs_testfile), ' file.recurse:', ' - source: salt://testappend', ' - dir_mode: 744', ' - file_mode: 644', ] try: ret = self.run_function( 'state.template_str', ['\n'.join(good_template)] ) self.assertSaltTrueReturn(ret) finally: if os.path.isdir(testcase_temp_dir): shutil.rmtree(testcase_temp_dir)
if order_field.startswith('-') and pfx == "-": ordering.append(order_field[1:]) else: ordering.append(pfx + order_field)
msg = "Input has different freq=A-DEC from PeriodIndex" with tm.assertRaisesRegexp(period.IncompatibleFrequency, msg): base <= Period('2011', freq='A')
if return_type not in BoxPlot._valid_return_types: raise ValueError("return_type must be {None, 'axes', 'dict', 'both'}")
config_course_cohorts(course, is_cohorted=True)
msg = "get() returned more than one Article -- it returned 2!" with self.assertRaisesMessage(MultipleObjectsReturned, msg): Article.objects.get(headline__startswith='Swallow',) with self.assertRaisesMessage(MultipleObjectsReturned, msg): Article.objects.get(pub_date__year=2005,) with self.assertRaisesMessage(MultipleObjectsReturned, msg): Article.objects.get(pub_date__year=2005, pub_date__month=7)
ts = _simple_pts('1990', '1992', freq='A-JUN')
loss[i] = np.dot(true_at_reversed_rank.cumsum(), false_at_reversed_rank)
m_space, m_source = model.get_monitoring_data_specs() state, target = m_space.make_theano_batch() y = target[:, 0] y_hat = model.fprop(state)[:, 0] wmape_numerator = abs(y - y_hat).sum() wmape_numerator = T.cast(wmape_numerator, config.floatX) for dataset_name, dataset in algorithm.monitoring_dataset.items(): if dataset_name: channel_name = '{0}_{1}'.format(dataset_name, self.channel_name_suffix) else: channel_name = self.channel_name_suffix model.monitor.add_channel(name=channel_name, ipt=(state, target), val=wmape_numerator, data_specs=(m_space, m_source), dataset=dataset)
df = DataFrame(index=["A", "B", "C"], columns=[1, 2, 3, 4, 5]) result = df.get_dtype_counts().sort_values() expected = Series({'object': 5}) assert_series_equal(result, expected)
query = self.q(css='.ui-loading-indicator') return query.present and 'is-hidden' not in query.attrs('class')[0].split()
self.alias_map = {} self.external_aliases = set()
from salt.modules import composer from salt.exceptions import CommandExecutionError, CommandNotFoundError, SaltInvocationError
try: currIndex = sysPath.index(path) if currIndex != index: sysPath.pop(currIndex) else: return True except ValueError: pass
self.storage.save('dotted.path/test', ContentFile("1")) self.storage.save('dotted.path/test', ContentFile("2")) files = sorted(os.listdir(os.path.join(self.storage_dir, 'dotted.path'))) self.assertFalse(os.path.exists(os.path.join(self.storage_dir, 'dotted_.path'))) self.assertEqual(files[0], 'test') six.assertRegex(self, files[1], 'test_%s' % FILE_SUFFIX_REGEX)
add_action(task_definition=task_definition, **kwargs)
youtube_text_api = copy.deepcopy(settings.YOUTUBE['TEXT_API']) youtube_text_api['params']['v'] = youtube_id youtube_transcript_name = youtube_video_transcript_name(youtube_text_api) if youtube_transcript_name: youtube_text_api['params']['name'] = youtube_transcript_name youtube_response = requests.get('http://' + youtube_text_api['url'], params=youtube_text_api['params'])
@ddt.data(*itertools.product([True, False], [True, False], [True, False])) @ddt.unpack def test_thread(self, is_author, is_privileged, is_cohorted): thread = Thread(user_id="5" if is_author else "6", type="thread") context = _get_context( requester_id="5", is_requester_privileged=is_privileged, is_cohorted=is_cohorted ) actual = get_editable_fields(thread, context) expected = {"abuse_flagged", "following", "read", "voted"} if is_author or is_privileged: expected |= {"topic_id", "type", "title", "raw_body"} if is_privileged and is_cohorted: expected |= {"group_id"} self.assertEqual(actual, expected) @ddt.data(*itertools.product([True, False], [True, False], ["question", "discussion"], [True, False])) @ddt.unpack def test_comment(self, is_author, is_thread_author, thread_type, is_privileged): comment = Comment(user_id="5" if is_author else "6", type="comment") context = _get_context( requester_id="5", is_requester_privileged=is_privileged, thread=Thread(user_id="5" if is_thread_author else "6", thread_type=thread_type) ) actual = get_editable_fields(comment, context) expected = {"abuse_flagged", "voted"} if is_author or is_privileged: expected |= {"raw_body"} if (is_thread_author and thread_type == "question") or is_privileged: expected |= {"endorsed"} self.assertEqual(actual, expected)
tm.assertRaisesRegexp(ValueError, 'axis', df.to_timestamp, axis=2)
frame = self.mframe
df = DataFrame( {'A': [1, 2, 3, 4, 5, 6], 'B': [3, 4, 5, 6, 7, 8]}, index=[0, 1, 0, 1, 2, 3]) self.assertRaises(KeyError, df.loc.__getitem__, tuple([slice(1, None)])) self.assertRaises(KeyError, df.loc.__getitem__, tuple([slice(0, None)])) self.assertRaises(KeyError, df.loc.__getitem__, tuple([slice(1, 2)]))
response.remove_headers = headers
error_message_selector = '.auto_enroll_csv .results .message-%s li.summary-item' % section_type self.wait_for_element_presence(error_message_selector, "%s message" % section_type.title()) return self.q(css=error_message_selector).text[0]
serializer_context = { 'request': request, 'block_structure': blocks, 'requested_fields': requested_fields or [], }
self.assertEqual(p.books.count(), 3)
self.client.force_login(self.changeuser) response = self.client.get(reverse('admin:admin_views_article_history', args=(self.a1.pk,))) self.assertEqual(response.status_code, 200)
def f(x): x = x.ravel()
value = value.lstrip('@')
instance.size = instance with transaction.atomic(): with self.assertRaises(TypeError): instance.save() instance.size = 2.5 instance.save() self.assertTrue(instance.id) instance.size = instance msg = ( 'Tried to update field model_fields.FloatModel.size with a model ' 'instance, <FloatModel: FloatModel object>. Use a value ' 'compatible with FloatField.' ) with transaction.atomic(): with self.assertRaisesMessage(TypeError, msg): instance.save() obj = FloatModel.objects.get(pk=instance.id) obj.size = obj with self.assertRaises(TypeError): obj.save()
for view_name in ['courseware', 'info', 'progress']: resp = self.client.get( reverse( view_name, kwargs={'course_id': unicode(course.id)} ) ) self.assertEquals(resp.status_code, 200)
def get_html(self): return self.descriptor.rendered_html
prng = period_range('1/1/2011', '1/1/2012', freq='M') new_prng = self.round_trip_pickle(prng) self.assertEqual(new_prng.freq, offsets.MonthEnd()) self.assertEqual(new_prng.freqstr, 'M')
merge_opts = ['--ff-only']
if not all_exists: all_exists = (j > 0) break images.append(img)
if isinstance(other, TimedeltaIndex): return True elif (len(other) > 0 and other.inferred_type not in ('floating', 'mixed-integer', 'integer', 'mixed-integer-float', 'mixed')): return True return False
xshp, = input_shapes out_shapes = [xshp] while len(out_shapes) < self.n_levels: s = out_shapes[-1] out_shapes.append((s[0], s[1]//2, s[2]//2,s[3])) return out_shapes
with self.assertRaises(UserNotFound): get_account_settings(self.default_request, username="does_not_exist") self.user.username = "does_not_exist" request = self.request_factory.get("/api/user/v1/accounts/") request.user = self.user with self.assertRaises(UserNotFound): get_account_settings(request)
return result, True
self.assertFalse(os.path.exists(path))
DATABASES = AUTH_TOKENS['DATABASES']
testStack = self.event_stack.value presenceReq = self.presence_req.value ryn = 'manor' name = 'unknown_name' self.assertTrue(name != testStack.local.name) presenceReq.append({'route': {'dst': (None, ryn, 'presence_req'), 'src': (None, name, None)}})
for mode in test_modes: key = 'Mode: {0}'.format(mode)
self.assertTrue(self.store.has_changes(xblock))
denied_banner = any(item.display for item in reverifications["denied"])
return [ aside_type for aside_type in super(LmsModuleSystem, self).applicable_aside_types(block) if aside_type != 'acid_aside' ]
for i in range(values.shape[1]): chunk = new_values[:, i * width:(i + 1) * width] mask_chunk = new_mask[:, i * width:(i + 1) * width]
middleware.process_request(request) request.session['hello'] = 'world'
continue
return _prod(self.dim[:self.ndim - len(self.readshape)])
self.user.is_active = True
y = set() while len(y) != y_size: c = np.searchsorted(cumulative_p_c, generator.rand(y_size - len(y))) y.update(c) y = list(y)
value = "".join(value.strip().replace(',', '').split()) self.ftypes = set() for ftype in value: try: self.ftypes.add(_FILE_TYPES[ftype]) except KeyError: raise ValueError('invalid file type "{0}"'.format(ftype))
cursor.execute("SHOW FULL TABLES") return [TableInfo(row[0], {'BASE TABLE': 't', 'VIEW': 'v'}.get(row[1])) for row in cursor.fetchall()]
X = diabetes.data Y = np.vstack([diabetes.target, diabetes.target ** 2]).T n_targets = Y.shape[1]
'track', 'eventtracking.django.apps.EventTrackingConfig',
with self.assertRaisesRegexp(Exception, 'Unknown parenthesis'): preview.LatexRendered('x^2', parens='not parens')
pass
now = datetime.datetime.now(UTC()) is_released = "unknown" mstart = block.start
alg.fit(X) alg.fit(X.tolist())
'course_modes',
max_depth = node_indicator.sum(axis=1).max() assert_less_equal(est.tree_.max_depth, max_depth)
self._assert_steps_displayed( response, [PayAndVerifyView.INTRO_STEP] + PayAndVerifyView.VERIFICATION_STEPS, PayAndVerifyView.INTRO_STEP ) self._assert_messaging(response, PayAndVerifyView.FIRST_TIME_VERIFY_MSG) self._assert_requirements_displayed(response, [ PayAndVerifyView.PHOTO_ID_REQ, PayAndVerifyView.WEBCAM_REQ, ])
arr = randn(50) arr[:10] = np.NaN arr[-10:] = np.NaN s = Series(arr)
raise NotImplementedError
expected_results = { 'action_name': 'certificates generated', 'total': 3, 'attempted': 3, 'succeeded': 3, 'failed': 0, 'skipped': 0 } self.assertCertificatesGenerated(task_input, expected_results)
self.user.set_password('new_password') self.user.save() self.middleware.process_request(self.request) self.assertIsNotNone(self.request.user) self.assertTrue(self.request.user.is_anonymous) self.assertIsNone(self.request.session.session_key)
alpha1 = dual_coef[class2 - 1, sv_locs[class1]:sv_locs[class1 + 1]] alpha2 = dual_coef[class1, sv_locs[class2]:sv_locs[class2 + 1]]
for block_key in block_structure.topological_traversal(): block_structure.set_transformer_block_field( block_key, cls, data_key, cls._create_block_value(block_key, data_key) )
report_csv_filename = report_store.links_for(self.course.id)[0][0] with open(report_store.path_to(self.course.id, report_csv_filename)) as csv_file: csv_file_data = csv_file.read() for data in expected_data: self.assertIn(data, csv_file_data)
predicted_probabilitiy = self.predict_proba(X) return self.classes_.take((np.argmax(predicted_probabilitiy, axis=1)), axis=0)
raise NotImplementedError( str(type(self)) + " does not implement " + "continue_learning.")
try: return _int_to_bytes(address, 4, 'big') except: raise ValueError("Address negative or too large for IPv4")
return (BlockTypeKeyV1(key.block_family, key.block_scope_id), key.field_name)
try: return self.maps[0].popitem() except KeyError: raise KeyError('No keys found in the first mapping.')
self.assert_grade(problem, 'choice_0', 'incorrect') self.assert_grade(problem, 'choice_1', 'incorrect') self.assert_grade(problem, 'choice_2', 'incorrect') self.assert_grade(problem, ['choice_0', 'choice_1', 'choice_2'], 'incorrect') self.assert_grade(problem, ['choice_0', 'choice_2'], 'incorrect') self.assert_grade(problem, ['choice_0', 'choice_1'], 'incorrect') self.assert_grade(problem, ['choice_1', 'choice_2'], 'correct')
labels = (label == 'root' and node_id == 0) or label == 'all'
return nanops.nanargmax(self.values)
if which_set in ['train', 'test']: data_x, data_y = load_data("{0}{1}_32x32.mat".format(path, which_set))
dot_app_user = UserFactory.create(password=USER_PASSWORD) dot_app = dot_models.Application.objects.create( name='test app', user=dot_app_user, client_type='confidential', authorization_grant_type='authorization-code', redirect_uris='http://localhost:8079/complete/edxorg/' ) self.dot_access_token = dot_models.AccessToken.objects.create( user=self.student, application=dot_app, expires=datetime.utcnow() + timedelta(weeks=1), scope='read write', token='16MGyP3OaQYHmpT1lK7Q6MMNAZsjwF' )
ret['result'] = None ret['comment'] = ( 'Sysctl option {0} would be changed to {1}'.format(name, value) ) return ret
if len(args) == 1: coords = args[0] else: coords = args
url(r'^admin/', admin.site.urls),
if not self.is_templatized: return self.path extension = { 'djangojs': 'c', 'django': 'py', }.get(self.domain) filename = '%s.%s' % (self.translatable.file, extension) return os.path.join(self.translatable.dirpath, filename)
response = self.client.get(reverse('admin:admin_views_customarticle_change', args=(article_pk,))) self.assertTemplateUsed(response, 'custom_admin/change_form.html') response = self.client.get(reverse('admin:admin_views_customarticle_delete', args=(article_pk,))) self.assertTemplateUsed(response, 'custom_admin/delete_confirmation.html') response = self.client.post(reverse('admin:admin_views_customarticle_changelist'), data={ 'index': 0, 'action': ['delete_selected'], '_selected_action': ['1'], }) self.assertTemplateUsed(response, 'custom_admin/delete_selected_confirmation.html') response = self.client.get(reverse('admin:admin_views_customarticle_history', args=(article_pk,))) self.assertTemplateUsed(response, 'custom_admin/object_history.html')
CreditRequirementStatus.remove_requirement_status( username, req_to_remove )
from __future__ import absolute_import
delete_item(category='chapter', name='chapter_2')
pass
size_tr_col = len(headers[self.tr_size_col])
if not isPublish or (isPublish and is_publish_root): ancestor_payload = { 'edit_info.subtree_edited_on': now, 'edit_info.subtree_edited_by': user_id } self._update_ancestors(xblock.scope_ids.usage_id, ancestor_payload)
rval = 1./ (self.model.alpha + self.model.w ) rval.name = 'var_s1' return rval
response = self.client.post(self.url, { "email": "someone+else@example.com", "name": "Someone Else", "username": self.USERNAME, "password": self.PASSWORD, "honor_code": "true", }) self.assertEqual(response.status_code, 409) response_json = json.loads(response.content) self.assertEqual( response_json, { "username": [{ "user_message": ( "It looks like {} belongs to an existing account. " "Try again with a different username." ).format( self.USERNAME ) }] } )
def inner(self, default_store, module_count, mongo_calls, sql_queries, *args, **kwargs): with modulestore().default_store(default_store): self.set_up_course(module_count=module_count) self.clear_caches() with self.assertNumQueries(sql_queries): with check_mongo_calls(mongo_calls): func(self, *args, **kwargs) return inner
if hasattr(os, 'symlink'): if os.path.exists(self.symlinked_dir): self.assertTrue(os.path.islink(self.symlinked_dir)) else: try: os.symlink(os.path.join(self.test_dir, 'templates'), self.symlinked_dir) except (OSError, NotImplementedError): raise SkipTest("os.symlink() is available on this OS but can't be used by this user.") os.chdir(self.test_dir) management.call_command('makemessages', locale=[LOCALE], verbosity=0, symlinks=True) self.assertTrue(os.path.exists(self.PO_FILE)) with open(self.PO_FILE, 'r') as fp: po_contents = force_text(fp.read()) self.assertMsgId('This literal should be included.', po_contents) self.assertLocationCommentPresent(self.PO_FILE, None, 'templates_symlinked', 'test.html') else: raise SkipTest("os.symlink() not available on this OS + Python version combination.")
module_location = module.location.map_into_course(target_id) _update_module_location(module, module_location.replace(revision=MongoRevisionKey.draft))
import salt.config import salt.loader from salt.modules import boto_elb
msg = ("Expected self.y to have dim 2, but it has %d. Maybe you are " "loading from an outdated pickle file?") if control.get_load_data(): if start is not None: self.X = preprocessed_dataset.X[start:stop, :] if self.y is not None: if self.y.ndim != 2: raise ValueError(msg % self.y.ndim) self.y = self.y[start:stop, :] assert self.X.shape[0] == stop - start else: self.X = preprocessed_dataset.X else: self.X = None if self.X is not None: if self.y is not None: assert self.y.shape[0] == self.X.shape[0]
self._unget_history = [num_bytes] + self._unget_history[:49] number_equal = len([current_number for current_number in self._unget_history if current_number == num_bytes]) if number_equal > 40: raise SuspiciousMultipartForm( "The multipart parser got stuck, which shouldn't happen with" " normal uploaded files. Check for malicious upload activity;" " if there is none, report this to the Django developers." )
if scipy.version.version >= LooseVersion('0.12'): from scipy.sparse.linalg import eigs, eigsh, svds else: eigs, eigsh, svds = _eigs, _eigsh, _svds
dr = date_range(datetime(2011, 6, 1, 0), periods=10, freq=datetools.Hour()) is_dst = np.array([1] * 10) localized = dr.tz_localize(tz) localized_is_dst = dr.tz_localize(tz, ambiguous=is_dst) self.assert_index_equal(localized, localized_is_dst)
batch_size = self.batch_size if hasattr(model, "force_batch_size"): if model.force_batch_size and model.force_batch_size > 0: if batch_size is not None: if batch_size != model.force_batch_size: if self.set_batch_size: model.set_batch_size(batch_size) else: raise ValueError("batch_size argument to " + str(type(self)) + "conflicts with model's " + "force_batch_size attribute") else: self.batch_size = model.force_batch_size if self.batch_size is None: raise NoBatchSizeError()
self._find_within( ".response_{} .discussion-response .post-update".format( response_id ) ).first.click() return ( not self.is_response_editor_visible(response_id) and self.is_response_visible(response_id) and self.get_response_body(response_id) == new_response_body )
__virtualname__ = 'winrepo'
with tm.assert_produces_warning(FutureWarning, check_stacklevel=False): from pandas.io.wb import search, download, get_countries
if len(names) == 1 and len(ret): return ret[names[0]]
def f(): s[l]
scheme = value.split('://')[0].lower() if scheme not in self.schemes: raise ValidationError(self.message, code=self.code)
ref_out = { 'retcode': 1, 'stdout': '', 'stderr': '' } with patch.dict('salt.modules.zypper.__salt__', {'cmd.run_all': MagicMock(return_value=ref_out)}): with self.assertRaisesRegexp(CommandExecutionError, "^Zypper command failure: Check Zypper's logs.$"): zypper.list_upgrades(refresh=False)
active = __salt__['mount.active'](extended=True) real_name = os.path.realpath(name) if device.startswith('/'): if 'bind' in opts and real_name in active: _device = device if active[real_name]['device'].startswith('/'): while True: if _device in active: _real_device = active[_device]['device'] opts = list(set(opts + active[_device]['opts'] + active[_device]['superopts'])) active[real_name]['opts'].append('bind') break _device = os.path.dirname(_device) real_device = _real_device else: if _device in active: opts = list(set(opts + active[_device]['opts'] + active[_device]['superopts'])) active[real_name]['opts'].append('bind') real_device = active[real_name]['device'] else: real_device = os.path.realpath(device) elif device.upper().startswith('UUID='): real_device = device.split('=')[1].strip('"').lower() elif device.upper().startswith('LABEL='): _label = device.split('=')[1] cmd = 'blkid -t LABEL={0}'.format(_label) res = __salt__['cmd.run_all']('{0}'.format(cmd)) if res['retcode'] > 0: ret['comment'] = 'Unable to find device with label {0}.'.format(_label) ret['result'] = False return ret else: for line in res['stdout']: dev_with_label = line.split(':')[0] device_list.append(dev_with_label) real_device = device_list[0] else: real_device = device
module = import_module(module_name) if not hasattr(module, name): raise ValueError( "Could not find manager %s in %s.\n" "Please note that you need to inherit from managers you " "dynamically generated with 'from_queryset()'." % (name, module_name) ) return (
generator = check_random_state(random_state) X = generator.normal(loc=0, scale=1, size=(n_samples, n_features)) y = generator.normal(loc=(X[:, 0] + 2 * X[:, 1] - 2 * X[:, 2] - 1.5 * X[:, 3]), scale=np.ones(n_samples)) return X, y
update_data = { self.test_preference_key: "new_value" } update_user_preferences(self.user, update_data, user=self.user) self.assertEqual( get_user_preference(self.user, self.test_preference_key), "new_value" )
def setUp(self): self.write_settings('settings.py', ['django.contrib.auth', 'django.contrib.contenttypes', 'admin_scripts', 'admin_scripts.complex_app']) def tearDown(self): self.remove_settings('settings.py') def test_builtin_command(self): "fulldefault: django-admin builtin commands fail with an error when no settings provided" args = ['check', 'admin_scripts'] out, err = self.run_django_admin(args) self.assertNoOutput(out) self.assertOutput(err, 'settings are not configured') def test_builtin_with_settings(self): "fulldefault: django-admin builtin commands succeed if a settings file is provided" args = ['check', '--settings=test_project.settings', 'admin_scripts'] out, err = self.run_django_admin(args) self.assertNoOutput(err) self.assertOutput(out, SYSTEM_CHECK_MSG) def test_builtin_with_environment(self): "fulldefault: django-admin builtin commands succeed if the environment contains settings" args = ['check', 'admin_scripts'] out, err = self.run_django_admin(args, 'test_project.settings') self.assertNoOutput(err) self.assertOutput(out, SYSTEM_CHECK_MSG) def test_builtin_with_bad_settings(self): "fulldefault: django-admin builtin commands fail if settings file (from argument) doesn't exist" args = ['check', '--settings=bad_settings', 'admin_scripts'] out, err = self.run_django_admin(args) self.assertNoOutput(out) self.assertOutput(err, "No module named '?bad_settings'?", regex=True) def test_builtin_with_bad_environment(self): "fulldefault: django-admin builtin commands fail if settings file (from environment) doesn't exist" args = ['check', 'admin_scripts'] out, err = self.run_django_admin(args, 'bad_settings') self.assertNoOutput(out) self.assertOutput(err, "No module named '?bad_settings'?", regex=True) def test_custom_command(self): "fulldefault: django-admin can't execute user commands unless settings are provided" args = ['noargs_command'] out, err = self.run_django_admin(args) self.assertNoOutput(out) self.assertOutput(err, "No Django settings specified") self.assertOutput(err, "Unknown command: 'noargs_command'") def test_custom_command_with_settings(self): "fulldefault: django-admin can execute user commands if settings are provided as argument" args = ['noargs_command', '--settings=test_project.settings'] out, err = self.run_django_admin(args) self.assertNoOutput(err) self.assertOutput(out, "EXECUTE: noargs_command") def test_custom_command_with_environment(self): "fulldefault: django-admin can execute user commands if settings are provided in environment" args = ['noargs_command'] out, err = self.run_django_admin(args, 'test_project.settings') self.assertNoOutput(err) self.assertOutput(out, "EXECUTE: noargs_command")
values = Series([u("FOO"), NA, u("bar"), u("Blurg")]) results = values.str.swapcase() exp = Series([u("foo"), NA, u("BAR"), u("bLURG")]) tm.assert_series_equal(results, exp)
return cls.objects.filter(order__isnull=False, course_id=course_id)
vm_image = config.get_cloud_config_value( 'image', vm_, __opts__, search_global=False ) image = show_image({'image': vm_image}, call='function') platform = ET.SubElement(content, 'platform') template = ET.SubElement(platform, 'template-info') template.attrib['name'] = vm_image os_info = ET.SubElement(platform, 'os-info') os_info.attrib['technology'] = image[vm_image]['technology'] os_info.attrib['type'] = image[vm_image]['osType']
return force_text(unquote(force_str(quoted_url)))
self.assertEqual([], list(User.objects.all())) call_command('manage_user', TEST_USERNAME, TEST_EMAIL) user = User.objects.get(username=TEST_USERNAME) self.assertEqual(user.username, TEST_USERNAME) self.assertEqual(user.email, TEST_EMAIL) self.assertIsNotNone(user.profile)
filename = '_' + fragment_name contents[filename] = fragment
lab = product(range(-1, n), range(-1, m)) mi = MultiIndex(levels=[list('abcde')[:n], list('WXYZ')[:m]], labels=np.random.permutation(list(lab)).T) self.assertEqual(len(mi), (n + 1) * (m + 1)) self.assertFalse(mi.has_duplicates) self.assertEqual(mi.get_duplicates(), []) tm.assert_numpy_array_equal(mi.duplicated(), np.zeros( len(mi), dtype='bool'))
res = store.select('wpneg', Term('items == -1')) expected = Panel({-1: wpneg[-1]}) tm.assert_panel_equal(res, expected)
callback = self.get_params['callback']
resp = self.submit_question_answer(name, {'2_1': self.incorrect_responses[name]}) self.reset_question_answer(name) resp = self.submit_question_answer(name, {'2_1': self.correct_responses[name]})
return txt.replace('\\', '\\\\').replace(':', '\\:')
if self.user: user = self.user.resolve(context) if not user.is_authenticated: flatpages = flatpages.filter(registration_required=False) else: flatpages = flatpages.filter(registration_required=False)
class TestForm(Form): field1 = CharField() field2 = CharField() field3 = CharField() field4 = CharField() field5 = CharField() field6 = CharField() field7 = CharField() field8 = CharField() field9 = CharField() field10 = CharField() field11 = CharField() field12 = CharField() field13 = CharField() field14 = CharField()
len_non_info_axes = [ len(_ax) for _i, _ax in enumerate(self.obj.axes) if _i != i ] if any([not l for l in len_non_info_axes]): if not is_list_like_indexer(value): raise ValueError("cannot set a frame with no " "defined index and a scalar") self.obj[key] = value return self.obj
about = modulestore().get_item(course.id.make_usage_key('about', 'overview')) html = get_preview_fragment(request, about, context).content self.assertNotRegexpMatches(html, r"data-block-type=[\"\']test_aside[\"\']") self.assertNotRegexpMatches(html, "Aside rendered")
return self.q(css=self._bounded_selector('.remove-admin-role')).text[0]
ps = tm.makePeriodSeries() shifted = ps.shift(1) unshifted = shifted.shift(-1) tm.assert_index_equal(shifted.index, ps.index) tm.assert_index_equal(unshifted.index, ps.index) tm.assert_numpy_array_equal(unshifted.valid().values, ps.values[:-1])
cright = right.copy() cright['d'] = cright['d'].astype('category') cleft = left.copy() cleft['b'] = cleft['b'].astype('category') result = pd.merge(cleft, cright, how='left', left_on='b', right_on='c') tm.assert_frame_equal(result, expected)
class SongForm(Form): artist = CharField() name = CharField()
with patch_edxnotes_api_settings("http://example.com"): self.assertEqual("http://example.com/some_path/", get_endpoint_function("/some_path"))
packages_sources={} installed_packages_list={}
this = self.reindex(items=items, major=major, minor=minor) other = other.reindex(items=items, major=major, minor=minor)
expected_textline_context = { 'STATIC_URL': '/dummy-static/', 'status': the_system.STATUS_CLASS('unsubmitted'), 'label': '', 'value': '', 'preprocessor': None, 'msg': '', 'inline': False, 'hidden': False, 'do_math': False, 'id': '1_2_1', 'trailing_text': '', 'size': None, }
if key_node.tag == u'!aggregate': log.warning('!aggregate applies on values only, not on keys') value_node.tag = key_node.tag key_node.tag = self.resolve_sls_tag(key_node)[0]
X, y = _rescale_data(X, y, sample_weight)
response = self.client.get('/check_session/') self.assertEqual(response.status_code, 200) self.assertEqual(response.content, b'YES')
silent_variable_failure = True
accuracy_scorer = make_scorer(accuracy_score) f1_scorer = make_scorer(f1_score)
url += '?auto' try: response = _urlopen(url, timeout=timeout).read().splitlines() except URLError: return 'error'
cache_key = CourseEnrollment.cache_key_name( instance.user.id, unicode(instance.course_id) ) cache.delete(cache_key)
qs = Author.objects.order_by().order_by('name') self.assertIn('ORDER BY', qs.query.get_compiler(qs.db).as_sql()[0])
class Point(OGRGeometry):
class OFTString(Field): pass
if 'COMMAND' in chunks[0]:
A = neighbors.kneighbors_graph(X, 1, mode='connectivity', include_self=True) assert_array_equal(A.toarray(), np.eye(A.shape[0]))
module = CapaFactory.create(max_attempts=0) self.assertFalse(module.should_show_check_button())
shp_file = os.path.join(TEST_DATA, 'gas_lines', 'gas_leitung.shp') model_def = ogrinspect(shp_file, 'MyModel', multi_geom=True) self.assertIn('geom = models.MultiLineStringField(srid=-1)', model_def)
self.selenium.find_element_by_link_text('Home').click() self.selenium.back() expected_unselected_values = [ str(self.arthur.id), str(self.bob.id), str(self.cliff.id), str(self.jason.id), str(self.jenny.id), str(self.john.id), ] expected_selected_values = [str(self.lisa.id), str(self.peter.id)] self.assertSelectOptions('#id_students_from', expected_unselected_values) self.assertSelectOptions('#id_students_to', expected_selected_values) self.assertSelectOptions('#id_alumni_from', expected_unselected_values) self.assertSelectOptions('#id_alumni_to', expected_selected_values)
for size, name in get_profile_image_names(self.user.username).items(): if exist: self.assertTrue(self.storage.exists(name)) with closing(Image.open(self.storage.path(name))) as img: self.assertEqual(img.size, (size, size)) self.assertEqual(img.format, 'JPEG') else: self.assertFalse(self.storage.exists(name))
_, _, exclude_dictionary = LmsSearchFilterGenerator.generate_field_filters(user=self.user) self.assertIn('org', exclude_dictionary) exclude_orgs = exclude_dictionary['org'] self.assertEqual(2, len(exclude_orgs)) self.assertEqual('LogistrationX', exclude_orgs[0]) self.assertEqual('TestMicrositeX', exclude_orgs[1])
response.set_cookie(settings.CSRF_COOKIE_NAME, request.META["CSRF_COOKIE"], max_age=settings.CSRF_COOKIE_AGE, domain=settings.CSRF_COOKIE_DOMAIN, path=settings.CSRF_COOKIE_PATH, secure=settings.CSRF_COOKIE_SECURE, httponly=settings.CSRF_COOKIE_HTTPONLY ) patch_vary_headers(response, ('Cookie',)) response.csrf_cookie_set = True return response
ref_geom = GEOSGeometry( 'MULTIPOINT(-97.516111 33.058333,-96.801611 32.782057,' '-95.363151 29.763374,-96.801611 32.782057)' )
#import yaml import salt.ext.six as six
UPSELL_TO_VERIFIED_MODES = [HONOR, AUDIT]
index = date_range('01-Jan-2013', periods=10, freq='H') arr = np.arange(10, dtype='int64') s1 = Series(arr, index=index) s2 = Series(arr, index=index) df = DataFrame(arr.reshape(-1, 1), index=index)
self.navigate_to_video()
from __future__ import unicode_literals
if is_edx_domain: changes = comprehensive_theme_changes(EDX_THEME_DIR) with override_settings(COMPREHENSIVE_THEME_DIR=EDX_THEME_DIR, **changes['settings']): with edxmako.save_lookups(): for template_dir in changes['template_paths']: edxmako.paths.add_lookup('main', template_dir, prepend=True) yield else: yield
wp = tm.makePanel4D() wp['obj1'] = 'foo' wp['obj2'] = 'bar' wp['bool1'] = wp['l1'] > 0 wp['bool2'] = wp['l2'] > 0 wp['int1'] = 1 wp['int2'] = 2 wp = wp.consolidate()
return self.book.save(self.path)
models.VeryLongModelNameZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ.objects.create()
warn_until( 'Carbon', 'It\'s time to start raising `SaltInvocationError` instead of ' 'returning warnings', _dont_call_warnings=True )
if self.depth == self.num_fields - 1:
DATE_INPUT_FORMATS = [
X_csc = sp.csc_matrix(X_C) X_coo = X_csc.tocoo() X_dok = X_csc.todok() X_int = X_csc.astype(np.int) X_float = X_csc.astype(np.float)
proba = sum(all_proba) / self.n_estimators
course_about = get_course_about_section(self.request, self.course, 'short_description') self.assertEqual(course_about, "A course about toys.")
return library_key.replace(version_guid=None, branch=None)
low['__jid__'] = jid low['__user__'] = user low['__tag__'] = tag
return getattr(self.queryables.get(self.lhs), 'metadata', None)
with remove_ccx(course_id) as (course_id, restore): return restore(self._modulestore.has_course( course_id, ignore_case=ignore_case, **kwargs ))
ret = '{0:<38} {1}\n'.format('Snapshot ID', 'Snapshot Name') for snap_id in snap_ids: snap_name = snapshot_id_to_name(name, snap_id, runas=runas) ret += (u'{{{0}}} {1}\n'.format(snap_id, _sdecode(snap_name))) return ret
update_wrapper(view, cls.dispatch, assigned=()) return view
current_path = os.environ.pop('PATH', None) try: self.assertIsNone(find_command('_missing_')) finally: if current_path is not None: os.environ['PATH'] = current_path
data_specs = (IndexSpace(max_labels=10, dim=1), 'targets') it = self.test.iterator(mode='sequential', data_specs=data_specs, batch_size=100) for y in it: pass
X = np.array([[0, 0, 5], [0, 5, 0], [3, 0, 0], [0, 0, 6], [6, 0, 0]]) y = ["eggs", "spam", "spam", "eggs", "spam"] Y = np.array([[0, 1, 1, 0, 1]]).T
options = self.xml.xpath('//optioninput[@id=$id]/option', id=answer_id) options = [option for option in options if option.text.strip() == student_answer] if options: option = options[0] hint_node = option.find('./optionhint') if hint_node is not None: new_cmap[answer_id]['msg'] += self.make_hint_div( hint_node, option.get('correct').upper() == 'TRUE', [student_answer], self.tags[0] )
return _graph_connected_component(graph, 0).sum() == graph.shape[0]
@property def title(self): return self._get_element_text(self.TITLE_SELECTOR) @property def children(self): children = self.q(css=self._bounded_selector('.note')) return [EdxNotesPageItem(self.browser, child.get_attribute("id")) for child in children] @property def notes(self): return [section.text for section in self.children]
with self.assertRaises(IntegrityError): Person.objects.update_or_create(first_name="Tom", last_name="Smith")
courses_list_by_groups, __ = _accessible_courses_list_from_groups(self.request)
EmptyPromise( lambda: self.field(field_id) is not None, "Field with id \"{0}\" is in DOM.".format(field_id) ).fulfill()
assert_almost_equal(result._y.values.flat, [1, 4, 5], check_dtype=False)
if err.ndim == 3: if (err_shape[0] != self.nseries) or \ (err_shape[1] != 2) or \ (err_shape[2] != len(self.data)): msg = "Asymmetrical error bars should be provided " + \ "with the shape (%u, 2, %u)" % \ (self.nseries, len(self.data)) raise ValueError(msg)
water = Book(title="Dive into Water", published=datetime.date(2001, 1, 1), editor=mark) self.assertEqual(water._state.db, 'default')
from __future__ import absolute_import import os import random import string
return self._get_structures_for_branch_and_locator(branch, self._create_library_locator, **kwargs)
return 4.0 / (max_squared_sum + int(fit_intercept) + 4.0 * alpha_scaled)
kernel = self.kernel return kernel == "precomputed" or callable(kernel)
with mock_order_endpoint(order_number=self.ORDER_NUMBER, exception=exceptions.HttpNotFoundError): response = self.client.get(self.path) self.assertEqual(response.status_code, 404)
#s.raw_command(netfn=6, command=1, retry=False) return True
import salt.loader import salt.utils import salt.utils.locales
return vpnservice
if asset_type == 'transcript' and self.q(css='#upload_error').present: return self.click_button('asset_submit') self._wait_for(lambda: not self.q(css=CLASS_SELECTORS['upload_dialog']).present, 'Upload Completed')
db_name = "foo'3" self._db_creation_loop(db_name=db_name, returning_name=db_name, test_conn=True, character_set='utf8', connection_user=self.user, connection_pass=self.password )
return render(request, 'context_processors/auth_attrs_test_access.html', {'session_accessed': request.session.accessed})
entries = {} for name, overrides in six.iteritems(mapped): overrides.setdefault('name', name) entries[name] = overrides map_[profile] = entries continue
with self.assertRaises(ImproperlyConfigured): Sitemap().get_urls()
if isinstance(other, SparseSeries): other = other.to_dense() dense_combined = self.to_dense().combine_first(other) return dense_combined.to_sparse(fill_value=self.fill_value)
try: import pymongo HAS_MONGODB = True except ImportError: HAS_MONGODB = False
re.compile(r'^\s*Files & Uploads'): [ 'js/base', 'jquery.ui', 'coffee/src/main', 'underscore', 'js/views/assets', 'js/views/asset' ],
return urlencode({ 'next': '/account/finish_auth?{}'.format(urlencode(params)) })
conf[opt] = [x.strip() for x in cp.get('main', opt).split(',')]
elif single_aligner and is_frame:
install_flags = '{0}'.format(pkginfo[version_num].get('install_flags')) if options and options.get('extra_install_flags'): install_flags = '{0} {1}'.format( install_flags, options.get('extra_install_flags', '') )
super(CourseEnrollmentEndFieldTest, self).setUp() self.course = CourseFactory.create(org='edX', number='dummy', display_name='Marketing Site Course') self.course_details_url = reverse_course_url('settings_handler', unicode(self.course.id))
return _write_styles('.xmodule_display', output_root, _list_modules())
assert_raises(ValueError, KFold, 1.5) assert_raises(ValueError, KFold, 2.0) assert_raises(ValueError, StratifiedKFold, 1.5) assert_raises(ValueError, StratifiedKFold, 2.0)
with self.assertRaises(BadHeaderError): send_mail('Subject\nMultiline', 'Content', 'from@example.com', ['to@example.com'])
old_cmap = CorrectMap() for i, answer_id in enumerate(answer_ids): queuekey = 1000 + i queuestate = CodeResponseTest.make_queuestate(queuekey, datetime.now(UTC)) old_cmap.update(CorrectMap(answer_id=answer_ids[i], queuestate=queuestate))
res = cidx3.get_loc('a') self.assertEqual(res, idx3.get_loc('a')) self.assertEqual(res, slice(0, 2, None))
return [] if all else None
for protocol in range(pickle.HIGHEST_PROTOCOL + 1): lazy_category = SimpleLazyObject(lambda: category) lazy_category.categoryinfo lazy_category_2 = SimpleLazyObject(lambda: category) with warnings.catch_warnings(record=True) as recorded: self.assertEqual(pickle.loads(pickle.dumps(lazy_category, protocol)), category) self.assertEqual(pickle.loads(pickle.dumps(lazy_category_2, protocol)), category) self.assertEqual(len(recorded), 0)
if adjacent: self.leaveWhitespace() self.adjacent = adjacent self.skipWhitespace = True self.joinString = joinString
result = df.drop_duplicates('AAA') expected = df[:2] tm.assert_frame_equal(result, expected)
evaluate_prerequisite(self.course, self.prob1.location, self.user.id) self.assertFalse(mock_module_score.called)
user = User.objects.get(username=self.USERNAME) request = RequestFactory().get("/api/user/v1/accounts/") request.user = user account_settings = get_account_settings(request)
return self.render()
marg_idx = numpy.arange(marginalize_odd, depth, 2) for i in marg_idx: from_im1 = T.dot(samples[i-1], W_list[i]) if i >= 1 else 0. from_ip1 = T.dot(samples[i+1], W_list[i+1].T) if i < depth-1 else 0 net_input = (from_im1 + from_ip1 + b_list[i]) * beta fe -= T.sum(T.nnet.softplus(net_input), axis=1)
self.store.convert_to_draft(self.vertical_x1a, self.user_id) item = self.store.get_item(self.vertical_x1a) self.assertTrue(self.store.has_published_version(item))
response = client.post(self.path, data=json.dumps(data), content_type=JSON) self.assertEqual(response.status_code, 403) self.assertIn('CSRF', response.content)
mocked_attr.return_value = {'foo': 'bar'} self.assertEqual(utils.get_course_chapters(self.course_key), [])
if hasattr(klass, '_default_manager'): return klass._default_manager.all() return klass
self._validate(is_numeric, batch) return 0
return True
for op in ['__eq__', '__le__', '__ge__']:
if select_settings: self.select_cohort_settings() if content_group is None: self.q(css=self._bounded_selector(self.no_content_group_button_css)).first.click() else: self._select_associated_content_group(content_group) self.save_cohort_settings()
url = reverse('activate', kwargs={'key': activation_key}) self.assert_request_status_code(200, url) self.assertTrue(User.objects.get(email=email).is_active)
with change_cwd("."): suite = DiscoverRunner().build_suite([]) self.assertEqual( suite._tests[0].id().split(".")[0], os.path.basename(os.getcwd()), )
with tm.assert_produces_warning(FutureWarning): tm.assert_series_equal( s.duplicated(take_last=True), expected) with tm.assert_produces_warning(FutureWarning): tm.assert_series_equal(s.drop_duplicates(take_last=True), s[~np.array(base)]) base = [False] * len(original) + [True, True] base[3] = True base[5] = True expected = Series(base, index=idx, name='a')
html_module = self.store.get_item(course_id.make_usage_key('html', 'with_styling')) self.assertIn('<p style="font:italic bold 72px/30px Georgia, serif; color: red; ">', html_module.data)
self.setup_system_xmodule_mocks_for_lti20_request_test() mock_request = self.get_signed_lti20_mock_request(self.GOOD_JSON_PUT) for bad_method in self.UNSUPPORTED_HTTP_METHODS: mock_request.method = bad_method response = self.xmodule.lti_2_0_result_rest_handler(mock_request, "user/abcd") self.assertEqual(response.status_code, 404)
query = self.q(css=self._bounded_selector("#cohort-select option")) return len(query) > 0, query
expressions.extend( [self._handle_param(arg, '', NUMERIC_TYPES) for arg in args[2:]] ) expressions.extend( [self._handle_param(arg, '', NUMERIC_TYPES) for arg in args[0:2]] )
import salt.utils.systemd
if not self.db_table: self.db_table = "%s_%s" % (self.app_label, self.model_name) self.db_table = truncate_name(self.db_table, connection.ops.max_name_length())
asides = None if asides_source: asides = [] for asd in asides_source: aside_fields = {} for asd_field_key, asd_field_val in asd.fields.iteritems(): aside_fields[asd_field_key] = asd_field_val.read_from(asd) asides.append({ 'aside_type': asd.scope_ids.block_type, 'fields': aside_fields }) return asides
original = pd.concat([original[col].astype('category') for col in original], axis=1)
class UserRegistration(Form):
if not isinstance(username, basestring): raise AccountUsernameInvalid(u"Username must be a string") if len(username) < USERNAME_MIN_LENGTH: raise AccountUsernameInvalid( u"Username '{username}' must be at least {min} characters long".format( username=username, min=USERNAME_MIN_LENGTH ) ) if len(username) > USERNAME_MAX_LENGTH: raise AccountUsernameInvalid( u"Username '{username}' must be at most {max} characters long".format( username=username, max=USERNAME_MAX_LENGTH ) ) try: validate_slug(username) except ValidationError: raise AccountUsernameInvalid( u"Username '{username}' must contain only A-Z, a-z, 0-9, -, or _ characters" )
self.assert_enrollment_status()
yield found + 1, "E201 whitespace after '%s'" % char
grouped = df.groupby('A') first = grouped.first() expected = df.ix[[1, 0], ['B', 'C', 'D', 'E', 'F']] expected.index = Index(['bar', 'foo'], name='A') expected = expected.sort_index() assert_frame_equal(first, expected)
import collections
import os.path import shutil
completed_courses = [ {'course_id': 'course-1', 'type': 'verified'}, {'course_id': 'course-2', 'type': 'prof-ed'}, ] mock_get_completed_courses.return_value = completed_courses tasks.award_program_certificates.delay(self.student.username).get() self.assertEqual( mock_get_completed_programs.call_args[0][1], completed_courses )
self.assertContains( response, configuration['microsites']['testmicrosite']['company_about_url'], ) self.assertContains( response, configuration['microsites']['testmicrosite']['company_privacy_url'], ) self.assertContains( response, configuration['microsites']['testmicrosite']['company_tos_url'], )
opts = self.opts codename = get_permission_codename('change', opts) return request.user.has_perm("%s.%s" % (opts.app_label, codename))
if (children_left[node_id] != children_right[node_id]): stack.append((children_left[node_id], parent_depth + 1)) stack.append((children_right[node_id], parent_depth + 1)) else: is_leaves[node_id] = True
X = X_digits_multi[:10] y = y_digits_multi[:10]
cur_block = unordered_structure[block]
warnings.extend(check_resolver(pattern))
s = Series(date_range('1/1/2000', periods=10))
try: output = modulestore().heartbeat() except HeartbeatFailure as fail: return JsonResponse({fail.service: unicode(fail)}, status=503)
dates = date_range('01-Jan-2015', '01-Dec-2015', freq='M') values2 = dates.view(np.ndarray).astype('datetime64[ns]') expected = Series(values2, dates)
draft_dir = OSFS(root_dir / 'test_export/drafts') self.assertTrue(draft_dir.exists('html')) self.assertTrue(draft_dir.exists('video')) self.assertIn(self.DRAFT_HTML + ".xml", draft_dir.listdir('html')) self.assertIn(self.DRAFT_VIDEO + ".xml", draft_dir.listdir('video')) self.assertNotIn(self.ORPHAN_DRAFT_HTML + ".xml", draft_dir.listdir('html'))
return np.array(self)
fmt = '%%(function)s(%s)' % settings['procedure_fmt']
packages_really_to_upgrade=[] for this_package in packages_to_upgrade: if this_package in installed_packages_list:
businesshours = self._get_business_hours_by_sec() return self._onOffset(dt, businesshours)
mean = zca_dataset.X.mean(axis=0) var = zca_dataset.X.std(axis=0) assert_allclose(mean, np.zeros(num_feat), atol=1e-2) assert_allclose(var, np.ones(num_feat), atol=1e-2)
try: log.info("Course import %s: Upload complete", courselike_key) _save_request_status(request, courselike_string, 1)
s = Series(range(5), [-2, -1, 1, 2, 3])
poudriere.__salt__ = {}
def start(self): pass
syncing.remove(name) continue
self.assertIn(settings.EDXMKTG_LOGGED_IN_COOKIE_NAME, self.client.cookies) self.assertIn(settings.EDXMKTG_USER_INFO_COOKIE_NAME, self.client.cookies)
log.error('PhotoVerification: Error parsing this error message: %s', self.error_msg) return _("There was an error verifying your ID photos.")
df = DataFrame(np.random.randint(0, 2, (4, 4)), columns=['a', 'b', 'c', 'd'])
get_version = { 'Linux': linux_diskstats, 'FreeBSD': freebsd_diskstats, }
self.assertTrue(np.isnan(self.ts[::2].cov(self.ts[1::2])))
topics_data = self._serialize_and_paginate( TopicsPagination, topics, request, BulkTeamCountTopicSerializer, {'course_id': course.id}, ) topics_data["sort_order"] = sort_order
with self.assertRaises(ValueError): d.save()
_("There was a problem with the staff answer to this problem: empty boundary.")
self._create_video_component() self.edit_component() self.open_advanced_tab() self.video.upload_translation('chinese_transcripts.srt', 'zh') self.save_unit_settings() self.assertTrue(self.video.is_captions_visible()) unicode_text = "好 各位同学".decode('utf-8') self.assertIn(unicode_text, self.video.captions_text) self.edit_component() self.open_advanced_tab() self.assertEqual(self.video.translations(), ['zh']) self.video.replace_translation('zh', 'uk', 'uk_transcripts.srt') self.save_unit_settings() self.assertTrue(self.video.is_captions_visible()) unicode_text = "Привіт, edX вітає вас.".decode('utf-8') self.assertIn(unicode_text, self.video.captions_text)
url = reverse('about_course', args=[self.course_with_visibility.id.to_deprecated_string()]) resp = self.client.get(url, HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME) self.assertEqual(resp.status_code, 200) self.assertNotIn("Enroll in {}".format(self.course_with_visibility.id.course), resp.content) self.assertIn("Add {} to Cart <span>($10 USD)</span>".format( self.course_with_visibility.id.course ), resp.content) self.assertIn('$("#add_to_cart_post").click', resp.content)
empty = Empty().setName("empty") lineStart = LineStart().setName("lineStart") lineEnd = LineEnd().setName("lineEnd") stringStart = StringStart().setName("stringStart") stringEnd = StringEnd().setName("stringEnd")
mock = MagicMock(return_value={'retcode': 0, 'stdout': ''}) with patch.dict(pip.__salt__, {'cmd.run_all': mock}): pip.install(pkg, install_options=install_options) mock.assert_called_once_with( expected, saltenv='base', runas=None, use_vt=False, python_shell=False, )
from django.contrib.gis.maps.google.gmap import GoogleMap google_map = GoogleMap() scripts = google_map.scripts self.assertIn(GOOGLE_MAPS_API_KEY, scripts) self.assertIn("new GMap2", scripts)
import salt.config import salt.minion import salt.payload import salt.transport import salt.loader import salt.minion import salt.utils import salt.utils.args import salt.utils.event import salt.utils.minions import salt.utils.verify import salt.utils.jid import salt.syspaths as syspaths from salt.exceptions import ( EauthAuthenticationError, SaltInvocationError, SaltReqTimeoutError, SaltClientError, PublishError )
parent_locations = self._get_raw_parent_locations(location, key_revision=parent_revision) if not parent_locations: parent_locations = self._get_raw_parent_locations(location, key_revision=MongoRevisionKey.published) if parent_locations: draft_parent = self.convert_to_draft(parent_locations[0], user_id) parent_locations = [draft_parent.location] for parent_location in parent_locations: if not is_item_direct_only and parent_location.category in DIRECT_ONLY_CATEGORIES: query = location.to_deprecated_son(prefix='_id.') del query['_id.revision'] if self.collection.find(query).count() > 1: continue
s = 'Month 1, 1999' assert to_datetime(s, errors='ignore') == s
cls.objects.create(checkpoint=checkpoint, user_id=user_id, course_id=course_id)
self.save()
for x, y in ((vb_vcrunexec, vb_vcrun), (vb_saltexec, vb_salt)): vb_lines = y.split('\n') batch += '\ndel ' + x + '\n@echo ' + vb_lines[0] + ' >' + \ x + '.vbs\n@echo ' + \ (' >>' + x + '.vbs\n@echo ').join(vb_lines[1:]) + \ ' >>' + x + '.vbs\ncscript.exe /NoLogo ' + x + '.vbs'
key = make_key(args, kwds, typed) if kwds or typed else args with lock: link = cache_get(key) if link is not None: root, = nonlocal_root link_prev, link_next, key, result = link link_prev[NEXT] = link_next link_next[PREV] = link_prev last = root[PREV] last[NEXT] = root[PREV] = link link[PREV] = last link[NEXT] = root stats[HITS] += 1 return result result = user_function(*args, **kwds) with lock: root, = nonlocal_root if key in cache: pass elif _len(cache) >= maxsize: oldroot = root oldroot[KEY] = key oldroot[RESULT] = result root = nonlocal_root[0] = oldroot[NEXT] oldkey = root[KEY] oldvalue = root[RESULT] root[KEY] = root[RESULT] = None del cache[oldkey] cache[key] = oldroot else: last = root[PREV] link = [last, root, key, result] last[NEXT] = root[PREV] = cache[key] = link stats[MISSES] += 1 return result
probs = np.array([[1, 1e-6, 0], [0.19, 0.6, 0.2], [-999, 0.51, 0.5], [1e-6, 1, 1e-9]]) probs /= np.abs(probs.sum(axis=1))[:, np.newaxis]
try: self.assertTrue(verify_socket('::', 18000, 18001)) except socket.error as serr: pass
self.browser.execute_script('$(".file-name-block").show();$(".file-input").show()') self.q(css='input[type="file"]')[0].send_keys(asset_file_path) self.q(css='input[type="file"]')[0].send_keys(asset_file_path) self._wait_for_button() click_css(self, '.submit-button', require_notification=False)
solve_triangular_args = {'check_finite': False}
y_proba_pred1 = clf.predict_proba(X1) assert_array_equal((y_proba_pred1[:, 1] > 0.5) + 1, y, 'solver %s' % solver) y_log_proba_pred1 = clf.predict_log_proba(X1) assert_array_almost_equal(np.exp(y_log_proba_pred1), y_proba_pred1, 8, 'solver %s' % solver)
sequence_sql = connections[using].ops.sequence_reset_sql(no_style(), [Site]) if sequence_sql: if verbosity >= 2: print("Resetting sequence") with connections[using].cursor() as cursor: for command in sequence_sql: cursor.execute(command)
perms_map = DjangoModelPermissions.perms_map.copy() perms_map['GET'] = perms_map['OPTIONS'] = perms_map['HEAD'] = perms_map['POST']
r2_scores = cross_val_score(reg, X, y, scoring="r2", cv=5) assert_array_almost_equal(r2_scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)
for i, hyperparameter in enumerate(kernel.hyperparameters): params = kernel.get_params() params[hyperparameter.name + "_bounds"] = "fixed" kernel_class = kernel.__class__ new_kernel = kernel_class(**params) _, K_gradient_new = new_kernel(X, eval_gradient=True) assert_equal(theta.shape[0], new_kernel.theta.shape[0] + 1) assert_equal(K_gradient.shape[2], K_gradient_new.shape[2] + 1) if i > 0: assert_equal(theta[:i], new_kernel.theta[:i]) assert_array_equal(K_gradient[..., :i], K_gradient_new[..., :i]) if i + 1 < len(kernel.hyperparameters): assert_equal(theta[i+1:], new_kernel.theta[i:]) assert_array_equal(K_gradient[..., i+1:], K_gradient_new[..., i:])
check_is_fitted(self, 'n_components_') return np.dot(W, self.components_)
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010")
cache_ttl = CourseAssetCacheTtlConfig.get_cache_ttl() if cache_ttl > 0 and not is_locked: newrelic.agent.add_custom_parameter('contentserver.cacheable', True)
try: import SoftLayer HAS_SLLIBS = True except ImportError: HAS_SLLIBS = False
self.assert_redirect_to_provider_looks_correct(self.client.get( pipeline.get_login_url(self.provider.provider_id, pipeline.AUTH_ENTRY_LOGIN)))
DATE_INPUT_FORMATS = [
self.creation_counter = Field.creation_counter Field.creation_counter += 1
try: super(ClassDecoratedTestCase, self).test_max_recursion_error() except RuntimeError: self.fail()
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
if len(cls.requisites) > 0: for req in cls.requisites: if req.requisite not in state.kwargs: state.kwargs[req.requisite] = [] state.kwargs[req.requisite].append(req())
actor = Actor.objects.create(name="Palin", age=27) response = self.client.get("%s?%s" % (reverse('admin:admin_views_actor_changelist'), IS_POPUP_VAR)) self.assertContains(response, 'data-popup-opener="%s"' % actor.pk)
A, B = _run_one_hot(X, X2, cat) C, D = _run_one_hot(X, X2, ind) assert_equal(A.shape, (2, n_features)) assert_equal(B.shape, (1, n_features)) assert_equal(C.shape, (2, n_features)) assert_equal(D.shape, (1, n_features)) assert_array_equal(toarray(A), toarray(C)) assert_array_equal(toarray(B), toarray(D))
result = get_result(arr, 20, min_periods=15) self.assertTrue(np.isnan(result[23])) self.assertFalse(np.isnan(result[24]))
normalized = text.normalize_newlines(b"abc\ndef\rghi\r\n") self.assertEqual(normalized, "abc\ndef\nghi\n") self.assertIsInstance(normalized, six.text_type)
return [mock.call(self.store, course.id) for course in courses]
op(a, b, axis=0)
import salt.ext.six as six
from django.conf import settings from django.contrib.auth.models import User from django.db import models, migrations
win_path.__salt__ = {}
node_position_model = manifold.LocallyLinearEmbedding( n_components=2, eigen_solver='dense', n_neighbors=6)
__virtualname__ = 'sys'
if isnull(rvalue).any() and isnull(self.categories).any(): nan_pos = np.where(isnull(self.categories))[0] lindexer[lindexer == -1] = nan_pos
from __future__ import absolute_import
response = method("/request_data/?foo=whiz", data={'foo': 'bang'}) self.assertEqual(response.context['get-foo'], 'bang')
class SomeForm(Form): hidden1 = CharField(widget=HiddenInput) custom = CharField() hidden2 = CharField(widget=HiddenInput) def as_p(self): return self._html_output( normal_row='<p%(html_class_attr)s>%(field)s %(field_name)s</p>', error_row='%s', row_ender='</p>', help_text_html=' %s', errors_on_separate_row=True, ) form = SomeForm() self.assertHTMLEqual( form.as_p(), '<p><input id="id_custom" name="custom" type="text" required /> custom' '<input id="id_hidden1" name="hidden1" type="hidden" />' '<input id="id_hidden2" name="hidden2" type="hidden" /></p>' )
if course_id: return cls.objects.filter(user=user, badge_class__course_id=course_id) return cls.objects.filter(user=user)
self.assertNotIn(self.module_key + '2', self.loader)
expected_1 = TimedeltaIndex( ['1 day', '3 day', '4 day', '5 day'], freq=None, name='idx')
if vgname and not clone_from: try: kwargs['vgname'] = vgname clone_from = _get_base(profile=profile, **kwargs) except (SaltInvocationError, CommandExecutionError) as exc: ret['comment'] = exc.strerror if changes: ret['changes'] = changes_dict return ret if not kwargs.get('snapshot') is False: kwargs['snapshot'] = True does_exist = exists(name, path=path) to_reboot = False remove_seed_marker = False if does_exist: pass elif clone_from: remove_seed_marker = True try: clone(name, clone_from, profile=profile, **kwargs) changes.append({'create': 'Container cloned'}) except (SaltInvocationError, CommandExecutionError) as exc: if 'already exists' in exc.strerror: changes.append({'create': 'Container already exists'}) else: ret['result'] = False ret['comment'] = exc.strerror if changes: ret['changes'] = changes_dict return ret cfg = _LXCConfig(name=name, network_profile=network_profile, nic_opts=nic_opts, bridge=bridge, path=path, gateway=gateway, autostart=autostart, cpuset=cpuset, cpushare=cpushare, memory=memory) old_chunks = read_conf(cfg.path, out_format='commented') cfg.write() chunks = read_conf(cfg.path, out_format='commented') if old_chunks != chunks: to_reboot = True else: remove_seed_marker = True cfg = _LXCConfig(network_profile=network_profile, nic_opts=nic_opts, cpuset=cpuset, path=path, bridge=bridge, gateway=gateway, autostart=autostart, cpushare=cpushare, memory=memory) with cfg.tempfile() as cfile: try: create(name, config=cfile.name, profile=profile, **kwargs) changes.append({'create': 'Container created'}) except (SaltInvocationError, CommandExecutionError) as exc: if 'already exists' in exc.strerror: changes.append({'create': 'Container already exists'}) else: ret['comment'] = exc.strerror if changes: ret['changes'] = changes_dict return ret cpath = os.path.join(bpath, name, 'config') old_chunks = [] if os.path.exists(cpath): old_chunks = read_conf(cpath, out_format='commented') new_cfg = _config_list(conf_tuples=old_chunks, cpu=cpu, network_profile=network_profile, nic_opts=nic_opts, bridge=bridge, cpuset=cpuset, cpushare=cpushare, memory=memory) if new_cfg: edit_conf(cpath, out_format='commented', lxc_config=new_cfg) chunks = read_conf(cpath, out_format='commented') if old_chunks != chunks: to_reboot = True
action_name = 'generating_exec_summary_report' task_fn = partial(upload_exec_summary_report, xmodule_instance_args) return run_main_task(entry_id, task_fn, action_name)
if ((self.n_neighbors is None or self.n_neighbors < self._fit_X.shape[0] // 2) and self.metric != 'precomputed'): if self.effective_metric_ in VALID_METRICS['kd_tree']: self._fit_method = 'kd_tree' else: self._fit_method = 'ball_tree' else: self._fit_method = 'brute'
class SongForm(forms.ModelForm): extra_data = forms.CharField() class Meta: model = Song fields = '__all__' class FieldsOnFormOnlyAdmin(admin.ModelAdmin): form = SongForm fields = ['extra_data', 'title'] errors = FieldsOnFormOnlyAdmin(Song, AdminSite()).check() self.assertEqual(errors, [])
wp = Panel(1, items=range(2), major_axis=range(3), minor_axis=range(4), dtype='float32') vals = np.empty((2, 3, 4), dtype='float32') vals.fill(1) assert_panel_equal(wp, Panel(vals, dtype='float32'))
course = getattr(self, course_attr) with self.assertNumQueries(expected_sql_queries): _update_xblocks_cache(course.id) expected_cache_data = getattr(self, course_attr + '_expected_cache_data') for usage_key, __ in expected_cache_data.items(): xblock_cache = XBlockCache.objects.get(usage_key=usage_key) for path_index, path in enumerate(xblock_cache.paths): for path_item_index, path_item in enumerate(path): self.assertEqual( path_item.usage_key, expected_cache_data[usage_key][path_index][path_item_index + 1] ) with self.assertNumQueries(3): _update_xblocks_cache(course.id)
choices = self.xml.xpath('choicegroup/choice') return [choice.get("name") for choice in choices]
axis = self._get_axis_number(kwargs.pop('axis', self._stat_axis_name)) if fill_method is None: data = self else: data = self.fillna(method=fill_method, limit=limit, axis=axis)
expected = CategoricalIndex([0, 1, 2], categories=[0, 1, 2], ordered=True) idx = Index(range(3)) result = CategoricalIndex(idx, categories=idx, ordered=True) tm.assert_index_equal(result, expected, exact=True)
if f.db_column is None and column_name is not None and len(column_name) > allowed_len: errors.append( checks.Error( 'Autogenerated column name too long for field "%s". ' 'Maximum length is "%s" for database "%s".' % (column_name, allowed_len, db_alias), hint="Set the column name manually using 'db_column'.", obj=cls, id='models.E018', ) )
problem = self.build_problem(answer=4, tolerance=0.1, credit_type='bongo') input_dict = {'1_2_1': '4'} with self.assertRaises(LoncapaProblemError): problem.grade_answers(input_dict)
if wt_detached: tags_found = _git_tag_points_at(cwd, wt_head, user) if tags_found: wt_ptr['tags'] = tags_found
object_repr = models.CharField(_('object repr'), max_length=200) action_flag = models.PositiveSmallIntegerField(_('action flag')) change_message = models.TextField(_('change message'), blank=True)
theano_args = [] for space, source in safe_zip(space_tuple, source_tuple): name = '%s[%s]' % (self.__class__.__name__, source) arg = space.make_theano_batch(name=name, batch_size=self.batch_size) theano_args.append(arg) theano_args = tuple(theano_args)
sally = Teacher.objects.create(name='Sally') john = Parent.objects.create(name='John') joe = Child.objects.create(name='Joe', teacher=sally, parent=john) iaf = InlineAdminForm(None, None, {}, {}, joe) parent_ct = ContentType.objects.get_for_model(Parent) self.assertEqual(iaf.original.content_type, parent_ct)
df = sql.read_sql_table("types_test_data", self.conn) check(df.DateColWithTz)
pass
if self.view_converter is None: raise Exception("Tried to call get_weights_view on a dataset " "that has no view converter") return self.view_converter.design_mat_to_weights_view(mat)
delete_course_and_groups(course_key, self.user.id)
return [ credential['credential']['program_id'] for credential in get_user_credentials(student) if 'program_id' in credential['credential'] and credential['status'] == 'awarded' ]
try: self.urlopen('/') except HTTPError as err: self.assertEqual(err.code, 404, 'Expected 404 response') else: self.fail('Expected 404 response')
for i in [tm.makeIntIndex(10), tm.makeFloatIndex(10), tm.makePeriodIndex(10)]: self.assertRaises(TypeError, lambda: frequencies.infer_freq(i))
from __future__ import absolute_import
mylocalrepo: git.config_unset: - name: foo.bar - value_regex: 'baz' - repo: /path/to/repo
self.assertEqual(unfiltered.index.names, ('major', 'minor'))
AccessTestData(xblock_access={1: [1], 2: [2]}), AccessTestData(partition_groups={2: 1, 3: 1}, xblock_access={1: [1], 2: [2]}), AccessTestData(partition_groups={1: 1, 2: 1, 3: 1}, xblock_access={1: [1], 2: [2]}),
self.assertRaises(NotImplementedError, lambda: pd.Categorical(np.array([list('abcd')])))
bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)
autodetector = MigrationAutodetector( loader.project_state(), ProjectState.from_apps(apps), questioner, )
self._setup_mock_request(mock_request, include_depth=(view_name == "create_sub_comment")) response = self.client.post(reverse(view_name, kwargs=view_kwargs), data=data) self.assertEqual(response.status_code, 400) for call in mock_request.call_args_list: self.assertEqual(call[0][0].lower(), "get")
X_4d = np.arange(10 * 5 * 3 * 2).reshape(10, 5, 3, 2) y_3d = np.arange(10 * 7 * 11).reshape(10, 7, 11) check_X = lambda x: x.shape[1:] == (5, 3, 2) check_y = lambda x: x.shape[1:] == (7, 11) clf = CheckingClassifier(check_X=check_X, check_y=check_y) grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}) grid_search.fit(X_4d, y_3d).score(X, y) assert_true(hasattr(grid_search, "grid_scores_"))
self.assertEqual(len(httpretty.httpretty.latest_requests), 3)
s = self.create_series() expected = s.reindex(s.index.take(np.arange(0, len(s.index), 2))) expected.index = expected.index.to_timestamp() expected.index.freq = to_offset('2D')
raise RuntimeError( 'Please set {0}._default_logging_logfile_'.format( self.__class__.__name__ ) )
if hasattr(middleware, 'process_response'): def callback(response): return middleware.process_response(request, response) response.add_post_render_callback(callback)
if is_categorical_dtype(key): c = key
idx = self.create_index() idx = idx.repeat(50) with pd.option_context("display.max_seq_items", None): repr(idx) self.assertFalse('...' in str(idx))
for freq in ['A', '2A', '3A']: p = Period('NaT', freq=freq) for o in [offsets.YearEnd(2)]: self.assertEqual((p + o).ordinal, tslib.iNaT) self.assertEqual((o + p).ordinal, tslib.iNaT)
if self.syndic_mode == 'sync': if 'retcode' not in event['data']: self.raw_events.append(event)
render_failure = (render_failure or default_render_failure) openid_response = openid_views.parse_openid_response(request) if not openid_response: return render_failure(request, 'This is an OpenID relying party endpoint.') if openid_response.status == SUCCESS: external_id = openid_response.identity_url oid_backend = openid_auth.OpenIDBackend() details = oid_backend._extract_user_details(openid_response) log.debug('openid success, details=%s', details) url = getattr(settings, 'OPENID_SSO_SERVER_URL', None) external_domain = "{0}{1}".format(OPENID_DOMAIN_PREFIX, url) fullname = '%s %s' % (details.get('first_name', ''), details.get('last_name', '')) return _external_login_or_signup( request, external_id, external_domain, details, details.get('email', ''), fullname, retfun=functools.partial(redirect, get_next_url_for_login_page(request)), ) return render_failure(request, 'Openid failure')
df.iloc[[0, 1], [0, 1]] = df.iloc[[0, 1], [0, 1]] assert_frame_equal(df, expected)
self.assertNotIn("strict-transport-security", self.process_response(secure=True))
self.course_outline_page.visit() self.set_name_and_verify( self.course_outline_page.section_at(0), 'Test Section', '', 'Test Section' )
IGNORABLE_404_URLS = []
Number.objects.filter(pk=self.n.pk).update(integer=F('integer') ** 2, float=F('float') ** 1.5) self.assertEqual(Number.objects.get(pk=self.n.pk).integer, 1764) self.assertEqual(Number.objects.get(pk=self.n.pk).float, Approximate(61.02, places=2))
rng = date_range('1/1/2000', '3/1/2000') idx = Index(rng, dtype=object)
role, instructions = part.split('=') role = role.upper()
est = DummyRegressor() est.fit(X_learn, y_learn) y_pred_learn = est.predict(X_learn) y_pred_test = est.predict(X_test)
self.check_val('True', True)
actual_events = self.wait_for_events( event_filter={'event_type': 'edx.course.student_notes.searched'}, number_of_matches=1 ) expected_events = [ {'event': {'search_string': search_string, 'number_of_results': number_of_results}} ] self.assert_events_match(expected_events, actual_events)
from __future__ import absolute_import
return value
for fg in (fg1, fg2): with self.assertRaises(GEOSException): fg._get_ptr()
self.video.set_url_field('http://youtu.be/t__eq_exist', 1) self.assertEqual(self.video.message('status'), 'No EdX Timed Transcript') self.assertTrue(self.video.is_transcript_button_visible('import'))
dynamath = [student_answers.get(k + '_dynamath', None) for k in idset]
return json.dumps({'type': self.__class__.__name__, 'coordinates': self.coords})
return []
django_settings.SOCIAL_AUTH_STRATEGY = 'third_party_auth.strategy.ConfigurationModelStrategy'
import salt.runner
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
if len(args) == 1: if isinstance(args[0], (tuple, list)): init_geoms = args[0] else: init_geoms = args else: init_geoms = args
self.assertEqual(len(mail.outbox), 1) msg = mail.outbox[0] self.assertEqual(msg.subject, subject) for fragment in body_fragments: self.assertIn(fragment, msg.body)
from __future__ import absolute_import
ret['comment'] = "Label {0} already set".format(name)
if emit_signals and bulk_ops_record.is_root: self.send_pre_publish_signal(bulk_ops_record, structure_key)
geos_relate = GEOSFuncFactory( 'GEOSRelate', argtypes=[GEOM_PTR, GEOM_PTR], restype=geos_char_p, errcheck=check_string )
import salt.log import salt.utils import salt.utils.master import salt.payload from salt.exceptions import SaltInvocationError from salt.fileserver import clear_lock as _clear_lock from salt.fileserver.gitfs import PER_REMOTE_OVERRIDES as __GITFS_OVERRIDES from salt.pillar.git_pillar \ import PER_REMOTE_OVERRIDES as __GIT_PILLAR_OVERRIDES from salt.runners.winrepo import PER_REMOTE_OVERRIDES as __WINREPO_OVERRIDES
raise SaltInvocationError(kwarg + ' cannot be None')
result = Timedelta(nanoseconds=100) expected = Timedelta('100ns') self.assertEqual(result, expected)
cdn_url = "http://cdn.example.com/static/image.png" with mock.patch('branding.api.staticfiles_storage.url', return_value=cdn_url): resp = self._get_footer()
node = etree.Element('unknown') descriptor.add_xml_to_node(node)
from util.testing import patch_testcase, patch_sessions patch_testcase() patch_sessions()
self.course_hierarchy = self.get_course_hierarchy() self.blocks = self.build_course(self.course_hierarchy) self.course = self.blocks['course']
mock_request.return_value = Mock(META={'HTTP_HOST': 'localhost'}) user = UserFactory.create(is_staff=True) CourseStaffRole(self.course.id).add_users(user) client = AjaxEnabledTestClient() client.login(username=user.username, password=user.password) response = self.client.ajax_post(self.course_setting_url, { 'advanced_modules': {"value": [""]} }) self.assertEqual(response.status_code, 200)
for f in use_expand_hidden: f = f.lower()+ "_" for x in use: if f in x: use.remove(x) archlist = portage.settings["PORTAGE_ARCHLIST"].split() for a in use[:]: if a in archlist: use.remove(a) masked = usemasked + useforced for a in use[:]: if a in masked: use.remove(a) return use
for test_case in solver_shrinkage: solver, shrinkage = test_case clf = LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage) y_pred = clf.fit(X, y).predict(X) assert_array_equal(y_pred, y, 'solver %s' % solver)
long_name += '1' self.assertEqual(len(long_name), 49) self.assertFalse(linode._validate_name(long_name))
mock_get_cache_ttl.return_value = 10 CourseEnrollment.enroll(self.non_staff_usr, self.course_key) self.assertTrue(CourseEnrollment.is_enrolled(self.non_staff_usr, self.course_key)) self.client.login(username=self.non_staff_usr, password='test') resp = self.client.get(self.url_locked) self.assertEqual(resp.status_code, 200) self.assertNotIn('Expires', resp) self.assertEquals('private, no-cache, no-store', resp['Cache-Control'])
import re import inspect
b.authors.remove(p) self.assertEqual(receiver._database, DEFAULT_DB_ALIAS) with self.override_router(): b.authors.remove(p) self.assertEqual(receiver._database, "other")
module = CapaFactory.create(attempts=attempts, max_attempts=attempts, done=True) self.assertFalse(module.should_show_reset_button())
X = dataset.get_design_matrix() m = X.shape[0] assert X.shape[1] == self.nvis
from crum import get_current_request
err_cnt += validate_category_hierarchy( module_store, course_id, "course", "chapter" ) err_cnt += validate_category_hierarchy( module_store, course_id, "chapter", "sequential" ) err_cnt += validate_category_hierarchy( module_store, course_id, "sequential", "vertical" ) warn_cnt += validate_course_policy(module_store, course_id) err_cnt += validate_no_non_editable_metadata( module_store, course_id, "vertical" ) err_cnt += validate_no_non_editable_metadata( module_store, course_id, "chapter" ) err_cnt += validate_no_non_editable_metadata( module_store, course_id, "sequential" )
__virtualname__ = 'kmod'
act.actor.lane_stack.value.server.close() testStack = self.store.fetch('.salt.test.lane.stack') if testStack: testStack.value.server.close()
jid_dir_name = jid_file.rpartition('/')[2] self.assertEqual(jid_dir_name, 'jid')
patch_extractor = sp_extract_patches(IR, IC, KR, KC, CH, RasterOrders.row_col_channel, RasterOrders.row_col_channel, subsample, border_mode, flip_patches=True).tocsc()
new_tab_list = [] for tab_id_locator in requested_tab_id_locators: tab = get_tab_by_tab_id_locator(old_tab_list, tab_id_locator) if tab is None: return JsonResponse( {"error": "Tab with id_locator '{0}' does not exist.".format(tab_id_locator)}, status=400 ) new_tab_list.append(tab)
try: carg = arg.astype(np.float64) return calc_with_mask(carg, com.notnull(carg)) except: pass
orig_settings_dict = self.connection.settings_dict new_settings_dict = orig_settings_dict.copy() new_settings_dict['NAME'] = '{}_{}'.format(orig_settings_dict['NAME'], number) return new_settings_dict
self.app_client = Client.objects.create( user=app_user, name='test client', url='http://localhost//', redirect_uri='http://localhost//', client_type=CONFIDENTIAL ) self.app_grant = Grant.objects.create( user=app_user, client=self.app_client, redirect_uri='http://localhost//' ) self.course.enable_ccx = True self.mstore.update_item(self.course, self.coach.id) self.auth = self.get_auth_token() self.master_course_chapters = get_course_chapters(self.master_course_key)
import logging import json import yaml
clear_counties()
np.random.seed(0) X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]] Y = [0] * 20 + [1] * 20
self.browser.execute_script( '$(".action-preview-username").val("{}").blur().change();'.format(username_or_email) ) self.wait_for_ajax()
L = np.empty((max_features, max_features), dtype=Gram.dtype)
values_int = values.astype(np.int64) consider_values = values_int != iNaT one_day_nanos = (86400 * 1e9) even_days = np.logical_and(consider_values, values_int % one_day_nanos != 0).sum() == 0 all_sub_day = np.logical_and( consider_values, np.abs(values_int) >= one_day_nanos).sum() == 0 if even_days: format = 'even_day' elif all_sub_day: format = 'sub_day' else: format = 'long' def _formatter(x): if x is None or lib.checknull(x): return nat_rep if not isinstance(x, Timedelta): x = Timedelta(x) result = x._repr_base(format=format) if box: result = "'{0}'".format(result) return result return _formatter
_ptr = None
i = 0 m = len(list_n_features) plt.figure('scikit-learn SGD regression benchmark results', figsize=(5 * 2, 4 * m)) for j in range(m): plt.subplot(m, 2, i + 1) plt.plot(list_n_samples, np.sqrt(elnet_results[:, j, 0]), label="ElasticNet") plt.plot(list_n_samples, np.sqrt(sgd_results[:, j, 0]), label="SGDRegressor") plt.plot(list_n_samples, np.sqrt(asgd_results[:, j, 0]), label="A-SGDRegressor") plt.plot(list_n_samples, np.sqrt(ridge_results[:, j, 0]), label="Ridge") plt.legend(prop={"size": 10}) plt.xlabel("n_train") plt.ylabel("RMSE") plt.title("Test error - %d features" % list_n_features[j]) i += 1
self.assertIsNone(default_storage.directory_permissions_mode) with self.settings(FILE_UPLOAD_DIRECTORY_PERMISSIONS=0o777): self.assertEqual(default_storage.directory_permissions_mode, 0o777)
from lxml import etree import unittest import xml.sax.saxutils as saxutils
old_email = self.user.email old_is_staff = self.user.is_staff self.user.email = u'foo@bar.com' self.user.is_staff = True self.user.save() self.assert_user_setting_event_emitted(setting='email', old=old_email, new=self.user.email) self.assert_user_setting_event_emitted(setting='is_staff', old=old_is_staff, new=self.user.is_staff)
self.payment_and_verification_flow.proceed_to_payment()
headers = {"Date": "Fri, 09 Nov 2001 01:08:47 -0000", "Message-ID": "foo"} subject, from_email, to = 'hello', 'from@example.com', 'to@example.com' text_content = 'This is an important message.' html_content = '<p>This is an <strong>important</strong> message.</p>' msg = EmailMultiAlternatives(subject, text_content, from_email, [to], headers=headers) msg.attach_alternative(html_content, "text/html") msg.attach("an attachment.pdf", b"%PDF-1.4.%...", mimetype="application/pdf") msg_bytes = msg.message().as_bytes() message = message_from_bytes(msg_bytes) self.assertTrue(message.is_multipart()) self.assertEqual(message.get_content_type(), 'multipart/mixed') self.assertEqual(message.get_default_type(), 'text/plain') payload = message.get_payload() self.assertEqual(payload[0].get_content_type(), 'multipart/alternative') self.assertEqual(payload[1].get_content_type(), 'application/pdf')
return value.lower()
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'cloudstack', vm_['profile'], vm_=vm_) is False: return False
references = [] for line in lines: line = line.strip() m = re.match(r'^.. \[([a-z0-9_.-])\]', line, re.I) if m: references.append(m.group(1))
try: import boto import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
results.append({ 'identifier': identifier, 'error': error, 'userDoesNotExist': user_does_not_exist })
qs = RasterModel.objects.filter(geom__dwithin=(rast, 500)) self.assertEqual(qs.count(), 1)
return capi.geos_length(self.ptr, byref(c_double()))
message = "Cannot call select_related() after .values() or .values_list()" with self.assertRaisesMessage(TypeError, message): list(Species.objects.values_list('name').select_related('genus'))
FormSet = modelformset_factory(AuthorMeeting, fields='__all__') Author.objects.create(pk=1, name='Charles Baudelaire') data = { 'form-TOTAL_FORMS': 1, 'form-INITIAL_FORMS': 0, 'form-MAX_NUM_FORMS': '', 'form-0-name': '', 'form-0-created': '', 'form-0-authors': list(Author.objects.values_list('id', flat=True)), } formset = FormSet(initial=[{'authors': Author.objects.all()}], data=data) self.assertFalse(formset.extra_forms[0].has_changed())
if self.theano_function_mode != mode: self._dirty = True self.theano_function_mode = mode
EmptyPromise(self.is_upload_finished, 'Upload Finished', timeout=30).fulfill()
def __init__(self, developer_message, user_message=None): self.developer_message = developer_message self.user_message = user_message
self.event = salt.utils.event.get_event( 'master', self.application.opts['sock_dir'], self.application.opts['transport'], opts=self.application.opts, listen=False)
if __opts__['test']: ret['comment'] = ('[stage: {0}] is already at desired state with an associated ' 'deployment matching the given [api_name: {1}] ' 'and [swagger_file: {2}].\n' 'Stage variables will be set ' 'to {3}.'.format(stage_name, api_name, swagger_file, stage_vars)) ret['result'] = None return swagger.overwrite_stage_variables(ret, stage_vars)
casted = mn.astype('O') _check_cast(casted, 'object')
response = self.client.get('/') self.assertContains( response, "Page not found <span>(404)</span>", status_code=404 )
from __future__ import absolute_import import json import logging
info['fmt'][quarter_start] = True info['min'] = True
try: cc_comment = Comment(id=comment_id).retrieve() _, context = _get_thread_and_context(request, cc_comment["thread_id"]) return cc_comment, context except CommentClientRequestError: raise CommentNotFoundError("Comment not found.")
self.assertIsNone(default_storage.file_permissions_mode) with self.settings(FILE_UPLOAD_PERMISSIONS=0o777): self.assertEqual(default_storage.file_permissions_mode, 0o777)
authentication_classes = []
runlevels = ['l1'] level_list_mock = MagicMock(return_value=self.__services({service_name: runlevels})) with patch.dict(gentoo_service.__salt__, {'cmd.run': level_list_mock}): with patch.dict(gentoo_service.__salt__, {'cmd.retcode': rc_update_mock}): self.assertFalse(gentoo_service.disable('name', runlevels='l1')) rc_update_mock.assert_called_once_with('rc-update delete name l1', python_shell=False) rc_update_mock.reset_mock()
non_int_round_dict = {'col1': 1, 'col2': [1, 2]} with self.assertRaises(TypeError): df.round(non_int_round_dict)
sys.path.append(project_path)
assert_raises(ValueError, plot_partial_dependence, clf, iris.data, [0, 1], grid_resolution=grid_resolution)
return ' '.join([self.selector, css])
self.assertFalse(result['success']) self.assertIn(u"badly-typed value", result['error_html'])
FORMAT_MODULE_PATH = None
y = multioutput_estimator_convert_y_2d(name, y)
if not is_numeric_v_string_like(nn, nn_at): comp = (nn == nn_at) if is_list_like(comp) and comp.all(): nv = v.copy() nv[m] = nn_at return nv
self.assertNotContains(response, "How it Works") self.assertNotContains(response, "Schools & Partners")
self.assertEqual(response.status_code, 200)
from sklearn.linear_model import Lasso
if (isinstance(ax, MultiIndex) and not (is_integer(i) or is_null_slice(i))): take_split_path = True break
return r['class'](obj)[0]
metrics_report = (Env.METRICS_DIR / "safecommit") _write_metric(violations_count_str, metrics_report) sh("cat {metrics_report}".format(metrics_report=metrics_report), ignore_error=True)
lhs = DataFrame(rand(5, 2) > 0.5) expect = ~lhs result = pd.eval(expr, engine=self.engine, parser=self.parser) assert_frame_equal(expect, result)
df = DataFrame({'a': [nan, 1, 2, nan, nan], 'b': [1, 2, 3, nan, nan], 'c': [nan, 1, 2, 3, 4]}, index=list('VWXYZ'))
self._rollback = len(boundary) + 6
raise NotImplementedError(str(type(self)) + " does not implement mf.")
offsets = [pd.offsets.Hour(2), timedelta(hours=2), np.timedelta64(2, 'h'), Timedelta(hours=2)]
backend = smtp.EmailBackend( username='not empty username', password='not empty password') try: with self.assertRaisesMessage(SMTPException, 'SMTP AUTH extension not supported by server.'): backend.open() finally: backend.close()
del params["name"] assert_name_error("Your legal name must be a minimum of two characters long")
if self.args: try: if ',' in self.args[1]: self.config['fun'] = self.args[1].split(',') self.config['arg'] = [[]] cmd_index = 0 if (self.args[2:].count(self.options.args_separator) == len(self.config['fun']) - 1): for arg in self.args[2:]: if arg == self.options.args_separator: cmd_index += 1 self.config['arg'].append([]) else: self.config['arg'][cmd_index].append(arg) else: for arg in self.args[2:]: if self.options.args_separator in arg: sub_args = arg.split(self.options.args_separator) for sub_arg_index, sub_arg in enumerate(sub_args): if sub_arg: self.config['arg'][cmd_index].append(sub_arg) if sub_arg_index != len(sub_args) - 1: cmd_index += 1 self.config['arg'].append([]) else: self.config['arg'][cmd_index].append(arg) if len(self.config['fun']) != len(self.config['arg']): self.exit(42, 'Cannot execute compound command without ' 'defining all arguments.\n') for i in range(len(self.config['arg'])): self.config['arg'][i] = salt.utils.args.parse_input( self.config['arg'][i]) else: self.config['fun'] = self.args[1] self.config['arg'] = self.args[2:] self.config['arg'] = \ salt.utils.args.parse_input(self.config['arg']) except IndexError: self.exit(42, '\nIncomplete options passed.\n\n')
payment_support_email = microsite.get_value('payment_support_email', settings.PAYMENT_SUPPORT_EMAIL) return _format_error_html( _( "Sorry! Our payment processor did not accept your payment. " "The decision they returned was {decision}, " "and the reason was {reason}. " "You were not charged. Please try a different form of payment. " "Contact us with payment-related questions at {email}." ).format( decision='<span class="decision">{decision}</span>'.format(decision=params['decision']), reason='<span class="reason">{reason_code}:{reason_msg}</span>'.format( reason_code=params['reason_code'], reason_msg=REASONCODE_MAP.get(params['reason_code']) ), email=payment_support_email ) )
key = labels._maybe_cast_indexer(key)
return edxval_api.get_video_info_for_course_and_profiles(unicode(course_id), video_profile_names)
image_id = email_image.get('Content-ID', '')[1:-1] self.assertIsNotNone(image_id) self.assertIn(image_id, html_content_first) self.assertIn( 'credit from Hogwarts School of Witchcraft and Wizardry for', html_content_first )
for unit in ['Y', 'M', 'W', 'D', 'y', 'w', 'd']: testit(unit, lambda x: x.upper()) for unit in ['days', 'day', 'Day', 'Days']: testit(unit, lambda x: 'D') for unit in ['h', 'm', 's', 'ms', 'us', 'ns', 'H', 'S', 'MS', 'US', 'NS']: testit(unit, lambda x: x.lower())
to_delete, to_create = _get_rule_changes(rules, sg['rules']) if to_create or to_delete: if __opts__['test']: msg = 'Security group {0} set to have rules modified.'.format(name) ret['comment'] = msg ret['result'] = None return ret if to_delete: deleted = True for rule in to_delete: _deleted = __salt__['boto_secgroup.revoke']( name, vpc_id=vpc_id, vpc_name=vpc_name, region=region, key=key, keyid=keyid, profile=profile, **rule) if not _deleted: deleted = False if deleted: msg = 'Removed rules on {0} security group.'.format(name) ret['comment'] = msg else: msg = 'Failed to remove rules on {0} security group.' ret['comment'] = msg.format(name) ret['result'] = False if to_create: created = True for rule in to_create: _created = __salt__['boto_secgroup.authorize']( name, vpc_id=vpc_id, vpc_name=vpc_name, region=region, key=key, keyid=keyid, profile=profile, **rule) if not _created: created = False if created: msg = 'Created rules on {0} security group.' ret['comment'] = ' '.join([ret['comment'], msg.format(name)]) else: msg = 'Failed to create rules on {0} security group.' ret['comment'] = ' '.join([ret['comment'], msg.format(name)]) ret['result'] = False ret['changes']['old'] = {'rules': sg['rules']} sg = __salt__['boto_secgroup.get_config'](name=name, group_id=None, region=region, key=key, keyid=keyid, profile=profile, vpc_id=vpc_id, vpc_name=vpc_name) ret['changes']['new'] = {'rules': sg['rules']} return ret
from __future__ import absolute_import, print_function import json import logging
data_home = get_data_home(data_home=data_home) lfw_home = join(data_home, "lfw_home") if funneled: archive_path = join(lfw_home, FUNNELED_ARCHIVE_NAME) data_folder_path = join(lfw_home, "lfw_funneled") archive_url = BASE_URL + FUNNELED_ARCHIVE_NAME else: archive_path = join(lfw_home, ARCHIVE_NAME) data_folder_path = join(lfw_home, "lfw") archive_url = BASE_URL + ARCHIVE_NAME if not exists(lfw_home): makedirs(lfw_home) for target_filename in TARGET_FILENAMES: target_filepath = join(lfw_home, target_filename) if not exists(target_filepath): if download_if_missing: url = BASE_URL + target_filename logger.warning("Downloading LFW metadata: %s", url) urllib.urlretrieve(url, target_filepath) else: raise IOError("%s is missing" % target_filepath) if not exists(data_folder_path): if not exists(archive_path): if download_if_missing: logger.warning("Downloading LFW data (~200MB): %s", archive_url) urllib.urlretrieve(archive_url, archive_path) else: raise IOError("%s is missing" % target_filepath) import tarfile logger.info("Decompressing the data archive to %s", data_folder_path) tarfile.open(archive_path, "r:gz").extractall(path=lfw_home) remove(archive_path) return lfw_home, data_folder_path
return cls.objects.filter(user=user, team__course_id=course_id).exists()
ALL_LANGUAGES = ( [u"aa", u"Afar"], [u"ab", u"Abkhazian"], [u"af", u"Afrikaans"], [u"ak", u"Akan"], [u"sq", u"Albanian"], [u"am", u"Amharic"], [u"ar", u"Arabic"], [u"an", u"Aragonese"], [u"hy", u"Armenian"], [u"as", u"Assamese"], [u"av", u"Avaric"], [u"ae", u"Avestan"], [u"ay", u"Aymara"], [u"az", u"Azerbaijani"], [u"ba", u"Bashkir"], [u"bm", u"Bambara"], [u"eu", u"Basque"], [u"be", u"Belarusian"], [u"bn", u"Bengali"], [u"bh", u"Bihari languages"], [u"bi", u"Bislama"], [u"bs", u"Bosnian"], [u"br", u"Breton"], [u"bg", u"Bulgarian"], [u"my", u"Burmese"], [u"ca", u"Catalan"], [u"ch", u"Chamorro"], [u"ce", u"Chechen"], [u"zh", u"Chinese"], [u"zh_HANS", u"Simplified Chinese"], [u"zh_HANT", u"Traditional Chinese"], [u"cu", u"Church Slavic"], [u"cv", u"Chuvash"], [u"kw", u"Cornish"], [u"co", u"Corsican"], [u"cr", u"Cree"], [u"cs", u"Czech"], [u"da", u"Danish"], [u"dv", u"Divehi"], [u"nl", u"Dutch"], [u"dz", u"Dzongkha"], [u"en", u"English"], [u"eo", u"Esperanto"], [u"et", u"Estonian"], [u"ee", u"Ewe"], [u"fo", u"Faroese"], [u"fj", u"Fijian"], [u"fi", u"Finnish"], [u"fr", u"French"], [u"fy", u"Western Frisian"], [u"ff", u"Fulah"], [u"ka", u"Georgian"], [u"de", u"German"], [u"gd", u"Gaelic"], [u"ga", u"Irish"], [u"gl", u"Galician"], [u"gv", u"Manx"], [u"el", u"Greek"], [u"gn", u"Guarani"], [u"gu", u"Gujarati"], [u"ht", u"Haitian"], [u"ha", u"Hausa"], [u"he", u"Hebrew"], [u"hz", u"Herero"], [u"hi", u"Hindi"], [u"ho", u"Hiri Motu"], [u"hr", u"Croatian"], [u"hu", u"Hungarian"], [u"ig", u"Igbo"], [u"is", u"Icelandic"], [u"io", u"Ido"], [u"ii", u"Sichuan Yi"], [u"iu", u"Inuktitut"], [u"ie", u"Interlingue"], [u"ia", u"Interlingua"], [u"id", u"Indonesian"], [u"ik", u"Inupiaq"], [u"it", u"Italian"], [u"jv", u"Javanese"], [u"ja", u"Japanese"], [u"kl", u"Kalaallisut"], [u"kn", u"Kannada"], [u"ks", u"Kashmiri"], [u"kr", u"Kanuri"], [u"kk", u"Kazakh"], [u"km", u"Central Khmer"], [u"ki", u"Kikuyu"], [u"rw", u"Kinyarwanda"], [u"ky", u"Kirghiz"], [u"kv", u"Komi"], [u"kg", u"Kongo"], [u"ko", u"Korean"], [u"kj", u"Kuanyama"], [u"ku", u"Kurdish"], [u"lo", u"Lao"], [u"la", u"Latin"], [u"lv", u"Latvian"], [u"li", u"Limburgan"], [u"ln", u"Lingala"], [u"lt", u"Lithuanian"], [u"lb", u"Luxembourgish"], [u"lu", u"Luba-Katanga"], [u"lg", u"Ganda"], [u"mk", u"Macedonian"], [u"mh", u"Marshallese"], [u"ml", u"Malayalam"], [u"mi", u"Maori"], [u"mr", u"Marathi"], [u"ms", u"Malay"], [u"mg", u"Malagasy"], [u"mt", u"Maltese"], [u"mn", u"Mongolian"], [u"na", u"Nauru"], [u"nv", u"Navajo"], [u"nr", u"Ndebele, South"], [u"nd", u"Ndebele, North"], [u"ng", u"Ndonga"], [u"ne", u"Nepali"], [u"nn", u"Norwegian Nynorsk"], [u"nb", u"Bokmål, Norwegian"], [u"no", u"Norwegian"], [u"ny", u"Chichewa"], [u"oc", u"Occitan"], [u"oj", u"Ojibwa"], [u"or", u"Oriya"], [u"om", u"Oromo"], [u"os", u"Ossetian"], [u"pa", u"Panjabi"], [u"fa", u"Persian"], [u"pi", u"Pali"], [u"pl", u"Polish"], [u"pt", u"Portuguese"], [u"ps", u"Pushto"], [u"qu", u"Quechua"], [u"rm", u"Romansh"], [u"ro", u"Romanian"], [u"rn", u"Rundi"], [u"ru", u"Russian"], [u"sg", u"Sango"], [u"sa", u"Sanskrit"], [u"si", u"Sinhala"], [u"sk", u"Slovak"], [u"sl", u"Slovenian"], [u"se", u"Northern Sami"], [u"sm", u"Samoan"], [u"sn", u"Shona"], [u"sd", u"Sindhi"], [u"so", u"Somali"], [u"st", u"Sotho, Southern"], [u"es", u"Spanish"], [u"sc", u"Sardinian"], [u"sr", u"Serbian"], [u"ss", u"Swati"], [u"su", u"Sundanese"], [u"sw", u"Swahili"], [u"sv", u"Swedish"], [u"ty", u"Tahitian"], [u"ta", u"Tamil"], [u"tt", u"Tatar"], [u"te", u"Telugu"], [u"tg", u"Tajik"], [u"tl", u"Tagalog"], [u"th", u"Thai"], [u"bo", u"Tibetan"], [u"ti", u"Tigrinya"], [u"to", u"Tonga (Tonga Islands)"], [u"tn", u"Tswana"], [u"ts", u"Tsonga"], [u"tk", u"Turkmen"], [u"tr", u"Turkish"], [u"tw", u"Twi"], [u"ug", u"Uighur"], [u"uk", u"Ukrainian"], [u"ur", u"Urdu"], [u"uz", u"Uzbek"], [u"ve", u"Venda"], [u"vi", u"Vietnamese"], [u"vo", u"Volapük"], [u"cy", u"Welsh"], [u"wa", u"Walloon"], [u"wo", u"Wolof"], [u"xh", u"Xhosa"], [u"yi", u"Yiddish"], [u"yo", u"Yoruba"], [u"za", u"Zhuang"], [u"zu", u"Zulu"] )
cmd = '. {0} ; {1} '.format(path, config) cmd += '| Select-Object -Property FullName, Extension, Exists, ' \ '@{Name="LastWriteTime";Expression={Get-Date ($_.LastWriteTime) -Format g}}'
parent = modulestore.get_parent_location(next_usage)
_AXES_MAP = { DataFrame: [0], Panel: [1, 2], Panel4D: [1, 2, 3], }
site_name = microsite.get_value('SITE_NAME', settings.SITE_NAME) parts = ("https" if is_secure else "http", site_name, url_path, '', '', '') return urlparse.urlunparse(parts)
with mock.patch( 'xmodule.modulestore.mixed.MixedModuleStore.clone_course', mock.Mock(side_effect=Exception()), ): source_course = CourseFactory.create() message_too_long = "traceback".rjust(CourseRerunState.MAX_MESSAGE_LENGTH * 2, '-') with mock.patch('traceback.format_exc', return_value=message_too_long): destination_course_key = self.post_rerun_request(source_course.id) rerun_state = CourseRerunState.objects.find_first(course_key=destination_course_key) self.assertEquals(rerun_state.state, CourseRerunUIStateManager.State.FAILED) self.assertTrue(rerun_state.message.endswith("traceback")) self.assertEqual(len(rerun_state.message), CourseRerunState.MAX_MESSAGE_LENGTH)
def _translate_newlines(self, data): if data is None or not data: return return data.replace('\r\n', os.linesep)
return self._decision_function(X)
if not settings.FEATURES.get('MILESTONES_APP', False): return [] from milestones import api as milestones_api return milestones_api.get_course_milestones(course_id)
conn.get_instance_profile(name) return True
return redirect(reverse('cas-login'))
test_routers = (object(),) with self.settings(DATABASE_ROUTERS=test_routers): self.assertSequenceEqual(router.routers, test_routers)
if passwordless: if not salt.utils.is_true(allow_passwordless): ret['comment'] = 'Either password or password_hash must be ' \ 'specified, unless allow_passwordless is True' ret['result'] = False return ret else: if __salt__['mysql.user_exists'](name, host, passwordless=True, unix_socket=unix_socket, password_column=password_column, **connection_args): ret['comment'] += ' with passwordless login' return ret else: err = _get_mysql_error() if err is not None: ret['comment'] = err ret['result'] = False return ret else: if __salt__['mysql.user_exists'](name, host, password, password_hash, unix_socket=unix_socket, password_column=password_column, **connection_args): ret['comment'] += ' with the desired password' if password_hash and not password: ret['comment'] += ' hash' return ret else: err = _get_mysql_error() if err is not None: ret['comment'] = err ret['result'] = False return ret
scorer = get_scorer('mean_squared_error') ridge_gcv4 = RidgeCV(fit_intercept=False, scoring=scorer) ridge_gcv4.fit(filter_(X_diabetes), y_diabetes) assert_equal(ridge_gcv4.alpha_, alpha_)
try: return getattr(self, '_%s_cache' % related_name) except AttributeError: pass
if isinstance(x, np.ndarray): return np.asarray(x) elif isinstance(x, Sequence): return x else: return list(x)
course = CourseFactory.create(start=datetime(2013, 9, 16, 7, 17, 28)) course = modulestore().get_course(course.id) return course
result = idx / 2
X = iris.data
from __future__ import print_function
n_samples = 500
target_srs = SpatialReference(srid)
if auto_created: self.creation_counter = Field.auto_creation_counter Field.auto_creation_counter -= 1 else: self.creation_counter = Field.creation_counter Field.creation_counter += 1
if not is_list_like(new_values) or self.ndim == 1: return _maybe_box_datetimelike(new_values)
self.assertEqual(df.asfreq('D').index.freq, 'D')
self.assertEqual(len(mail.outbox), 1) self.assertEqual(len(mail.outbox[0].to), 1) self.assertEquals(mail.outbox[0].to[0], self.instructor.email) self.assertEquals(mail.outbox[0].subject, 'test subject for myself') self.assertEquals( mail.outbox[0].from_email, u'"{course_display_name}" Course Staff <{course_name}-no-reply@example.com>'.format( course_display_name=self.course.display_name, course_name=self.course.id.course ) )
self.input_space.validate(state_below) if self.requires_reformat: if not isinstance(state_below, tuple): for sb in get_debug_values(state_below): if sb.shape[0] != self.dbm.batch_size: raise ValueError("self.dbm.batch_size is %d but got " + "shape of %d" % (self.dbm.batch_size, sb.shape[0])) assert reduce(operator.mul, sb.shape[1:]) == self.input_dim state_below = self.input_space.format_as(state_below, self.desired_space) if iter_name is None: iter_name = 'anon' if state_above is not None: assert layer_above is not None msg = layer_above.downward_message(state_above) msg.name = 'msg_from_' + layer_above.layer_name + '_to_' + \ self.layer_name + '[' + iter_name + ']' else: msg = None if double_weights: state_below = 2. * state_below state_below.name = self.layer_name + '_'+iter_name + '_2state' z = self.transformer.lmul(state_below) + self.b if self.layer_name is not None and iter_name is not None: z.name = self.layer_name + '_' + iter_name + '_z' if msg is not None: z = z + msg h = T.tanh(self.beta * z) return h
data_x = np.cast[config.floatX](data['data']) data_x = data_x[MNISTPlus.idx[which_set]]
self.course_outline.visit() self.course_outline.open_subsection_settings_dialog(1) self.course_outline.select_access_tab() self.assertTrue(self.course_outline.gating_prerequisite_checkbox_is_visible()) self.assertTrue(self.course_outline.gating_prerequisites_dropdown_is_visible()) self.assertTrue(self.course_outline.gating_prerequisite_min_score_is_visible())
break
self.create_programs_config() self.mock_programs_api(data={'results': []})
with make_uploaded_file(extension=extension, content_type=content_type) as uploaded_file: self.check_validation_result(uploaded_file, expected_failure_message)
self.fmtlist = ["%td" if x.startswith("%td") else x for x in self.fmtlist]
n_components = 100 n_features = 500
result = ci[:3].append(ci[3:]) tm.assert_index_equal(result, ci, exact=True)
courses = split_store.get_courses(BRANCH_NAME_DRAFT)
shutil.rmtree(clone_parent_dir)
axes = plotting._flatten(axes) axes = [ax for ax in axes if ax.get_visible()] return axes
self.h_space.validate(downward_state) return self.transformer.lmul_T(downward_state)
baseqs = Author.objects.order_by('name') self.assertIn( '<= (2011 || ', str(baseqs.filter(birthdate__testyear__lte=2011).query)) self.assertIn( '-12-31', str(baseqs.filter(birthdate__testyear__lte=2011).query))
email_image = email_payload_first[1]
self.set_group_access( block_specified, {self.animal_partition.id: [self.cat_group.id]}, ) self.check_access(self.red_cat, block_accessed, True) self.check_access(self.blue_dog, block_accessed, False) self.check_access(self.white_mouse, block_accessed, False) self.check_access(self.gray_worm, block_accessed, False) self.ensure_staff_access(block_accessed)
self.assertRedirects(response, '/permanent_redirect_view/', target_status_code=301)
values = (c_double * 6)(*values) capi.set_ds_geotransform(self._ptr, byref(values)) self._flush()
if name not in loader.file_mapping: return {}
exc_info_on_loglevel=logging.DEBUG
return {'STATIC_URL': settings.STATIC_URL}
elif (len(arr_value.shape) and arr_value.shape[0] == values.shape[0] and np.prod(arr_value.shape) == np.prod(values.shape)): values[indexer] = value values = values.astype(arr_value.dtype)
for key, value in configs: self.setConfig(key, value)
self.user.is_active = False self.user.save()
for child in xblock.get_children():
if support_sample_weight: if sample_weight is None: curr_sample_weight = np.ones((n_samples,)) else: curr_sample_weight = sample_weight.copy()
link_tgt = self.repo[tree[path].oid].data path = os.path.normpath( os.path.join(os.path.dirname(path), link_tgt) )
return dict(dic1.items() + dic2.items())
user.is_active = False user.save() user_fetched = UserModel._default_manager.get(pk=user.pk) self.assertEqual(user_fetched.is_active, True)
__virtualname__ = 'whoosh'
from salt.states import drac
instance = self multiprocessing_enabled = self.opts.get('multiprocessing', True) if multiprocessing_enabled: if sys.platform.startswith('win'): instance = None with default_signals(signal.SIGINT, signal.SIGTERM): process = SignalHandlingMultiprocessingProcess( target=self._target, args=(instance, self.opts, data, self.connected) ) else: process = threading.Thread( target=self._target, args=(instance, self.opts, data, self.connected), name=data['jid'] )
if not scopes: scopes = ['default'] return ' '.join(scopes)
exclude_func = lambda s: s['user__email'] in exclude return [make(s) for s in values if not exclude_func(s)]
if idx >= len(self): raise IndexError(idx) self.f.seek(self.f_start + idx * self.elsize * self.readsize) return numpy.fromfile(self.f, dtype=self.magic_t, count=self.readsize).reshape(self.returnshape)
import logging import copy
State.objects.create(name='Puerto Rico')
__virtualname__ = 'syslog'
import logging from uuid import uuid4 try: import couchdb HAS_COUCH = True except ImportError: HAS_COUCH = False
sample = None if len(ndims) > 1: max_ndim = max(ndims) for obj in objs: if obj.ndim == max_ndim and np.sum(obj.shape): sample = obj break
reg = RidgeClassifier(class_weight='balanced') reg.fit(X, y) assert_array_equal(reg.predict([[0.2, -1.0]]), np.array([1]))
assert_equal((804414, 47236), X1.shape) assert_equal((804414, 103), Y1.shape) assert_equal((804414,), s1.shape) assert_equal(103, len(cat_list))
with self.assertRaises(ValueError): (DataFrame(np.ones((10, 10))) .rolling(window=3, center=True, axis=2).mean())
raise
self.assertEqual(list(iter_format_modules('de')), [default_mod])
self.lib_page.click_duplicate_button(first_block_id) self.assertEqual(len(self.lib_page.xblocks), 2) second_block_id = self.lib_page.xblocks[1].locator self.assertNotEqual(first_block_id, second_block_id)
return self._ic['aic']
cache_key = self._cache_key_for_kvs_key(kvs_key) if cache_key not in self._cache: raise KeyError(kvs_key.field_name) return self._cache[cache_key][kvs_key.field_name]
placeholder_rows, param_rows = zip(*sql_and_param_pair_rows)
self.cart.order_type = 'business' self.cart.save() CourseRegCodeItem.add_to_order(self.cart, self.course_key, 2) self.cart.purchase() reg_code = CourseRegistrationCode.order_generated_registration_codes(self.course_key)[0] enrollment = CourseEnrollment.enroll(self.user, self.course_key) redemption = RegistrationCodeRedemption( registration_code=reg_code, redeemed_by=self.user, course_enrollment=enrollment ) redemption.save() test_redemption = RegistrationCodeRedemption.registration_code_used_for_enrollment(enrollment) self.assertEqual(test_redemption.id, redemption.id)
df = orig.copy() df.ix["j", :] = ["b", 2] tm.assert_frame_equal(df, exp_single_row)
if len(self.frame.columns) == 0 or len(self.frame.index) == 0: info_line = (u('Empty %s\nColumns: %s\nIndex: %s') % (type(self.frame).__name__, self.frame.columns, self.frame.index)) strcols = [[info_line]] else: strcols = self.fmt._to_str_columns()
if block.service_declaration(service_name) is not None: if service_name == "user": return DjangoXBlockUserService(self._user) if service_name == "studio_user_permissions": return StudioPermissionsService(self._user) return None
if parts[1] == 'job': logger.debug('In job part 1') if parts[3] == 'new': logger.debug('In new job') self.process_new_job_event(salt_data) elif parts[3] == 'ret': logger.debug('In ret') self.process_ret_job_event(salt_data) if salt_data['data']['fun'] == 'grains.items': self.process_minion_update(salt_data) elif parts[1] == 'key': logger.debug('In key') self.process_key_event(salt_data) elif parts[1] == 'presence': self.process_presence_events(salt_data, token, opts)
Donut.objects.create( name='Date Test 2007', baked_date=datetime.datetime(year=2007, month=12, day=31), consumed_at=datetime.datetime(year=2007, month=12, day=31, hour=23, minute=59, second=59), ) Donut.objects.create( name='Date Test 2006', baked_date=datetime.datetime(year=2006, month=1, day=1), consumed_at=datetime.datetime(year=2006, month=1, day=1), ) self.assertEqual("Date Test 2007", Donut.objects.filter(baked_date__year=2007)[0].name) self.assertEqual("Date Test 2006", Donut.objects.filter(baked_date__year=2006)[0].name) Donut.objects.create( name='Apple Fritter', consumed_at=datetime.datetime(year=2007, month=4, day=20, hour=16, minute=19, second=59), ) self.assertEqual( ['Apple Fritter', 'Date Test 2007'], list(Donut.objects.filter(consumed_at__year=2007).order_by('name').values_list('name', flat=True)) ) self.assertEqual(0, Donut.objects.filter(consumed_at__year=2005).count()) self.assertEqual(0, Donut.objects.filter(consumed_at__year=2008).count())
return [mark_safe(force_text(obj)) for obj in value]
assert self.h_space.axes == ('b', 'c', 0, 1) assert self.output_space.axes == ('b', 'c', 0, 1) mx = s.max(axis=3).max(axis=2).max(axis=0) assert hasattr(mx.owner.op, 'grad') mn = s.min(axis=3).max(axis=2).max(axis=0) assert hasattr(mn.owner.op, 'grad') assert mx.ndim == 1 assert mn.ndim == 1 r = mx - mn rval += (1. - r).mean() * c
missing_node_cache(prov_dir, nodes, provider, opts)
df = tm.makeTimeDataFrame() df['string'] = 'foo' df['string2'] = 'bar' store.append('f', df, data_columns=['string', 'string2']) assert(col('f', 'index').is_indexed is True) assert(col('f', 'string').is_indexed is True) assert(col('f', 'string2').is_indexed is True)
if nowait: return 'FOR UPDATE NOWAIT' else: return 'FOR UPDATE'
from __future__ import absolute_import
from salt.states import ssh_known_hosts
self.q(css=selector)[0].send_keys(value)
from sklearn.mixture import sample_gaussian x = sample_gaussian([0, 0], [[4, 3], [1, .1]], covariance_type='full', random_state=42) assert_true(np.isfinite(x).all())
self.assertEqual(buffer_o, expected_data) self.assertFalse(term.isalive())
msg_text = self.register_page.wait_for_auth_status_message() self.assertEqual(self.register_page.current_form, "register") self.assertIn("You've successfully signed into Dummy", msg_text) self.assertIn("We just need a little more information", msg_text)
return not re.search(r'to be filled', val, flags=re.IGNORECASE) \ and not re.search(r'un(known|specified)|no(t|ne)? (asset|provided|defined|available|present|specified)', val, flags=re.IGNORECASE)
test_course_data = self.assert_created_course(number_suffix=uuid4().hex) self.assertTrue(are_permissions_roles_seeded(_get_course_id(self.store, test_course_data)))
self = object.__new__(cls) self._constructor_args = (args, kwargs) return self
rfecv_sparse = RFECV(estimator=SVC(kernel="linear"), step=1, cv=5) X_sparse = sparse.csr_matrix(X) rfecv_sparse.fit(X_sparse, y) X_r_sparse = rfecv_sparse.transform(X_sparse) assert_array_equal(X_r_sparse.toarray(), iris.data)
self.store.convert_to_draft(item_location, self.user_id) item = self.store.get_item(item_location) self.assertTrue(self.store.has_published_version(item))
user_input = '[{"a":"target1"}, \ {"b":"target2"},{"c":"target3"},{"b":"target5"}, \ {"c":"target6"}, {"a":"target7"},{"b":"target8"},{"c":"target9"}, \ {"a":"target1"}]' correct_answer = [ { 'draggables': ['a', 'a', 'a'], 'targets': ['target1', 'target4', 'target7', 'target10'], 'rule': 'anyof+number' }, { 'draggables': ['b', 'b', 'b'], 'targets': ['target2', 'target5', 'target8'], 'rule': 'anyof+number' }, { 'draggables': ['c', 'c', 'c'], 'targets': ['target3', 'target6', 'target9'], 'rule': 'anyof+number' } ] self.assertTrue(draganddrop.grade(user_input, correct_answer))
if service_name == 'SSH' or service_name == 'ssh': temp_service_name = 'TSM-SSH' else: temp_service_name = service_name
lr = "pa1" if self.loss == "epsilon_insensitive" else "pa2" return self._partial_fit(X, y, alpha=1.0, C=self.C, loss="epsilon_insensitive", learning_rate=lr, n_iter=1, sample_weight=None, coef_init=None, intercept_init=None)
target_id = self.store.make_course_key('MITx', '111', '2013_Spring') course_data = { 'org': target_id.org, 'number': target_id.course, 'display_name': 'Robot Super Course', 'run': target_id.run } _create_course(self, target_id, course_data)
cert = request._req.subprocess_env.get(certkey, '')
enrollment.deactivate() self.assertFalse(CourseEnrollment.is_enrolled(user, course_id)) self.assert_unenrollment_event_was_emitted(user, course_id)
pass
self.assertNotIn('country_code', request.session) self.assertNotIn('ip_address', request.session) self.country_middleware.process_request(request) self.assertEqual('CN', request.session.get('country_code')) self.assertEqual( '2001:da8:20f:1502:edcf:550b:4a9c:207d', request.session.get('ip_address'))
user_partititons = self.course.user_partitions
self.assertNotContains(resp, 'This is a Test Microsite footer')
try: valid_strings = set( ('0', '1', '2', '3', '4', '5', '6', 's', 'S', '-s', 'single')) with salt.utils.fopen('/proc/cmdline') as fp_: for line in fp_: for arg in line.strip().split(): if arg in valid_strings: runlevel = arg break except Exception: pass
kwargs = {'hostname': host, 'creds': creds}
if self.optimizer not in self._optimizer_types: raise ValueError("optimizer should be one of %s" % self._optimizer_types)
digest = hashlib.sha256(aes).hexdigest() ret['sig'] = salt.crypt.private_encrypt(self.master_key.key, digest) eload = {'result': True, 'act': 'accept', 'id': load['id'], 'pub': load['pub']} self.event.fire_event(eload, salt.utils.event.tagify(prefix='auth')) return ret
return xml_import_data.policy.get(policy_key(usage_id), {})
product = partial(reduce, operator.mul) return sum(product(len(v) for v in p.values()) if p else 1 for p in self.param_grid)
option2=main2
init_space, source = self.data_specs X_space, init_y_space = init_space.components new_y_space = VectorSpace(dim=num_classes) new_space = CompositeSpace((X_space, new_y_space)) self.data_specs = (new_space, source)
self._install_masquerade(self.course_staff) self.assertEqual( 'student', access.get_user_role(self.course_staff, self.course_key) )
with self.assertRaises(FieldError): Book.objects.all().aggregate(num_authors=Count('foo'))
n_samples_per_fold = np.zeros(n_folds)
X = np.dot(h, W) + noise
idx = DatetimeIndex(['2000-01-01', '2000-01-02', '2000-01-03']) self.assertEqual(idx.freqstr, None) tm.assert_index_equal(idx.to_period(), expected)
self._corrupt_switch = True
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
before = self.make_project_state([self.author_name]) after = self.make_project_state([self.book, self.author_with_book_order_wrt]) autodetector = MigrationAutodetector(before, after) changes = autodetector._detect_changes() self.assertNumberMigrations(changes, 'testapp', 1) self.assertOperationTypes(changes, 'testapp', 0, ["AddField", "AlterOrderWithRespectTo"]) self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name="author", name="book") self.assertOperationAttributes(changes, 'testapp', 0, 1, name="author", order_with_respect_to="book")
import salt.log.setup
cls._field_list_filters.insert( cls._take_priority_index, (test, list_filter_class)) cls._take_priority_index += 1
#html_use_modindex = True
def setUp(self): super(CustomValidationTestCase, self).setUp() self.user = User.objects.create_user( username='darkhelmet', password='12345', email='darkhelmet@spaceball_one.org', ) self.validator = EdxOAuth2Validator() self.request_factory = RequestFactory() def test_active_user_validates(self): self.assertTrue(self.user.is_active) request = self.request_factory.get('/') self.assertTrue(self.validator.validate_user('darkhelmet', '12345', client=None, request=request)) def test_inactive_user_validates(self): self.user.is_active = False self.user.save() request = self.request_factory.get('/') self.assertTrue(self.validator.validate_user('darkhelmet', '12345', client=None, request=request))
from __future__ import absolute_import import copy import contextlib
with self.assertRaisesRegexp(CommandError, ".* is not a library key"): call_command('reindex_library', unicode(self.first_course.id)) with self.assertRaisesRegexp(CommandError, ".* is not a library key"): call_command('reindex_library', unicode(self.second_course.id)) with self.assertRaisesRegexp(CommandError, ".* is not a library key"): call_command( 'reindex_library', unicode(self.second_course.id), unicode(self._get_lib_key(self.first_lib)) )
new_data = self[res]
destroy(vm_['name'])
s = df['dt'].copy() s = klass([v for v in s.values] + [pd.NaT])
help = ('Usage: ' 'git_add_course repository_url [directory to check out into] [repository_branch] ' '\n{0}'.format(_('Import the specified git repository and optional branch into the ' 'modulestore and optionally specified directory.')))
with patch_logger('django.server', 'info'): WSGIRequestHandler(request, '192.168.0.2', server)
do_external_auth = 'ExternalAuthMap' in request.session if do_external_auth: eamap = request.session['ExternalAuthMap'] try: validate_email(eamap.external_email) params["email"] = eamap.external_email except ValidationError: pass if eamap.external_name.strip() != '': params["name"] = eamap.external_name params["password"] = eamap.internal_password log.debug(u'In create_account with external_auth: user = %s, email=%s', params["name"], params["email"])
parameters = { 'Format': 'JSON', 'Version': DEFAULT_ALIYUN_API_VERSION, 'AccessKeyId': access_key_id, 'SignatureVersion': '1.0', 'SignatureMethod': 'HMAC-SHA1', 'SignatureNonce': str(uuid.uuid1()), 'TimeStamp': timestamp, }
for user, course_id, opt_in_pref in args: self.assertIn({ "email": user.email.encode('utf-8'), "full_name": user.profile.name.encode('utf-8'), "course_id": unicode(course_id).encode('utf-8'), "is_opted_in_for_email": unicode(opt_in_pref), "preference_set_datetime": ( self._latest_pref_set_datetime(self.user) if kwargs.get("expect_pref_datetime", True) else self.DEFAULT_DATETIME_STR ) }, output[1:])
if os.path.isdir(base_dir / url_name): self._load_extra_content(system, course_descriptor, category, base_dir / url_name, course_dir)
return self.name_to_event_type_map[self.name]
course = Course( validated_data["id"], self._new_course_mode_models(validated_data["modes"]), verification_deadline=validated_data["verification_deadline"] ) course.save() return course
self.assertFalse(SignatureValidator(self.lti_consumer).check_nonce(nonce))
for i in range(0, lenbin - 1): r_bin = binner[i + 1]
split_test = self._update_partition_id(-50) self.assertEqual(2, len(split_test.children)) self.assertEqual(initial_group_id_to_child, split_test.group_id_to_child)
old_name = settings.DATABASES[db.DEFAULT_DB_ALIAS]["NAME"] with mock.patch('django.db.connections', new=tested_connections): tested_connections['default'].creation.destroy_test_db(old_name, verbosity=0, keepdb=True) self.assertEqual(tested_connections['default'].settings_dict["NAME"], old_name)
if block.fields[field_name].is_set_on(block): return getattr(block, field_name) else: return default_value
try: groupSID, domainName, objectType = win32security.LookupAccountName(None, pgroup) except pywinerror: err += 'Group does not exist\n'
MAXSIZE = int((1 << 31) - 1)
import os import sys import time import errno import select import logging import tempfile import subprocess
cmd = ['git', 'ls-remote', 'origin', '-h', 'refs/heads/{0}'.format(branch), ] try: output = cmd_log(cmd, rdir) except subprocess.CalledProcessError as ex: log.exception('Getting a list of remote branches failed: %r', ex.output) raise GitImportErrorCannotBranch() if branch not in output: raise GitImportErrorRemoteBranchMissing() cmd = ['git', 'branch', '-a', ] try: output = cmd_log(cmd, rdir) except subprocess.CalledProcessError as ex: log.exception('Getting a list of local branches failed: %r', ex.output) raise GitImportErrorCannotBranch() branches = [] for line in output.split('\n'): branches.append(line.replace('*', '').strip())
df = pd.DataFrame({'a': [0, 1], 'b': [2.2, 3.3], 'c': 1}) expected = 'a,b,c\n0,2.20,1\n1,3.30,1\n' self.assertEqual( df.set_index('a').to_csv(float_format='%.2f'), expected)
m2m_inherited = models.ManyToManyField(Relation, related_name='m2m_concrete_rel') friends_inherited = models.ManyToManyField('self', related_name='friends_concrete', symmetrical=True) following_inherited = models.ManyToManyField('self', related_name='followers_concrete', symmetrical=False)
microsite.clear() with patch('django.conf.settings.MICROSITE_CONFIGURATION', False): self.assertEqual( microsite.get_all_orgs(), set() )
items = [("%03d" % i, i) for i in range(1000)] rng = Random(42) d_sorted = dict(items) rng.shuffle(items) d_shuffled = dict(items)
THIRD_PARTY_AUTH_BACKENDS = ["google-oauth2", "facebook"] THIRD_PARTY_AUTH_PROVIDERS = ["Google", "Facebook"]
if expr_form == 'range' and HAS_RANGE: tgt = self._convert_range_to_list(tgt) expr_form = 'list'
a = DataFrame({0: [1, 2], 1: [3, 4], 2: [5, 6]}) b = DataFrame({0: [np.nan, 8], 1: [9, np.nan], 2: [np.nan, np.nan]}) do_not_replace = b.isnull() | (a > b)
lev_loc = len(level) level = level.insert(lev_loc, k)
del self._actions[name]
data = salt.utils.alias_function(items, 'data')
_validate_username(username) _validate_password(password, username) _validate_email(email)
import salt.loader import salt.syspaths
while field.remote_field is not None: field = field.remote_field.get_related_field() return field.to_python
from __future__ import unicode_literals
continue
if size is None: size = self._chunksize return self.read(nrows=size)
res = df.groupby('gender').hist() tm.close()
__slots__ = ('template', 'args', 'kwargs', '_message') def __init__(self, template, *args, **kwargs): self.template = template self.args = args self.kwargs = kwargs self._message = None def __unicode__(self): if self._message is None: self._message = self.template.format(*self.args, **self.kwargs) return self._message def __repr__(self): return unicode(self) def __len__(self): return len(unicode(self)) def __getitem__(self, index): return unicode(self)[index]
problem = self.build_problem(answer='(1, 5)') input_dict = {'1_2_1': ''} correct_map = problem.grade_answers(input_dict) correctness = correct_map.get_correctness('1_2_1') self.assertEqual(correctness, 'incorrect')
before = self.make_project_state([]) after = self.make_project_state([self.book, self.author_with_book_order_wrt]) autodetector = MigrationAutodetector(before, after) changes = autodetector._detect_changes() self.assertNumberMigrations(changes, 'testapp', 1) self.assertOperationTypes(changes, 'testapp', 0, ["CreateModel", "AlterOrderWithRespectTo"]) self.assertOperationAttributes(changes, 'testapp', 0, 1, name="author", order_with_respect_to="book") self.assertNotIn("_order", [name for name, field in changes['testapp'][0].operations[0].fields])
if low['name'] == comp['file'] or low['__id__'] == comp['file']: fn = low['name'] if os.path.isdir(fn): if _is_child(fn, name): if walk_d: walk_ret = set() _process_by_walk_d(fn, walk_ret) keep.update(walk_ret) else: keep.update(_process(fn)) else: keep.add(fn)
result = A.dot(b)
child_descriptor = Mock(name='child_descriptor') child_descriptor._xmodule.student_view.return_value.content = u'<p>This is a secret</p>' child_descriptor.student_view = child_descriptor._xmodule.student_view child_descriptor.displayable_items.return_value = [child_descriptor] child_descriptor.runtime = descriptor_system child_descriptor.xmodule_runtime = get_test_system() child_descriptor.render = lambda view, context=None: descriptor_system.render(child_descriptor, view, context) child_descriptor.location = source_location.replace(category='html', name='child')
elidable = False
self.assert_middleware_usage(pre_middleware, True, False, False, True, False) self.assert_middleware_usage(bad_middleware, True, False, False, True, False) self.assert_middleware_usage(post_middleware, False, False, False, True, False)
names = expr.names overlap = names & _ne_builtins if overlap: s = ', '.join(map(repr, overlap)) raise NumExprClobberingError('Variables in expression "%s" ' 'overlap with builtins: (%s)' % (expr, s))
self.send_delete(self.client, expected_status=404)
X = check_array(X, accept_sparse='csr', dtype=np.float64) self._check_parameters() self._fit(X)
ls._set_single(0, (-50, 25)) self.assertEqual(ls.coords, ((-50.0, 25.0), (4.0, 1.0), (6.0, -1.0)), 'LineString _set_single')
from netaddr import IPAddress from netaddr.core import AddrFormatError
retained = None for flav in flavor: parser = _parser_dispatch(flav) p = parser(io, compiled_match, attrs, encoding)
if abs(r - round(r)) > 0.01: return False r = int(round(r))
import yaml import salt.ext.six as six
expected = DataFrame( {0: Series(1, index=range(4))}, columns=['A', 'B', 0])
if not base.startswith(resolved(settings.DATA_DIR)): raise SuspiciousOperation("Attempted to import course outside of data dir")
import sys import contextlib import os from salt.ext.six.moves import range from salt.ext.six.moves import map
if key in obj._data.items: return None
ItemFactory.create( parent_location=self.course.location, category="discussion", discussion_id=topic_name_to_id(self.course, "Topic A"), discussion_category="Chapter", discussion_target="Discussion", start=datetime.now() ) discussion_topics = { "Topic B": {"id": "Topic B"}, }
log = logging.getLogger(__name__)
X_checked = assert_warns(DeprecationWarning, check_array, [42], ensure_2d=True) assert_array_equal(np.array([[42]]), X_checked)
post_data = { 'site': self.site.pk, 'title': 'Changed', 'hist': 'Some content', 'created_0': '2008-03-18', 'created_1': '11:54', } change_url = reverse('admin:admin_utils_article_change', args=[quote(self.a1.pk)]) response = self.client.post(change_url, post_data) self.assertRedirects(response, reverse('admin:admin_utils_article_changelist')) logentry = LogEntry.objects.filter(content_type__model__iexact='article').latest('id') self.assertEqual(logentry.get_change_message(), 'Changed title and hist.') with translation.override('fr'): self.assertEqual(logentry.get_change_message(), 'Modification de title et hist.') add_url = reverse('admin:admin_utils_article_add') post_data['title'] = 'New' response = self.client.post(add_url, post_data) self.assertRedirects(response, reverse('admin:admin_utils_article_changelist')) logentry = LogEntry.objects.filter(content_type__model__iexact='article').latest('id') self.assertEqual(logentry.get_change_message(), 'Added.') with translation.override('fr'): self.assertEqual(logentry.get_change_message(), 'Ajout.')
self.assertIsNotNone(get_template_request_context())
DATE_FORMAT = r'Y \m. E j \d.' TIME_FORMAT = 'H:i' DATETIME_FORMAT = r'Y \m. E j \d., H:i' YEAR_MONTH_FORMAT = r'Y \m. F' MONTH_DAY_FORMAT = r'E j \d.' SHORT_DATE_FORMAT = 'Y-m-d' SHORT_DATETIME_FORMAT = 'Y-m-d H:i'
from __future__ import absolute_import import time
usage_key = UsageKey.from_string('i4x://edX/apis/html/interactive') usage_key.replace(course_key=self.course.id) self.assertEqual(Bookmark.get_path(usage_key), [])
s = Series(['a3', 'b3', 'd4c2'], name='series_name') with tm.assertRaisesRegexp(ValueError, "no capture groups"): s.str.extractall(r'[a-z]')
self.assertContains(response, 'cooked_eggs', status_code=500) self.assertContains(response, 'scrambled', status_code=500) self.assertContains(response, 'sauce', status_code=500) self.assertContains(response, 'worcestershire', status_code=500)
field_data_cache = FieldDataCache.cache_for_descriptor_descendents(course_id, student, module_descriptor) student_data = KvsFieldData(DjangoKeyValueStore(field_data_cache))
return _after_ignition_network_profile(cmd, ret, name, network_profile, path, nic_opts)
df = pd.DataFrame({'event': ['start', 'start'], 'change': [1234, 5678]}, index=pd.DatetimeIndex(['2014-09-10', '2013-10-10'])) grouped = df.groupby([pd.TimeGrouper(freq='M'), 'event']) self.assertEqual(len(grouped.groups), 2) self.assertEqual(grouped.ngroups, 2) self.assertIn((pd.Timestamp('2014-09-30'), 'start'), grouped.groups) self.assertIn((pd.Timestamp('2013-10-31'), 'start'), grouped.groups)
TIOCSWINSZ = getattr(termios, 'TIOCSWINSZ', -2146929561) if TIOCSWINSZ == 2148037735: TIOCSWINSZ = -2146929561 packed = struct.pack('HHHH', rows, cols, 0, 0) fcntl.ioctl(self.child_fd, TIOCSWINSZ, packed)
def __init__(self, content=None): self.__content = BytesIO() self.__len = 0 self.read_started = False if content is not None: self.write(content) def __len__(self): return self.__len def read(self, num_bytes=None): if not self.read_started: self.__content.seek(0) self.read_started = True if num_bytes is None: num_bytes = self.__len or 0 assert self.__len >= num_bytes, "Cannot read more than the available bytes from the HTTP incoming data." content = self.__content.read(num_bytes) self.__len -= num_bytes return content def write(self, content): if self.read_started: raise ValueError("Unable to write a payload after he's been read") content = force_bytes(content) self.__content.write(content) self.__len += len(content)
cat = self if inplace else self.copy() cat.categories = new_categories if not inplace: return cat
if psutil.version_info >= (0, 3, 0): return True return (False, 'The ps execution module cannot be loaded: the psutil python module version {0} is less than 0.3.0'.format(psutil.version_info))
if not os.path.exists(cyg_cache_dir): os.mkdir(cyg_cache_dir) elif os.path.exists(cyg_setup_path): os.remove(cyg_setup_path)
class ContactForm(Form): subject = CharField() message = CharField(widget=Textarea(attrs={'rows': 80, 'cols': 20}))
elif (self.is_datetime_lhs and (self.is_timedelta_rhs or self.is_offset_rhs)):
_WINDOW_TYPES.update((v, v) for k, v in list(_WINDOW_TYPES.items())) _ADDITIONAL_CLUSTER_TYPES = set(("entity", "time"))
if result is None: try: result = tslib.array_strptime( arg, format, exact=exact, errors=errors) except tslib.OutOfBoundsDatetime: if errors == 'raise': raise result = arg except ValueError: if not infer_datetime_format: if errors == 'raise': raise result = arg
raise NotImplementedError(str(type(self)) + " does not implement " "get_updates.")
X = np.arange(20).reshape(5, -1) y = [0, 0, 1, 1, 1] clf = SVC(probability=False) gs = GridSearchCV(clf, {}, cv=2).fit(X, y) assert_false(hasattr(gs, "predict_proba"))
t = Template('{% load i18n %}{% trans "May" context message_context %}') rendered = t.render(Context({'message_context': 'month name'})) self.assertEqual(rendered, 'Mai') t = Template('{% load i18n %}{% trans "May" context message_context %}') rendered = t.render(Context({'message_context': 'verb'})) self.assertEqual(rendered, 'Kann')
self._cleanup_index_file() super(StudioLibraryContainerTest, self).tearDown()
clf = AdaBoostRegressor(random_state=0) clf.fit(boston.data, boston.target) score = clf.score(boston.data, boston.target) assert score > 0.85
trellised_layers = [] for layer in layers: data = layer.data if self.by[0] == '.': grouped = data.groupby(self.by[1]) elif self.by[1] == '.': grouped = data.groupby(self.by[0]) else: grouped = data.groupby(self.by) groups = list(grouped.groups.keys()) if self.by[0] == '.' or self.by[1] == '.': shingle1 = set([g for g in groups]) else: shingle1 = set([g[0] for g in groups]) shingle2 = set([g[1] for g in groups]) if self.by[0] == '.': self.rows = 1 self.cols = len(shingle1) elif self.by[1] == '.': self.rows = len(shingle1) self.cols = 1 else: self.rows = len(shingle1) self.cols = len(shingle2) trellised = [[None for _ in range(self.cols)] for _ in range(self.rows)] self.group_grid = [[None for _ in range( self.cols)] for _ in range(self.rows)] row = 0 col = 0 for group, data in grouped: new_layer = deepcopy(layer) new_layer.data = data trellised[row][col] = new_layer self.group_grid[row][col] = group col += 1 if col >= self.cols: col = 0 row += 1 trellised_layers.append(trellised) return trellised_layers
values = _concat._concat_compat([b.values for b in blocks])
return self._folds[0].get_output_space()
return parent
submissions_score_reset_handler(None, **SUBMISSION_RESET_KWARGS) expected_reset_kwargs = { 'sender': None, 'points_possible': 0, 'points_earned': 0, 'user_id': 42, 'course_id': 'CourseID', 'usage_id': 'i4x://org/course/usage/123456' } self.signal_mock.assert_called_once_with(**expected_reset_kwargs)
raise SaltCloudConfigError( 'Either an instance (or list of names) or a provider must be ' 'specified, but not both.' )
try: module = getattr(self, '_xmodule', None) if not module: module = self except UndefinedContext: module = self
self.xml_data = "course info 463139"
KEY_EXPIRATION_IN_SECONDS = 86400
site_title = ugettext_lazy('Django site admin')
return self.q(css='.action-publish').first
try: c.verify(cacert, sig, der_cert, algo) assert dict(cert.get_subject().get_components())['CN'] == username, "Certificate's CN should match the username" log.info('Successfully authenticated certificate: {0}'.format(pem)) return True except (OpenSSL.crypto.Error, AssertionError): log.info('Failed to authenticate certificate: {0}'.format(pem)) return False
kernel = RBF(1.0) gpr = GaussianProcessRegressor(kernel=kernel, optimizer=None).fit(X, y) assert_equal(np.exp(gpr.kernel_.theta), 1.0)
response2 = self.client.get('/template_response_view/') self.assertEqual(response2.status_code, 200)
mixed = Series(['hello', np.NaN], index=[0, 1]) self.assertEqual(mixed.dtype, np.object_) self.assertIs(mixed[1], np.NaN)
course = modulestore().get_course(course_key, depth=0) if course is None: raise CourseNotFoundError return course
try: kwargs['format'] = _FORMAT_MAP[format.lower()] except: raise TypeError("invalid HDFStore format specified [{0}]" .format(format))
if connections[self.db].ops.spatialite: if z != 0.0: raise NotImplementedError('SpatiaLite does not support 3D scaling.') s = {'procedure_fmt': '%(geo_col)s,%(x)s,%(y)s', 'procedure_args': {'x': x, 'y': y}, 'select_field': GeomField(), } else: s = {'procedure_fmt': '%(geo_col)s,%(x)s,%(y)s,%(z)s', 'procedure_args': {'x': x, 'y': y, 'z': z}, 'select_field': GeomField(), } return self._spatial_attribute('scale', s, **kwargs)
return self.reset_attempts_button.click()
continue
if refresh: refresh_db()
assert_raises(ValueError, neighbors.NearestNeighbors, algorithm='blah')
if api_key is None and profile is None: raise Exception("Missing api_key and profile") if profile: if isinstance(profile, string_types): _profile = __salt__['config.option'](profile) elif isinstance(profile, dict): _profile = profile
self.workd[yslice] = self.OP(self.workd[xslice])
fresult = first.value
self.assertIsInstance(BadgeClass().backend, DummyBackend)
pass
from __future__ import absolute_import
truncator = text.Truncator( '<p>The quick <a href="xyz.html"\n id="mylink">brown fox</a> jumped over the lazy dog.</p>' ) self.assertEqual( '<p>The quick <a href="xyz.html"\n id="mylink">brown...</a></p>', truncator.words(3, '...', html=True) )
mlp = MLP(layers=[Linear(layer_name='h', dim=5, irange=0.01, max_col_norm=0.01)]) conditional = DiagonalGaussian(mlp=mlp, name='conditional') vae = DummyVAE() conditional.set_vae(vae) input_space = VectorSpace(dim=5) conditional.initialize_parameters(input_space=input_space, ndim=5)
y_true = np.ones((1, n_labels)) assert_equal(lrap_score(y_true, y_score), 1.) assert_equal(lrap_score(y_true, y_score_ties), 1.)
ret = self.run_state( 'user.present', name='salt_test', fullname=u'Sølt Test', roomnumber=u'①③②', workphone=u'٣٤١٢', homephone=u'६८७' ) self.assertSaltTrueReturn(ret)
merge_cells = False
(STUDENT, {1: StartDateType.released}, {}, {1, 3, 4, 6}), (STUDENT, {2: StartDateType.released}, {}, {2, 5, 6}), (STUDENT, {1: StartDateType.released, 2: StartDateType.released}, {}, {1, 2, 3, 4, 5, 6}),
request = self.request_factory.get(self.chapter_url) self.assertRaisesRegexp(Http404, 'Invalid course_key or usage_key', views.jump_to, request, 'bar', ())
for key, matrix in matrices.items(): del result[key]
if getattr(sys, 'frozen', False): application_path = os.path.dirname(sys.executable) elif __file__: application_path = os.path.dirname(__file__)
def __init__(self, dataset, n_folds=3, shuffle=False, random_state=None, **kwargs): y = self.get_y(dataset) cv = StratifiedValidationKFold(y, n_folds, shuffle, random_state) super(StratifiedDatasetValidationKFold, self).__init__(dataset, cv, **kwargs)
self.outline.visit() subsection = self.outline.section('Test Section').subsection('Test Subsection') unit = subsection.expand_subsection().unit('Test Unit').go_to() container = unit.xblocks[0].go_to_container() acid_block = AcidView(self.browser, container.xblocks[0].preview_selector) self.validate_acid_block_preview(acid_block)
query = self.q(css=SETTINGS_NAME_SELECTOR) return query.attrs('id')
verify_group_id_not_present(profiled_user=self.student, pass_group_id=False) verify_group_id_not_present(profiled_user=self.moderator, pass_group_id=False)
if self.n_topics <= 0: raise ValueError("Invalid 'n_topics' parameter: %r" % self.n_topics) if self.total_samples <= 0: raise ValueError("Invalid 'total_samples' parameter: %r" % self.total_samples) if self.learning_offset < 0: raise ValueError("Invalid 'learning_offset' parameter: %r" % self.learning_offset) if self.learning_method not in ("batch", "online"): raise ValueError("Invalid 'learning_method' parameter: %r" % self.learning_method)
vals = [end_intv, Period('2006-12-31', 'w')] self.assertRaises(ValueError, PeriodIndex, vals) vals = np.array(vals) self.assertRaises(ValueError, PeriodIndex, vals)
if stdout is None and stderr is None: self.assertFalse(term.isalive())
msg = ("The elements of 'usecols' must " "either be all strings, all unicode, or all integers") if usecols is not None: usecols_dtype = lib.infer_dtype(usecols) if usecols_dtype not in ('integer', 'string', 'unicode'): raise ValueError(msg) return usecols
mp = OGRGeometry('MULTIPOINT(5 23, 0 0, 10 50)') self.assertEqual((0.0, 0.0, 10.0, 50.0), mp.extent) poly = OGRGeometry(self.geometries.polygons[3].wkt) ring = poly.shell x, y = ring.x, ring.y xmin, ymin = min(x), min(y) xmax, ymax = max(x), max(y) self.assertEqual((xmin, ymin, xmax, ymax), poly.extent)
structure_json = CompressedTextField(verbose_name='Structure JSON', blank=True, null=True)
mixed_index = ['foo', ('qux', 'one'), 'two'] self.assertRaises(KeyError, self.index.drop, mixed_index) dropped = self.index.drop(mixed_index, errors='ignore') expected = self.index[[2, 3, 5]] self.assert_index_equal(dropped, expected)
return self.find_css("#start_time").present
self.assertTrue(self._has_changes(draft_xblock.location))
response = self.client.get('/permission_protected_view_exception/') self.assertEqual(response.status_code, 403)
if not pipsearch: pkg_404_comms.append( 'There was no error installing package \'{0}\' ' 'although it does not show when calling ' '\'pip.freeze\'.'.format(pkg) ) else: pkg_name = _find_key(prefix, pipsearch) if pkg_name.lower() in already_installed_packages: continue ver = pipsearch[pkg_name] ret['changes']['{0}=={1}'.format(pkg_name, ver)] = 'Installed'
return (lhs_mask & rhs_mask).nonzero()[0]
asides = block.runtime.get_asides(block) self.assertEqual(len(asides), 1, "Found {} asides but expected only test_aside".format(asides)) self.assertIsInstance(asides[0], AsideTestType) category = block.scope_ids.block_type self.assertEqual(asides[0].data_field, "Exported data_field {} aside data".format(category)) self.assertEqual(asides[0].content, "Exported content {} Aside".format(category.capitalize())) for child in block.get_children(): check_block(child)
if text_message is None: text_message = html_to_text(html_message)
if not has_studio_write_access(request.user, source_course_key): raise PermissionDenied()
if not isinstance(xblock.location, LibraryUsageLocator): modulestore().has_changes(modulestore().get_course(xblock.location.course_key, depth=None))
CUSTOM_THEME_OVERRIDES = { 'embargo': BlockedMessage( description='Embargo', template='static_templates/theme-embargo.html' ) }
request = RequestFactory().get("dummy_url") request.user = user mako_middleware_process_request(request) def call_single_thread(): return views.single_thread( request, unicode(self.course.id), discussion_id, thread_id ) if should_have_access: self.assertEqual(call_single_thread().status_code, 200) else: with self.assertRaises(Http404): call_single_thread()
self.sale_invoice_1 = Invoice.objects.create( total_amount=1234.32, company_name='Test1', company_contact_name='TestName', company_contact_email='Test@company.com', recipient_name='Testw', recipient_email='test1@test.com', customer_reference_number='2Fwe23S', internal_reference="A", course_id=self.course.id, is_valid=True ) self.invoice_item = CourseRegistrationCodeInvoiceItem.objects.create( invoice=self.sale_invoice_1, qty=1, unit_price=1234.32, course_id=self.course.id )
module = inspect.getmodule(method) if module is not None: if not module.__name__.startswith('pylearn2'): return method_errors
panel4dc = self.panel4d.copy() df = panel4dc.iloc[0, 0] df.iloc[:] = 1 panel4dc.iloc[0, 0] = df self.assertTrue((panel4dc.iloc[0, 0].values == 1).all())
course_id = self.course_id.replace( self.course_info['run'], 'other_run' ) self.stub_api(course_id=course_id) self.auth() self.listing_page.visit() self.assertTrue(self.listing_page.is_sidebar_present) self.assertFalse(self.listing_page.are_cards_present)
self.assertRaises(ValueError, utils.find_json, LORUM_IPSUM)
self.assertEqual(2, ls_orig.coord_dim) self.assertAlmostEqual(ls_trans.x[0], ls_orig.x[0], prec) self.assertAlmostEqual(ls_trans.y[0], ls_orig.y[0], prec)
default_variables = [ ('i', 1j, None), ('j', 1j, None), ('e', 2.7183, 1e-4), ('pi', 3.1416, 1e-4),
hq = 0 for psample in psamples[1:]: temp = \ - psample.get_value() * numpy.log(1e-5 + psample.get_value()) \ - (1.-psample.get_value()) \ * numpy.log(1. - psample.get_value() + 1e-5) hq += numpy.sum(temp, axis=1)
INVOICE_CORP_ADDRESS = ENV_TOKENS.get('INVOICE_CORP_ADDRESS', INVOICE_CORP_ADDRESS) INVOICE_PAYMENT_INSTRUCTIONS = ENV_TOKENS.get('INVOICE_PAYMENT_INSTRUCTIONS', INVOICE_PAYMENT_INSTRUCTIONS)
X, y = hastie_X, hastie_y ForestEstimator = FOREST_ESTIMATORS[name] clf = ForestEstimator(n_estimators=5, max_depth=1, warm_start=False, random_state=1) clf.fit(X, y)
last_name_label = _(u"Last Name")
for tt in set([str, compat.text_type]): result = df.astype(tt)
return {"result": "inclusion_one_default_from_template - Expected result: %s, %s" % (one, two)}
if 'pub_key' not in vm_ and 'priv_key' not in vm_: log.debug('Generating minion keys for \'{0[name]}\''.format(vm_)) vm_['priv_key'], vm_['pub_key'] = salt.utils.cloud.gen_keys( salt.config.get_cloud_config_value( 'keysize', vm_, __opts__ ) ) data = conn.server_show_libcloud(vm_['instance_id']) if vm_['key_filename'] is None and 'change_password' in __opts__ and __opts__['change_password'] is True: vm_['password'] = sup.secure_password() conn.root_password(vm_['instance_id'], vm_['password'])
user = UserSerializer() class Meta(object): model = UserPreference depth = 1
record = { 'handle': handle, 'description': dmi_raw.pop(0).strip(), 'type': int(htype) }
search_engine = SearchEngine.get_search_engine(index="library_index") if search_engine: filter_clause = { "library": unicode(normalize_key_for_search(library.location.library_key)), "content_type": CapaDescriptor.INDEX_CONTENT_TYPE, "problem_types": capa_type } search_result = search_engine.search(field_dictionary=filter_clause) results = search_result.get('results', []) return [LibraryUsageLocator.from_string(item['data']['id']) for item in results] else: return [key for key in library.children if self._filter_child(key, capa_type)]
return self._save_asset_metadata_list([asset_metadata, ], user_id, import_only)
self.system.error_tracker(msg) return 'Oops, couldn't load grommet'
return '%s'
result = self.mixed_frame.cov() expected = self.mixed_frame.ix[:, ['A', 'B', 'C', 'D']].cov() tm.assert_frame_equal(result, expected)
for row in unicodecsv.DictReader(csv_file): if row.get('Username') == username: self.assertEqual(row[column_header], expected_cell_content)
pass
STUDENT_VIEW = 'student_view'
comps = win_installer.split('/') local_path = '/'.join(comps[:-1]) installer = comps[-1] with salt.utils.fopen(win_installer, 'rb') as inst_fh: smb_conn.putFile('C$', 'salttemp/{0}'.format(installer), inst_fh.read)
django_conversions = conversions.copy() django_conversions.update({ FIELD_TYPE.TIME: backend_utils.typecast_time, FIELD_TYPE.DECIMAL: backend_utils.typecast_decimal, FIELD_TYPE.NEWDECIMAL: backend_utils.typecast_decimal, datetime.datetime: adapt_datetime_warn_on_aware_datetime, })
if six.PY3: trans = ''.maketrans('', '', text_characters) nontext = data.translate(trans) else: trans = string.maketrans('', '') nontext = data.translate(trans, text_characters)
wkb_reader_create = GEOSFuncFactory('GEOSWKBReader_create', restype=WKB_READ_PTR) wkb_reader_destroy = GEOSFuncFactory('GEOSWKBReader_destroy', argtypes=[WKB_READ_PTR])
if not has_rejectfile_option: cmd.append('--reject-file=-')
modules = get_accessible_discussion_modules(course, self.non_staff_user) self.assertEqual(len(modules), 2) self.assertTrue( any(module.display_name == 'scheduled' for module in modules) )
if call != 'action': raise SaltCloudSystemExit( 'The instance action must be called with -a or --action.' ) log.info("Starting machine: %s", name) vb_start_vm(name) machine = vb_get_machine(name) del machine["name"] return treat_machine_dict(machine)
if com.is_numeric_v_string_like(arr, x): mask = False else: mask = arr == x
labels = np.array([0, 0, 0, 0, 0], dtype=np.int64)
import salt.payload import salt.utils import salt.utils.files import salt.utils.jid import salt.exceptions
if not hasattr(self, 'mask_weights'): self.mask_weights = None
inp, rows, cols, outp = range(4) raw = self._filters.get_value(borrow=borrow) return np.transpose(raw, (outp, rows, cols, inp))
monitoring_train = DenseDesignMatrix(X=X) monitoring_test = DenseDesignMatrix(X=Y)
database += token try: if exploded_grant[position_tracker + 1] == '.': phrase = 'tables' except IndexError: break
if not self.sources_list: if 'sources' in self.xml_attributes and isinstance(self.xml_attributes['sources'], basestring): self.sources_list = [ self.location.course_key.make_usage_key_from_deprecated_string(item) for item in ConditionalDescriptor.parse_sources(self.xml_attributes) ]
hg.__salt__ = {}
powers = self.powers_ if input_features is None: input_features = ['x%d' % i for i in range(powers.shape[1])] feature_names = [] for row in powers: inds = np.where(row)[0] if len(inds): name = " ".join("%s^%d" % (input_features[ind], exp) if exp != 1 else input_features[ind] for ind, exp in zip(inds, row[inds])) else: name = "1" feature_names.append(name) return feature_names
try: return render_to_response( 'student_profile/learner_profile.html', learner_profile_context(request, username, request.user.is_staff) ) except (UserNotAuthorized, UserNotFound, ObjectDoesNotExist): raise Http404
return self.report_download_links.map(lambda el: el.text)
inp = iter(inp) mlb = MultiLabelBinarizer(classes=[1, 3, 2]) assert_array_equal(mlb.fit(inp).transform(inp), indicator_mat)
labname = self._encode(_pad_bytes(self.labname[:32], 33)) bio.write(labname)
main_page_content_css = '.page-content-main' self.wait_for( lambda: len(self.q(css=main_page_content_css).text) == 1, description="Body text is present" ) return self.q(css=main_page_content_css).text[0]
for B in combinations(features, k): for b in product(*[values[B[j]] for j in range(k)]): mask_b = np.ones(n_samples, dtype=np.bool)
content = 'outside <strong>inside</strong> after' payload = get_response(content, 'June 22, 2000') self.assertHTMLEqual(content, payload['content'], "text outside tag")
self.assertTrue(self.video.downloaded_transcript_contains_text('srt', '00:00:00,260'))
bob = User.objects.db_manager('other').create_user('bob', 'bob@example.com') bob_profile = UserProfile.objects.using('other').create(user=bob, flavor='crunchy frog')
df = tm.makeDataFrame() df['obj1'] = 'foo' df['obj2'] = 'bar' df['bool1'] = df['A'] > 0 df['bool2'] = df['B'] > 0 df['bool3'] = True df['int1'] = 1 df['int2'] = 2 df['timestamp1'] = Timestamp('20010102') df['timestamp2'] = Timestamp('20010103') df['datetime1'] = datetime.datetime(2001, 1, 2, 0, 0) df['datetime2'] = datetime.datetime(2001, 1, 3, 0, 0) df.ix[3:6, ['obj1']] = np.nan df = df.consolidate()._convert(datetime=True)
'django.contrib.auth.hashers.SHA1PasswordHasher', 'django.contrib.auth.hashers.MD5PasswordHasher',
return '1234'
return None
elif storage in self._used_storages: storage._store([], response) self._used_storages.remove(storage)
verification_status, verification_msg = SoftwareSecurePhotoVerification.user_status(user)
norm_expected = expecteds.copy() for k in norm_expected: norm_expected[k] = Timestamp(norm_expected[k].date())
cmd = '/bin/pkg uninstall -v {0}'.format(pkg2rm) out = __salt__['cmd.run_all'](cmd, output_loglevel='trace')
result = _lexsort_indexer(keys, orders=False, na_position='first') exp = list(range(5)) + list(range(105, 110)) + list(range(104, 4, -1)) tm.assert_numpy_array_equal(result, np.array(exp))
if len(self.axes) > 0: all_axes = self._get_subplots() nrows, ncols = self._get_axes_layout() _handle_shared_axes(axarr=all_axes, nplots=len(all_axes), naxes=nrows * ncols, nrows=nrows, ncols=ncols, sharex=self.sharex, sharey=self.sharey) for ax in self.axes: if self.yticks is not None: ax.set_yticks(self.yticks) if self.xticks is not None: ax.set_xticks(self.xticks) if self.ylim is not None: ax.set_ylim(self.ylim) if self.xlim is not None: ax.set_xlim(self.xlim) ax.grid(self.grid) if self.title: if self.subplots: self.fig.suptitle(self.title) else: self.axes[0].set_title(self.title)
problem_not_specified = self.build_problem(answer="Second") problems = [problem_specified, problem_not_specified]
class CustomUserNonListRequiredFields(AbstractBaseUser): username = models.CharField(max_length=30, unique=True) date_of_birth = models.DateField() USERNAME_FIELD = 'username' REQUIRED_FIELDS = 'date_of_birth' errors = checks.run_checks(app_configs=self.apps.get_app_configs()) self.assertEqual(errors, [ checks.Error( "'REQUIRED_FIELDS' must be a list or tuple.", obj=CustomUserNonListRequiredFields, id='auth.E001', ), ])
for mode in configured_modes: CourseModeFactory.create( course_id=self.course.id, mode_slug=mode, mode_display_name=mode, )
return type( str('RelatedObjectDoesNotExist'), (self.field.remote_field.model.DoesNotExist, AttributeError), {} )
return self._spatial_attribute('mem_size', {}, **kwargs)
from __future__ import unicode_literals
self.assert_matching_events_were_emitted( event_filter={'name': u'edx.instructor.report.downloaded', 'report_url': report_url} )
images, filters = inputs images_ev, filters_ev = evals if 'Cuda' not in str(type(images)): raise TypeError("inputs must be cuda") if 'Cuda' not in str(type(filters)): raise TypeError("filters must be cuda") if filters_ev is not None: sol = self(images, filters_ev) else: sol = None if images_ev is not None: if sol is not None: sol += self(images_ev, filters) else: sol = self(images_ev, filters) return [sol]
exam_id = update_exam( exam_id=exam['id'], exam_name=timed_exam.display_name, time_limit_mins=timed_exam.default_time_limit_minutes, due_date=timed_exam.due, is_proctored=timed_exam.is_proctored_exam, is_practice_exam=timed_exam.is_practice_exam, is_active=True, hide_after_due=timed_exam.hide_after_due, ) msg = 'Updated timed exam {exam_id}'.format(exam_id=exam['id']) log.info(msg)
if self.out_of_bounds not in ["raise", "nan", "clip"]: raise ValueError("The argument ``out_of_bounds`` must be in " "'nan', 'clip', 'raise'; got {0}" .format(self.out_of_bounds))
out = self._str_indent(out,indent) return '\n'.join(out)
from salt.modules import solr import os
metric_tag_fields = [ 'course_id', 'group_id', 'pinned', 'closed', 'anonymous', 'anonymous_to_peers', 'endorsed', 'read' ]
import integration
"Ordered dictionary"
except Exception as exc: log.error('There was an error::') if hasattr(exc, 'code') and hasattr(exc, 'msg'): log.error(' Code: {0}: {1}'.format(exc.code, exc.msg)) log.error(' Content: \n{0}'.format(getattr(exc, 'read', lambda: str(exc))())) return False
variables = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country'] continuous = set(['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']) assert all(var in variables for var in continuous) assert all(map(lambda l: len(l) == len(variables), features)) pieces = []
[pd.Timestamp('2011-01-01'), pd.Timestamp('2011-01-02'), pd.Timestamp('2011-01-03'), pd.NaT], [pd.Timestamp('2011-01-01', tz='US/Eastern'), pd.Timestamp('2011-01-02', tz='US/Eastern'), pd.Timestamp('2011-01-03', tz='US/Eastern'), pd.NaT], [pd.Timedelta('1 days'), pd.Timedelta('2 days'), pd.Timedelta('3 days'), pd.NaT]]
self.adding = True
length_zero = frame.reindex([]) self.assertEqual(len(length_zero), 0) self.assertEqual(len(length_zero.columns), len(frame.columns)) self.assertEqual(len(length_zero['A']), 0)
return _get_backing_memmap(b)
class Alfa(models.Model): name = models.CharField(max_length=10, null=True)
batch_size = self._flat_data_specs[0].batch_size(theano_args)
if changing_email: try: student_views.do_email_change_request(existing_user, new_email) except ValueError as err: raise AccountUpdateError( u"Error thrown from do_email_change_request: '{}'".format(err.message), user_message=err.message )
self.assertEqual( res_json['message'], u"{user} does not exist in the LMS. Please check your spelling and retry.".format(user=invalid_user), )
UNITS = ((b'\xd0\xbc\xd0\xb5\xd1\x81.', b'\xd0\xbc\xd0\xb5\xd1\x81.'), (b'\xd1\x88\xd1\x82.', b'\xd1\x88\xd1\x82.')) f = ChoiceField(choices=UNITS) self.assertEqual(f.clean('\u0448\u0442.'), '\u0448\u0442.') self.assertEqual(f.clean(b'\xd1\x88\xd1\x82.'), '\u0448\u0442.')
target_shim_file = '.{0}'.format(binascii.hexlify(os.urandom(6))) self.shell.send(shim_tmp_file.name, target_shim_file)
if hasattr(obj, 'form') and not issubclass(obj.form, BaseModelForm): return must_inherit_from(parent='BaseModelForm', option='form', obj=obj, id='admin.E016') else: return []
data_obj = (data_create, data_compare) generic_obj = (generic_create, generic_compare) fk_obj = (fk_create, fk_compare) m2m_obj = (m2m_create, m2m_compare) im2m_obj = (im2m_create, im2m_compare) im_obj = (im_create, im_compare) o2o_obj = (o2o_create, o2o_compare) pk_obj = (pk_create, pk_compare) inherited_obj = (inherited_create, inherited_compare) uuid_obj = uuid.uuid4()
eps = 0.8 min_samples = 10 metric = 'euclidean' core_samples, labels = dbscan(X, metric=metric, eps=eps, min_samples=min_samples)
submissions_score_set_handler(None, **SUBMISSION_SET_KWARGS) self.get_user_mock.assert_called_once_with('anonymous_id')
random_state = check_random_state(0) p = random_state.rand(n_samples) * (2 * np.pi - 0.55) t = random_state.rand(n_samples) * np.pi
example_messages = [str(i) for i in range(5)] set_cookie_data(cookie_storage, [CookieStorage.not_finished], encode_empty=True) set_session_data(session_storage, example_messages)
cmd = '--{0}-{1}={2} --permanent'.format(action, _type, name)
index = Index([1, 2.0 + 3.0j, np.nan]) formatted = index.format() expected = [str(index[0]), str(index[1]), u('NaN')] self.assertEqual(formatted, expected)
modeladmin = DecadeFilterBookAdminWithNoneReturningLookups(Book, site) request = self.request_factory.get('/', {}) changelist = self.get_changelist(request, Book, modeladmin) filterspec = changelist.get_filters(request)[0] self.assertEqual(len(filterspec), 0)
return form.save(commit=False)
'correct': _('This answer is correct.'), 'incorrect': _('This answer is incorrect.'), 'partially-correct': _('This answer is partially correct.'), 'unanswered': _('This answer is unanswered.'), 'unsubmitted': _('This answer is unanswered.'), 'queued': _('This answer is being processed.'),
clf = RandomForestClassifier(n_estimators=25) clf.fit(X_train_valid, y_train_valid) clf_probs = clf.predict_proba(X_test) score = log_loss(y_test, clf_probs)
min_files = ( 'salt/__init__.py', 'salt/utils', 'salt/utils/__init__.py', 'salt/utils/validate', 'salt/utils/validate/__init__.py', 'salt/utils/validate/path.py', 'salt/utils/decorators', 'salt/utils/decorators/__init__.py', 'salt/utils/cache.py', 'salt/utils/xdg.py', 'salt/utils/odict.py', 'salt/utils/minions.py', 'salt/utils/dicttrim.py', 'salt/utils/sdb.py', 'salt/utils/migrations.py', 'salt/utils/files.py', 'salt/utils/parsers.py', 'salt/utils/locales.py', 'salt/utils/lazy.py', 'salt/utils/s3.py', 'salt/utils/dictupdate.py', 'salt/utils/verify.py', 'salt/utils/args.py', 'salt/utils/kinds.py', 'salt/utils/xmlutil.py', 'salt/utils/debug.py', 'salt/utils/jid.py', 'salt/utils/openstack', 'salt/utils/openstack/__init__.py', 'salt/utils/openstack/swift.py', 'salt/utils/async.py', 'salt/utils/process.py', 'salt/utils/jinja.py', 'salt/utils/rsax931.py', 'salt/utils/context.py', 'salt/utils/minion.py', 'salt/utils/error.py', 'salt/utils/aws.py', 'salt/utils/timed_subprocess.py', 'salt/utils/zeromq.py', 'salt/utils/schedule.py', 'salt/utils/url.py', 'salt/utils/yamlencoding.py', 'salt/utils/network.py', 'salt/utils/http.py', 'salt/utils/gzip_util.py', 'salt/utils/vt.py', 'salt/utils/templates.py', 'salt/utils/aggregation.py', 'salt/utils/yamlloader.py', 'salt/utils/event.py', 'salt/serializers', 'salt/serializers/__init__.py', 'salt/serializers/yamlex.py', 'salt/template.py', 'salt/_compat.py', 'salt/loader.py', 'salt/client', 'salt/client/__init__.py', 'salt/ext', 'salt/ext/__init__.py', 'salt/ext/six.py', 'salt/ext/ipaddress.py', 'salt/version.py', 'salt/syspaths.py', 'salt/defaults', 'salt/defaults/__init__.py', 'salt/defaults/exitcodes.py', 'salt/renderers', 'salt/renderers/__init__.py', 'salt/renderers/jinja.py', 'salt/renderers/yaml.py', 'salt/modules', 'salt/modules/__init__.py', 'salt/modules/test.py', 'salt/modules/selinux.py', 'salt/modules/cmdmod.py', 'salt/minion.py', 'salt/pillar', 'salt/pillar/__init__.py', 'salt/textformat.py', 'salt/log', 'salt/log/__init__.py', 'salt/log/handlers', 'salt/log/handlers/__init__.py', 'salt/log/mixins.py', 'salt/log/setup.py', 'salt/cli', 'salt/cli/__init__.py', 'salt/cli/caller.py', 'salt/cli/daemons.py', 'salt/cli/salt.py', 'salt/cli/call.py', 'salt/fileserver', 'salt/fileserver/__init__.py', 'salt/transport', 'salt/transport/__init__.py', 'salt/transport/client.py', 'salt/exceptions.py', 'salt/grains', 'salt/grains/__init__.py', 'salt/grains/extra.py', 'salt/scripts.py', 'salt/state.py', 'salt/fileclient.py', 'salt/crypt.py', 'salt/config.py', 'salt/beacons', 'salt/beacons/__init__.py', 'salt/payload.py', 'salt/output', 'salt/output/__init__.py', 'salt/output/nested.py', )
self._for_write = True try: return self.get(**lookup), False except self.model.DoesNotExist: return self._create_object_from_params(lookup, params)
svc = svm.SVC(kernel='linear', C=0.1, decision_function_shape='ovo') clf = svc.fit(iris.data, iris.target)
largefile = request.FILES['file_field2'] obj = FileModel() obj.testfile.save(largefile.name, largefile)
self.assertContains(response, "\n1 recommendation\n")
block_id = BlockKey.from_usage_key(parent_usage_key) if block_id not in new_structure['blocks']: raise ItemNotFoundError(parent_usage_key)
if not organizations_enabled(): return [] from organizations import api as organizations_api return organizations_api.get_organization(organization_id)
visbias_a = visbias
super(VectorSpace, self)._validate_impl(is_numeric, batch)
df = DataFrame(np.random.randn(3, 3), columns=[['i', 'i', 'j'], ['A', 'A', 'B']], index=[['i', 'i', 'j'], ['X', 'X', 'Y']])
self.assertTrue(self._is_location_published(location)) self.assertFalse(modulestore().has_changes(modulestore().get_item(location)))
_translations = {} _active = local()
self.assertEqual( Person.objects.last(), p2) self.assertEqual( Person.objects.order_by('-name').last(), p1) self.assertEqual( Person.objects.filter(birthday__lte=datetime(1955, 1, 1)).last(), p1) self.assertIs( Person.objects.filter(birthday__lte=datetime(1940, 1, 1)).last(), None)
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010")
data = payload.format(score=0.8) return self._send_lti2(data)
match_targets = ["pcre", "glob", "list"] if self.opts['zmq_filtering'] and load['tgt_type'] in match_targets: match_ids = self.ckminions.check_minions(load['tgt'], expr_form=load['tgt_type'] )
class BinaryTree(models.Model): name = models.CharField(max_length=100) parent = models.ForeignKey('self', models.SET_NULL, null=True, blank=True)
if self._handle is not None: self._handle.close() self._handle = None
assert_raises(ValueError, ForestEstimator(min_samples_leaf=-1).fit, X, y) assert_raises(ValueError, ForestEstimator(min_samples_leaf=0).fit, X, y)
def test_str_to_bool_true(self): self.assertTrue(str_to_bool('True')) self.assertTrue(str_to_bool('true')) self.assertTrue(str_to_bool('trUe')) def test_str_to_bool_false(self): self.assertFalse(str_to_bool('Tru')) self.assertFalse(str_to_bool('False')) self.assertFalse(str_to_bool('false')) self.assertFalse(str_to_bool('')) self.assertFalse(str_to_bool(None)) self.assertFalse(str_to_bool('anything')) def test_str_to_bool_errors(self): def test_raises_error(val): with self.assertRaises(AttributeError): self.assertFalse(str_to_bool(val)) test_raises_error({}) test_raises_error([]) test_raises_error(1) test_raises_error(True)
if self._isClockwise(poly.exterior_ring): poly.exterior_ring = list(reversed(poly.exterior_ring))
result = s.sum(skipna=False) self.assertEqual(int(result), v.sum(dtype='int64')) result = s.min(skipna=False) self.assertEqual(int(result), 0) result = s.max(skipna=False) self.assertEqual(int(result), v[-1])
self.assertEqual(qs.count(), 1) self.assertEqual(qs[0].links__sum, l.id) l.delete() qs = qs.all() self.assertEqual(qs.count(), 1) self.assertIs(qs[0].links__sum, None) self.assertEqual(qs.filter(links__sum__isnull=True).count(), 1) self.assertEqual(qs.filter(links__sum__isnull=False).count(), 0)
user = User.objects.get(email=self.notenrolled_student.email) self.assertTrue(CourseEnrollment.is_enrolled(user, self.course.id))
continue
win_system.__salt__ = {} win_system.__opts__ = {}
return module.sort_key or module.discussion_target
if __PLATFORM.startswith('win'): ROOT_DIR = r'c:\salt' else: ROOT_DIR = '/'
self.lc_block.refresh_children() self.assertTrue(self.lc_block.validate())
s = Series()
entry = InstructorTask.objects.get(pk=entry_id) if len(entry.subtasks) == 0: format_str = "Unexpected task_id '{}': unable to find subtasks of instructor task '{}': rejecting task {}" msg = format_str.format(current_task_id, entry, new_subtask_status) TASK_LOG.warning(msg) dog_stats_api.increment('instructor_task.subtask.duplicate.nosubtasks', tags=[entry.course_id]) raise DuplicateTaskException(msg)
resp = self._update_item( lc_block.location, {"source_library_id": "library-v1:NOT+FOUND"}, ) self.assertEqual(resp.status_code, 200) lc_block = modulestore().get_item(lc_block.location)
'ENABLE_COURSE_SORTING_BY_START_DATE': True,
company_name = models.CharField(max_length=255, null=True, blank=True) company_contact_name = models.CharField(max_length=255, null=True, blank=True) company_contact_email = models.CharField(max_length=255, null=True, blank=True) recipient_name = models.CharField(max_length=255, null=True, blank=True) recipient_email = models.CharField(max_length=255, null=True, blank=True) customer_reference_number = models.CharField(max_length=63, null=True, blank=True) order_type = models.CharField(max_length=32, default='personal', choices=OrderTypes.ORDER_TYPES)
module_store_setting['default']['OPTIONS']['stores'] = convert_old_stores_into_list( get_mixed_stores(module_store_setting) ) assert isinstance(get_mixed_stores(module_store_setting), list)
with transaction.atomic(): return wrapped_func(*args, **kwargs)
return model.log_prob(X) - self.noise.log_prob(X)
gpg_info_file = '{0}/gpg-agent-info-salt'.format(gnupghome) with salt.utils.fopen(gpg_info_file, 'r') as fow: gpg_raw_info = fow.readlines()
test1 = rnorm / bnorm test2 = arnorm / (anorm * rnorm + eps) test3 = 1 / (acond + eps) t1 = test1 / (1 + anorm * xnorm / bnorm) rtol = btol + atol * anorm * xnorm / bnorm
self._find_within( "#wmd-{}-button-edit-post-body-{}".format( content_type, response_id, ) ).click() self.q(css='#new-url-input').fill(url) self.q(css='#new-url-desc-input').fill(description) if is_decorative: self.q(css='#img-is-decorative').click() self.q(css='input[value="OK"]').click()
vertical = self.store.create_child( self.user_id, sequential.location, 'vertical', block_id='moon_unit' )
p = self.PersonModel.objects.get(name='Joe') self.assertEqual(p.mugshot.was_opened, False) self.assertEqual(p.headshot.was_opened, False) self.check_dimensions(p, 4, 8, 'mugshot') self.check_dimensions(p, 8, 4, 'headshot') self.assertEqual(p.mugshot.was_opened, True) self.assertEqual(p.headshot.was_opened, True) p.mugshot.was_opened = False p.headshot.was_opened = False self.check_dimensions(p, 4, 8, 'mugshot') self.check_dimensions(p, 8, 4, 'headshot') self.assertEqual(p.mugshot.was_opened, False) self.assertEqual(p.headshot.was_opened, False)
from salt.modules import htpasswd
rng = np.random.RandomState(0) X = rng.random_sample(size=(300, 50)) Y = rng.random_sample(size=(300, 50)) X /= X.sum(axis=1)[:, np.newaxis] Y /= Y.sum(axis=1)[:, np.newaxis]
check_ortho(pls_ca.x_weights_, "x weights are not orthogonal") check_ortho(pls_ca.y_weights_, "y weights are not orthogonal")
self.send_get(client=self.client, url=reverse('bookmarks'), query_parameters='page_size=2&page=2') self.assert_bookmark_event_emitted( mock_tracker, event_name='edx.bookmark.listed', list_type='all_courses', bookmarks_count=3, page_size=2, page_number=2 )
elif course_overview.location.org in orgs_to_exclude: continue
if is_object_dtype(args[0]): raise TypeError(e) raise
return self._constructor(result, index=new_index, columns=new_columns)._convert(datetime=True, copy=False)
#html_file_suffix = ''
width = self.ext.config['dailymotion_width'][0] height = self.ext.config['dailymotion_height'][0] return flash_object(url, width, height)
self.team_management_page.value_for_text_field( field_id='name', value='EdX is a massive open online course (MOOC) provider and online learning platform. ' 'It hosts online university-level courses in a wide range of disciplines to a worldwide ' 'audience, some at no charge. It also conducts research into learning based on how ' 'people use its platform. EdX was created for students and institutions that seek to' 'transform themselves through cutting-edge technologies, innovative pedagogy, and ' 'rigorous courses. More than 70 schools, nonprofits, corporations, and international' 'organizations offer or plan to offer courses on the edX website. As of 22 October 2014,' 'edX has more than 4 million users taking more than 500 courses online.', press_enter=False ) self.team_management_page.submit_form()
return self.q(css=self.get_selector(css=css_selector))
patcher = patch('shoppingcart.models.analytics') self.mock_tracker = patcher.start() self.addCleanup(patcher.stop)
from pandas.tseries.plotting import tsplot import matplotlib.pyplot as plt
raise NotImplementedError()
if len(a) != len(b): raise AssertionError('Operands to nancorr must have same size') if min_periods is None: min_periods = 1 valid = notnull(a) & notnull(b) if not valid.all(): a = a[valid] b = b[valid] if len(a) < min_periods: return np.nan f = get_corr_func(method) return f(a, b)
with connection.schema_editor() as editor: editor.create_model(AuthorWithDefaultHeight) columns = self.column_classes(AuthorWithDefaultHeight) self.assertTrue(columns['height'][1][6]) old_field = AuthorWithDefaultHeight._meta.get_field("height") new_field = PositiveIntegerField(default=42) new_field.set_attributes_from_name("height") with connection.schema_editor() as editor: editor.alter_field(AuthorWithDefaultHeight, old_field, new_field) columns = self.column_classes(AuthorWithDefaultHeight) self.assertFalse(columns['height'][1][6])
df = DataFrame({'a': [1, 222, 33333, 4], 'b': [u'あ', u'いいい', u'う', u'ええええええ']}, index=['a', 'bb', 'c', 'ddd']) expected = (u" a b\na 1 あ\n" u"bb 222 いいい\nc 33333 う\n" u"ddd 4 ええええええ") self.assertEqual(_rep(df), expected)
mock_uses_shib.return_value = True
'ping_interval': int,
pro.editor = george pro.save()
for max_leaf_nodes, name in product((None, 1000), ALL_TREES.keys()): TreeEstimator = ALL_TREES[name]
to_concat, name = self._ensure_compat_append(other) to_concat = [self._is_dtype_compat(c) for c in to_concat] codes = np.concatenate([c.codes for c in to_concat]) return self._create_from_codes(codes, name=name)
try: iter(obj) except TypeError: return False return True
if __salt__['mysql.user_exists'](name, host, **connection_args): if __opts__['test']: ret['result'] = None ret['comment'] = 'User {0}@{1} is set to be removed'.format( name, host) return ret if __salt__['mysql.user_remove'](name, host, **connection_args): ret['comment'] = 'User {0}@{1} has been removed'.format(name, host) ret['changes'][name] = 'Absent' return ret else: err = _get_mysql_error() if err is not None: ret['comment'] = err ret['result'] = False return ret else: err = _get_mysql_error() if err is not None: ret['comment'] = err ret['result'] = False return ret
for num, item in enumerate(extra_data, start=1): key = u"merchant_defined_data{num}".format(num=num) params[key] = item
from pandas.core.algorithms import value_counts result = value_counts(self, sort=sort, ascending=ascending, normalize=normalize, bins=bins, dropna=dropna) return result
if "application/json" in request.META.get('CONTENT_TYPE', '') and request.body: try: request.json = json.loads(request.body) except ValueError: return JsonResponseBadRequest({"error": "Invalid JSON"}) else: request.json = {}
try: course = get_course_with_access(request.user, 'load', course_key, check_if_enrolled=check_if_enrolled) except UserNotEnrolled: raise Http404("Course not found.")
Author.objects.create(name='Foo') Author.objects.create(name='Bar')
states = [] def __init__(self, opts): pass
pro = Book.objects.create(title="Pro Django", published=datetime.date(2008, 12, 16)) review1 = Review.objects.create(source="Python Monthly", content_object=pro)
verify_status_by_course = check_verify_status_by_course(user, course_enrollments) cert_statuses = { enrollment.course_id: cert_info(request.user, enrollment.course_overview, enrollment.mode) for enrollment in course_enrollments }
rnd = np.random.RandomState(0) X = 3 * rnd.uniform(size=(20)) y = X.astype(np.int) y = multioutput_estimator_convert_y_2d(name, y) estimator = Estimator() set_testing_parameters(estimator)
h5file, node = self.init_hdf5(h_file_n, ([sizes[which_set], image_size], [sizes[which_set], 1]), title="SVHN Dataset", y_dtype='int')
self._fire_master( 'Syndic {0} started at {1}'.format( self.opts['id'], time.asctime() ), 'syndic_start', sync=False, ) self._fire_master( 'Syndic {0} started at {1}'.format( self.opts['id'], time.asctime() ), tagify([self.opts['id'], 'start'], 'syndic'), sync=False, )
return utils.CursorWrapper(cursor, self)
try: proc_dir = os.path.join(__opts__['cachedir'], 'proc') jid_file = os.path.join(proc_dir, kwargs['__pub_jid']) if os.path.isfile(jid_file): serial = salt.payload.Serial(__opts__) with salt.utils.fopen(jid_file, 'rb') as fn_: jid_dict = serial.load(fn_)
course_key = CourseLocator(org="edX", course="101", run="2015") root_block_key = self.draft_store.make_course_usage_key(course_key) self.assertEqual(root_block_key.block_type, "course") self.assertEqual(root_block_key.name, "2015")
base_url = reverse( 'third_party_auth_user_mapping_api', kwargs={'provider_id': PROVIDER_ID_TESTSHIB} ) params = QueryDict('', mutable=True) for attr in ['username', 'remote_id']: if attr in query_params: params.setlist(attr, query_params[attr]) url = "{}?{}".format(base_url, params.urlencode()) response = self.client.get(url, HTTP_X_EDX_API_KEY=VALID_API_KEY) self._verify_response(response, expect_code, expect_data)
for subs_id in youtube_subs.values(): filename = 'subs_{0}.srt.sjson'.format(subs_id) content_location = StaticContent.compute_location( self.course.id, filename ) self.assertTrue(contentstore().find(content_location))
return {"result": "inclusion_no_params_with_context - Expected result (context value: %s)" % context['value']}
self._write(str(t))
mask = isnull(val) if dropna: inc[idx] = 1 inc[mask] = 0 else: inc[mask & np.r_[False, mask[:-1]]] = 0 inc[idx] = 1
#html_title = None
with salt.utils.flopen(filename, 'rb') as _fp: _contents = _fp.read() result = gpg.encrypt(_contents, recipients, passphrase=gpg_passphrase, output=output)
CourseModeFactory.create( course_id=self.course.id, mode_slug='professional', mode_display_name='professional', )
import salt.utils.sdb
def benchmark(clf): print('_' * 80) print("Training: ") print(clf) t0 = time() clf.fit(X_train, y_train) train_time = time() - t0 print("train time: %0.3fs" % train_time)
self.assertIn( 'oauth_body_hash="00hq6RNueFa8QiEjhep5cJRHWAI%3D"', prepped_req.headers['Authorization'] )
assert_raises( AssertionError, monitor.update_channels, ['train_objective'], start=2, end=1 )
XA_checked, XB_checked = check_pairwise_arrays(XA, XB) assert_equal(XA_checked.dtype, np.float32) assert_equal(XB_checked.dtype, np.float32)
import logging import pythoncom import threading
assert_frame_equal(df.loc[:, []], df.iloc[:, :0], check_index_type=True, check_column_type=True) assert_frame_equal(df.loc[[], :], df.iloc[:0, :], check_index_type=True, check_column_type=True) assert_frame_equal(df.loc[[]], df.iloc[:0, :], check_index_type=True, check_column_type=True)
test_model = CourseMetadata.update_from_json( self.course, { "edxnotes": {"value": "true"}, }, user=self.user ) self.assertNotIn('edxnotes', test_model)
self.assertEqual( list(Person.objects.using('default').filter(edited__title='Pro Django').values_list('name', flat=True)), ['George Vilches'] ) self.assertEqual( list(Person.objects.using('other').filter(edited__title='Pro Django').values_list('name', flat=True)), [] )
self.register_post_comment_response({}, thread_id="test_thread") data = self.minimal_data.copy() data["endorsed"] = True saved = self.save_and_reserialize(data) self.assertEqual( httpretty.last_request().parsed_body, { "course_id": [unicode(self.course.id)], "body": ["Test body"], "user_id": [str(self.user.id)], "endorsed": ["True"], } ) self.assertTrue(saved["endorsed"]) self.assertIsNone(saved["endorsed_by"]) self.assertIsNone(saved["endorsed_by_label"]) self.assertIsNone(saved["endorsed_at"])
if dtype is None: dtype = theano.config.floatX return theano.shared(theano._asarray(value, dtype=dtype), name=name, borrow=borrow)
return
act.actor.lane_stack.value.server.close() testStack = self.store.fetch('.salt.test.lane.stack') if testStack: testStack.value.server.close()
if data_specs is None: data_specs = (self._get_sources, self._get_spaces) [mode, batch_size, num_batches, rng, data_specs] = self._init_iterator( mode, batch_size, num_batches, rng, data_specs) convert = None return FiniteDatasetIterator(self, mode(self.get_num_examples(), batch_size, num_batches, rng), data_specs=data_specs, return_tuple=return_tuple, convert=convert)
K = safe_sparse_dot(X, X.T, dense_output=True) v, Q = linalg.eigh(K) QT_y = np.dot(Q.T, y) return v, Q, QT_y
self.lc_block = self._add_library_content_block(self.course, self.lib_key)
queryset = AustraliaCity.objects.filter(point__distance_lte=('POINT(5 23)', D(km=100), 'spheroid', '4')) with self.assertRaises(ValueError): len(queryset)
targets = [x for x in pkg_params if x in old] if not targets: return {}
ax = _check_plot_works(df.plot, yerr=df_err, logy=True) self._check_has_errorbars(ax, xerr=0, yerr=2) ax = _check_plot_works(df.plot, yerr=df_err, logx=True, logy=True) self._check_has_errorbars(ax, xerr=0, yerr=2) ax = _check_plot_works(df.plot, yerr=df_err, loglog=True) self._check_has_errorbars(ax, xerr=0, yerr=2)
pass
return
known_lazy = { ('django.db.models.fields.related', 'resolve_related_class'): field_error, ('django.db.models.fields.related', 'set_managed'): None, ('django.dispatch.dispatcher', 'connect'): signal_connect_error, }
return condition, needed_inner
JSON_SCHEMA_DRAFT_4 = 'http://json-schema.org/draft-04/schema#'
self.vw.default_parts.add(self.sunroof)
'raet_port': int, 'raet_alt_port': int, 'raet_mutable': bool, 'raet_main': bool, 'raet_clear_remotes': bool, 'raet_clear_remote_masters': bool, 'raet_road_bufcnt': int, 'raet_lane_bufcnt': int, 'cluster_mode': bool, 'cluster_masters': list, 'sqlite_queue_dir': str,
if 'sysctl.default_config' in __salt__: config = __salt__['sysctl.default_config']() else: config = '/etc/sysctl.conf'
else: raise NotImplemented("cannot align with a higher dimensional " "NDFrame")
self.assertRaises(TypeError, rwindow._flex_binary_moment, 5, 6, None)
proc = Popen(["kubectl", "--namespace=default", "get", "secrets", name, "-o", "json"], stdout=PIPE) kubectl_out = json.loads(proc.communicate()[0]) b = kubectl_out.get("data", {}) self.assertTrue(isinstance(kubectl_out, dict)) self.assertEqual(expected_data, b)
true_sum = pred_sum = tp_sum = np.zeros(len(labels))
res = df.copy() res.iloc[lambda x: [1, 3]] = 0 exp = df.copy() exp.iloc[[1, 3]] = 0 tm.assert_frame_equal(res, exp)
prefs = self.get_json(USER_PREFERENCE_LIST_URI)["results"] for pref in prefs: if pref["user"]["id"] == target_pref.user.id and pref["key"] == target_pref.key: return pref["url"] self.fail()
self.assertTrue(is_survey_required_for_course(self.course))
if self.n_iter: if current_log_likelihood > max_log_prob: max_log_prob = current_log_likelihood best_params = {'weights': self.weights_, 'means': self.means_, 'covars': self.covars_} if self.verbose > 1: print('\tBetter parameters were found.')
data['changed'] = salt.fileserver.diff_mtime_map(old_mtime_map, new_mtime_map)
from napalm import get_network_driver HAS_NAPALM = True
from __future__ import absolute_import import os import copy import tempfile import json import datetime import pprint
res = c1.searchsorted(['bread', 'eggs'], side='right') chk = s1.searchsorted(['bread', 'eggs'], side='right')
try: course_key = CourseKey.from_string(course_id) user = User.objects.get(id=user_id) course = modulestore().get_course(course_key)
err = st_.verify_data(kwargs) if err: __context__['retcode'] = 1 return err
data = {"Testing invalid"} response = self.client.post( reverse('verify_student_results_callback'), data=data, content_type='application/json', HTTP_AUTHORIZATION='test BBBBBBBBBBBBBBBBBBBB: testing', HTTP_DATE='testdate' ) self.assertIn('Invalid JSON', response.content) self.assertEqual(response.status_code, 400)
defaults = { "formfield_callback": partial(self.formfield_for_dbfield, request=request), } defaults.update(kwargs) return modelformset_factory( self.model, self.get_changelist_form(request), extra=0, fields=self.list_editable, **defaults )
SWAGGER_OBJ_V2_FIELDS_REQUIRED = ('swagger', 'info', 'basePath', 'schemes', 'paths', 'definitions') SWAGGER_OPERATION_NAMES = ('get', 'put', 'post', 'delete', 'options', 'head', 'patch') SWAGGER_VERSIONS_SUPPORTED = ('2.0',)
DEBUG = False SESSION_COOKIE_SECURE = False SESSION_SAVE_EVERY_REQUEST = False SESSION_SERIALIZER = 'django.contrib.sessions.serializers.PickleSerializer'
self.assertContains(resp, 'visible_course')
user: myuser@pam or myuser@pve password: mypassword url: hypervisor.domain.tld driver: proxmox verify_ssl: True
ovr = OneVsRestClassifier(LinearSVC(random_state=0)) pred = ovr.fit(iris.data, iris.target).predict(iris.data) assert_equal(len(ovr.estimators_), n_classes)
student_answers = kwargs['student_answers'] if (self.answer_id in student_answers and student_answers[self.answer_id] in self.correct_choices): return CorrectMap(self.answer_id, correctness='correct') elif ( self.answer_id in student_answers and student_answers[self.answer_id] in self.partial_choices ): choice_index = self.partial_choices.index(student_answers[self.answer_id]) credit_amount = self.partial_values[choice_index] return CorrectMap(self.answer_id, correctness='partially-correct', npoints=credit_amount) else: return CorrectMap(self.answer_id, 'incorrect')
expec = DataFrame({'a': mix['a'], 'b': [nan, 'b', '.', '.'], 'c': mix['c']}) res = dfmix.replace('a', {'b': nan}, regex=True) res2 = dfmix.copy() res2.replace('a', {'b': nan}, regex=True, inplace=True) assert_frame_equal(res, expec) assert_frame_equal(res2, expec)
result = self.client.get_html(self._url()) self.assertIn('Test certificate', result.content) self.assertIn('Test description', result.content)
try: post_dict = urlparse.parse_qs(contents, keep_blank_values=True) return { key: list_val[0] for key, list_val in post_dict.items() }
if len(new_overlays) == 0: srcline = 'source /var/lib/layman/make.conf' makeconf = _get_makeconf() if __salt__['file.contains'](makeconf, 'layman'): __salt__['file.sed'](makeconf, srcline, '')
self.assertFalse(CourseEnrollment.is_enrolled(self.user, self.course.id))
res = grainsmod.filter_by(dict2, default='xxx', base='default') self.assertEqual( res, dictupdate.update(copy.deepcopy(dict2['default']), dict2['MockedOS']) )
return _client().cache_local_file(path)
with new_connection.cursor(): pass new_connection.queries_log.clear()
y = np.asarray([1, 1, 1, 2, 2, 2, 3]) sample_weight = assert_warns(DeprecationWarning, compute_sample_weight, "auto", y) expected_auto = np.asarray([.6, .6, .6, .6, .6, .6, 1.8]) assert_array_almost_equal(sample_weight, expected_auto) sample_weight = compute_sample_weight("balanced", y) expected_balanced = np.array([0.7777, 0.7777, 0.7777, 0.7777, 0.7777, 0.7777, 2.3333]) assert_array_almost_equal(sample_weight, expected_balanced, decimal=4)
elif trigger_types[trigger_type] == TASK_TRIGGER_BOOT: trigger.Id = 'OnBoot_ID1'
RequestContext(HttpRequest()).new().new()
if context is None: context = self.serializer_context context['requested_fields'].extend([ 'children', 'display_name', 'graded', 'format', 'block_counts', 'student_view_data', 'student_view_multi_device', 'lti_url', 'visible_to_staff_only', ])
avg = _ensure_numeric(values.sum(axis=axis, dtype=np.float64)) / count if axis is not None: avg = np.expand_dims(avg, axis) sqr = _ensure_numeric((avg - values)**2) np.putmask(sqr, mask, 0) result = sqr.sum(axis=axis, dtype=np.float64) / d
from salt.states import rabbitmq_plugin
smode = __salt__['config.manage_mode'](stats['mode']) mode = __salt__['config.manage_mode'](mode) if mode is not None and mode != smode: changes['mode'] = mode return changes
course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
if self.multiply is not None: batch *= self.multiply return batch
self.assertEqual(response.status_code, 204)
panel4dc = self.panel4d.copy() panel4dc.iloc[0] = 1 panel4dc.iloc[1] = True panel4dc.iloc[2] = 'foo' self.assertTrue((panel4dc.iloc[0].values == 1).all()) self.assertTrue(panel4dc.iloc[1].values.all()) self.assertTrue((panel4dc.iloc[2].values == 'foo').all())
raise ValueError( 'ignore_lst must be a list of parameters to ignore ' '%s (type %s) was given' % (ignore_lst, type(ignore_lst)))
form_data['categories'] = [six.text_type(self.c1.id), six.text_type(self.c2.id)] f = ArticleForm(form_data) new_art = f.save(commit=False)
from __future__ import print_function
if isinstance(block_locator.block_id, LocalId): self.local_modules[block_locator] = module
pass
import salt import salt.utils import salt.version import salt.loader import salt.ext.six as six from salt.utils.decorators import depends
honor, _ = self.create_mode('honor', 'Honor') self.assertFalse(CourseMode.has_payment_options(self.course_key))
result = embargo_api.check_course_access(self.course.id, ip_address='0.0.0.0') self.assertTrue(result)
set_use_numexpr(True)
from __future__ import unicode_literals
return (isinstance(obj, slice) and obj.start is None and obj.stop is None and obj.step is None)
attrs = [ (name, " ".join(sorted(value.split(" ")))) if name == "class" else (name, value) for name, value in attrs ] element = Element(tag, attrs) self.current.append(element) if tag not in self.SELF_CLOSING_TAGS: self.open_tags.append(element) self.element_positions[element] = self.getpos()
get_connection(using).savepoint_commit(sid)
if not np.any(Y_ == i): continue plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)
ae = Autoencoder(5, 7, act_enc='tanh', act_dec='cos', tied_weights=True) model = DeepComposedAutoencoder([ae]) model._ensure_extensions() data = np.random.randn(10, 5).astype(config.floatX) model.perform(data)
beta = linalg.solve_triangular(G, np.dot(Q.T, Yt))
response = self.send_get( client=self.client, url=reverse( 'bookmarks_detail', kwargs={'username': self.user.username, 'usage_id': unicode(self.sequential_1.location)} ), query_parameters=query_params ) data = response.data self.assertIsNotNone(data) self.assert_bookmark_data_is_valid(self.bookmark_1, data, check_optional_fields=check_optional_fields)
run_command = (test_command + 'prepare').format(mode) __salt__['cmd.run'](run_command)
comb = dict([(k, v.filled()) if hasattr( v, 'filled') else (k, v) for k, v in comb])
'ad_select': '0', 'tx_queues': '16', 'miimon': '100', 'arp_interval': '250', 'downdelay': '200', 'lacp_rate': '0', 'max_bonds': '1', 'updelay': '0', 'use_carrier': 'on', 'xmit_hash_policy': 'layer2',
ret['comment'] = remote_loc.capitalize() \ if rev == 'HEAD' \ else remote_loc ret['comment'] += ( ' is already present and local HEAD ({0}) does not ' 'match, but update_head=False. HEAD has not been ' 'updated locally.'.format(local_rev[:7]) ) return ret
y_true = np.array([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1]) y_pred = np.array([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1])
origin = bool(re.search('/', pkgname))
return assertIs(expr, None, msg)
with tm.assertRaisesRegexp(ValueError, 'fill value must be in categories'): idx.fillna(2.0)
certs_api.set_cert_generation_enabled(self.COURSE_KEY, True) self._assert_enabled_for_course(self.COURSE_KEY, True)
assert_raises(ValueError, clf.fit(X_, y_).predict, rng.random_sample((3, 12)))
self.assertRaises( ValueError, lambda: DataFrame([pd.Categorical(list('abc')), pd.Categorical(list('abdefg'))]))
i = len(os.path.commonprefix([start_list, path_list]))
def __init__(self, op, operand): super(UnaryOp, self).__init__(op, (operand,)) self.operand = operand try: self.func = _unary_ops_dict[op] except KeyError: raise ValueError('Invalid unary operator {0!r}, valid operators ' 'are {1}'.format(op, _unary_ops_syms)) def __call__(self, env): operand = self.operand(env) return self.func(operand) def __unicode__(self): return pprint_thing('{0}({1})'.format(self.op, self.operand)) @property def return_type(self): operand = self.operand if operand.return_type == np.dtype('bool'): return np.dtype('bool') if (isinstance(operand, Op) and (operand.op in _cmp_ops_dict or operand.op in _bool_ops_dict)): return np.dtype('bool') return np.dtype('int')
path = os.path.join(TEST_ROOT, 'uploads/', file_name) world.browser.execute_script("$('input.file-input').css('display', 'block')") world.browser.attach_file('file', os.path.abspath(path)) close_css = 'a.close-button' world.css_click(close_css)
response = requests.get(self._get_url("api/v1/annotations"), params={ "user": "dummy-user-id", "page_size": 10 }) self.assertTrue(response.ok) self._verify_pagination_info( response=response.json(), total_notes=5, num_pages=1, notes_per_page=5, start=0, current_page=1, next_page=None, previous_page=None )
if ccxs.exists(): return ccxs[0] return None
self.cert_status = cert_status with patch('student.views.cert_info', side_effect=self.mock_cert): response = self.client.post( reverse('change_enrollment'), {'enrollment_action': 'unenroll', 'course_id': self.course.id} ) self.assertEqual(response.status_code, status_code) if status_code == 200: course_enrollment.assert_called_with(self.user, self.course.id) else: course_enrollment.assert_not_called()
user2 = UserFactory() with self.assertRaises(PermissionDenied): reindex_course_and_check_access(self.course.id, user2)
url(r'^get_students_problem_grades$', 'class_dashboard.dashboard_data.get_students_problem_grades', name="get_students_problem_grades"),
result = _get_xblock_parent(self.vert1, 'unit') self.assertIsNone(result)
import ioflo.base.deeding
new_children = set(children) - set(xblock.children) for new_child in new_children: old_parent_location = store.get_parent_location(new_child) if old_parent_location: old_parent = store.get_item(old_parent_location) old_parent.children.remove(new_child) old_parent = _update_with_callback(old_parent, user) else: return JsonResponse({"error": "Invalid data, possibly caused by concurrent authors."}, 400)
library_fixture.add_children( XBlockFixtureDesc("html", "Html1", data='html1'), XBlockFixtureDesc("html", "Html2", data='html2'), XBlockFixtureDesc("html", "Html3", data='html3'), XBlockFixtureDesc("html", "Html4", data='html4'), )
p = Publisher.objects.create(name="Acme Publishing") book, created = p.books.update_or_create(name="The Book of Ed & Fred") self.assertTrue(created) self.assertEqual(p.books.count(), 1)
self.assertFalse(buildout._has_old_distribute(self.py_st)) self.assertFalse(buildout._has_setuptools7(self.py_dis)) self.assertTrue(buildout._has_setuptools7(self.py_st)) self.assertFalse(buildout._has_setuptools7(self.py_blank))
matplotlib.use(mplbackend)
with check_exact_number_of_calls(store, '_get_cached_metadata_inheritance_tree', 1):
fig = plt.figure(figsize=(15, 8)) plt.suptitle("Manifold Learning with %i points, %i neighbors" % (1000, n_neighbors), fontsize=14)
d = Decimal(str(ogr_field.value))
if os.system('sphinx-build -b latex -d build/doctrees ' 'source build/latex'): raise SystemExit("Building LaTeX failed.")
add_organization({ 'name': 'Test Organization', 'short_name': 'orgX', 'description': 'Testing Organization Description', }) with modulestore().default_store(store): response = self.client.ajax_post(self.course_create_rerun_url, { 'org': 'orgX', 'number': 'CS101', 'display_name': 'Course with web certs enabled', 'run': '2015_T2' }) self.assertEqual(response.status_code, 200) data = parse_json(response) new_course_key = CourseKey.from_string(data['course_key']) course_orgs = get_course_organizations(new_course_key) self.assertEqual(len(course_orgs), 1) self.assertEqual(course_orgs[0]['short_name'], 'orgX')
argmin = idxmin argmax = idxmax
Login.objects.update(description='description') with self.assertNumQueries(1): Login.objects.distinct('description').order_by('pk').filter( orgunit__name__isnull=False ).delete() self.assertFalse(Login.objects.filter(pk=self.l1.pk).exists()) self.assertTrue(Login.objects.filter(pk=self.l2.pk).exists())
self.run_function('user.delete', [user, True, True]) if not self.run_function('user.add', [user]): self.skipTest('Failed to create the \'{0}\' user'.format(user))
pieces["closest-tag"] = None count_out = run_command(GITS, ["rev-list", "HEAD", "--count"], cwd=root)
hash_func = sha256() for data_item in [self.version, self.session_id, user_id]: hash_func.update(unicode(data_item)) hash_func.update('|') return hash_func.hexdigest()
macro_measure = metric(y_true, y_pred, average="macro") assert_almost_equal(macro_measure, np.mean(label_measure))
class_name = str(self.__class__) shape = self.shape dims = u('Dimensions: %s') % ' x '.join( ["%d (%s)" % (s, a) for a, s in zip(self._AXIS_ORDERS, shape)]) def axis_pretty(a): v = getattr(self, a) if len(v) > 0: return u('%s axis: %s to %s') % (a.capitalize(), pprint_thing(v[0]), pprint_thing(v[-1])) else: return u('%s axis: None') % a.capitalize() output = '\n'.join( [class_name, dims] + [axis_pretty(a) for a in self._AXIS_ORDERS]) return output
return {}
xml = etree.fromstring(expr) xml = self.formulaInstance.preprocess_pmathml(xml) test = etree.tostring(xml)
with assert_raises(TypeError): StudioValidation("id").set_summary("foo")
locator = BlockUsageLocator(course_locator, block_type='chapter', block_id='chapter1') self.assertTrue( modulestore().has_item(locator), "couldn't find chapter1" )
recons = DataFrame.from_items(items, columns=['C', 'B', 'A']) tm.assert_frame_equal(recons, self.frame.ix[:, ['C', 'B', 'A']])
retc = ret.copy()
kwargs['explanation_text'] = None return super(CodeResponseXMLFactory, self).build_xml(**kwargs)
return ((-123.30, -41.32), (174.78, 48.46))
self.assertEqual(300, self.DEFAULT_TIMEOUT)
from __future__ import absolute_import import inspect import logging import time from functools import wraps from collections import defaultdict
if 'check_same_thread' in kwargs and kwargs['check_same_thread']: warnings.warn( 'The `check_same_thread` option was provided and set to ' 'True. It will be overridden with False. Use the ' '`DatabaseWrapper.allow_thread_sharing` property instead ' 'for controlling thread shareability.', RuntimeWarning ) kwargs.update({'check_same_thread': False}) if self.features.can_share_in_memory_db: kwargs.update({'uri': True}) return kwargs
self.client.login(username=self.instructor.username, password="test") self._assert_certificates_visible(False)
tracker.emit( SETTING_CHANGE_INITIATED, { "setting": "email", "old": context['old_email'], "new": context['new_email'], "user_id": user.id, } )
z = self.beta * (T.dot(state_below, self.ising_weights()) + self.ising_b()) return z
embargo_redirect = embargo_api.redirect_if_blocked( course_key, user=request.user, ip_address=get_ip(request), url=request.path ) if embargo_redirect: return redirect(embargo_redirect)
sortorder = None
return f.ix[i]
subnet = yaml_data['minion'][0]
return self.prior.get_params()
digits = load_digits() data = digits.data
if is_request_from_mobile_web_view(request): return True if getattr(settings, 'MOBILE_APP_USER_AGENT_REGEXES', None): user_agent = request.META.get('HTTP_USER_AGENT') if user_agent: for user_agent_regex in settings.MOBILE_APP_USER_AGENT_REGEXES: if re.search(user_agent_regex, user_agent): return True return False
if bool(actual_data) != bool(data): ret.update({item: {'old': actual_data, 'new': data}})
AVAIL_SVR_DIRS = []
rng1 = pd.period_range('1/1/2000', freq='D', periods=5) other1 = pd.period_range('1/6/2000', freq='D', periods=5) expected1 = pd.period_range('1/1/2000', freq='D', periods=10)
self.white_label_course = CourseFactory.create() self.white_label_course_mode = CourseModeFactory.create( course_id=self.white_label_course.id, mode_slug=CourseMode.HONOR, min_price=10, suggested_prices='10', )
from __future__ import absolute_import import os import shutil import tempfile
response = self.client.get('/lastmod-sitemaps/ascending.xml') self.assertEqual(response['Last-Modified'], 'Sat, 20 Apr 2013 05:00:00 GMT')
can_import_settings = False url_schemes = ['http', 'https', 'ftp'] leave_locale_alone = True rewrite_template_suffixes = ( ('.py-tpl', '.py'), )
resource = None for field in identifier_fields: if field in data: resource = get_resource(resource_name, data[field], identifier_fields, profile, subdomain, api_key) if resource is not None: break
if expected_error_msg is None: self.assertTrue(response_dict.get("success")) self.assertIsNone(response_dict.get("msg")) self.assertFalse(self._user_in_cohort(username, cohort)) else: self.assertFalse(response_dict.get("success")) self.assertEqual(response_dict.get("msg"), expected_error_msg)
est.fit(data, y) threshold = 0.5 * np.mean(est.feature_importances_) mask = est.feature_importances_ > threshold assert_array_equal(X_transform, data[:, mask])
s = Series(np.random.randn(10)) result = np.ones_like(s) expected = Series(1, index=range(10), dtype='float64')
self.assertIn( settings.MICROSITE_CONFIGURATION['test_microsite']["urls"]['ABOUT'], data['company_about_url'] )
if slug is not None and (pk is None or self.query_pk_and_slug): slug_field = self.get_slug_field() queryset = queryset.filter(**{slug_field: slug})
with self.assertRaises(TemplateSyntaxError): self.engine.get_template('basic-syntax06')
mocked_popen_wrapper.return_value = ( "xgettext (GNU gettext-tools) 0.18.1\n" "Copyright (C) 1995-1998, 2000-2010 Free Software Foundation, Inc.\n" "License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n" "This is free software: you are free to change and redistribute it.\n" "There is NO WARRANTY, to the extent permitted by law.\n" "Written by Ulrich Drepper.\n", '', 0) cmd = MakeMessagesCommand() self.assertEqual(cmd.gettext_version, (0, 18, 1))
self.assertIsNotNone(json_response['display_name']) self.assertIsNotNone(json_response['id']) self.assertIsNotNone(json_response['category']) self.assertTrue(json_response['published']) if json_response.get('child_info', None): for child_response in json_response['child_info']['children']: self.assert_correct_json_response(child_response)
LogoutPage(self.browser).visit() self._auto_auth("STAFF_TESTER", "staff101@example.com", True) self.course_outline.visit() self.course_outline.open_subsection_settings_dialog() self.assertTrue(self.course_outline.proctoring_items_are_displayed())
section = ItemFactory.create(parent=self.course, category='chapter', display_name='Test Section') subsection = ItemFactory.create(parent=section, category='sequential', display_name='Test Subsection') vertical = ItemFactory.create(parent=subsection, category='vertical', display_name='Test Unit') self.reverification = ItemFactory.create( parent=vertical, category='edx-reverification-block', display_name='Test Verification Block' ) self.section_location = section.location self.subsection_location = subsection.location self.vertical_location = vertical.location self.reverification_location = unicode(self.reverification.location) self.reverification_assessment = self.reverification.related_assessment
if not any( (salt.utils.compare_versions(ver1=x, oper='>=', ver2=candidate, cmp_func=version_cmp) for x in installed) ): ret[name] = candidate
v0 = random_state.uniform(-1, 1, K.shape[0]) self.lambdas_, self.alphas_ = eigsh(K, n_components, which="LA", tol=self.tol, maxiter=self.max_iter, v0=v0)
idx = Index(['three', 'one', 'two']) result = idx.join(self.index, level='second') tm.assertIsInstance(result, MultiIndex)
if not isinstance(batch, np.ndarray) \ and str(type(batch)) != "<type 'CudaNdarray'>": raise TypeError("The value of a VectorSequenceSpace batch " "should be a numpy.ndarray, or CudaNdarray, " "but is %s." % str(type(batch))) if batch.ndim != 2: raise ValueError("The value of a VectorSequenceSpace batch " "must be 2D, got %d dimensions for %s." % (batch.ndim, batch)) if batch.shape[1] != self.dim: raise ValueError("The width of a VectorSequenceSpace 'batch' " "must match with the space's window" "dimension, but batch has dim %d and " "this space's dim is %d." % (batch.shape[1], self.dim))
objects = NoneToEmptyManager() user = models.ForeignKey(User, db_index=True) anonymous_user_id = models.CharField(unique=True, max_length=32) course_id = CourseKeyField(db_index=True, max_length=255, blank=True) unique_together = (user, course_id)
strpfmt = '{0}_%a_%b_%d_%H-%M-%S_%f_%Y'.format(basename)
new_group = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, user_partition) self.assertEqual(old_group.id, new_group.id)
from pandas.tseries.api import DatetimeIndex periods = 1000 ind = DatetimeIndex(start='2012/1/1', freq='5min', periods=periods) df = DataFrame({'high': np.arange(periods), 'low': np.arange(periods)}, index=ind) grouped = df.groupby(lambda x: datetime(x.year, x.month, x.day))
s = Series(['foo'] * 100, dtype='O') s[::2] = np.nan result = s.unique() self.assertEqual(len(result), 2)
DECIMAL_SEPARATOR = ','
return self.q(css='.create-team.wrapper-msg .copy')[0].text
current_umask = os.umask(0o027) verify_files([logfile], self.config['user']) os.umask(current_umask)
self.set_year_of_birth(current_year - 14) self.assertFalse(self.profile.requires_parental_consent()) self.assertFalse(self.profile.requires_parental_consent(date=datetime.date(current_year, 1, 1)))
test_list = ('1', '2', '3', '4', '11', '12', '13', '101', '102', '103', '111', 'something else', None) result_list = ('1<sup>er</sup>', '2<sup>e</sup>', '3<sup>e</sup>', '4<sup>e</sup>', '11<sup>e</sup>', '12<sup>e</sup>', '13<sup>e</sup>', '101<sup>er</sup>', '102<sup>e</sup>', '103<sup>e</sup>', '111<sup>e</sup>', 'something else', 'None') with translation.override('fr-fr'): self.humanize_tester(test_list, result_list, 'ordinal', lambda x: x)
publications = models.ManyToManyField(Publication, name='publications') tags = models.ManyToManyField(Tag, related_name='tags')
r.COOKIES = {settings.LANGUAGE_COOKIE_NAME: 'zh-hans'} r.META = {'HTTP_ACCEPT_LANGUAGE': 'de'} self.assertEqual(g(r), 'zh-hans')
clf = DecisionTreeClassifier(random_state=0) clf.fit(iris.data, iris.target) clf2 = DecisionTreeClassifier(random_state=0, max_leaf_nodes=len(iris.data)) clf2.fit(iris.data, iris.target)
from sklearn.tree._tree import TREE_LEAF X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1) k = 4 for name, TreeEstimator in ALL_TREES.items(): est = TreeEstimator(max_depth=None, max_leaf_nodes=k + 1).fit(X, y) tree = est.tree_ assert_equal((tree.children_left == TREE_LEAF).sum(), k + 1)
return self.as_sql(compiler, connection, template='%(expressions)s::%(db_type)s')
from sklearn import datasets datasets.mldata.urlopen = urlopen
self.assertEqual(Timedelta('10 days 1 hour'), timedelta(days=10, hours=1)) self.assertEqual(Timedelta('10 days 1 h'), timedelta(days=10, hours=1)) self.assertEqual(Timedelta('10 days 1 h 1m 1s'), timedelta( days=10, hours=1, minutes=1, seconds=1)) self.assertEqual(Timedelta('-10 days 1 h 1m 1s'), - timedelta(days=10, hours=1, minutes=1, seconds=1)) self.assertEqual(Timedelta('-10 days 1 h 1m 1s'), - timedelta(days=10, hours=1, minutes=1, seconds=1)) self.assertEqual(Timedelta('-10 days 1 h 1m 1s 3us'), - timedelta(days=10, hours=1, minutes=1, seconds=1, microseconds=3)) self.assertEqual(Timedelta('-10 days 1 h 1.5m 1s 3us'), - timedelta(days=10, hours=1, minutes=1, seconds=31, microseconds=3))
for whitelist_country in whitelist: CountryAccessRule.objects.create( rule_type=CountryAccessRule.WHITELIST_RULE, restricted_course=self.restricted_course, country=Country.objects.get(country=whitelist_country) )
if _loose_version < '0.14.1': del data['frame']['mixed_dup'] del data['panel']['mixed_dup'] if _loose_version < '0.17.0': del data['series']['period'] del data['scalars']['period'] return data
expected = Series( data=['A', 'B', 'C'], index=pd.to_timedelta([0, 10, 20], unit='s') )
ns_td = Timedelta(1, 'ns') self.assertNotEqual(hash(ns_td), hash(ns_td.to_pytimedelta()))
return BlockUsageLocator( course_key=self.course_key, block_type=block_type, block_id=block_id )
msg = "Cannot resolve keyword 'attached_comment_set' into field." with self.assertRaisesMessage(FieldError, msg): Post.objects.filter(attached_comment_set__is_spam=True)
delete_item(category='html', name='test_html')
HOURS_PER_DAY = 24. MIN_PER_HOUR = 60. SEC_PER_MIN = 60.
if os.path.isfile(cache_file): os.remove(cache_file)
return [t.value for t in TagAvailableValues.objects.filter(category=self)]
expected_log_prob_v_given_hs = self.expected_log_prob_v_given_hs(stats, H_hat = H_hat, S_hat = S_hat) expected_log_prob_s_given_h = self.expected_log_prob_s_given_h(stats) expected_log_prob_h = self.expected_log_prob_h(stats) rval = expected_log_prob_v_given_hs + expected_log_prob_s_given_h + expected_log_prob_h assert len(rval.type.broadcastable) == 0 return rval
return [mock.call(self.store, self._get_lib_key(lib)) for lib in libraries]
self.assertEqual(self.func(None), [base.W004])
self.client.logout() self.verify_user_email(EMAIL) (uri, _headers, body) = self.lti.sign( uri=LTI_TPA_LOGIN_URL, http_method='POST', headers={'Content-Type': FORM_ENCODED}, body={'user_id': LTI_USER_ID} ) login_2_response = self.client.post(path=uri, content_type=FORM_ENCODED, data=body) self.assertEqual(login_2_response.status_code, 302) self.assertEqual(login_2_response['Location'], LTI_TPA_COMPLETE_URL) continue_2_response = self.client.get(login_2_response['Location']) self.assertEqual(continue_2_response.status_code, 302) self.assertTrue(continue_2_response['Location'].endswith(reverse('dashboard')))
log.trace('IPCMessagePublisher: binding to socket: {0}'.format(self.socket_path)) if isinstance(self.socket_path, int): self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) self.sock.setblocking(0) self.sock.bind(('127.0.0.1', self.socket_path)) self.sock.listen(128) else: self.sock = tornado.netutil.bind_unix_socket(self.socket_path)
_MIN_ELEMENTS = 10000
self.assertEqual('/%257Eme/places/1/', reverse('places', args=[1]))
course_key = SlashSeparatedCourseKey.from_deprecated_string(course_id) if has_instructor_access_for_class(request.user, course_key): try: data = dashboard_data.get_d3_section_grade_distrib(course_key, section)
from mock import call
shifted_gaussian = np.random.randn(n_samples, 2) + np.array([20, 20])
from __future__ import absolute_import, print_function import os import sys import types import signal import getpass import logging import optparse import traceback import yaml from functools import partial
article.headline = lazy article.save() self.assertEqual(article.headline, notlazy) Article.objects.update(headline=lazy) article = Article.objects.get() self.assertEqual(article.headline, notlazy) Article.objects.all().delete() Article.objects.bulk_create([Article(headline=lazy, pub_date=datetime.now())]) article = Article.objects.get() self.assertEqual(article.headline, notlazy)
with option_context('max_info_columns', 4): buf = StringIO() df.info(buf=buf, verbose=verbose) res = buf.getvalue() self.assertEqual(len(res.strip().split('\n')), len_)
outliers_index = rand_gen.permutation(n_samples)[:n_outliers] outliers_offset = 10. * \ (rand_gen.randint(2, size=(n_outliers, n_features)) - 0.5) data[outliers_index] += outliers_offset inliers_mask = np.ones(n_samples).astype(bool) inliers_mask[outliers_index] = False
pass
vmconfig['changed']['set_{0}'.format(collection)][prop] = vmconfig['state'][collection][prop]
well_known = ( TestSRS( 'GEOGCS["WGS 84",DATUM["WGS_1984",SPHEROID["WGS 84",6378137,298.257223563,' 'AUTHORITY["EPSG","7030"]],TOWGS84[0,0,0,0,0,0,0],AUTHORITY["EPSG","6326"]],' 'PRIMEM["Greenwich",0,AUTHORITY["EPSG","8901"]],UNIT["degree",0.01745329251994328,' 'AUTHORITY["EPSG","9122"]],AUTHORITY["EPSG","4326"]]', wk='WGS84', name='WGS 84', attrs=(('GEOGCS|AUTHORITY', 1, '4326'), ('SPHEROID', 'WGS 84')), ), TestSRS( 'GEOGCS["WGS 72",DATUM["WGS_1972",SPHEROID["WGS 72",6378135,298.26,' 'AUTHORITY["EPSG","7043"]],AUTHORITY["EPSG","6322"]],' 'PRIMEM["Greenwich",0,AUTHORITY["EPSG","8901"]],' 'UNIT["degree",0.01745329251994328,AUTHORITY["EPSG","9122"]],' 'AUTHORITY["EPSG","4322"]]', wk='WGS72', name='WGS 72', attrs=(('GEOGCS|AUTHORITY', 1, '4322'), ('SPHEROID', 'WGS 72')), ), TestSRS( 'GEOGCS["NAD27",DATUM["North_American_Datum_1927",' 'SPHEROID["Clarke 1866",6378206.4,294.9786982138982,' 'AUTHORITY["EPSG","7008"]],AUTHORITY["EPSG","6267"]],' 'PRIMEM["Greenwich",0,AUTHORITY["EPSG","8901"]],' 'UNIT["degree",0.01745329251994328,AUTHORITY["EPSG","9122"]],' 'AUTHORITY["EPSG","4267"]]', wk='NAD27', name='NAD27', attrs=(('GEOGCS|AUTHORITY', 1, '4267'), ('SPHEROID', 'Clarke 1866')) ), TestSRS( 'GEOGCS["NAD83",DATUM["North_American_Datum_1983",' 'SPHEROID["GRS 1980",6378137,298.257222101,' 'AUTHORITY["EPSG","7019"]],AUTHORITY["EPSG","6269"]],' 'PRIMEM["Greenwich",0,AUTHORITY["EPSG","8901"]],' 'UNIT["degree",0.01745329251994328,AUTHORITY["EPSG","9122"]],' 'AUTHORITY["EPSG","4269"]]', wk='NAD83', name='NAD83', attrs=(('GEOGCS|AUTHORITY', 1, '4269'), ('SPHEROID', 'GRS 1980')), ), TestSRS( 'PROJCS["NZGD49 / Karamea Circuit",GEOGCS["NZGD49",' 'DATUM["New_Zealand_Geodetic_Datum_1949",' 'SPHEROID["International 1924",6378388,297,' 'AUTHORITY["EPSG","7022"]],' 'TOWGS84[59.47,-5.04,187.44,0.47,-0.1,1.024,-4.5993],' 'AUTHORITY["EPSG","6272"]],PRIMEM["Greenwich",0,' 'AUTHORITY["EPSG","8901"]],UNIT["degree",0.01745329251994328,' 'AUTHORITY["EPSG","9122"]],AUTHORITY["EPSG","4272"]],' 'PROJECTION["Transverse_Mercator"],' 'PARAMETER["latitude_of_origin",-41.28991152777778],' 'PARAMETER["central_meridian",172.1090281944444],' 'PARAMETER["scale_factor",1],PARAMETER["false_easting",300000],' 'PARAMETER["false_northing",700000],' 'UNIT["metre",1,AUTHORITY["EPSG","9001"]],AUTHORITY["EPSG","27216"]]', wk='EPSG:27216', name='NZGD49 / Karamea Circuit', attrs=(('PROJECTION', 'Transverse_Mercator'), ('SPHEROID', 'International 1924')), ), )
self.browser.refresh() self.cohort_management_page.wait_for_page() self.instructor_dashboard_page.select_cohort_management() self.cohort_management_page.wait_for_page() self.cohort_discussion_topics_are_visible()
return descriptor.location.library_key
print_filename = True
from __future__ import absolute_import, print_function
out = StringIO() export_graphviz(clf, out_file=out, filled=True, impurity=False, proportion=True, special_characters=True, rounded=True) contents1 = out.getvalue() contents2 = 'digraph Tree {\n' \ 'node [shape=box, style="filled, rounded", color="black", ' \ 'fontname=helvetica] ;\n' \ 'edge [fontname=helvetica] ;\n' \ '0 [label=<X<SUB>0</SUB> &le; 0.0<br/>samples = 100.0%<br/>' \ 'value = [0.5, 0.5]>, fillcolor="#e5813900"] ;\n' \ '1 [label=<samples = 50.0%<br/>value = [1.0, 0.0]>, ' \ 'fillcolor="#e58139ff"] ;\n' \ '0 -> 1 [labeldistance=2.5, labelangle=45, ' \ 'headlabel="True"] ;\n' \ '2 [label=<samples = 50.0%<br/>value = [0.0, 1.0]>, ' \ 'fillcolor="#399de5ff"] ;\n' \ '0 -> 2 [labeldistance=2.5, labelangle=-45, ' \ 'headlabel="False"] ;\n' \ '}'
name = self.env.add_tmp(np.float32(right.value)) right = self.term_type(name, self.env)
self.course_nav.go_to_vertical('Test Vertical-0')
api.remove_credit_requirement_status("staff", self.course_key, "grade", "grade") req_status = api.get_credit_requirement_status(self.course_key, "staff", namespace="grade", name="grade") self.assertIsNone(req_status[0]["status"]) self.assertIsNone(req_status[0]["status_date"]) self.assertIsNone(req_status[0]["reason"])
warnings.warn( 'Storage.created_time() is deprecated in favor of get_created_time().', RemovedInDjango20Warning, stacklevel=2, ) raise NotImplementedError('subclasses of Storage must provide a created_time() method')
usage = '%%prog %s [options] %s' % (subcommand, self.args) if self.help: return '%s\n\n%s' % (usage, self.help) else: return usage
values = _prep_ndarray(values, copy=copy)
self.assertEqual([name for name, mgr in food_state.managers], ['food_mgr']) self.assertTrue(all(isinstance(name, six.text_type) for name, mgr in food_state.managers)) self.assertEqual(food_state.managers[0][1].args, ('a', 'b', 1, 2))
tm._skip_if_no_pytz()
all_locales = [] for basedir in basedirs: locale_dirs = filter(os.path.isdir, glob.glob('%s/*' % basedir)) all_locales.extend(map(os.path.basename, locale_dirs))
value = self._resolve_lookup(context)
return render(request, [], {})
import salt.config import salt.utils import salt.utils.jid import salt.utils.process import salt.utils.args import salt.loader import salt.minion import salt.payload import salt.syspaths import salt.exceptions import salt.log.setup as log_setup import salt.defaults.exitcodes from salt.utils.odict import OrderedDict from salt.utils.process import os_is_running, default_signals, SignalHandlingMultiprocessingProcess
w = rng.randn(num_features)
if len(left) != len(right): raise_assert_detail(obj, 'Series length are different', '{0}, {1}'.format(len(left), left.index), '{0}, {1}'.format(len(right), right.index))
self.assertTrue( self.run_function('system.set_disable_keyboard_on_lock', [True])) self.assertTrue( self.run_function('system.get_disable_keyboard_on_lock'))
return self.q(css='body.view-course-create-rerun').present
strong_css = '.your_words strong' target_text = set([world.css_text(strong_css, i) for i in range(2)]) assert set(['text1', 'text2']) == target_text
perim_m = [18404.3550889361, 15627.2108551001, 20632.5588368978, 17094.5996143697] tol = 2 if oracle else 7 qs = SouthTexasZipcode.objects.annotate(perimeter=Perimeter('poly')).order_by('name') for i, z in enumerate(qs): self.assertAlmostEqual(perim_m[i], z.perimeter.m, tol)
foo = Foo()
for _ in xrange(0, max_iters): last_d = doc_topic_d
base_url = reverse('create_mode', args=[unicode(self.course.id)]) self.client.get(base_url)
unique = hashlib.md5(":".join(str(d) for d in self.directories)).hexdigest() self.template_args['module_directory'] = os.path.join(self.__original_module_directory, unique)
mixed = Series(['a', NA, 'b', True, datetime.today(), 'foo', None, 1, 2.]) mixed = mixed.str.upper() rs = Series(mixed).str.lower() xp = Series(['a', NA, 'b', NA, NA, 'foo', NA, NA, NA]) tm.assertIsInstance(rs, Series) tm.assert_series_equal(rs, xp)
result = self.bulk.get_structure(self.course_key, version_guid) self.assertConnCalls( call.get_structure(self.course_key.as_object_id(version_guid), self.course_key) ) self.assertEqual(result, self.conn.get_structure.return_value) self.assertCacheNotCleared()
ret = {'result': False, 'credentials': None, 'credentials_metadata': None} profile_data = copy.deepcopy(profile) if profile_data.get('disabled', False): ret['result'] = True return ret.get(key) token_version = profile_data.get('token_version', 1) try: url = profile_data['url'] auth_key = profile_data['auth_key'] auth_context = profile_data['auth_context'] role = auth_context['from'] except (KeyError, TypeError): msg = ('profile has undefined url, auth_key or auth_context') log.debug(msg) return ret.get(key) region = profile_data.get('region', 'us-east-1') token_duration = profile_data.get('token_duration', 60) retries = profile_data.get('retries', 5) token_cache_file = profile_data.get('token_cache_file') backoff = profile_data.get('backoff', 1) client = confidant.client.ConfidantClient( url, auth_key, auth_context, token_lifetime=token_duration, token_version=token_version, token_cache_file=token_cache_file, region=region, retries=retries, backoff=backoff ) try: data = client.get_service( role, decrypt_blind=True ) except confidant.client.TokenCreationError: return ret.get(key) if not data['result']: return ret.get(key) ret = confidant.formatter.combined_credential_pair_format(data) ret['result'] = True return ret.get(key)
results = {} for k, v in kwargs.items(): results[k] = com._apply_if_callable(v, data)
file_handle.write(asbytes(length.ljust(_MAX_LEN))) file_handle.write(zlib.compress(asbytes(data), compress))
final_template, final_locals = process_template(template, _globals) _globals.update(final_locals)
pc_clf.fit(this_X_train, y_train + 1, sample_weight=sw_train) prob_pos_pc_clf_relabeled = pc_clf.predict_proba(this_X_test)[:, 1] assert_array_almost_equal(prob_pos_pc_clf, prob_pos_pc_clf_relabeled)
pass
if 'ppa_auth' in kwargs: ppa_auth = '{0}@'.format(kwargs['ppa_auth']) repo = LP_PVT_SRC_FORMAT.format(ppa_auth, owner_name, ppa_name, dist) else: repo = LP_SRC_FORMAT.format(owner_name, ppa_name, dist)
if self._output_field is not None: if self.for_save: val = self.output_field.get_db_prep_save(val, connection=connection) else: val = self.output_field.get_db_prep_value(val, connection=connection) if val is None: return 'NULL', [] return '%s', [val]
if 'class_' in attrs: attrs['class'] = attrs.pop('class_')
line_X = np.arange(-5, 5) line_y = model.predict(line_X[:, np.newaxis]) line_y_ransac = model_ransac.predict(line_X[:, np.newaxis])
cases = [Series(period_range('20130101', periods=5, freq='D'), name='xxx')] for s in cases: for prop in ok_for_period: if prop != 'freq': compare(s, prop)
qemu_img.__salt__ = {}
expected = { "action": "enroll", "auto_enroll": False, "results": [ { "identifier": self.notenrolled_student.email, "before": { "enrollment": False, "auto_enroll": False, "user": True, "allowed": False, }, "after": { "enrollment": True, "auto_enroll": False, "user": True, "allowed": False, } } ] }
staff_user = UserFactory(is_staff=True, password=self.test_password) staff_client = APIClient() staff_client.login(username=staff_user.username, password=self.test_password) response = staff_client.get(self.teams_url) self.assertContains(response, "TeamsTabFactory", status_code=200)
return
state_config = config if config else {} config = { 'import': True, 'import_dirs': None, 'device_dir': None if __grains__['kernel'] != 'SunOS' else '/dev/rdsk', 'force': False } config.update(state_config) log.debug('zpool.present::{0}::config - {1}'.format(name, config))
if kwargs.get('refresh_db', True): refresh_db() return { repo: { 'architectures': getattr(mod_source, 'architectures', []), 'comps': mod_source.comps, 'disabled': mod_source.disabled, 'file': mod_source.file, 'type': mod_source.type, 'uri': mod_source.uri, 'line': mod_source.line } }
result = {} for field in self.fields.values(): if field.scope == scope and field.is_set_on(self): try: result[field.name] = field.read_json(self) except TypeError as exception: exception_message = "{message}, Block-location:{location}, Field-name:{field_name}".format( message=exception.message, location=unicode(self.location), field_name=field.name ) raise TypeError(exception_message) return result
data = datasets.fetch_20newsgroups(subset='all') assert_equal(len(data['data']), len(data.data)) assert_equal(len(data['target']), len(data.target)) assert_equal(len(data['filenames']), len(data.filenames))
if isinstance(axis, (tuple, list)) and len(axis) == 2: return self._apply_2d(f, axis=axis)
continue
assert_frame_equal(trunced['ItemA'], expected, check_names=False)
return fnd
import salt.utils
self.wait_for( lambda: self._is_element_visible(".MathJax_SVG"), description="MathJax Preview is rendered" )
import salt.utils
BODY_SELECTOR = "#structure-panel" TAB_SELECTOR = ".tab#view-course-structure" CHILD_SELECTOR = ".note-group" CHILD_CLASS = EdxNotesChapterGroup
y = np.array([[1, 0], [2, 0], [1, 0], [1, 3]]) est = clone(clf) est.fit(X, y) y_pred = est.predict(X) assert_equal(y.shape, y_pred.shape)
lvals = [] for name_full in unique_names: name_base = name_full.split('.', 1)[0] if name_base in frame.f_code.co_varnames: if name_base in locals.keys(): try: value = safe_repr(eval(name_full, locals)) except: value = "undefined" else: value = "undefined" name = name_full lvals.append('%s = %s' % (name, value)) #elif print_globals: if lvals: lvals = '%s%s' % (INDENT, ('\n%s' % INDENT).join(lvals)) else: lvals = ''
MEDIA_ROOT = ENV_TOKENS.get('MEDIA_ROOT', MEDIA_ROOT) MEDIA_URL = ENV_TOKENS.get('MEDIA_URL', MEDIA_URL)
for dtype in [np.int32, np.float64, np.float32, np.bool_, np.int64, object]: arr = np.array([], dtype=dtype) result = com._possibly_downcast_to_dtype(arr, 'int64') tm.assert_almost_equal(result, np.array([], dtype=np.int64)) assert result.dtype == np.int64
self.attempt_login(403, return_to="http://apps.cs50.edx.or")
return dict(zip(('request', 'user', 'course_key', 'course_mode', 'amount'), patched_create_order.call_args[0]))
return swapped_for
assertNumProblems(expected_display_name, 1)
shutil.rmtree(root_dir)
import salt.utils import salt.utils.decorators as decorators import salt.ext.six as six
return make_immutable_fields_list( "local_concrete_fields", (f for f in self.local_fields if f.concrete) )
for subsection in section.get_children(): c_subsection += 1 subsection_name = own_metadata(subsection).get('display_name', '')
for obj in objects: instance = obj.__class__.objects.get(id=obj.pk) self.assertEqual( obj.data, instance.data, "Objects with PK=%d not equal; expected '%s' (%s), got '%s' (%s)" % ( obj.pk, obj.data, type(obj.data), instance, type(instance.data), ) )
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, mock_open, NO_MOCK, NO_MOCK_REASON )
modulestore().delete_course(self.course.id, self.user_id) self.assertIsNone(modulestore().get_course(self.course.id)) response = self.search() self.assertEqual(response["total"], 0)
url: 'https://confidant-production.example.com' auth_context: from: example-production-iad to: confidant-production-iad user_type: service auth_key: "alias/authnz" token_cache_file: /run/confidant/confidant_token token_duration: 60 keyid: 98nh9h9h908h09kjjk key: jhf908gyeghehe0he0g8h9u0j0n0n09hj09h0 region: us-east-1
X = np.array([[1, 1, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0], [0, 1, 0, 1, 0, 0], [0, 1, 1, 0, 0, 1]])
if self.estimators_ is None or len(self.estimators_) == 0: raise NotFittedError("Estimator not fitted, call `fit`" " before making predictions`.")
with self.assertRaises(FieldError): Book.objects.annotate(mean_age=Avg('authors__age')).annotate(Avg('mean_age'))
current_chapter = None if 'chapters' in textbook: for entry in textbook['chapters']: entry['url'] = remap_static_url(entry['url'], course) if chapter is not None and int(chapter) <= (len(textbook['chapters'])): current_chapter = textbook['chapters'][int(chapter) - 1] else: current_chapter = textbook['chapters'][0] viewer_params += current_chapter['url'] current_url = current_chapter['url']
if not classname.replace("_", "").isalnum(): raise InvalidTestName("Python variables should be letters, numbers, and underscores: " + classname) globals()[classname] = plugin print "Loading XBlock test: " + classname xblock_loaded = True
return _deferredSkip( lambda: not all(getattr(connection.features, feature, False) for feature in features), "Database doesn't support feature(s): %s" % ", ".join(features) )
evt = salt.utils.event.get_event('minion', opts=self.opts, listen=False) evt.fire_event({'complete': True, 'schedule': self.opts['schedule']}, tag='/salt/minion/minion_schedule_enabled_complete')
est_en = SGDClassifier(alpha=0.001, penalty='elasticnet', l1_ratio=0.9999999999, random_state=42).fit(X, y) est_l1 = SGDClassifier(alpha=0.001, penalty='l1', random_state=42).fit(X, y) assert_array_almost_equal(est_en.coef_, est_l1.coef_)
if self.opts['priv']: if not os.path.isfile(self.opts['priv']): print('Private-key {0} does not exist'.format(self.opts['priv'])) return self.privkey = self.opts['priv']
import json
if start is not None and start.tz is not None: start = start.replace(tzinfo=None)
return [Attribute('file'), Attribute('missing', None)]
password = self.cleaned_data["password"] if ( self.enforce_username_neq_password and "username" in self.cleaned_data and self.cleaned_data["username"] == password ): raise ValidationError(_("Username and password fields cannot match")) if self.enforce_password_policy: try: validate_password_length(password) validate_password_complexity(password) validate_password_dictionary(password) except ValidationError, err: raise ValidationError(_("Password: ") + "; ".join(err.messages)) return password
connections['default'].allow_thread_sharing = False exceptions = [] do_thread() self.assertIsInstance(exceptions[0], DatabaseError)
FILTERED_LIST = [ 'cohort_config', 'xml_attributes', 'start', 'end', 'enrollment_start', 'enrollment_end', 'tabs', 'graceperiod', 'show_timezone', 'format', 'graded', 'hide_from_toc', 'pdf_textbooks', 'user_partitions',
data = np.array([['R0C0', 'R0C1', 'R0C2', 'R0C3', 'R0C4'], ['R1C0', 'R1C1', 'R1C2', 'R1C3', 'R1C4'], ['R2C0', 'R2C1', 'R2C2', 'R2C3', 'R2C4'], ['R3C0', 'R3C1', 'R3C2', 'R3C3', 'R3C4'], ['R4C0', 'R4C1', 'R4C2', 'R4C3', 'R4C4']]) columns = ['C_l0_g0', 'C_l0_g1', 'C_l0_g2', 'C_l0_g3', 'C_l0_g4'] mi = MultiIndex(levels=[['R_l0_g0', 'R_l0_g1', 'R_l0_g2', 'R_l0_g3', 'R_l0_g4'], ['R_l1_g0', 'R_l1_g1', 'R_l1_g2', 'R_l1_g3', 'R_l1_g4']], labels=[[0, 1, 2, 3, 4], [0, 1, 2, 3, 4]], names=['R0', 'R1']) si = Index(['R_l0_g0', 'R_l0_g1', 'R_l0_g2', 'R_l0_g3', 'R_l0_g4'], name='R0')
required.append(item_name)
split_course_key = CourseLocator(org='orgASD', course='course_01213', run='Run_0_hhh_hhh_hhh') course_updates_url_split = reverse_course_url('course_info_handler', split_course_key) response = self.client.get(course_updates_url_split) self.assertEqual(response.status_code, 404)
fields = {f for f in fields if f != 'pk'}
for idxr in [lambda x: x.ix, lambda x: x]:
MobileApiConfig(video_profiles=" mobile_low , mobile_high,youtube ").save() video_profile_list = MobileApiConfig.get_video_profiles() self.assertEqual( video_profile_list, [u'mobile_low', u'mobile_high', u'youtube'] )
def __init__(self, new_checks, deployment_checks=None): from django.core.checks.registry import registry self.registry = registry self.new_checks = new_checks self.deployment_checks = deployment_checks super(override_system_checks, self).__init__() def enable(self): self.old_checks = self.registry.registered_checks self.registry.registered_checks = self.new_checks self.old_deployment_checks = self.registry.deployment_checks if self.deployment_checks is not None: self.registry.deployment_checks = self.deployment_checks def disable(self): self.registry.registered_checks = self.old_checks self.registry.deployment_checks = self.old_deployment_checks
expected = df.ix[[0, 1, 2, 3]] result = df.drop_duplicates((('AA', 'AB'), 'B')) tm.assert_frame_equal(result, expected)
event_name = '.'.join(['edx', 'certificate', event_name]) if course is None: course = modulestore().get_course(course_id, depth=0) context = { 'org_id': course.org, 'course_id': unicode(course_id) } data = { 'user_id': user.id, 'course_id': unicode(course_id), 'certificate_url': get_certificate_url(user.id, course_id) } event_data = event_data or {} event_data.update(data) with tracker.get_tracker().context(event_name, context): tracker.emit(event_name, event_data)
from pylearn2.space import VectorSpace, CompositeSpace, Conv2DSpace from pylearn2.utils import sharedX from pylearn2.utils.rng import make_np_rng, make_theano_rng from pylearn2.models import Model from pylearn2.blocks import Block from pylearn2.utils import wraps from pylearn2.utils.data_specs import DataSpecsMapping import theano
from __future__ import print_function from __future__ import absolute_import import socket
super(AutoAuthEnabledTestCase, self).setUp() self.url = '/auto_auth' self.client = Client()
obj = Book.objects.select_related('publisher').annotate( num_authors=Count('authors')).values().get(isbn='013790395') self.assertEqual(obj, { 'contact_id': self.a8.id, 'id': self.b5.id, 'isbn': '013790395', 'name': 'Artificial Intelligence: A Modern Approach', 'num_authors': 2, 'pages': 1132, 'price': Decimal("82.8"), 'pubdate': datetime.date(1995, 1, 15), 'publisher_id': self.p3.id, 'rating': 4.0, })
from __future__ import absolute_import
from __future__ import absolute_import try: import pwd HAS_PWD = True except ImportError: HAS_PWD = False import copy import logging
clf = QuadraticDiscriminantAnalysis(store_covariances=True).fit(X6, y6) assert_true(hasattr(clf, 'covariances_'))
self.course_info = { 'org': 'test_org', 'number': 'course_' + self.unique_id[:5], 'run': 'test_' + self.unique_id, 'display_name': 'Test Course ' + self.unique_id }
if len(resolved_envs) == 1 or saltenv in resolved_envs: sls_targets = fnmatch.filter( self.avail[saltenv], inc_sls ) or [inc_sls]
try: child = super(XModuleMixin, self).get_child(usage_id) except ItemNotFoundError: log.warning(u'Unable to load item %s, skipping', usage_id) dog_stats_api.increment( "xmodule.item_not_found_error", tags=[ u"course_id:{}".format(usage_id.course_key), u"block_type:{}".format(usage_id.block_type), u"parent_block_type:{}".format(self.location.block_type), ] ) return None if child is None: return None child.runtime.export_fs = self.runtime.export_fs return child
click_css(self, 'a.duplicate-button', source_index)
continue
if messages_len == 1: payload = self.serial.loads(messages[0]) elif messages_len == 2: if messages[0] not in ('broadcast', self.hexid): log.debug('Publish received for not this minion: {0}'.format(messages[0])) raise tornado.gen.Return(None) payload = self.serial.loads(messages[1]) else: raise Exception(('Invalid number of messages ({0}) in zeromq pub' 'message from master').format(len(messages_len))) ret = yield self._decode_payload(payload) raise tornado.gen.Return(ret)
self.data[col_loc] = col
continue
response = self.send_get( client=self.client, url=reverse( 'bookmarks_detail', kwargs={'username': self.user.username, 'usage_id': 'i4x://arbi/100/html/340ef1771a0940'} ), expected_status=404 ) self.assertEqual( response.data['user_message'], 'Bookmark with usage_id: i4x://arbi/100/html/340ef1771a0940 does not exist.' ) self.assertEqual( response.data['developer_message'], 'Bookmark with usage_id: i4x://arbi/100/html/340ef1771a0940 does not exist.' )
else: msg = _(u"Error: {msg}").format(msg=inst.message)
iris = datasets.load_iris() X = iris.data y = iris.target
data.take(indexer, out=out, axis=i)
self._add_default_notes(["apple", "banana", "kiwi", "pear", "pumpkin", "squash", "zucchini"]) self.notes_page.visit() self._scroll_to_tag_and_verify("pear", 3)
scaler_batch = MaxAbsScaler().fit(X)
text = f.widget.format_value(result) self.assertEqual(text, "13:30:00")
self.assertEqual(406, self.client.post(self.url, {}).status_code) self.assertEqual(406, self.client.post(self.url, {'not_course_id': ''}).status_code)
from __future__ import unicode_literals
root = etree.Element("problem")
formset = self.make_choiceformset([('Calexico', '100'), ('', '')], initial_forms=1) self.assertTrue(formset.is_valid()) self.assertEqual([form.cleaned_data for form in formset.forms], [{'votes': 100, 'choice': 'Calexico'}, {}])
def my_view(request): return HttpResponse("OK") my_safe_view = require_safe(my_view) request = HttpRequest() request.method = 'GET' self.assertIsInstance(my_safe_view(request), HttpResponse) request.method = 'HEAD' self.assertIsInstance(my_safe_view(request), HttpResponse) request.method = 'POST' self.assertIsInstance(my_safe_view(request), HttpResponseNotAllowed) request.method = 'PUT' self.assertIsInstance(my_safe_view(request), HttpResponseNotAllowed) request.method = 'DELETE' self.assertIsInstance(my_safe_view(request), HttpResponseNotAllowed)
from __future__ import absolute_import
df = DataFrame({'a': [u'あああああ', u'い', u'う', u'えええ'], 'b': [u'あ', u'いいい', u'う', u'ええええええ']}, index=pd.Index([u'あ', u'い', u'うう', u'え'], name=u'おおおお')) expected = (u" a b\nおおおお \nあ あああああ あ\n" u"い い いいい\nうう う う\nえ えええ ええええええ" ) self.assertEqual(_rep(df), expected)
if self.opts.get('permissive_pki_access', False) and stat.S_IWGRP & fmode.st_mode: return True elif stat.S_IWGRP & fmode.st_mode: return False
return dict(_typ=self._typ, _subtyp=self._subtyp, _data=self._data, _default_fill_value=self._default_fill_value, _default_kind=self._default_kind)
connected_minions = None return_count = 0
from __future__ import absolute_import import time
if line.lstrip().startswith('#') or line.isspace(): continue if line.startswith('iface'): sline = line.split()
autosum += [" %s%s" % (prefix, param)]
import salt.ext.six as six import salt.utils from salt.modules import network from salt.exceptions import CommandExecutionError if six.PY2: import salt.ext.ipaddress
return request.user.is_active and request.user.is_staff
return chapter_titles.index(title.lower()) + 1
resp = self.client.put(self.exam_url) self.assertEqual(resp.status_code, 405)
store = self._verify_modulestore_support(location.course_key, 'unpublish') return store.unpublish(location, user_id, **kwargs)
if loss_l in ('l1', 'l2'): old_loss = self.loss self.loss = {'l1': 'hinge', 'l2': 'squared_hinge'}.get(loss_l) warnings.warn(msg % (old_loss, self.loss, old_loss, '1.0'), DeprecationWarning)
idx = MultiIndex.from_tuples([('a', 'one'), ('a', 'two'), ('b', 'one'), ('b', 'two')]) s = Series([1, 2, 3, 4], index=idx) s.index.set_names(['L1', 'L2'], inplace=True) result = s.xs('one', level='L2') expected = Series([1, 3], index=['a', 'b']) expected.index.set_names(['L1'], inplace=True) assert_series_equal(result, expected)
self.assertQuerysetEqual( self.a.friends.all(), [ "Bill", "Chuck", "David" ], attrgetter("name"), ordered=False ) self.assertQuerysetEqual( self.b.friends.all(), [ "Anne", ], attrgetter("name") ) self.assertQuerysetEqual( self.c.friends.all(), [ "Anne", "David" ], attrgetter("name"), ordered=False ) self.assertQuerysetEqual( self.d.friends.all(), [ "Anne", "Chuck", ], attrgetter("name"), ordered=False )
learn_rates = [annealed * self.learning_rates[p] for p in self.params]
result = d['a'].fillna(False) | d['b'] expected = Series([True, True]) assert_series_equal(result, expected)
def __hash__(self): return hash(self._params())
result = self.panel.apply(lambda x: x * 2, axis=['items', 'major_axis']) expected = (self.panel * 2).transpose('minor_axis', 'major_axis', 'items') assert_panel_equal(result, expected) result = self.panel.apply(lambda x: x * 2, axis=['major_axis', 'items']) assert_panel_equal(result, expected)
result = DatetimeIndex([Timestamp('2011-01-01 10:00', tz='Asia/Tokyo'), Timestamp('2011-01-02 10:00', tz='Asia/Tokyo')], name='idx') exp = DatetimeIndex( [Timestamp('2011-01-01 10:00'), Timestamp('2011-01-02 10:00') ], tz='Asia/Tokyo', name='idx') self.assert_index_equal(result, exp, exact=True) self.assertTrue(isinstance(result, DatetimeIndex))
__virtualname__ = 'pkg'
company_identifier = microsite.get_value('LINKEDIN_COMPANY_ID', self.company_identifier) params = OrderedDict([ ('_ed', company_identifier), ('pfCertificationName', self._cert_name(course_name, cert_mode).encode('utf-8')), ('pfCertificationUrl', cert_url), ('source', source) ]) tracking_code = self._tracking_code(course_key, cert_mode, target) if tracking_code is not None: params['trk'] = tracking_code return u'http://www.linkedin.com/profile/add?{params}'.format( params=urlencode(params) )
accessible_discussion_ids = [ module.discussion_id for module in get_accessible_discussion_modules(course, user, include_all=include_all) ] return course.top_level_discussion_topic_ids + accessible_discussion_ids
def __init__(self, location, msg): super(SerializationError, self).__init__(msg) self.location = location
def setUp(self): boto_cloudtrail.__context__ = {} context.clear() conn_parameters['key'] = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(50))
self.verify_team_info( name=self.team['name'], description=self.team['description'], location='Afghanistan', language='Afar' ) self.verify_and_navigate_to_edit_team_page() self.fill_create_or_edit_form() expected_events = [ { 'event_type': 'edx.team.changed', 'event': { 'team_id': self.team['id'], 'field': 'country', 'old': 'AF', 'new': 'PK', 'truncated': [], } }, { 'event_type': 'edx.team.changed', 'event': { 'team_id': self.team['id'], 'field': 'name', 'old': self.team['name'], 'new': self.TEAMS_NAME, 'truncated': [], } }, { 'event_type': 'edx.team.changed', 'event': { 'team_id': self.team['id'], 'field': 'language', 'old': 'aa', 'new': 'en', 'truncated': [], } }, { 'event_type': 'edx.team.changed', 'event': { 'team_id': self.team['id'], 'field': 'description', 'old': self.team['description'], 'new': self.TEAM_DESCRIPTION, 'truncated': [], } }, ] with self.assert_events_match_during( event_filter=self.only_team_events, expected_events=expected_events, ): self.team_management_page.submit_form() self.team_page.wait_for_page() self.verify_team_info( name=self.TEAMS_NAME, description=self.TEAM_DESCRIPTION, location='Pakistan', language='English' )
if step_score: self.scores_.append(step_score(self.estimator_, features)) self.n_features_ = support_.sum() self.support_ = support_ self.ranking_ = ranking_
data = 'a b c\n1 2 3' msg = 'does not support'
fstypes.add('*')
def setUp(self): super(CommentViewSetDeleteTest, self).setUp() self.url = reverse("comment-detail", kwargs={"comment_id": "test_comment"}) self.comment_id = "test_comment" def test_basic(self): self.register_get_user_response(self.user) cs_thread = make_minimal_cs_thread({ "id": "test_thread", "course_id": unicode(self.course.id), }) self.register_get_thread_response(cs_thread) cs_comment = make_minimal_cs_comment({ "id": self.comment_id, "course_id": cs_thread["course_id"], "thread_id": cs_thread["id"], "username": self.user.username, "user_id": str(self.user.id), }) self.register_get_comment_response(cs_comment) self.register_delete_comment_response(self.comment_id) response = self.client.delete(self.url) self.assertEqual(response.status_code, 204) self.assertEqual(response.content, "") self.assertEqual( urlparse(httpretty.last_request().path).path, "/api/v1/comments/{}".format(self.comment_id) ) self.assertEqual(httpretty.last_request().method, "DELETE") def test_delete_nonexistent_comment(self): self.register_get_comment_error_response(self.comment_id, 404) response = self.client.delete(self.url) self.assertEqual(response.status_code, 404)
df = DataFrame({'A': list('abc')}, dtype='category') expected = Series(list('abc'), dtype='category', name='A') tm.assert_series_equal(df['A'], expected)
return line.endswith('\n' if isinstance(line, six.text_type) else b'\n')
all_requirements = { self.ACCOUNT_ACTIVATION_REQ: not is_active, self.PHOTO_ID_REQ: False, self.WEBCAM_REQ: False, } display_steps = set(step['name'] for step in display_steps) for step, step_requirements in self.STEP_REQUIREMENTS.iteritems(): if step in display_steps: for requirement in step_requirements: all_requirements[requirement] = True return all_requirements
response = self.client.get('/get_view/') self.assertEqual(response.resolver_match.url_name, 'get_view')
pass
self.browser.refresh() return self.components
ret['comment'] = 'Event module not available. Beacon enable job failed.'
exif_dict = piexif.load(exif) exif_dict['0th'][piexif.ImageIFD.Orientation] = orientation return piexif.dump(exif_dict)
http_methods_and_views = ( ('get', get_view), ('post', post_view), ('put', _generic_view), ('patch', _generic_view), ('delete', _generic_view), ('head', _generic_view), ('options', _generic_view), ('trace', trace_view), )
course = Mock( id=SlashSeparatedCourseKey('edX', 'test', '2012_Fall'), catalog_visibility=CATALOG_VISIBILITY_ABOUT ) self.assertFalse(access._has_access_course(user, 'see_in_catalog', course)) self.assertTrue(access._has_access_course(user, 'see_about_page', course)) self.assertTrue(access._has_access_course(staff, 'see_in_catalog', course)) self.assertTrue(access._has_access_course(staff, 'see_about_page', course))
self.assertRaises(KeyError, store.select_column, 'df', 'foo')
axarr = np.empty(nplots, dtype=object)
colg = self._gotitem(self._selection, ndim=2, subset=obj) return colg.aggregate(how, _level=None)
p = locale.find('_') if p >= 0: return locale[:p].lower() + '-' + locale[p + 1:].lower() else: return locale.lower()
columns = {} self.query.deferred_to_data(columns, self.query.get_loaded_field_names_cb) return columns
course_wiki_home = reverse('course_wiki', kwargs={'course_id': course.id.to_deprecated_string()}) referer = reverse("progress", kwargs={'course_id': self.toy.id.to_deprecated_string()}) resp = self.client.get(course_wiki_home, follow=True, HTTP_REFERER=referer) course_wiki_page = referer.replace('progress', 'wiki/' + self.toy.wiki_slug + "/") ending_location = resp.redirect_chain[-1][0] self.assertEquals(ending_location, 'http://testserver' + course_wiki_page) self.assertEquals(resp.status_code, 200) self.has_course_navigator(resp)
proc.start()
for block_key in block_structure.topological_traversal(): assert ( block_structure.get_transformer_block_field( block_key, cls, data_key, ) == cls._create_block_value(block_key, data_key) )
from pandas import compat import nose
return self.incr_version(key, -delta, version)
self.form_data.setlist('requested_fields', ['field1', 'field2'])
assert self.get_mlp() is None self.mlp = mlp
return self.client_address[0]
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
legacy_profile = UserProfile.objects.get(id=self.user.id) name_change_info = legacy_profile.get_meta()["old_names"] self.assertEqual(expected_entries, len(name_change_info)) return name_change_info
( '/ns-included1/test3/inner/42/37/', 'urlobject-view', 'testapp', 'inc-ns1:test-ns3', 'inc-ns1:test-ns3:urlobject-view', views.empty_view, tuple(), {'arg1': '42', 'arg2': '37'} ), ( '/ns-included1/ns-included4/ns-included2/test3/inner/42/37/', 'urlobject-view', 'testapp', 'inc-ns1:inc-ns4:inc-ns2:test-ns3', 'inc-ns1:inc-ns4:inc-ns2:test-ns3:urlobject-view', views.empty_view, tuple(), {'arg1': '42', 'arg2': '37'} ), ( '/app-included/test3/inner/42/37/', 'urlobject-view', 'inc-app:testapp', 'inc-app:test-ns3', 'inc-app:test-ns3:urlobject-view', views.empty_view, tuple(), {'arg1': '42', 'arg2': '37'} ), ( '/app-included/ns-included4/ns-included2/test3/inner/42/37/', 'urlobject-view', 'inc-app:testapp', 'inc-app:inc-ns4:inc-ns2:test-ns3', 'inc-app:inc-ns4:inc-ns2:test-ns3:urlobject-view', views.empty_view, tuple(), {'arg1': '42', 'arg2': '37'} ),
self.setup_logfile_logger() verify_log(self.config)
cmd = 'which zfs'
if step: rstep = step(self._step, other)
if miller(points) in (correct_answer['miller'].replace(' ', ''), negative(correct_answer['miller']).replace(' ', '')): return True
elif mode == 'buckling': mode = 4 if OPinv is None: Minv_matvec = get_OPinv_matvec(A, M, sigma, symmetric=True, tol=tol) else: Minv_matvec = _aslinearoperator_with_dtype(OPinv).matvec matvec = _aslinearoperator_with_dtype(A).matvec M_matvec = None
try:
assert np.allclose(X, np.dot(S_, A_.T) + ica.mean_)
from salt.modules import incron
args = [] if packages is not None: args.append('--remove-packages {pkgs}'.format(pkgs=packages)) LOG.debug('args: {0}'.format(args)) if not _check_cygwin_installed(cyg_arch): LOG.debug('We\'re convinced cygwin isn\'t installed') return True return _run_silent_cygwin(cyg_arch=cyg_arch, args=args, mirrors=mirrors)
_ret = _key_enabled( key_metadata, enabled, region, key, keyid, profile ) ret['changes'] = dictupdate.update(ret['changes'], _ret['changes']) ret['comment'] = ' '.join([ret['comment'], _ret['comment']]) if not _ret['result']: ret['result'] = _ret['result'] if ret['result'] is False: return ret _ret = _key_rotation( key_metadata, key_rotation, region, key, keyid, profile ) ret['changes'] = dictupdate.update(ret['changes'], _ret['changes']) ret['comment'] = ' '.join([ret['comment'], _ret['comment']]) if not _ret['result']: ret['result'] = _ret['result'] return ret
data = kwargs course = course if course else self.test_course_1 data.update({ 'name': name, 'course_id': str(course.id), 'description': description, }) return data
__MP_LOGGING_QUEUE_HANDLER = SaltLogQueueHandler(queue or get_multiprocessing_logging_queue()) logging.root.addHandler(__MP_LOGGING_QUEUE_HANDLER) logging.root.setLevel(logging.GARBAGE) logging.getLogger(__name__).debug( 'Multiprocessing queue logging configured for the process running ' 'under PID: {0}'.format(os.getpid()) ) time.sleep(0.0001)
return self.prepopulated_fields
with self.assertRaises(OGRIndexError): layer.__getitem__(-1) with self.assertRaises(OGRIndexError): layer.__getitem__(50000)
if isinstance(self.remote_field.model, six.string_types): kwargs['to'] = self.remote_field.model else: kwargs['to'] = "%s.%s" % ( self.remote_field.model._meta.app_label, self.remote_field.model._meta.object_name, ) swappable_setting = self.swappable_setting if swappable_setting is not None: if hasattr(kwargs['to'], "setting_name"): if kwargs['to'].setting_name != swappable_setting: raise ValueError( "Cannot deconstruct a ForeignKey pointing to a model " "that is swapped in place of more than one model (%s and %s)" % (kwargs['to'].setting_name, swappable_setting) ) from django.db.migrations.writer import SettingsReference kwargs['to'] = SettingsReference( kwargs['to'], swappable_setting, ) return name, path, args, kwargs
self.seq1 = ItemFactory.create( parent_location=self.chapter1.location, category='sequential', display_name='untitled sequential 1' ) self.seq2 = ItemFactory.create( parent_location=self.chapter1.location, category='sequential', display_name='untitled sequential 2' )
raise NotImplementedError('subclasses of BaseDatabaseOperations may require a datetime_extract_sql() method')
for name in ALL_TREES: yield check_min_weight_fraction_leaf, name, "iris"
X -= X.min()
return [self.mu, self.beta ]
resp = self.client.post(reverse('change_enrollment'), { 'enrollment_action': 'enroll', 'course_id': course.id.to_deprecated_string(), 'check_access': True, }) result = resp.status_code == 200 if verify: self.assertTrue(result) return result
X = self._validate_X_predict(X, check_input) return self.tree_.apply(X)
self.student_a = UserFactory.create(username='student_a', email='student_a@example.com') CourseEnrollmentFactory.create(user=self.student_a, course_id=self.course.id) self.student_b = UserFactory.create(username='student_b', email='student_b@example.com') CourseEnrollmentFactory.create(user=self.student_b, course_id=self.course.id)
with salt.utils.fopen('{0}/{1}.key'.format(csr_path, csr_filename), 'w+') as priv_key: priv_key.write( OpenSSL.crypto.dump_privatekey(OpenSSL.crypto.FILETYPE_PEM, key) )
if not issubclass(item, ListFilter): return must_inherit_from(parent='ListFilter', option=label, obj=obj, id='admin.E113') elif issubclass(item, FieldListFilter): return [ checks.Error( "The value of '%s' must not inherit from 'FieldListFilter'." % label, obj=obj.__class__, id='admin.E114', ) ] else: return []
wua_session = win32com.client.Dispatch('Microsoft.Update.Session')
LANGUAGES_BIDI = ["he", "ar", "fa", "ur"]
expected = self.create_index() if not isinstance(expected, MultiIndex): expected.name = 'foo' result = pd.Index(expected) tm.assert_index_equal(result, expected)
readback = contentstore().find(content.location) locked = getattr(content, 'locked', False) response_payload = { 'asset': _get_asset_json( content.name, content.content_type, readback.last_modified_at, content.location, content.thumbnail_location, locked ), 'msg': _('Upload completed') }
assert D_sparse.nnz < D.shape[0] * (D.shape[0] - 1) core_sparse, labels_sparse = dbscan(D_sparse, eps=.8, min_samples=10, metric='precomputed') core_dense, labels_dense = dbscan(D, eps=.8, min_samples=10, metric='precomputed') assert_array_equal(core_dense, core_sparse) assert_array_equal(labels_dense, labels_sparse)
_win(kernel32.DuplicateHandle, wintypes.BOOL,
self.chapter = self.store.get_item(self.chapter.location)
course_key = course.id section_data = { 'section_key': 'cohort_management', 'section_display_name': _('Cohorts'), 'access': access, 'course_cohort_settings_url': reverse( 'course_cohort_settings', kwargs={'course_key_string': unicode(course_key)} ), 'cohorts_url': reverse('cohorts', kwargs={'course_key_string': unicode(course_key)}), 'upload_cohorts_csv_url': reverse('add_users_to_cohorts', kwargs={'course_id': unicode(course_key)}), 'discussion_topics_url': reverse('cohort_discussion_topics', kwargs={'course_key_string': unicode(course_key)}), 'verified_track_cohorting_url': reverse( 'verified_track_cohorting', kwargs={'course_key_string': unicode(course_key)} ), } return section_data
try: record = cls.objects.get(course_id=course_id) return record.embargoed except cls.DoesNotExist: return False
SEARCH_ENGINE = "search.tests.mock_search_engine.MockSearchEngine" MOCK_SEARCH_BACKING_FILE = ( TEST_ROOT / "index_file.dat" ).abspath()
if f_ordered: new_values = new_values.T
BINS = frozenset(('svc', 'supervise', 'svok')) return all(salt.utils.which(b) for b in BINS)
def isatty(self): return True
req = self._get_GET_csrf_cookie_request() CsrfViewMiddleware().process_view(req, csrf_exempt(token_view), (), {}) resp = token_view(req) self._check_token_present(resp)
self.q(css='.label-username').first.click()
plt.figure(figsize=(7, 7)) plt.subplot(4, 1, 1) plt.xlim(0, 512) plt.title("Sparse signal") plt.stem(idx, w[idx])
from salt.exceptions import CommandExecutionError import salt.utils
if cmnt: regex = '{0}({1}){2}'.format( '^' if regex.startswith('^') else '', regex.lstrip('^').rstrip('$'), '$' if regex.endswith('$') else '') else: regex = r'^{0}\s*({1}){2}'.format( char, regex.lstrip('^').rstrip('$'), '$' if regex.endswith('$') else '')
from __future__ import unicode_literals
return iter(getattr(d, _iteritems)(**kw))
labels = np.array([1, 1, 1, 2, 2]) assert_raises(ValueError, cval.LabelKFold, labels, n_folds=3)
offsets = [pd.offsets.Hour(2), timedelta(hours=2), np.timedelta64(2, 'h'), Timedelta(hours=2)]
self.assertEqual('10000', nformat(self.l, decimal_sep='.', decimal_pos=0, grouping=0, force_grouping=True))
component = self.store.get_item(component.location) component.display_name = 'Changed Display Name' self.store.update_item(component, self.user_id) self.assertTrue(self._has_changes(vertical.location))
return set([p.id for p in course.user_partitions])
def rows(self): query = use_read_replica_if_available( OrderItem.objects.filter( status="purchased", fulfilled_time__gte=self.start_date, fulfilled_time__lt=self.end_date, ).order_by("fulfilled_time")) for item in query: yield [ item.fulfilled_time, item.order_id, item.status, item.qty, item.unit_cost, item.line_cost, item.currency, item.line_desc, item.report_comments, ] def header(self): return [ _("Purchase Time"), _("Order ID"), _("Status"), _("Quantity"), _("Unit Cost"), _("Total Cost"), _("Currency"), _("Description"), _("Comments") ]
ct = self.get(pk=id) self._add_to_cache(self.db, ct)
super(GitExportError, self).__init__(unicode(message))
_params = [sharedX(rng.randn(5)), sharedX(rng.randn(5, 3)), sharedX(rng.randn(4, 4, 4))]
else:
if hasattr(error, 'order'): _record_payment_info(params, error.order) else: log.info(json.dumps(params)) return { 'success': False,
self.video.wait_for_state('pause')
try: return cls.objects.get(Q(invoice_id=invoice_id), Q(status='completed') | Q(status='refunded')) except InvoiceTransaction.DoesNotExist: return None
resp = ProxyImprovement.objects.select_related().get( reporter__name__icontains='butor' ) self.assertEqual( repr(resp), '<ProxyImprovement: ProxyImprovement:improve that>' )
X = X.copy('F')
assert_greater(np.abs(s[:k] - sa).max(), 0.01)
assert_less(np.sqrt(np.mean((X_iso - X_iso2) ** 2)), 2 * noise_scale)
self.reapeds.value['alpha'] = createStack('1.1.1.1') self.reapeds.value['beta'] = createStack('1.2.3.4') testStack = self.event_stack.value presenceReq = self.presence_req.value ryn = 'manor' presenceReq.append({'route': {'dst': (None, ryn, 'presence_req'), 'src': (None, testStack.local.name, None)}, 'data': {'state': 'reaped'}})
pull_result = __salt__['dockerng.pull']( image, client_timeout=client_timeout, )
@python_2_unicode_compatible class RelatedModel(models.Model): test_gfk = GenericRelation('RelationModel', content_type_field='gfk_ctype', object_id_field='gfk_id') exact = models.NullBooleanField()
queryset = CourseMode.objects.all()
mocked_course = Mock(name='mocked_course', lti_passports=['lti_id:test_client:test_secret']) modulestore = Mock(name='modulestore') modulestore.get_course.return_value = mocked_course runtime = Mock(name='runtime', modulestore=modulestore) self.xmodule.descriptor.runtime = runtime self.xmodule.lti_id = "lti_id"
self.course.discussion_topics = {} self.course.save() self.discussion_num = 0 self.instructor = InstructorFactory(course_key=self.course.id)
return subnet
if not allow_future: if generic_view.uses_datetime_field: now = timezone.now() else: now = timezone_today() lookup['%s__lte' % date_field] = now
self.set_group_access(block_specified, {self.animal_partition.id: []}) self.check_access(self.red_cat, block_accessed, True) self.check_access(self.blue_dog, block_accessed, True) self.check_access(self.white_mouse, block_accessed, True) self.check_access(self.gray_worm, block_accessed, True) self.ensure_staff_access(block_accessed)
self._execute_query() if not connections[self.using].features.can_use_chunked_reads: result = list(self.cursor) else: result = self.cursor return iter(result)
X_FRAME_OPTIONS = 'SAMEORIGIN'
projected_distances = projected_distances[non_identical]
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
request = get_request_for_user(self.user) answer_entrance_exam_problem(self.course, request, self.problem_1)
LogoutPage(self.browser).visit() StudioAutoAuthPage(self.browser, username=username, email=email, course_id=self.course_id, staff=staff).visit()
msg = "Expected \d+ fields in line \d+, saw \d+" with tm.assertRaisesRegexp(ValueError, msg): df = self.read_csv(StringIO(csv))
if partition_id not in user_groups: return False
adapter = DOPAdapter() def setUp(self): super(DOPAdapterTestCase, self).setUp() self.user = UserFactory() self.public_client = self.adapter.create_public_client( name='public client', user=self.user, redirect_uri=DUMMY_REDIRECT_URL, client_id='public-client-id', ) self.confidential_client = self.adapter.create_confidential_client( name='confidential client', user=self.user, redirect_uri=DUMMY_REDIRECT_URL, client_id='confidential-client-id', ) @ddt.data( ('confidential', constants.CONFIDENTIAL), ('public', constants.PUBLIC), ) @ddt.unpack def test_create_client(self, client_name, client_type): client = getattr(self, '{}_client'.format(client_name)) self.assertIsInstance(client, models.Client) self.assertEqual(client.client_id, '{}-client-id'.format(client_name)) self.assertEqual(client.client_type, client_type) def test_get_client(self): client = self.adapter.get_client(client_type=constants.CONFIDENTIAL) self.assertIsInstance(client, models.Client) self.assertEqual(client.client_type, constants.CONFIDENTIAL) def test_get_client_not_found(self): with self.assertRaises(models.Client.DoesNotExist): self.adapter.get_client(client_id='not-found') def test_get_client_for_token(self): token = models.AccessToken( user=self.user, client=self.public_client, ) self.assertEqual(self.adapter.get_client_for_token(token), self.public_client) def test_get_access_token(self): token = models.AccessToken.objects.create( token='token-id', client=self.public_client, user=self.user, expires=now() + timedelta(days=30), ) self.assertEqual(self.adapter.get_access_token(token_string='token-id'), token)
@property def tuple(self): "Returns a tuple version of the geometry from the coordinate sequence." return self._cs.tuple coords = tuple
dirname, filename = os.path.split(filename) return os.path.normpath(os.path.join(dirname, self.get_valid_name(filename)))
currency = kwargs.get('currency', 'usd') if order.currency != currency and order.orderitem_set.exists(): raise InvalidCartItem(_("Trying to add a different currency into the cart"))
header_urls = {'logout': reverse('logout')}
return super_float("".join(parse_result))
self.assertEqual(Article.objects.filter(publications__title__startswith="Science").count(), 4) self.assertEqual(Article.objects.filter(publications__title__startswith="Science").distinct().count(), 3) self.assertQuerysetEqual( Article.objects.filter(publications__in=[self.p1.id, self.p2.id]).distinct(), [ '<Article: Django lets you build Web apps easily>', '<Article: NASA finds intelligent life on Earth>', '<Article: NASA uses Python>', '<Article: Oxygen-free diet works wonders>', ]) self.assertQuerysetEqual( Article.objects.filter(publications__in=[self.p1.id, self.p2]).distinct(), [ '<Article: Django lets you build Web apps easily>', '<Article: NASA finds intelligent life on Earth>', '<Article: NASA uses Python>', '<Article: Oxygen-free diet works wonders>', ] ) self.assertQuerysetEqual( Article.objects.filter(publications__in=[self.p1, self.p2]).distinct(), [ '<Article: Django lets you build Web apps easily>', '<Article: NASA finds intelligent life on Earth>', '<Article: NASA uses Python>', '<Article: Oxygen-free diet works wonders>', ] )
modulestore = self._verify_modulestore_support(parent_usage_key.course_key, 'create_child') return modulestore.create_child(user_id, parent_usage_key, block_type, block_id=block_id, fields=fields, **kwargs)
msg = ('Authentication failure of type "eauth" occurred for ' 'user {0}.').format(load.get('username', 'UNKNOWN')) log.warning(msg) return dict(error=dict(name='EauthAuthenticationError', message=msg))
httpretty.register_uri( httpretty.GET, "http://localhost:4567/api/v1/search/threads", body=json.dumps({ "collection": threads, "page": 1, "num_pages": num_pages, "corrected_text": rewrite, "thread_count": len(threads), }), status=200 )
cert = GeneratedCertificate.eligible_certificates.get(user=user, course_id=course_key) self.assertEqual(cert.status, expected_status)
assert np.all(dist < 5 * bandwidth)
instructor_dashboard = reverse('instructor_dashboard', kwargs={'course_id': self.course.id.to_deprecated_string()}) resp = self.client.get(instructor_dashboard) self.assertEqual(resp.status_code, 200) self.assertIn('Number Redeemed', resp.content) self.assertIn('<td>0</td>', resp.content)
self._verify_masquerade_for_all_groups()
return s
if prefix != '': if prefix: if not isinstance(prefix, six.string_types): prefix = str(prefix) else: prefix = os.path.basename(cwd) + '/' command.extend(['--prefix', prefix])
kernels_available = kernel_metrics() for kern in kernels_available: trans = Nystroem(n_components=2, kernel=kern, random_state=rnd) X_transformed = trans.fit(X).transform(X) assert_equal(X_transformed.shape, (X.shape[0], 2))
block_bits = width if (not max_bits or width < max_bits) else max_bits block_size = 2 ** block_bits
keys = list(compat.iterkeys(arg)) result = compat.OrderedDict()
timedelta_NaT = np.timedelta64('NaT')
clf = RandomizedLasso(verbose=False, alpha=1, random_state=42, scaling=scaling, selection_threshold=selection_threshold) feature_scores = clf.fit(X, y).scores_ assert_array_equal(np.argsort(F)[-3:], np.argsort(feature_scores)[-3:])
with self.assertNumQueries(2, using="default"): with self.assertNumQueries(1, using="student_module_history"): self.kvs.set_many(kv_dict)
magic_t, elsize, ndim, dim, dim_size = read_header(f, debug) f_start = f.tell() rval = None if isinstance(f, (gzip.GzipFile, bz2.BZ2File)): assert subtensor is None, \ "Haven't implemented the subtensor case for gzip file" d = f.read(_prod(dim) * elsize) rval = numpy.fromstring(d, dtype=magic_t).reshape(dim) del d elif subtensor is None: rval = numpy.fromfile(f, dtype=magic_t, count=_prod(dim)).reshape(dim) elif isinstance(subtensor, slice): if subtensor.step not in (None, 1): raise NotImplementedError('slice with step', subtensor.step) if subtensor.start not in (None, 0): bytes_per_row = _prod(dim[1:]) * elsize f.seek(f_start + subtensor.start * bytes_per_row) dim[0] = min(dim[0], subtensor.stop) - subtensor.start rval = numpy.fromfile(f, dtype=magic_t, count=_prod(dim)).reshape(dim) else: raise NotImplementedError('subtensor access not written yet: ', subtensor) return rval
assert which_set in ('train', 'test', 'both') if which_set == 'both': return (get_norb_file_paths(which_norb, 'train', norb_file_type) + get_norb_file_paths(which_norb, 'test', norb_file_type)) norb_file_types = ('cat', 'dat', 'info') if norb_file_type not in norb_file_types: raise ValueError("Expected norb_file_type to be one of %s, " "but it was '%s'" % (str(norb_file_types), norb_file_type)) instance_list = '01235' if which_set == 'test' else '46789' if which_norb == 'small': templates = ['smallnorb-5x%sx9x18x6x2x96x96-%sing-%%s.mat' % (instance_list, which_set)] else: numbers = range(1, 3 if which_set == 'test' else 11) templates = ['norb-5x%sx9x18x6x2x108x108-%sing-%02d-%%s.mat' % (instance_list, which_set, n) for n in numbers] original_files_dir = os.path.join(norb_dir, 'original') return [os.path.join(original_files_dir, t % norb_file_type) for t in templates]
for idx, signatory in enumerate(signatories): if len(signatories) > 2 and idx == len(signatories) - 1: continue else: self._create_fake_images([signatory['signature_image_path']])
self._init_socket()
the_html = problem.get_html() self.assertRegexpMatches(the_html, r"<div>.*\[.*'Banana'.*'Apple'.*'Chocolate'.*'Donut'.*\].*</div>") response = problem.responders.values()[0] self.assertFalse(response.has_mask()) self.assertEqual(response.unmask_order(), ['choice_1', 'choice_0', 'choice_2', 'choice_3']) self.assertEqual(the_html, problem.get_html(), 'should be able to call get_html() twice')
topping = Topping.objects.create(name='Salami') pizza = Pizza.objects.create(name='Americano') pizza.toppings.add(topping) response = self.client.get(reverse('admin:admin_views_topping_add')) self.assertEqual(response.status_code, 200)
resp = self.client.get('/') self.assertEquals(resp['X-Frame-Options'], 'DENY')
expected_fields = self.advanced_settings.expected_settings_names displayed_fields = self.advanced_settings.displayed_settings_names self.assertEquals(set(displayed_fields), set(expected_fields))
s1 = ugettext_lazy('Add %(name)s') self.assertEqual(s, s1) s2 = gettext_lazy('Add %(name)s') s3 = gettext_lazy('Add %(name)s') self.assertEqual(s2, s3) self.assertEqual(s, s2) s4 = ugettext_lazy('Some other string') self.assertNotEqual(s, s4)
#this adds lti passports to system mocked_course = Mock(lti_passports=['lti_id:test_client:test_secret']) modulestore = Mock() modulestore.get_course.return_value = mocked_course runtime = Mock(modulestore=modulestore) self.xmodule.descriptor.runtime = runtime self.xmodule.lti_id = "lti_id" key, secret = self.xmodule.get_client_key_secret() expected = ('test_client', 'test_secret') self.assertEqual(expected, (key, secret))
result = store.select( 'wp', [Term('major_axis', '>', datetime.datetime(2000, 1, 2))]) expected = wp.loc[:, wp.major_axis > Timestamp('20000102')] assert_panel_equal(result, expected)
import logging LOG_OVERRIDES = [ ('track.contexts', logging.CRITICAL), ('track.middleware', logging.CRITICAL), ('dd.dogapi', logging.CRITICAL), ('django_comment_client.utils', logging.CRITICAL), ] for log_name, log_level in LOG_OVERRIDES: logging.getLogger(log_name).setLevel(log_level)
rng = np.random.RandomState(1) X = np.sort(200 * rng.rand(100, 1) - 100, axis=0) y = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T y[::5, :] += (0.5 - rng.rand(20, 2))
self.assertTrue(self.store.has_changes(self.store.get_item(self.course.location)))
course_id=course_overview.id
tables.parameters.MAX_NUMEXPR_THREADS = 1 tables.parameters.MAX_BLOSC_THREADS = 1 tables.parameters.MAX_THREADS = 1
pass
unique_together = ('username', 'course', 'provider') get_latest_by = 'created'
admin_email_handler.include_html = False try: self.client.get('/', HTTP_HOST='evil.com') finally: admin_email_handler.include_html = old_include_html
disable_animations(self) self.wait_for_confirmation_prompt() self.q(css='.prompt button.action-primary').first.click() self.wait_for_element_invisibility('.prompt', 'wait for pop up to disappear') self.wait_for_ajax()
if self.seed is None: self.seed = self.lcp.seed
sites = [] i = 0 while 1: j = text.find(substr, i) if j == -1: break sites.append(j) i = j + 1 return sites
start_index = self._uncommented_start_index(template, start_index) while start_index is not None: close_char_index = template.find(close_char, start_index) if close_char_index < 0: return None open_char_index = template.find(open_char, start_index, close_char_index) parse_string = ParseString(template, start_index, close_char_index)
name = self.fk.name kwargs = { 'label': getattr(form.fields.get(name), 'label', capfirst(self.fk.verbose_name)) } if self.fk.remote_field.field_name != self.fk.remote_field.model._meta.pk.name: kwargs['to_field'] = self.fk.remote_field.field_name
person_names, file_paths = [], [] for person_name in sorted(listdir(data_folder_path)): folder_path = join(data_folder_path, person_name) if not isdir(folder_path): continue paths = [join(folder_path, f) for f in listdir(folder_path)] n_pictures = len(paths) if n_pictures >= min_faces_per_person: person_name = person_name.replace('_', ' ') person_names.extend([person_name] * n_pictures) file_paths.extend(paths)
self._set_partitions([ UserPartition( id=0, name="Cohort user partition", scheme=UserPartition.get_scheme("cohort"), description="Cohorted user partition", groups=[], ), UserPartition( id=1, name="Verification user partition", scheme=UserPartition.get_scheme("verification"), description="Verification user partition", groups=[ Group(id=0, name="Group C"), ], ), ])
if _any(self.left_on) and _any(self.right_on): for lk, rk in zip(self.left_on, self.right_on): if is_lkey(lk): left_keys.append(lk) if is_rkey(rk): right_keys.append(rk)
if parsed[3]: path += str(";") + force_str(parsed[3]) path = uri_to_iri(path).encode(UTF_8) return path.decode(ISO_8859_1) if six.PY3 else path
to_string(monitor)
if not value: return None else: return connection.ops.Adapter(self.get_prep_value(value))
self.axes = axes self._update_topo_space()
for name, Tree in CLF_TREES.items(): clf = Tree(random_state=0) clf.fit(X, y) assert_array_equal(clf.predict(T), true_result, "Failed with {0}".format(name))
verbose = getattr(options, "verbose", None) cmd = "i18n_tool extract" if verbose: cmd += " -vv" sh(cmd)
self.wait_until(lambda d: len(d.window_handles) == num_windows, timeout)
if not children_only: ret, perms = __salt__['file.check_perms'](name, ret, user, group, dir_mode, follow_symlinks)
train_ll, test_ll, logz = dbm_metrics.estimate_likelihood([W], [vbias, hbias], trainset, testset, pos_mf_steps=100) assert (real_logz - logz) < 2.0 assert (real_ais_train_ll - train_ll) < 2.0 assert (real_ais_test_ll - test_ll) < 2.0
DATE_INPUT_FORMATS = [
self.assertIn( 'Failed to setup the Syslog logging handler', '\n'.join(ret[1]) ) self.assertEqual(ret[2], 2)
default_store = os.environ.get('DEFAULT_STORE', 'draft') course_key = CourseLocator( self.course_info['course_org'], self.course_info['course_num'], self.course_info['course_run'], deprecated=(default_store == 'draft') ) url = "/".join([BASE_URL, self.url_path, urllib.quote_plus(unicode(course_key))]) return url if url[-1] is '/' else url + '/'
vert_block = ItemFactory.create( category="vertical", parent_location=library.location, user_id=self.user_id, publish_item=False, modulestore=self.store, ) child_block = ItemFactory.create( category="html", parent_location=vert_block.location, user_id=self.user_id, publish_item=False, metadata={"data": "Hello world", }, modulestore=self.store, ) self.assertEqual(child_block.parent.replace(version_guid=None, branch=None), vert_block.location)
logout_url = reverse('logout') response = self.client.post(logout_url)
sparse = self.dense1.to_sparse() sparse3 = self.dense3.to_sparse()
self._lookup_joins = join_list
__virtualname__ = 'pkg'
self.instructor = InstructorFactory(course_key=self.course.id) set_user_preference(self.instructor, LANGUAGE_KEY, 'zh-cn') self.client.login(username=self.instructor.username, password='test')
rng = np.random.RandomState(0) rand_data = RandomData(rng, scale=7) n_components = rand_data.n_components
def test_to_json(self): desc = FormDescription("post", "/submit") desc.add_field( "name", label="label", field_type="text", default="default", placeholder="placeholder", instructions="instructions", required=True, restrictions={ "min_length": 2, "max_length": 10 }, error_messages={ "required": "You must provide a value!" } ) self.assertEqual(desc.to_json(), json.dumps({ "method": "post", "submit_url": "/submit", "fields": [ { "name": "name", "label": "label", "type": "text", "defaultValue": "default", "placeholder": "placeholder", "instructions": "instructions", "required": True, "restrictions": { "min_length": 2, "max_length": 10, }, "errorMessages": { "required": "You must provide a value!" } } ] })) def test_invalid_field_type(self): desc = FormDescription("post", "/submit") with self.assertRaises(InvalidFieldError): desc.add_field("invalid", field_type="invalid") def test_missing_options(self): desc = FormDescription("post", "/submit") with self.assertRaises(InvalidFieldError): desc.add_field("name", field_type="select") def test_invalid_restriction(self): desc = FormDescription("post", "/submit") with self.assertRaises(InvalidFieldError): desc.add_field("name", field_type="text", restrictions={"invalid": 0})
self.certificates_section.add_certificate_exception(self.user_name, '')
if id_: id_ += '_0' return id_
from salttesting.helpers import ( ensure_in_syspath, skip_if_not_root, skip_if_binaries_missing ) from salttesting import skipIf ensure_in_syspath('../../')
_STORER_MAP = { u('TimeSeries'): 'LegacySeriesFixed', u('Series'): 'LegacySeriesFixed', u('DataFrame'): 'LegacyFrameFixed', u('DataMatrix'): 'LegacyFrameFixed', u('series'): 'SeriesFixed', u('sparse_series'): 'SparseSeriesFixed', u('frame'): 'FrameFixed', u('sparse_frame'): 'SparseFrameFixed', u('wide'): 'PanelFixed', u('sparse_panel'): 'SparsePanelFixed', }
return _data_api().get_course_enrollment(user_id, course_id)
with tm.assert_produces_warning(FutureWarning, check_stacklevel=False): sparse_moving = ols(y=y.to_sparse(), x=x.to_sparse(), weights=weights, window_type=window_type, window=window, **kwds) _compare_ols_results(moving, sparse_moving)
self.dashboard_page.visit() self.assertFalse(self.dashboard_page.pre_requisite_message_displayed())
r.COOKIES = {settings.LANGUAGE_COOKIE_NAME: 'es-us'} r.META = {} self.assertEqual(g(r), 'es')
loader = jinja2.FileSystemLoader( context, os.path.dirname(tmplpath))
assert_true(check_random_state(None) is np.random.mtrand._rand) assert_true(check_random_state(np.random) is np.random.mtrand._rand)
import salt.fileserver as fs import salt.modules import salt.utils import salt.utils.s3 as s3
if self.sparse: return self._transform(X, fitting=False) else: dtype = self.dtype vocab = self.vocabulary_ X = _tosequence(X) Xa = np.zeros((len(X), len(vocab)), dtype=dtype) for i, x in enumerate(X): for f, v in six.iteritems(x): if isinstance(v, six.string_types): f = "%s%s%s" % (f, self.separator, v) v = 1 try: Xa[i, vocab[f]] = dtype(v) except KeyError: pass return Xa
self._create_course_unit(subtitles=True) self.video.show_captions() self.video.focus_caption_line(2) self.assertTrue(self.video.is_caption_line_focused(2))
cm, __ = self.create_mode('honor', 'honor', 0, '', 'USD') self.assertEqual(cm.currency, 'usd') cm.currency = 'GHS' cm.save() self.assertEqual(cm.currency, 'ghs')
return Series(self._df_resid_raw, index=self._result_index)
_skip_if_no_xlrd()
parent = SomeParentModel.objects.create(name='a') SomeChildModel.objects.create(name='b', position='0', parent=parent) SomeChildModel.objects.create(name='c', position='1', parent=parent) response = self.client.get(reverse('admin:admin_inlines_someparentmodel_change', args=(parent.pk,))) self.assertNotContains(response, '<td class="field-position">') self.assertContains(response, ( '<input id="id_somechildmodel_set-1-position" ' 'name="somechildmodel_set-1-position" type="hidden" value="1" />'))
@python_2_unicode_compatible class Poet(models.Model): name = models.CharField(max_length=100)
if isinstance(key, string_types): self.env.swapkey(self.local_name, key, new_value=value)
return [('127.0.0.1:11211 (1)', {})]
if not __opts__['test']: try: __salt__['firewalld.allow_icmp'](name, icmp_type, permanent=True) except CommandExecutionError as err: ret['comment'] = 'Error: {0}'.format(err) return ret
examples = examples[1:]
self.assertEqual( list( TestObject.objects .extra(select=OrderedDict((('foo', 'first'), ('bar', 'second'), ('whiz', 'third')))) .values_list('first', 'second') ), [('first', 'second')] )
skip_if_no_sklearn() mapping = {'dataset_iterator': 'StratifiedDatasetShuffleSplit'} test_yaml = test_yaml_dataset_iterator % mapping trainer = yaml_parse.load(test_yaml) trainer.main_loop()
from __future__ import unicode_literals
self.assertEqual(response.status_code, 404)
from __future__ import absolute_import, generators, print_function, with_statement import re import logging
plt.plot(X1_tp[:, 0], X1_tp[:, 1], 'o', alpha=alpha, color='blue') plt.plot(X1_fp[:, 0], X1_fp[:, 1], '*', alpha=alpha,
if N.size(imgshp)==2: imgshp = (1,)+imgshp
if exc.errno != errno.EEXIST: raise else: ret['comment'] = 'File {0} exists and cannot be overwritten'.format(name)
import os
self.right = right self.is_offset_rhs = self._is_offset(right) self.is_datetime64_rhs = is_datetime64_dtype(rvalues) self.is_datetime64tz_rhs = is_datetime64tz_dtype(rvalues) self.is_datetime_rhs = (self.is_datetime64_rhs or self.is_datetime64tz_rhs) self.is_timedelta_rhs = is_timedelta64_dtype(rvalues) self.is_integer_rhs = rvalues.dtype.kind in ('i', 'u') self.is_floating_rhs = rvalues.dtype.kind == 'f'
with tm.assert_produces_warning(FutureWarning): s.iget_value(1)
response = self.post_with_bearer_token('/oauth2-test/') self.assertEqual(response.status_code, status.HTTP_200_OK)
return render_message_to_string( subject_template, message_template, self.get_email_params_ccx() )
def test_singular_matrix(): from nose.tools import assert_raises M = np.ones((10, 3)) f = ignore_warnings assert_raises(ValueError, f(manifold.locally_linear_embedding), M, 2, 1, method='standard', eigen_solver='arpack')
client = get_file_client.return_value client.cache_file.return_value = self.top_file.name
self.session._session_key = '1234567' self.assertIsNone(self.session.session_key)
_list = self.run_function('beacons.list', return_yaml=False) self.assertTrue(_list['ps']['enabled'])
filterspec = changelist.get_filters(request)[0][1] self.assertEqual(force_text(filterspec.title), 'publication decade') choices = list(filterspec.choices(changelist)) self.assertEqual(choices[1]['display'], 'the 1980\'s') self.assertEqual(choices[1]['selected'], True) self.assertEqual(choices[1]['query_string'], '?publication-decade=the+80s')
arr_ndim = arr.ndim if isinstance(patch_shape, numbers.Number): patch_shape = tuple([patch_shape] * arr_ndim) if isinstance(extraction_step, numbers.Number): extraction_step = tuple([extraction_step] * arr_ndim) patch_strides = arr.strides slices = [slice(None, None, st) for st in extraction_step] indexing_strides = arr[slices].strides patch_indices_shape = ((np.array(arr.shape) - np.array(patch_shape)) // np.array(extraction_step)) + 1 shape = tuple(list(patch_indices_shape) + list(patch_shape)) strides = tuple(list(indexing_strides) + list(patch_strides)) patches = as_strided(arr, shape=shape, strides=strides) return patches
if mp is not None: try: _sem = mp.Semaphore()
if hasattr(mod, 'run'): mod.run()
import time import salt.ext.six as six from salt.ext.six.moves import range
import integration
required_methods = [ "get_credit_requirement_namespace", "get_credit_requirement_name", "get_credit_requirement_display_name" ] for method_name in required_methods: if not callable(getattr(xblock, method_name, None)): LOGGER.error( "XBlock %s is marked as a credit requirement but does not " "implement %s", unicode(xblock), method_name ) return False return True
requires_pytz = unittest.skipIf(pytz is None, "this test requires pytz")
self.assertEqual(res.context['next_month'], datetime.date(2000, 2, 1)) self.assertEqual(res.context['previous_month'], datetime.date(1999, 12, 1))
self.reset_tracker() different_client = APIClient() different_client.login(username=different_user.username, password=TEST_PASSWORD) with make_image_file() as image_file: response = different_client.post(self.url, {'file': image_file}, format='multipart') self.check_response(response, 404) self.check_images(False) self.check_has_profile_image(False) self.assertFalse(mock_log.info.called) self.assert_no_events_were_emitted()
item = self.store.get_item(item_location) item.visible_to_staff_only = True self.store.update_item(item, self.user.id)
self.assertEquals(xmodule_tabs.CourseTabList.get_tab_by_id(self.course.tabs, tab.tab_id), tab)
if not physical_line.rstrip() and line_number == len(lines): return 0, "W391 blank line at end of file"
df2 = orig2.copy() with tm.assert_produces_warning(FutureWarning): df2.drop_duplicates(take_last=True, inplace=True) with tm.assert_produces_warning(FutureWarning): expected = orig2.drop_duplicates(['A', 'B'], take_last=True) result = df2 tm.assert_frame_equal(result, expected)
result = store.select('df1', ['A>0', 'B>0']) expected = store.select_as_multiple( ['df1'], where=['A>0', 'B>0'], selector='df1') tm.assert_frame_equal(result, expected) expected = store.select_as_multiple( 'df1', where=['A>0', 'B>0'], selector='df1') tm.assert_frame_equal(result, expected)
prepr = '[%s]' % ','.join(map(pprint_thing, self)) return '%s(%s)' % (self.__class__.__name__, prepr)
global is_initialized global additional_environ if not is_initialized: initialize() additional_environ = environ if isinstance(stream, six.string_types): string = stream else: string = stream.read() proxy_graph = yaml.load(string, **kwargs) if instantiate: return _instantiate(proxy_graph) else: return proxy_graph
df2 = df.set_index(['A', 'B']) df2['C'] = 3. df3 = df2.unstack('B') result = df3.get_dtype_counts() expected = Series({'int64': 2, 'float64': 2}) assert_series_equal(result, expected)
log.info( 'Failed to get mtime on {0}, ' 'dangling symlink ?'.format(file_path)) continue
__virtualname__ = 'ip'
for kernel in ['epanechnikov', 'exponential', 'linear', 'cosine']: kde = KernelDensity(bandwidth, kernel=kernel).fit(X) assert_raises(NotImplementedError, kde.sample, 100)
self.full_path = full_path self.directory = os.path.dirname(full_path) self.is_file = os.path.isfile(full_path) self.violations = []
df = DataFrame( {'A': [1, 2, 3, 4, 5, 6], 'B': [3, 4, 5, 6, 7, 8]}, index=[0, 1, 0, 1, 2, 3]).sort_index( axis=0) result = df.loc[1:] expected = DataFrame( {'A': [2, 4, 5, 6], 'B': [4, 6, 7, 8]}, index=[1, 1, 2, 3]) assert_frame_equal(result, expected)
if not self.can_import_settings: raise CommandError("Incompatible values of 'leave_locale_alone' " "(%s) and 'can_import_settings' (%s) command " "options." % (self.leave_locale_alone, self.can_import_settings)) from django.utils import translation saved_locale = translation.get_language() translation.deactivate_all()
cls.course_hidden_visibility = CourseFactory.create( display_name='Hidden_course', org='TestMicrositeX', catalog_visibility=CATALOG_VISIBILITY_NONE, emit_signals=True, )
values = ['2014', '2013/02', '2013/01/02', '2013/02/01 9H', '2013/02/01 09:00'] for v in values:
self.check_html( self.widget(choices=nested_choices), 'nestchoice', ('vinyl', 'dvd'), attrs={'id': 'media'}, html=html, )
pass
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
self.client.post(reverse('admin:login'), self.super_login) FilteredManager.objects.create(pk=1) FilteredManager.objects.create(pk=2) response = self.client.get(reverse('admin:admin_views_filteredmanager_changelist')) self.assertContains(response, "PK=1") self.assertContains(response, "PK=2") self.assertEqual( self.client.get(reverse('admin:admin_views_filteredmanager_history', args=(1,))).status_code, 200 ) self.assertEqual( self.client.get(reverse('admin:admin_views_filteredmanager_history', args=(2,))).status_code, 200 )
raise NotImplementedError
@setup({'linebreaksbr01': '{{ a|linebreaksbr }} {{ b|linebreaksbr }}'}) def test_linebreaksbr01(self): output = self.engine.render_to_string('linebreaksbr01', {"a": "x&\ny", "b": mark_safe("x&\ny")}) self.assertEqual(output, "x&amp;<br />y x&<br />y") @setup({'linebreaksbr02': '{% autoescape off %}{{ a|linebreaksbr }} {{ b|linebreaksbr }}{% endautoescape %}'}) def test_linebreaksbr02(self): output = self.engine.render_to_string('linebreaksbr02', {"a": "x&\ny", "b": mark_safe("x&\ny")}) self.assertEqual(output, "x&<br />y x&<br />y")
class PickyAuthenticationForm(AuthenticationForm): def confirm_login_allowed(self, user): if user.username == "inactive": raise forms.ValidationError("This user is disallowed.") raise forms.ValidationError("Sorry, nobody's allowed in.")
source_md5 = _get_md5(name, source) if source_md5 == __salt__['file.get_sum'](dest, 'md5'): log.debug('{0}:{1} and {2} are the same file, skipping copy' .format(name, source, dest)) return True
with self.assertRaisesRegexp(GitExportError, unicode(GitExportError.XML_EXPORT_FAIL)): git_export_utils.export_to_git( course_key, 'file://{0}'.format(self.bare_repo_dir))
body = msgpack.loads(body) if six.PY3: body = salt.transport.frame.decode_embedded_strs(body)
actual_field = None for field in form_desc["fields"]: if field["name"] == expected_field["name"]: actual_field = field break
wtf_numpy = np.zeros((pool_size_1,)) for i in xrange(pool_size_1): wtf_numpy[i] = on_probs[i] on_probs = wtf_numpy
def fit(self, X, y): self.y = y return self def predict(self, X): return self.y
tags = _database_tags('initialized', sender, kwargs) dog_stats_api.increment('edxapp.db.model', tags=tags)
if parts[0].endswith('sh:'): out = ' '.join(parts[1:]) ret['comment'] = out
s = Series([1, 2, 3, None, 5]) f(s)
msg_regxp = (r"Floating-point under-/overflow occurred at epoch #.*" " Scaling input data with StandardScaler or MinMaxScaler" " might help.") assert_raises_regexp(ValueError, msg_regxp, model.fit, X, y)
s = pd.Series(np.arange(9, dtype='int64'), index=pd.MultiIndex.from_product( [['A', 'B', 'C'], ['foo', 'bar', 'baz']], names=['one', 'two'])).sortlevel()
estimator.set_params(n_components=1)
if self.inferred_type not in ['floating', 'mixed-integer-float', 'string', 'unicode', 'mixed']: return self._invalid_indexer('label', key)
table = _normalize(table, normalize=normalize, margins=False)
return self._is_element_visible(".response_{} .edit-post-body".format(response_id))
X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]] y = [1, 1, 1, 1, 1, 1]
self.assertIsNone(annotated_books.query.alias_map["aggregation_book"].join_type) self.assertIsNone(excluded_books.query.alias_map["aggregation_book"].join_type)
try: from yaml import CSafeLoader as SafeLoader from yaml import CSafeDumper as SafeDumper except ImportError: from yaml import SafeLoader, SafeDumper
DELAY = 0.5
monitoring_dataset = DenseDesignMatrix(X=X)
if not status: ret['result'] = False return ret
with self.assertRaises(FieldError): TaggedItem.objects.filter(vegetable__isnull=True)
self.assertEqual(self._get_enrollments(), [])
df.values[0, 0] = np.nan df.values[5, 3] = np.nan
return ensure_compat('rolling', 'apply', arg, window=window, freq=freq, center=center, min_periods=min_periods, func_kw=['func', 'args', 'kwargs'], func=func, args=args, kwargs=kwargs)
import integration import salt.loader import inspect import yaml
return self.selenium.execute_script( 'return django.jQuery("%s").css("%s")' % (selector, attribute))
with patch.dict(local_cache.__opts__, {'keep_jobs': 0.00000001}): local_cache.clean_old_jobs()
clf = self.factory() assert_raises(ValueError, clf.fit, X2, Y2, coef_init=np.zeros((2, 2)))
post_data = {'name': 'John Doe'} self.assertEqual(Person.objects.count(), 0) response = self.client.post(reverse('admin_custom_urls:admin_custom_urls_person_add'), post_data) persons = Person.objects.all() self.assertEqual(len(persons), 1) redirect_url = reverse('admin_custom_urls:admin_custom_urls_person_history', args=[persons[0].pk]) self.assertRedirects(response, redirect_url)
if [ -z "$debian_chroot" ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi
for element in t.nodes[:-1]: self._dispatch(element) self._write(", ")
if not resultclasses: raise TypeError("You must pass at least one argument to keep_lazy().") def decorator(func): lazy_func = lazy(func, *resultclasses) @wraps(func) def wrapper(*args, **kwargs): for arg in list(args) + list(six.itervalues(kwargs)): if isinstance(arg, Promise): break else: return func(*args, **kwargs) return lazy_func(*args, **kwargs) return wrapper return decorator
resp = self.client.ajax_post( self.url, data={'tabs': [{'tab_id': tab_id} for tab_id in invalid_tab_ids]}, ) self.check_invalid_tab_id_response(resp)
n_samples, n_features = X.shape n_components_ = H.shape[0] if sparseness is None: Wt, gradW, iterW = _nls_subproblem(X.T, H.T, W.T, tolW, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio) elif sparseness == 'data': Wt, gradW, iterW = _nls_subproblem( safe_vstack([X.T, np.zeros((1, n_samples))]), safe_vstack([H.T, np.sqrt(beta) * np.ones((1, n_components_))]), W.T, tolW, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio) elif sparseness == 'components': Wt, gradW, iterW = _nls_subproblem( safe_vstack([X.T, np.zeros((n_components_, n_samples))]), safe_vstack([H.T, np.sqrt(eta) * np.eye(n_components_)]), W.T, tolW, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio) return Wt.T, gradW.T, iterW
git_range = "origin/master...%s" % commit try: check_output("git fetch origin master".split()) filenames = check_output("git diff --name-only".split() + [git_range]) except CalledProcessError: exit("git introspection failed.") filenames = filenames.decode('utf-8').split() for filename in filenames: if filename.startswith(u'doc/') or filename.startswith(u'examples/'): exit("detected doc impacting file modified by PR in range %s: %s" % (git_range, filename))
with mock.patch(self.YESNO_PATCH_LOCATION) as patched_yes_no: patched_yes_no.return_value = False with mock.patch(self.REINDEX_PATH_LOCATION) as patched_index, \ mock.patch(self.MODULESTORE_PATCH_LOCATION, mock.Mock(return_value=self.store)): call_command('reindex_library', all=True) patched_yes_no.assert_called_once_with(ReindexCommand.CONFIRMATION_PROMPT, default='no') patched_index.assert_not_called()
if name == 'ROOT': return settings.MKTG_URLS.get('ROOT') return settings.MKTG_URLS.get('ROOT') + settings.MKTG_URLS.get(name)
onlyrepo = True for arepo in six.iterkeys(repos): if arepo == repo: continue if repos[arepo]['file'] == repofile: onlyrepo = False
from salt.ext.six.moves import shlex_quote as _cmd_quote
self.postgis_types_reverse = self.get_postgis_types() self.data_types_reverse.update(self.postgis_types_reverse)
with self.assertRaises(TemplateSyntaxError): self.engine.render_to_string('if-tag-single-eq', {'foo': 1})
TEST_DIR = path(__file__).dirname() PLATFORM_ROOT = TEST_DIR.parent.parent.parent.parent.parent.parent TEST_DATA_ROOT = PLATFORM_ROOT / TEST_DATA_DIR COURSE_DATA_DIR = TEST_DATA_ROOT / COURSE_NAME
DATE_INPUT_FORMATS = [
_maybe_remove(store, 'df') store.append('df', df[:5], min_itemsize=200) store.append('df', df[5:], min_itemsize=200) tm.assert_frame_equal(store['df'], df)
CourseFinanceAdminRole(self.course.id).remove_users(self.instructor)
request = Request(self.environ) request.authorization = "bad authorization header" request.body = self.get_request_body() response = self.xmodule.grade_handler(request, '') real_response = self.get_response_values(response) expected_response = { 'action': None, 'code_major': 'failure', 'description': 'OAuth verification error: Malformed authorization header', 'messageIdentifier': self.defaults['messageIdentifier'], } self.assertEqual(response.status_code, 200) self.assertDictEqual(expected_response, real_response)
import salt.utils import salt.output from salt.utils.locales import sdecode
arr = SparseArray(data=1, sparse_index=IntIndex(1, [0]), dtype=None) exp = SparseArray([1], dtype=None) tm.assert_sp_array_equal(arr, exp) self.assertEqual(arr.dtype, np.int64) self.assertTrue(np.isnan(arr.fill_value))
t = self.engine.from_string('{{ my_doodad.value }}') self.assertEqual(t.render(c), '')
diff_quality_percentage_pass = True
pipe = Pipeline([('transf', TransfT()), ('clf', FitParamT())]) pipe.fit(X=None, y=None, clf__should_succeed=True) assert_true(pipe.predict(None)) assert_true(pipe.named_steps['transf'].a is None) assert_true(pipe.named_steps['transf'].b is None)
project = u'edX' copyright = u'2013, EdX Doc Team'
vals = [[1, -1, 2.], [2, -2, 3.]] rs = DataFrame(vals, columns=['A', 'A', 'B']) xp = DataFrame(vals) xp.columns = ['A', 'A', 'B'] assert_frame_equal(rs, xp)
for idx, old_item in enumerate(update_config): if idx < len(config): continue _namespace = '[{0}]'.format(idx) if namespace: _namespace = '{0}{1}'.format(namespace, _namespace) changes[_namespace] = { 'new': None, 'old': old_item, } del update_config[len(config):]
if np.any(np.less_equal(precision, 0.0)): raise ValueError("'%s precision' should be " "positive" % covariance_type)
xval = self.origin.x + self.scale.x * self.width yval = self.origin.y + self.scale.y * self.height xmin = min(xval, self.origin.x) xmax = max(xval, self.origin.x) ymin = min(yval, self.origin.y) ymax = max(yval, self.origin.y)
state_frame = self._get_context_stack_frame(context) if self not in state_frame: state_frame[self] = None
reorderable_items = set() if view_name == 'reorderable_container_child_preview': reorderable_items.add(xblock.location)
if var.startswith('_(') and var.endswith(')'): self.translate = True var = var[2:-1] try: self.literal = mark_safe(unescape_string_literal(var)) except ValueError: if var.find(VARIABLE_ATTRIBUTE_SEPARATOR + '_') > -1 or var[0] == '_': raise TemplateSyntaxError("Variables and attributes may " "not begin with underscores: '%s'" % var) self.lookups = tuple(var.split(VARIABLE_ATTRIBUTE_SEPARATOR))
flags = [('Name', name)]
person = Person.objects.create(name="Foo McBar") self.assertEqual(len(Person.objects.all()), 1) self.assertEqual(len(MyPerson.objects.all()), 1) self.assertEqual(MyPerson.objects.get(name="Foo McBar").id, person.id) self.assertFalse(MyPerson.objects.get(id=person.id).has_special_name())
non_displayed_tabs = set(old_tab_list) - set(new_tab_list) new_tab_list.extend(non_displayed_tabs)
renamed = self.frame.T.rename(index={'C': 'foo', 'D': 'bar'}) tm.assert_index_equal(renamed.index, pd.Index(['A', 'B', 'foo', 'bar']))
channel = salt.transport.Channel.factory(__opts__)
self.client.logout() login = self.client.login(username='testclient', password='password') self.assertTrue(login, 'Could not log in') self.client.logout() self.client.logout()
if mask.all(): continue
if self.is_professional_slug(self.mode_slug) and self.expiration_datetime is not None: raise ValidationError( _(u"Professional education modes are not allowed to have expiration_datetime set.") ) if self.is_verified_slug(self.mode_slug) and self.min_price <= 0: raise ValidationError(_(u"Verified modes cannot be free."))
import salt.ext.six as six from salt.ext.six.moves import range
self._create_students(num_emails - 1) expected_fails = 0 expected_succeeds = num_emails expected_retries = 10 with patch('bulk_email.tasks.get_connection', autospec=True) as get_conn: get_conn.return_value.send_messages.side_effect = cycle( chain(repeat(exception, expected_retries), [None]) ) self._test_run_with_task( send_bulk_course_email, 'emailed', num_emails, expected_succeeds, failed=expected_fails, retried_nomax=(expected_retries * num_emails) )
pairwise = True if pairwise is None else pairwise
child_item.set_staff_lock(True) parent_item.set_staff_lock(True) self.assertTrue(parent_item.has_staff_lock_warning) self.assertTrue(child_item.has_staff_lock_warning) parent_item.set_staff_lock(False) self.assertFalse(parent_item.has_staff_lock_warning) self.assertTrue(child_item.has_staff_lock_warning)
_init()
with tm.assert_produces_warning(FutureWarning, check_stacklevel=False): return f(obj, window=window, min_periods=min_periods, freq=freq, center=center, **kwargs)
output = self.engine.render_to_string('basic-syntax11', {'var': SomeClass()}) if self.engine.string_if_invalid: self.assertEqual(output, 'INVALID') else: self.assertEqual(output, '')
if tornado.util.errno_from_exception(e) == errno.ECONNABORTED: continue raise
from __future__ import absolute_import import tornado.stack_context import tornado.gen from tornado.testing import AsyncTestCase, gen_test import threading import time
Book.authors.through.objects.using('default').delete()
1: pgettext('month name', 'January'), 2: pgettext('month name', 'February'), 3: pgettext('month name', 'March'), 4: pgettext('month name', 'April'), 5: pgettext('month name', 'May'), 6: pgettext('month name', 'June'), 7: pgettext('month name', 'July'), 8: pgettext('month name', 'August'), 9: pgettext('month name', 'September'), 10: pgettext('month name', 'October'), 11: pgettext('month name', 'November'), 12: pgettext('month name', 'December'),
from __future__ import absolute_import
return (p.name != 'self' and p.kind != p.VAR_KEYWORD and p.kind != p.VAR_POSITIONAL)
if isinstance(self.columns, MultiIndex): raise ValueError('Cannot sort by column %s in a ' 'multi-index you need to explicity ' 'provide all the levels' % str(by))
request2 = self.request_factory.post('/create_account', data=postvars) request2.session = client.session request2.user = AnonymousUser()
pass
with self.modify_settings(INSTALLED_APPS={'append': 'i18n.resolution'}): activate('de')
if len(urlsplit(value).netloc) > 253: raise ValidationError(self.message, code=self.code)
env = geom.envelope env_w, env_h = self.get_width_height(env.extent) center = env.centroid
if not api_url: self.api_url = getattr(settings, 'GOOGLE_MAPS_URL', GOOGLE_MAPS_URL) % self.version else: self.api_url = api_url
return ( CertificateGenerationConfiguration.current().enabled and CertificateGenerationCourseSetting.is_enabled_for_course(course_key) )
self.client.login(username="jack", password="test") self._check_verification_status_off('verified', 'You\'re enrolled as a verified student') self._check_verification_status_off('honor', 'You\'re enrolled as an honor code student') self._check_verification_status_off('audit', '')
if not __salt__['user.info'](user): user = __salt__['user.current']() if not user: user = 'SYSTEM'
try: DefaultPerson.objects.get_or_create() except AssertionError: self.fail("If all the attributes on a model have defaults, we " "shouldn't need to pass any arguments.")
status_headings = sorted( set([status for course in cert_data for status in cert_data[course]]) )
self._set_team_configuration_and_membership(create_membership=False) self.team_page.visit() self.assert_team_details(is_member=False, num_members=0)
from __future__ import unicode_literals
return course_mode_tuple.slug in cls.CREDIT_MODES
opt_args['objective_error'] = objective_error opt_args['kwargs']['angle'] = self.angle opt_args['kwargs']['verbose'] = self.verbose
course = course or self.course role = Role.objects.create(name=role_name, course_id=course.id) role.users = users
ip1 = ipaddress.IPv4Network('1.1.1.0/24') ip2 = ipaddress.IPv4Network('1.1.1.0/32') ip3 = ipaddress.IPv4Network('1.1.2.0/24')
@setup({'timeuntil01': '{{ a|timeuntil }}'}) def test_timeuntil01(self): output = self.engine.render_to_string('timeuntil01', {'a': datetime.now() + timedelta(minutes=2, seconds=10)}) self.assertEqual(output, '2\xa0minutes')
return self.db_type(connection)
self.assertEqual(len(courses), 2)
BROKER_HEARTBEAT = 10.0 BROKER_HEARTBEAT_CHECKRATE = 2
self.assertEqual(ewkt_val, OGRGeometry(ewkt_val).ewkt) ewkt_val = 'SRID=4326;%s' % ewkt_val geom = OGRGeometry(ewkt_val) self.assertEqual(ewkt_val, geom.ewkt) self.assertEqual(4326, geom.srs.srid)
course_id = CourseKeyField(max_length=255, db_index=True)
entry_needs_updating = True entry_needs_saving = False task_output = None
try:
raise NotImplementedError('subclasses of ListFilter must provide a queryset() method')
return { cls.STRING_PAYLOAD, cls.ROOT_EXTRA_FIELDS, cls.CONTEXT_EXTRA_FIELDS, }
for col_name, df_col in data_frame.iteritems(): if com.is_datetime64tz_dtype(df_col): data_frame[col_name] = _handle_date_column(df_col)
self.assertFalse(self.storage.exists('test.file')) f = ContentFile('custom contents') f_name = self.storage.save('test.file', f) self.addCleanup(self.storage.delete, f_name) atime = self.storage.get_accessed_time(f_name) self.assertEqual(atime, datetime.fromtimestamp(os.path.getatime(self.storage.path(f_name)))) self.assertLess(timezone.now() - self.storage.get_accessed_time(f_name), timedelta(seconds=2))
err = SearchIndexingError mock_index_dictionary.return_value = err
p.undergroundbar = b
try: return getattr(self._module_system, name) except AttributeError: return getattr(self._descriptor_system, name)
email = user.email if user.is_authenticated() else request.POST.get('email')
query_parameters = 'course_id={}&page_size={}'.format(urllib.quote(self.course_id), page_size) self.send_get(client=self.client, url=reverse('bookmarks'), query_parameters=query_parameters) self.assert_bookmark_event_emitted( mock_tracker, event_name='edx.bookmark.listed', course_id=self.course_id, list_type='per_course', bookmarks_count=expected_bookmarks_count, page_size=expected_page_size, page_number=expected_page_number )
if not isinstance(data, (list, tuple)): data = list(data)
ret = salt.utils.cloud.bootstrap(vm_, __opts__)
return True
if labels is None: raise ValueError("The labels parameter should not be None") return len(np.unique(labels))
df1 = DataFrame(index=np.arange(10)) df1['bool'] = True df1['string'] = 'foo'
counts = {}
p1_db = self.PersonModel.objects.get(name="Joe") self.assertEqual(p1_db.mugshot == p2.mugshot, False) self.assertEqual(p1_db.mugshot != p2.mugshot, True)
if alarm_actions: alarm_actions = convert_to_arn(alarm_actions, region=region, key=key, keyid=keyid, profile=profile) if insufficient_data_actions: insufficient_data_actions = convert_to_arn(insufficient_data_actions, region=region, key=key, keyid=keyid, profile=profile) if ok_actions: ok_actions = convert_to_arn(ok_actions, region=region, key=key, keyid=keyid, profile=profile)
return type(self).censor_updates != Model.censor_updates
return {'name': name, 'changes': {}, 'result': True, 'comment': ''}
df = DataFrame(dict({ 'A': np.asarray(lrange(10), dtype='float64'), 'B': pd.Timestamp('20010101')})) df.iloc[3:6, :] = np.nan
top_idx = set(component.argsort()[-3:][::-1]) assert_true(tuple(sorted(top_idx)) in correct_idx_grps)
self.assertEqual( list( TestObject.objects .values() .extra(select=OrderedDict((('foo', 'first'), ('bar', 'second'), ('whiz', 'third')))) ), [{ 'bar': 'second', 'third': 'third', 'second': 'second', 'whiz': 'third', 'foo': 'first', 'id': obj.pk, 'first': 'first' }] )
from salttesting.unit import TestCase from salttesting.helpers import ensure_in_syspath
del mpoly self.assertTrue(prep.covers(Point(5, 5)))
query = self._course_key_to_son(location.course_key) query['definition.children'] = location.to_deprecated_string()
accepts = request.META.get('HTTP_ACCEPT', '*/*')
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 5)
self.assertIn('; %s' % http_cookies.Morsel._reserved['httponly'], str(example_cookie)) self.assertTrue(example_cookie['httponly'])
event_info['correct_map'] = correct_map.get_dict() event_info['success'] = success event_info['attempts'] = self.attempts self.track_function_unmask('problem_rescore', event_info)
action = models.CharField(max_length=100, db_index=True)
'rule': None
for kind1, kind2 in [('line', 'area'), ('area', 'line')]: ax = high.plot(kind=kind1, stacked=True) ax = low.plot(kind=kind2, stacked=True, ax=ax)
self.assertEqual(response.status_code, 200)
return self.client.patch( self.url, json.dumps(request_data), content_type="application/merge-patch+json" )
endblock = parser.next_token() acceptable_endblocks = ('endblock', 'endblock %s' % block_name) if endblock.contents not in acceptable_endblocks: parser.invalid_block_tag(endblock, 'endblock', acceptable_endblocks)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
assertion, args = assertion_tuple[0], assertion_tuple[1:] getattr(self, assertion)(*args)
EmptyPromise( lambda: self.q(css='.assetupload-modal .action-upload').present, 'Signature image upload dialog opened' ).fulfill()
expected_x = idxh.to_period().asi8.astype(np.float64) expected_y = np.zeros(len(expected_x), dtype=np.float64) for i in range(3): l = ax.lines[3 + i] self.assertEqual(PeriodIndex(data=l.get_xdata()).freq, idxh.freq) self.assert_numpy_array_equal(l.get_xdata(orig=False), expected_x) expected_y += high[i].values self.assert_numpy_array_equal(l.get_ydata(orig=False), expected_y)
return name
update = salt.utils.alias_function(refresh_db, 'update')
linter = JavaScriptLinter() results = FileResults('') linter.check_javascript_file_is_safe(data['template'], results) self._validate_data_rules(data, results)
disable_animations(annotation_component_page)
template = self.cleaned_data["plain_template"] self._validate_template(template) return template
list_check = lambda x: isinstance(x, list) clf = CheckingClassifier(check_X=list_check) scores = cross_val_score(clf, X.tolist(), y2.tolist())
url = self.homework.location.to_deprecated_string() found_unit = tools.find_unit(self.course, url) self.assertEqual(found_unit.location, self.homework.location)
override, _ = StudentFieldOverride.objects.get_or_create( course_id=block.runtime.course_id, location=block.location, student_id=user.id, field=name) field = block.fields[name] override.value = json.dumps(field.to_json(value)) override.save()
add_message(request, constants.SUCCESS, message, extra_tags=extra_tags, fail_silently=fail_silently)
for method_name in ('get', 'head'): method = getattr(self.client, method_name) response = method("/request_data/", data={'foo': 'whiz'}) self.assertEqual(response.context['get-foo'], 'whiz')
for (freq, s1, s2) in [('N', t1, t2), ('U', t1, t2), ('L', t1a, TimedeltaIndex(['-1 days +00:00:00', '-2 days +23:58:58', '-2 days +23:57:56'], dtype='timedelta64[ns]', freq=None) ), ('S', t1a, TimedeltaIndex(['-1 days +00:00:00', '-2 days +23:58:58', '-2 days +23:57:56'], dtype='timedelta64[ns]', freq=None) ), ('12T', t1c, TimedeltaIndex(['-1 days', '-1 days', '-1 days'], dtype='timedelta64[ns]', freq=None) ), ('H', t1c, TimedeltaIndex(['-1 days', '-1 days', '-1 days'], dtype='timedelta64[ns]', freq=None) ), ('d', t1c, pd.TimedeltaIndex([-1, -1, -1], unit='D') )]:
return '%(classname)s(%(components)s)' % \ dict(classname=self.__class__.__name__, components=', '.join([str(c) for c in self.components]))
if not err_msg: err_msg = u"Unknown error" error_rows.append(student_fields + [err_msg]) task_progress.failed += 1 continue
log_msg = u"Grade matches for %(type)s module %(id)s for student %(student)s in course %(course_id)s" LOG.debug(log_msg, { "type": module.module_type, "id": module.module_state_key, "student": module.student.username, "course_id": module.course_id, })
response = self.client.post(self.url, data) self.assertHttpBadRequest(response)
from __future__ import absolute_import
else: return False
chapter = ItemFactory.create(parent_location=self.course.location, category='chapter') self.assertEqual(xblock_type_display_name(chapter), u'Section') self.assertEqual(xblock_type_display_name('chapter'), u'Section')
truncator = text.Truncator('-B\u030AB\u030A----8') self.assertEqual('-B\u030A...', truncator.chars(5)) self.assertEqual('-B\u030AB\u030A-...', truncator.chars(7)) self.assertEqual('-B\u030AB\u030A----8', truncator.chars(8))
result = p.select(lambda x: x in ('ItemA', 'ItemC'), axis='items') expected = p.reindex(items=['ItemA', 'ItemC']) self.assert_panel4d_equal(result, expected)
_skip_console_logging_config_ = True _logfile_config_setting_name_ = 'key_logfile' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'key')
time.sleep(0.5)
return type(self) == type(other)
msg = 'Error occurred while evaluating CustomResponse' log.warning(msg, exc_info=True)
text = f.widget.format_value(result) self.assertEqual(text, '21.12.2010')
panel = self._add_entity_effects(panel) panel = self._add_categorical_dummies(panel, mapping) return panel
def apply_me(args): if len(args) == 1: return args[0] else: rval = ifelse(TT.eq(args[0], zero), false, apply_me(args[1:]), name=name + str(len(args))) return rval return apply_me(args)
set_credit_requirements(course.id, requirements)
source_block_keys = [ src_key for src_key in (BlockKey.from_usage_key(src) for src in source_children) if src_key not in unexpected ] dest_block_keys = [BlockKey.from_usage_key(dest) for dest in dest_children] for unexp in unexpected: self.assertNotIn(unexp, dest_block_keys) self.assertEqual(source_block_keys, dest_block_keys)
test_discussion = self.store.create_child(self.user.id, course.location, 'discussion', 'test_discussion')
from __future__ import absolute_import import copy import fnmatch import itertools import logging import os import re import string
pass
argtypes = [WKB_READ_PTR, c_char_p, c_size_t] restype = GEOM_PTR errcheck = staticmethod(check_geom)
d = TreeEstimator(random_state=0, max_depth=max_depth).fit(X, y) s = TreeEstimator(random_state=0, max_depth=max_depth).fit(X_sparse, y)
image_id = inspect_image(name)['Id']
from salt.serializers.yamlex import deserialize
df = DataFrame({'A': 1, 'B': 'foo', 'C': 'bar', 'D': Timestamp("20010101"), 'E': datetime(2001, 1, 2, 0, 0)}, index=np.arange(10)) result = df.get_dtype_counts() expected = Series({'int64': 1, datetime64name: 2, objectname: 2}) result.sort_index() expected.sort_index() tm.assert_series_equal(result, expected)
if os.path.isfile(name) or os.path.islink(name): if os.path.islink(name) and follow_symlinks: real_name = os.path.realpath(name) else: real_name = name
clf = GaussianNB(priors=np.array([2., 1.])) assert_raises(ValueError, clf.fit, X, y)
xl_format = self.book.add_format()
global_message = models.ForeignKey(GlobalStatusMessage) course_key = CourseKeyField(max_length=255, blank=True, db_index=True) message = models.TextField(blank=True, null=True) def __unicode__(self): return unicode(self.course_key)
try: indent = min(len(s) - len(s.lstrip()) for s in docstring if s.strip()) except ValueError: indent = 0
non_sids = set(['include', 'exclude', 'extend']) for sid, states in six.iteritems(data): if sid in non_sids or sid.startswith('__'): continue if '__sls__' not in states or states['__sls__'] == sls: break else: raise SaltRenderError('Can\'t determine the first state in the sls file!') reqin = {state_name(next(six.iterkeys(data[sid]))): sid} data[start_sid] = {STATE_FUNC: [{'require_in': [reqin]}]}
evt1 = me.get_event(wait=0, tag='evt1', no_block=False) self.assertGotEvent(evt1, {'data': 'foo1'})
HAS_WHOOSH = False try: import whoosh.index import whoosh.fields import whoosh.store import whoosh.qparser HAS_WHOOSH = True except ImportError: pass
z_name = z.name if z_name is None: z_name = 'anon_z' batch_size, zr, zc, ch = z.shape r, c = pool_shape flat_z = [] for i in xrange(r): for j in xrange(c): cur_part = z[:, i:zr:r, j:zc:c, :] assert cur_part.ndim == 4 if z_name is not None: cur_part.name = z_name + '[%d,%d]' % (i, j) flat_z.append(cur_part.dimshuffle(0, 1, 2, 3, 'x')) flat_z.append(T.zeros_like(flat_z[-1])) stacked_z = T.concatenate(flat_z, axis=4) batch_size, rows, cols, channels, outcomes = stacked_z.shape reshaped_z = stacked_z.reshape((batch_size * rows * cols * channels, outcomes)) dist = T.nnet.softmax(reshaped_z) dist = dist.reshape((batch_size, rows, cols, channels, outcomes)) p = 1. - dist[:, :, :, :, len(flat_z)-1] p.name = 'p(%s)' % z_name h = T.alloc(0., batch_size, zr, zc, ch) idx = 0 for i in xrange(r): for j in xrange(c): h = T.set_subtensor(h[:, i:zr:r, j:zc:c, :], dist[:, :, :, :, idx]) idx += 1 h.name = 'h(%s)' % z_name return p, h
_default_db = settings.DATABASES[DEFAULT_DB_ALIAS]['ENGINE'].rsplit('.')[-1] oracle = _default_db == 'oracle' postgis = _default_db == 'postgis' mysql = _default_db == 'mysql' spatialite = _default_db == 'spatialite'
from __future__ import absolute_import import grp import pwd
width, height = image_obj.size self.assertEqual(width, height) actual_sizes[width] = name
return self._dtype
elif isinstance(key, BaseGrouper): return key, [], obj
log_prob_of = (Y * log_prob).sum(axis=1) masked = log_prob_of * drop_mask_Y assert masked.ndim == 1
import os import stat import logging
if not user or user.is_anonymous(): return None
self.opts = opts
n_queries = 5 queries = X[rng.randint(0, n_samples, n_queries)] distances, neighbors = lshf.radius_neighbors(queries, return_distance=True)
d_clf = Lasso(alpha=0.1, fit_intercept=False, max_iter=max_iter, tol=1e-7) d_clf.fit(X_train.toarray(), y_train) assert_almost_equal(d_clf.dual_gap_, 0, 4) assert_greater(d_clf.score(X_test, y_test), 0.85)
from __future__ import absolute_import import sys import re
import salt.pillar import salt.utils from salt.defaults import DEFAULT_TARGET_DELIM from salt.exceptions import CommandExecutionError
y = zca_dataset.adjust_to_be_viewed_with(x, 2*x, True) z = zca_dataset.adjust_for_viewer(x) assert_allclose(z/2, y) y = zca_dataset.adjust_to_be_viewed_with(x, 2*x, False) z = x/np.abs(x).max() assert_allclose(z/2, y)
pandas.set_option('io.hdf.dropna_table', False) _maybe_remove(store, 'df3') store.append('df3', df[:10]) store.append('df3', df[10:]) tm.assert_frame_equal(store['df3'], df)
EmptyPromise( lambda: self.outline_page.is_browser_on_page(), "loaded page {!r}".format(self.outline_page), timeout=30 ).fulfill()
locale_dirs = filter(os.path.isdir, glob.glob('%s/*' % self.default_locale_path)) all_locales = map(os.path.basename, locale_dirs)
import integration import salt.utils import salt.utils.find
return True
phi_rec = ifelse(cond1, phi_hi, TT.switch(cond2, phi_hi, phi_lo), name='phi_rec') a_rec = ifelse(cond1, a_hi, TT.switch(cond2, a_hi, a_lo), name='a_rec') a_hi = ifelse(cond1, a_j, TT.switch(cond2, a_lo, a_hi), name='a_hi') phi_hi = ifelse(cond1, phi_aj, TT.switch(cond2, phi_lo, phi_hi), name='phi_hi')
import logging import codecs try: import sqlite3 HAS_SQLITE3 = True except ImportError: HAS_SQLITE3 = False
value.srid = self.srid
config = sconfig.minion_config('/etc/salt/minion') self.assertEqual(config['log_file'], env_fpath) os.environ.clear() os.environ.update(original_environ)
if mode == 'horizontal': self.selenium.find_element_by_id(choose_all_link).click() elif mode == 'vertical': for option in self.selenium.find_elements_by_css_selector(from_box + ' > option'): option.click() self.selenium.find_element_by_id(choose_link).click() self.assertSelectOptions(from_box, []) self.assertSelectOptions(to_box, [ str(self.lisa.id), str(self.peter.id), str(self.arthur.id), str(self.bob.id), str(self.cliff.id), str(self.jason.id), str(self.jenny.id), str(self.john.id), ]) self.assertActiveButtons(mode, field_name, False, False, False, True)
#sys.exit(retcode)
inp = ["spam", "ham", "eggs", "ham", "0"] expected = np.array([[0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0]]) got = lb.fit_transform(inp) assert_array_equal(lb.classes_, ['0', 'eggs', 'ham', 'spam']) assert_array_equal(expected, got) assert_array_equal(lb.inverse_transform(got), inp)
response.set_cookie( settings.CROSS_DOMAIN_CSRF_COOKIE_NAME, request.META['CSRF_COOKIE'], max_age=settings.CSRF_COOKIE_AGE, domain=settings.CROSS_DOMAIN_CSRF_COOKIE_DOMAIN, path=settings.CSRF_COOKIE_PATH, secure=True ) log.debug( "Set cross-domain CSRF cookie '%s' for domain '%s'", settings.CROSS_DOMAIN_CSRF_COOKIE_NAME, settings.CROSS_DOMAIN_CSRF_COOKIE_DOMAIN )
BODY_SELECTOR = None EDIT_BUTTON_SELECTOR = '.xblock-field-value-edit' NAME_SELECTOR = '.item-title' NAME_INPUT_SELECTOR = '.xblock-field-input' NAME_FIELD_WRAPPER_SELECTOR = '.xblock-title .wrapper-xblock-field' STATUS_MESSAGE_SELECTOR = '> div[class$="status"] .status-message' CONFIGURATION_BUTTON_SELECTOR = '.action-item .configure-button'
geoqueryset_methods = ( 'area', 'bounding_circle', 'centroid', 'difference', 'distance', 'distance_spheroid', 'envelope', 'force_rhr', 'geohash', 'gml', 'intersection', 'kml', 'length', 'mem_size', 'num_geom', 'num_points', 'perimeter', 'point_on_surface', 'reverse', 'scale', 'snap_to_grid', 'svg', 'sym_difference', 'transform', 'translate', 'union', 'unionagg', )
self.assertEqual(correct_map.get_correctness('1_2_1'), 'correct') self.assertEqual(correct_map.get_correctness('1_2_2'), 'correct') self.assertEqual(correct_map.get_correctness('1_2_3'), 'correct')
default_error_messages = { 'required': _('This field is required.'), } empty_values = list(validators.EMPTY_VALUES)
Z = Z.reshape(xx.shape) plt.figure(1) plt.clf() plt.imshow(Z, interpolation='nearest', extent=(xx.min(), xx.max(), yy.min(), yy.max()), cmap=plt.cm.Paired, aspect='auto', origin='lower')
print "{0:>26}".format(course_id.to_deprecated_string()), for heading in status_headings: if heading in cert_data[course_id]: print "{:>16}".format(cert_data[course_id][heading]), else: print " " * 16, print
self.store.publish(self.vertical_x1a, self.user_id) self.store.publish(problem_location, self.user_id)
if not salt.utils.check_include_exclude( os.path.relpath(nfn, root), None, exclude_pat): return removed.add(nfn) if not __opts__['test']: try: os.remove(nfn) except OSError: __salt__['file.remove'](nfn)
source_image = VirtualHardDisk(uri=vm_['image']) img_ref = None if win_installer: os_type = 'Windows' else: os_type = 'Linux'
template = key.lower() if '.' not in template: template = "%s.%s" % (template, settings.STATIC_TEMPLATE_VIEW_DEFAULT_FILE_EXTENSION)
data = {'changed': False, 'backend': 'hgfs'} data['changed'], repos = _clear_old_remotes() for repo in repos: if os.path.exists(repo['lockfile']): log.warning( 'Update lockfile is present for hgfs remote {0}, skipping. ' 'If this warning persists, it is possible that the update ' 'process was interrupted. Removing {1} or running ' '\'salt-run fileserver.clear_lock hgfs\' will allow updates ' 'to continue for this remote.' .format(repo['url'], repo['lockfile']) ) continue _, errors = lock(repo) if errors: log.error('Unable to set update lock for hgfs remote {0}, ' 'skipping.'.format(repo['url'])) continue log.debug('hgfs is fetching from {0}'.format(repo['url'])) repo['repo'].open() curtip = repo['repo'].tip() try: repo['repo'].pull() except Exception as exc: log.error( 'Exception {0} caught while updating hgfs remote {1}' .format(exc, repo['url']), exc_info_on_loglevel=logging.DEBUG ) else: newtip = repo['repo'].tip() if curtip[1] != newtip[1]: data['changed'] = True repo['repo'].close() clear_lock(repo)
class Thingy(models.Model): name = models.CharField(max_length=255)
for key, value in kwargs.items(): value = value and "on" or "off" if key in old and value != old[key]: new.update({key: value}) diff.append('{0}: {1}'.format(key, value))
os.utime(path_test, (fstats_orig.st_mtime-age, fstats_orig.st_atime-age))
num_students = len(students) self.assertDictContainsSubset({'attempted': num_students, 'succeeded': num_students, 'failed': 0}, result)
Xs = [[0, 1, 1], [0, 0, 1], [1, 0, 0], [1, 1, 0]] y = [1] dummy_score = lambda X, y: (X[0], X[0]) for X in Xs: sel = SelectPercentile(dummy_score, percentile=34) X1 = ignore_warnings(sel.fit_transform)([X], y) assert_equal(X1.shape[1], 1) assert_best_scores_kept(sel)
if sys.platform.startswith('win'): import win32file else: import resource
log.debug( 'ext_tops.mongo: no document found in collection {0}'.format( collection ) ) return {}
start_date = Timestamp(start_date) end_date = Timestamp(end_date) filter_start_date = start_date filter_end_date = end_date if self.year is not None: dt = Timestamp(datetime(self.year, self.month, self.day)) if return_name: return Series(self.name, index=[dt]) else: return [dt] dates = self._reference_dates(start_date, end_date) holiday_dates = self._apply_rule(dates) if self.days_of_week is not None: holiday_dates = holiday_dates[np.in1d(holiday_dates.dayofweek, self.days_of_week)] if self.start_date is not None: filter_start_date = max(self.start_date.tz_localize( filter_start_date.tz), filter_start_date) if self.end_date is not None: filter_end_date = min(self.end_date.tz_localize( filter_end_date.tz), filter_end_date) holiday_dates = holiday_dates[(holiday_dates >= filter_start_date) & (holiday_dates <= filter_end_date)] if return_name: return Series(self.name, index=holiday_dates) return holiday_dates
testStack = self.store.fetch('.salt.test.road.stack').value statsReq = self.store.fetch('.salt.stats.event_req').value tag = tagify('road', 'stats') minionName = roadStack.value.local.name unknownName = 'unknownName' statsReq.append({'route': {'dst': (minionName, None, 'stats_req'), 'src': (unknownName, None, None)}, 'tag': tag})
log.debug('Could not find any values for key \'{0}\'. ' 'Setting to \'{0}\' to an empty string.'.format(row)) ret[row] = '' continue
fake_key = SlashSeparatedCourseKey('a', 'b', 'c') self.assertRaises(Http404, lambda: cohorts.is_course_cohorted(fake_key))
CS_batch = CS.make_theano_batch() new_VS_batch = CS.format_as(CS_batch, VS) new_CS_batch = CS.undo_format_as(new_VS_batch, VS) assert new_CS_batch is CS_batch CS_batch = CS_non_default.make_theano_batch() new_VS_batch = CS_non_default.format_as(CS_batch, VS) new_CS_batch = CS_non_default.undo_format_as(new_VS_batch, VS) assert new_CS_batch is CS_batch
integer_field_ranges = dict( BaseDatabaseOperations.integer_field_ranges, PositiveSmallIntegerField=(0, 65535), PositiveIntegerField=(0, 4294967295), )
ceil_ = lambda x: int(ceil(x)) if nrows == -1 and ncols > 0: layout = nrows, ncols = (ceil_(float(nplots) / ncols), ncols) elif ncols == -1 and nrows > 0: layout = nrows, ncols = (nrows, ceil_(float(nplots) / nrows)) elif ncols <= 0 and nrows <= 0: msg = "At least one dimension of layout must be positive" raise ValueError(msg)
self.assertTrue(xb_user.opt_attrs[ATTR_KEY_IS_AUTHENTICATED]) self.assertEqual(xb_user.emails[0], dj_user.email) self.assertEqual(xb_user.full_name, dj_user.profile.name) self.assertEqual(xb_user.opt_attrs[ATTR_KEY_USERNAME], dj_user.username) self.assertEqual(xb_user.opt_attrs[ATTR_KEY_USER_ID], dj_user.id) self.assertFalse(xb_user.opt_attrs[ATTR_KEY_USER_IS_STAFF])
dog_stats_api.start(**options)
DATES = (datetime.date(2000, 6, 30), datetime.date(2000, 6, 15), datetime.date(2000, 6, 3)) for date in DATES: Podcast.objects.create(release_date=date) url = reverse('admin:admin_views_podcast_changelist') response = self.client.get(url) for date in DATES: self.assert_contains_day_link(response, date) self.assert_non_localized_year(response, 2000)
self.assert_index_equal(right.union(left), the_union)
store = self._get_modulestore_for_courselike(asset_key.course_key) return store.set_asset_metadata_attrs(asset_key, {attr: value}, user_id)
return re.sub(ur'[;/]', _quote_slashes, text)
e = { 'required': 'REQUIRED', 'invalid_choice': '%(value)s IS INVALID CHOICE', 'list': 'NOT A LIST OF VALUES', } f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e) self.assertFormErrors(['REQUIRED'], f.clean, '') self.assertFormErrors(['NOT A LIST OF VALUES'], f.clean, '3') self.assertFormErrors(['4 IS INVALID CHOICE'], f.clean, ['4'])
override_field_for_ccx( ccx_course_object, master_course_object, 'max_student_enrollments_allowed', valid_input['max_students_allowed'] )
result = _nargsort(items2, kind='mergesort', ascending=True, na_position='first') exp = list(range(5)) + list(range(105, 110)) + list(range(5, 105)) tm.assert_numpy_array_equal(result, np.array(exp, dtype=np.int64))
g = wkb_r().read(geo_input)
if not isinstance(X, (KDTree, BallTree)): X, y = check_X_y(X, y, "csr", multi_output=True) if y.ndim == 1 or y.ndim == 2 and y.shape[1] == 1: if y.ndim != 1: warnings.warn("A column-vector y was passed when a 1d array " "was expected. Please change the shape of y to " "(n_samples, ), for example using ravel().", DataConversionWarning, stacklevel=2) self.outputs_2d_ = False y = y.reshape((-1, 1)) else: self.outputs_2d_ = True check_classification_targets(y) self.classes_ = [] self._y = np.empty(y.shape, dtype=np.int) for k in range(self._y.shape[1]): classes, self._y[:, k] = np.unique(y[:, k], return_inverse=True) self.classes_.append(classes) if not self.outputs_2d_: self.classes_ = self.classes_[0] self._y = self._y.ravel() return self._fit(X)
from datetime import datetime, timedelta import numpy as np import pandas.tseries.frequencies as frequencies from pandas.tseries.frequencies import get_freq_code as _gfc from pandas.tseries.index import DatetimeIndex, Int64Index, Index from pandas.tseries.tdi import TimedeltaIndex from pandas.tseries.base import DatelikeOps, DatetimeIndexOpsMixin from pandas.tseries.tools import parse_time_string import pandas.tseries.offsets as offsets
n_features = 10 n_samples = 100 n1, n2 = 5, 10 rng = np.random.RandomState(0) X = rng.random_sample((n_samples, n_features)) expected = {(True, n1): (n_features, n1), (True, n2): (n_features, n2), (False, n1): (n_features, n2), (False, n2): (n_features, n2)} for whiten in [True, False]: for n_components in [n1, n2]: n_components_ = (n_components if n_components is not None else X.shape[1]) ica = FastICA(n_components=n_components, random_state=rng, whiten=whiten) with warnings.catch_warnings(record=True): Xt = ica.fit_transform(X) expected_shape = expected[(whiten, n_components_)] assert_equal(ica.mixing_.shape, expected_shape) X2 = ica.inverse_transform(Xt) assert_equal(X.shape, X2.shape)
tol = 10 ** (-6) assert_true(scaler.mean_ is not None) assert_allclose(scaler_incr.var_, scaler.var_, rtol=tol) assert_allclose(scaler_incr.scale_, scaler.scale_, rtol=tol)
import integration
return data['alias'], data['driver'], ()
if isinstance(value, string_types) and ' ' in value: value = re.sub(r'\s+', '\t', value)
from __future__ import absolute_import import os import re import logging import glob
return def_id.block_type
raise
with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
request = self.factory.get('////absolute-uri') self.assertEqual( request.build_absolute_uri(), 'http://testserver//absolute-uri' )
stamp = Timestamp('2011-4-16', tz='US/Eastern') dt_tz = stamp.to_pydatetime() ts = Timestamp.fromordinal(dt_tz.toordinal(), tz='US/Eastern') self.assertEqual(ts.to_pydatetime(), dt_tz)
check_is_fitted(self, 'classes_') y = column_or_1d(y, warn=True) classes = np.unique(y) _check_numpy_unicode_bug(classes) if len(np.intersect1d(classes, self.classes_)) < len(classes): diff = np.setdiff1d(classes, self.classes_) raise ValueError("y contains new labels: %s" % str(diff)) return np.searchsorted(self.classes_, y)
train.save = MethodType(only_run_extensions, train)
method = missing.clean_reindex_fill_method(method) target = _ensure_index(target) if tolerance is not None: tolerance = self._convert_tolerance(tolerance) pself, ptarget = self._possibly_promote(target) if pself is not self or ptarget is not target: return pself.get_indexer(ptarget, method=method, limit=limit, tolerance=tolerance) if not com.is_dtype_equal(self.dtype, target.dtype): this = self.astype(object) target = target.astype(object) return this.get_indexer(target, method=method, limit=limit, tolerance=tolerance) if not self.is_unique: raise InvalidIndexError('Reindexing only valid with uniquely' ' valued Index objects') if method == 'pad' or method == 'backfill': indexer = self._get_fill_indexer(target, method, limit, tolerance) elif method == 'nearest': indexer = self._get_nearest_indexer(target, limit, tolerance) else: if tolerance is not None: raise ValueError('tolerance argument only valid if doing pad, ' 'backfill or nearest reindexing') if limit is not None: raise ValueError('limit argument only valid if doing pad, ' 'backfill or nearest reindexing') indexer = self._engine.get_indexer(target._values) return com._ensure_platform_int(indexer)
from __future__ import absolute_import import os import shutil
return self._geomset_attribute('sym_difference', geom, **kwargs)
log.exception( u'Unable to emit {event} event for user {user} and order {order}'.format( event=event_name, user=self.user.id, order=self.id) )
non_editable_fields = super(HtmlDescriptor, self).non_editable_metadata_fields non_editable_fields.append(HtmlDescriptor.use_latex_compiler) return non_editable_fields
def __init__(self, preference_errors): self.preference_errors = preference_errors
import salt.utils from salt.exceptions import SaltInvocationError, CommandExecutionError from salt.modules.freebsdports import _normalize, _options_file_exists
REQUIRE_DEBUG = DEBUG
params = [("auth_entry", auth_entry)] if redirect_url: params.append(("next", redirect_url)) return u"{url}?{params}".format( url=reverse("social:begin", kwargs={"backend": backend_name}), params=urllib.urlencode(params) )
print("Top wikipedia pages according to principal singular vectors") pprint([names[i] for i in np.abs(U.T[0]).argsort()[-10:]]) pprint([names[i] for i in np.abs(V[0]).argsort()[-10:]])
raise NotImplementedError
self.store.publish(item_location, self.user_id) item = self.store.get_item(item_location) self.assertTrue(self.store.has_published_version(item)) _check_asides(item)
request = self.factory.get(url) request.META['SSL_CLIENT_S_DN'] = self.AUTH_DN.format(self.USER_NAME, self.USER_EMAIL) request.user = AnonymousUser() middleware = SessionMiddleware() middleware.process_request(request) request.session.save() MakoMiddleware().process_request(request) return request
nnbrs = NearestNeighbors(algorithm='brute', metric='cosine').fit(X)
views = [] for p in urlpatterns: if hasattr(p, 'url_patterns'): try: patterns = p.url_patterns except ImportError: continue views.extend(extract_views_from_urlpatterns( patterns, base + p.regex.pattern, (namespace or []) + (p.namespace and [p.namespace] or []) )) elif hasattr(p, 'callback'): try: views.append((p.callback, base + p.regex.pattern, namespace, p.name)) except ViewDoesNotExist: continue else: raise TypeError(_("%s does not appear to be a urlpattern object") % p) return views
del locale if not encoding: encoding = sys.getdefaultencoding() or 'ascii'
filename = 'subs_{0}.srt.sjson'.format(subs_id) content_location = StaticContent.compute_location(self.course.id, filename) try: content = contentstore().find(content_location) contentstore().delete(content.location) except NotFoundError: pass
if isinstance(labels, MultiIndex) and level is not None: if (hasattr(result, 'ndim') and not np.prod(result.shape) and len(keyarr)): raise KeyError("cannot index a multi-index axis " "with these keys")
self.assertDoesNotOptimize( [ migrations.CreateModel("Foo", [ ("a", models.IntegerField()), ("b", models.IntegerField()), ]), alter, migrations.AlterField("Foo", "b", models.CharField(max_length=255)), ], )
braces = '}' * num_braces + "\n" rval = (basic_setup + setup_nv_images + setup_nv_targets + setup_nv_denoms + do_normalize + braces) rval = rval % locals() return rval
self.a.friends.clear()
from openedx.core.djangoapps.credit.api.eligibility import ( is_credit_course, set_credit_requirement_status as api_set_credit_requirement_status )
for key in list(to_cleanup.keys()): instance = to_cleanup.pop(key) del instance
non_int_round_Series = Series(non_int_round_dict) with self.assertRaises(TypeError): df.round(non_int_round_Series)
self._assert_reg_field( no_extra_fields_setting, { u"name": u"email", u"defaultValue": u"bob@example.com", u"type": u"email", u"required": True, u"label": u"Email", u"placeholder": u"username@domain.com", u"restrictions": { "min_length": EMAIL_MIN_LENGTH, "max_length": EMAIL_MAX_LENGTH }, } )
if before is None and after is None and not match: match = content
response = self._get_reverify_page() self.assertContains(response, "reverify-container")
try:
self.service.set_credit_requirement_status( self.user.id, unicode(self.course.id), 'grade', 'grade' )
self.make_course(html_textbooks=[HTML_BOOK]) url = self.make_url('html_book', book_index=1, chapter=1) response = self.client.get(url) self.assertEqual(response.status_code, 404)
from __future__ import absolute_import
get_size = namespaced_function(get_size, globals()) get_image = namespaced_function(get_image, globals()) avail_locations = namespaced_function(avail_locations, globals()) avail_images = namespaced_function(avail_images, globals()) avail_sizes = namespaced_function(avail_sizes, globals()) script = namespaced_function(script, globals()) destroy = namespaced_function(destroy, globals()) list_nodes = namespaced_function(list_nodes, globals()) list_nodes_full = namespaced_function(list_nodes_full, globals()) list_nodes_select = namespaced_function(list_nodes_select, globals()) show_instance = namespaced_function(show_instance, globals())
self.assert_no_xss(response, xss_content)
store = self._verify_modulestore_support(location.course_key, 'convert_to_draft') return store.convert_to_draft(location, user_id)
if len(children) == 1: return children[0] children_latex = [k.latex for k in children] latex = "".join(children_latex) tall = any(k.tall for k in children) return LatexRendered(latex, tall=tall)
from __future__ import absolute_import
with patch('capa.capa_problem.LoncapaProblem.supports_rescoring') as mock_supports_rescoring: mock_supports_rescoring.return_value = False with self.assertRaises(NotImplementedError): module.rescore_problem()
sys.stdout.write("{0}\next_mods\n".format(OPTIONS.delimiter)) sys.exit(EX_MOD_DEPLOY)
return self.child_at(index)
ret['comment'] = 'Event module not available. Schedule add failed.'
return (self.get_input_space(), self.get_input_source())
mock_get_completed_courses.return_value = [ {'course_id': first_course_id, 'mode': MODES.verified}, {'course_id': second_course_id, 'mode': MODES.honor}, ] self._assert_progress( meter, factories.Progress( id=program_id, completed=self._extract_names(program, 0), in_progress=self._extract_names(program, 1) ) )
errstring = "Invalid course_key: 'InvalidCourseID'." with self.assertRaisesRegexp(CommandError, errstring): call_command('export', "InvalidCourseID", self.temp_dir_1)
if os.path.isfile(name): os.remove(name) ret = self.run_state( 'file.append', name=name, text='cheese', makedirs=True ) self.assertSaltTrueReturn(ret)
self.assertContains(response, 'Third query list: 2') self.assertContains(response, 'Fourth query list: 3')
raise TypeError( "SparseArray does not support item assignment via setitem")
if profile and alias not in self.opts['profiles'][profile]['provider'].split(':')[0]: continue
site.delete() with self.assertRaises(ObjectDoesNotExist): get_current_site(request)
self.client.logout() response = self.client.get(self.url) self.assertEqual(response.status_code, 302)
self.assertTrue(np.isnan(self.bseries.fill_value)) tm.assertIsInstance(self.bseries.sp_index, BlockIndex) self.assertTrue(np.isnan(self.iseries.fill_value)) tm.assertIsInstance(self.iseries.sp_index, IntIndex)
a = Article(None, 'Second article', datetime(2005, 7, 29)) a.save() self.assertEqual(a.headline, 'Second article') self.assertEqual(a.pub_date, datetime(2005, 7, 29, 0, 0))
if sys.version_info[:2] < (2, 6): raise nose.SkipTest("file:// not supported with Python < 2.6")
result = f.clean('2010-12-21 13:30:05') self.assertEqual(result, datetime(2010, 12, 21, 13, 30, 5))
request, strategy = self.get_request_and_strategy( auth_entry=pipeline.AUTH_ENTRY_LOGIN, redirect_uri='social:complete') request.backend.auth_complete = mock.MagicMock(return_value=self.fake_auth_complete(strategy)) pipeline.analytics.track = mock.MagicMock() request.user = self.create_user_models_for_existing_account( strategy, 'user@example.com', 'password', self.get_username(), skip_social_auth=True)
print('=' * 80) print("Naive Bayes") results.append(benchmark(MultinomialNB(alpha=.01))) results.append(benchmark(BernoulliNB(alpha=.01)))
import pprint import logging import time
from pandas import Series return Series(self._data.get_dtype_counts())
x = np.linspace(0, resolution - 1, resolution) x = ((2 / ((np.sqrt(3 * width) * np.pi ** 1 / 4))) * (1 - ((x - center) ** 2 / width ** 2)) * np.exp((-(x - center) ** 2) / (2 * width ** 2))) return x
self.assertFalse(self.user.is_active)
if not organizations: raise Exception( 'Configuration error. Microsite {key} does not have any ORGs mapped to it!'.format( key=microsite_object.key ) )
with assertRaisesRegexp(CParserError, 'Passed header=\[0,1,2\] are too many ' 'rows for this multi_index of columns'): read_csv(path, tupleize_cols=False, header=lrange(3), index_col=0)
qn = self.quote_name_unless_alias qn2 = self.connection.ops.quote_name result = [] opts = self.query.get_meta() for name in self.query.distinct_fields: parts = name.split(LOOKUP_SEP) _, targets, alias, joins, path, _ = self._setup_joins(parts, opts, None) targets, alias, _ = self.query.trim_joins(targets, joins, path) for target in targets: if name in self.query.annotation_select: result.append(name) else: result.append("%s.%s" % (qn(alias), qn2(target.column))) return result
for t in [r, g]: result = t.agg({'A': np.sum, 'B': lambda x: np.std(x, ddof=1)}) rcustom = t['B'].apply(lambda x: np.std(x, ddof=1)) expected = pd.concat([r['A'].sum(), rcustom], axis=1) assert_frame_equal(result, expected, check_like=True)
for field in self._cfg: if (field not in _Swagger.SWAGGER_OBJ_V2_FIELDS and not _Swagger.VENDOR_EXT_PATTERN.match(field)): raise ValueError('Invalid Swagger Object Field: {0}'.format(field))
self.assertEqual(response.status_code, 200) msg = Messages.NO_ECOM_API.format(username=self.user.username, course_id=self.course.id) self.assertResponseMessage(response, msg)
df = pd.DataFrame([[1, 2], [3, 4]], columns=pd.date_range('1/1/2013', '1/2/2013'), index=['A', 'B'])
self.first_cohort.delete() self.assertEqual( cohorts.get_group_info_for_cohort(self.first_cohort), (None, None), ) with self.assertRaises(CourseUserGroupPartitionGroup.DoesNotExist): CourseUserGroupPartitionGroup.objects.get( course_user_group_id=self.first_cohort.id )
raise NotImplementedError(str(type(self)) + " needs to implement " "is_stochastic.")
([], '', CourseMode.DEFAULT_MODE_SLUG),
self.filters[name] = filter_func for attr in ('expects_localtime', 'is_safe', 'needs_autoescape'): if attr in flags: value = flags[attr] setattr(filter_func, attr, value) if hasattr(filter_func, "_decorated_function"): setattr(filter_func._decorated_function, attr, value) filter_func._filter_name = name return filter_func
q = Author.objects.none() self.assertQuerysetEqual(q.values(), []) self.assertQuerysetEqual(q.values_list(), [])
v1 = date_range('2012-1-1', periods=3, freq='D') v2 = date_range('2012-1-2', periods=3, freq='D') rs = Series(v2) - Series(v1) xp = Series(1e9 * 3600 * 24, rs.index).astype('int64').astype('timedelta64[ns]') assert_series_equal(rs, xp) self.assertEqual(rs.dtype, 'timedelta64[ns]')
try: course_reg = CourseRegistrationCode.objects.get(code=code) except CourseRegistrationCode.DoesNotExist: return HttpResponseNotFound(_("Discount does not exist against code '{code}'.").format(code=code))
response = self.client.get(self.programs_path) self.assertEquals(response.status_code, status_code) return response
rng = np.random.RandomState(0) n_samples, n_features = 10, 5
return date + datetime.timedelta(days=7 - self._get_weekday(date))
if use_sum: raise NotImplementedError() V_hat = V_hat_unmasked assert hasattr(V_hat, 'owner') owner = V_hat.owner assert owner is not None op = owner.op block_grad = False if is_block_gradient(op): assert isinstance(op.scalar_op, theano.scalar.Identity) block_grad = True real, = owner.inputs owner = real.owner op = owner.op if not hasattr(op, 'scalar_op'): raise ValueError("Expected V_hat_unmasked to be generated by an Elemwise op, got "+str(op)+" of type "+str(type(op))) assert isinstance(op.scalar_op, T.nnet.sigm.ScalarSigmoid) z ,= owner.inputs if block_grad: z = block_gradient(z) if V.ndim != V_hat.ndim: raise ValueError("V and V_hat_unmasked should have same ndim, but are %d and %d." % (V.ndim, V_hat.ndim)) unmasked_cost = V * T.nnet.softplus(-z) + (1 - V) * T.nnet.softplus(z) assert unmasked_cost.ndim == V_hat.ndim if drop_mask is None: masked_cost = unmasked_cost else: masked_cost = drop_mask * unmasked_cost return masked_cost.mean()
migration_loader = MigrationLoader(connection) self.assertEqual( migration_loader.graph.forwards_plan(("migrations", "0001_initial")), [ ('contenttypes', '0001_initial'), ('auth', '0001_initial'), ("migrations", "0001_initial"), ], ) project_state = migration_loader.project_state(("migrations", "0001_initial")) self.assertEqual(len([m for a, m in project_state.models if a == "migrations"]), 1)
import salt.config import salt.loader from salt.modules import boto_cloudtrail
with warnings.catch_warnings(record=True) as w: warnings.simplefilter("always") parsed = read_stata( fname, convert_categoricals=convert_categoricals, convert_dates=convert_dates)
from __future__ import absolute_import import json import logging as logger
with check_mongo_calls_range(max_finds=5, max_sends=3): self._update_partitions(reload_items=False)
X6 = np.array([[0, 0], [-2, -2], [-2, -1], [-1, -1], [-1, -2], [1, 3], [1, 2], [2, 1], [2, 2]]) y6 = np.array([1, 1, 1, 1, 1, 2, 2, 2, 2]) y7 = np.array([1, 2, 3, 2, 3, 1, 2, 3, 1])
result = Series(i) assert_series_equal(result, expected)
import salt.ext.six as six from salt.exceptions import SaltSystemExit
OGRException = GDALException
'organizations',
raise NotImplementedError( str(type(self)) + " does not implement encode.")
A = np.array([[600, 470, 170, 430, 300], [600, 470, 170, 430, 300], [600, 470, 170, 430, 300], [600, 470, 170, 430, 300]]).T idx = 2 X1 = A[:idx, :] X2 = A[idx:, :]
least_squares, info = solve_cholesky(L[:n_active, :n_active], sign_active[:n_active], lower=True)
mdict = copy.deepcopy(self.dict1) mdict['C']['F']['G'] = ['a', 'b'] res = dictupdate.update(copy.deepcopy(mdict), {'C': {'F': {'G': ['c', 'd']}}}, merge_lists=True) mdict['C']['F']['G'] = ['a', 'b', 'c', 'd'] self.assertEqual(res, mdict)
stream_stdout=None, stream_stderr=None, ):
request._dont_enforce_csrf_checks = not self.enforce_csrf_checks
rng = np.random.RandomState(0) n_topics, X = _build_sparse_mtx() lda = LatentDirichletAllocation(n_topics=n_topics, learning_method='batch', random_state=rng) lda.fit(X.toarray())
time.sleep(1) return info(name).get('home') == home
return self.q(css='.wrapper-last-draft').first.text[0]
STATIC_URL = "/static/" STATICFILES_FINDERS = ( 'django.contrib.staticfiles.finders.FileSystemFinder', ) STATICFILES_DIRS = ( (TEST_ROOT / "staticfiles" / "lms").abspath(), )
import logging import re import os from salt.ext.six.moves import map
from __future__ import absolute_import
comps = fun.split('.') if len(comps) < 2: __context__['retcode'] = 1 return 'Invalid function passed'
pass
return
a, b = first, second assert a is b, "%s: %r is not %r" % (msg.format(a, b), a, b)
PROFILE_IMAGE_BACKEND = ENV_TOKENS.get('PROFILE_IMAGE_BACKEND', PROFILE_IMAGE_BACKEND) PROFILE_IMAGE_SECRET_KEY = AUTH_TOKENS.get('PROFILE_IMAGE_SECRET_KEY', PROFILE_IMAGE_SECRET_KEY) PROFILE_IMAGE_MAX_BYTES = ENV_TOKENS.get('PROFILE_IMAGE_MAX_BYTES', PROFILE_IMAGE_MAX_BYTES) PROFILE_IMAGE_MIN_BYTES = ENV_TOKENS.get('PROFILE_IMAGE_MIN_BYTES', PROFILE_IMAGE_MIN_BYTES) PROFILE_IMAGE_DEFAULT_FILENAME = 'images/profiles/default'
if any([line.startswith('{0}.'.format(root)) for root in roots]): comps = line.split(': ' if ': ' in line else ' = ', 1) if len(comps) == 2: ret[comps[0]] = comps[1] else: ret[comps[0]] = '' elif comps[0]: ret[comps[0]] += '{0}\n'.format(line) else: continue
expected = { "action": "enroll", "auto_enroll": False, "results": [ { "identifier": self.notenrolled_student.email, "before": { "enrollment": False, "auto_enroll": False, "user": True, "allowed": False, }, "after": { "enrollment": True, "auto_enroll": False, "user": True, "allowed": False, } } ] }
return value.rjust(int(arg))
purchased_callback.assert_called_with()
else: ret = _load_result(existing, ret)
exists_ce = is_active is not None and is_active full_name = user.profile.name
expected = { u'version': CERTIFICATE_SCHEMA_VERSION, u'name': u'Test certificate', u'description': u'Test description', u'is_active': True, u'signatories': [] } response = self.client.ajax_post( self._url(), data=CERTIFICATE_JSON ) self.assertEqual(response.status_code, 201) self.assertIn("Location", response) content = json.loads(response.content) certificate_id = self._remove_ids(content) self.assertEqual(content, expected) self.assert_event_emitted( 'edx.certificate.configuration.created', course_id=unicode(self.course.id), configuration_id=certificate_id, )
y_true, y_pred, _ = make_prediction(binary=False)
if salt.utils.is_windows(): return (False, 'The network execution module cannot be loaded on Windows: use win_network instead.') return True
self.course_outline.select_advanced_tab()
self.assertContains( response, settings.MICROSITE_CONFIGURATION['test_microsite']['logo_image_url'], ) self.assertContains( response, settings.MICROSITE_CONFIGURATION['test_microsite']["urls"]['ABOUT'], ) self.assertContains( response, settings.MICROSITE_CONFIGURATION['test_microsite']["urls"]['PRIVACY'], ) self.assertContains( response, settings.MICROSITE_CONFIGURATION['test_microsite']["urls"]['TOS_AND_HONOR'], )
zfs.__salt__ = {}
self.mapbias = sharedX( numpy.zeros(self.nmap), name='mb', borrow=True )
df = DataFrame({'a': [1, 1]}) ds = Series([2], index=[1], name='b') result = df.join(ds, on='a') expected = DataFrame({'a': [1, 1], 'b': [2, 2]}, index=df.index) tm.assert_frame_equal(result, expected)
new_module_store_setting['default']['OPTIONS']['stores'] = convert_old_stores_into_list( module_store_setting ) module_store_setting = new_module_store_setting
cursor.execute("SELECT CURRVAL(pg_get_serial_sequence('%s','%s'))" % ( self.quote_name(table_name), pk_name)) return cursor.fetchone()[0]
from salt.modules import win_network
self.assertRaises(PossibleDataLossError, store.open, 'w') store.close() self.assertFalse(store.is_open)
cause_copy = sharedX(np.zeros((num_samples,))).dimshuffle(0,'x') v_state = v_state[0,:] + cause_copy p, h = h1_state h1_state = (p[0,:] + cause_copy, h[0,:] + cause_copy) p, h = h2_state h2_state = (p[0,:] + cause_copy, h[0,:] + cause_copy)
the_system = test_capa_system() the_system.render_template = mock.Mock() the_system.render_template.return_value = "<div>Input Template Render</div>"
if value == self.empty_value or value in self.empty_values: return self.empty_value new_value = [] for choice in value: try: new_value.append(self.coerce(choice)) except (ValueError, TypeError, ValidationError): raise ValidationError( self.error_messages['invalid_choice'], code='invalid_choice', params={'value': choice}, ) return new_value
before = self.make_project_state([self.author_name_nested_deconstructible_1]) after = self.make_project_state([self.author_name_nested_deconstructible_changed_kwarg]) autodetector = MigrationAutodetector(before, after) changes = autodetector._detect_changes() self.assertEqual(len(changes), 1)
fname = '${PYLEARN2_DATA_PATH}/mnistplus/mnistplus' if label_type == 'azimuth': fname += '_azi' if label_type == 'rotation': fname += '_rot' label_type = 'label' if label_type == 'texture_id': fname += '_tex' label_type = 'label'
val = Timestamp('20130101 12:01:02') self.assertFalse(val == 'foo') self.assertFalse(val == 10.0) self.assertFalse(val == 1) self.assertFalse(val == long(1)) self.assertFalse(val == []) self.assertFalse(val == {'foo': 1}) self.assertFalse(val == np.float64(1)) self.assertFalse(val == np.int64(1))
return set()
is_collection = False
class GetDateShowHiddenInitial(Form): mydate = DateField(widget=SelectDateWidget, show_hidden_initial=True)
if request.method not in ('GET', 'HEAD', 'OPTIONS', 'TRACE'): if getattr(request, '_dont_enforce_csrf_checks', False): return self._accept(request)
if isinstance(data, mrecords.MaskedRecords): mgr = _masked_rec_array_to_mgr(data, index, columns, dtype, copy)
mlp = MLP(layers=[Linear(layer_name='h', dim=5, irange=0.01)]) conditional = DummyConditional(mlp=mlp, name='conditional') vae = DummyVAE() conditional.set_vae(vae) testing.assert_same_object(conditional.get_vae(), vae)
self.store.convert_to_draft(self.vertical_x1a, self.user_id) self.store.convert_to_draft(self.vertical_y1a, self.user_id)
pass
if attribute not in ABOUT_ATTRIBUTES + ['video']: raise ValueError("'{0}' is not a valid course about attribute.".format(attribute)) usage_key = course_key.make_usage_key('about', attribute) try: value = modulestore().get_item(usage_key).data except ItemNotFoundError: value = None return value
if isinstance(opts['file_ignore_glob'], str): opts['file_ignore_glob'] = [opts['file_ignore_glob']]
if self.is_color: for i in xrange(3): self.image[:, :, i] = self.background[i] * .5 + .5 else: self.image[:] = self.background * .5 + .5 self.cur_pos = (0, 0)
return GISLookup._check_geo_field(self.model._meta, field_name)
cmd = '{vmadm} validate {mode} {brand} -f {vmadm_json_file}'.format( vmadm=vmadm, mode=mode, brand=get(uuid)['brand'] if uuid is not None else '', vmadm_json_file=vmadm_json_file ) res = __salt__['cmd.run_all'](cmd, python_shell=True) retcode = res['retcode'] if retcode != 0: ret['Error'] = _exit_status(retcode) if 'stderr' in res: if res['stderr'][0] == '{': ret['Error'] = json.loads(res['stderr']) else: ret['Error'] = res['stderr'] return ret cmd = '{vmadm} {mode} {uuid} -f {vmadm_json_file}'.format( vmadm=vmadm, mode=mode, uuid=uuid if uuid is not None else '', vmadm_json_file=vmadm_json_file ) res = __salt__['cmd.run_all'](cmd, python_shell=True) retcode = res['retcode'] if retcode != 0: ret['Error'] = _exit_status(retcode) if 'stderr' in res: if res['stderr'][0] == '{': ret['Error'] = json.loads(res['stderr']) else: ret['Error'] = res['stderr'] return ret else: salt.utils.safe_rm(vmadm_json_file)
df = DataFrame([{"a": 1}, {"a": 3, "b": 2}]) df['c'] = np.nan self.assertEqual(df['c'].dtype, np.float64)
self.client.post('/password_reset/', {'email': 'staffmember@example.com'}) self.assertEqual(len(mail.outbox), 1) return self._read_signup_email(mail.outbox[0])
import copy import logging import json
for name, clf in zip(names, classifiers): ax = plt.subplot(len(datasets), len(classifiers) + 1, i) clf.fit(X_train, y_train) score = clf.score(X_test, y_test)
if not self.q(css="input.timed_exam").present: return False
y = iris.target colors = "bry"
rs2 = ser.replace([np.nan, 'foo', 'bar'], [-1, -2, -3]) tm.assert_series_equal(rs, rs2)
lang_code = self._get_inactive_language_code() post_data = dict(language=lang_code, next='//unsafe/redirection/') response = self.client.post('/i18n/setlang/', post_data, HTTP_X_REQUESTED_WITH='XMLHttpRequest') self.assertEqual(response.url, '/') self.assertEqual(self.client.session[LANGUAGE_SESSION_KEY], lang_code)
response = self.course_fixture.session.post( LMS_BASE_URL + '/api/team/v0/teams/', data=json.dumps(team_data), headers=self.course_fixture.headers ) self.assertEqual(response.status_code, 200) return json.loads(response.text)
course = CourseFactory.create( org=course_location.org, number=course_location.course, run=course_location.run, default_store=store ) self._add_role_access_to_user(user, course.id) return course
assert_raises(ValueError, auc, [0.0], [0.1])
if when > data['_when']: data['_when'] = when data['_when_run'] = True
if settings_dict['NAME'] == '': raise ImproperlyConfigured( "settings.DATABASES is improperly configured. " "Please supply the NAME value.") conn_params = { 'database': settings_dict['NAME'] or 'postgres', } conn_params.update(settings_dict['OPTIONS']) conn_params.pop('isolation_level', None) if settings_dict['USER']: conn_params['user'] = settings_dict['USER'] if settings_dict['PASSWORD']: conn_params['password'] = force_str(settings_dict['PASSWORD']) if settings_dict['HOST']: conn_params['host'] = settings_dict['HOST'] if settings_dict['PORT']: conn_params['port'] = settings_dict['PORT'] return conn_params
self.assertRaises(ValueError, TimedeltaIndex, ['1 days', '2 days', '4 days'], freq='D')
if field.many_to_many and field.remote_field.through._meta.auto_created: return self.create_model(field.remote_field.through) definition, params = self.column_sql(model, field, include_default=True) if definition is None: return db_params = field.db_parameters(connection=self.connection) if db_params['check']: definition += " CHECK (%s)" % db_params['check'] sql = self.sql_create_column % { "table": self.quote_name(model._meta.db_table), "column": self.quote_name(field.column), "definition": definition, } self.execute(sql, params) if not self.skip_default(field) and field.default is not None: sql = self.sql_alter_column % { "table": self.quote_name(model._meta.db_table), "changes": self.sql_alter_column_no_default % { "column": self.quote_name(field.column), } } self.execute(sql) if field.db_index and not field.unique: self.deferred_sql.append(self._create_index_sql(model, [field])) if field.remote_field and self.connection.features.supports_foreign_keys and field.db_constraint: self.deferred_sql.append(self._create_fk_sql(model, field, "_fk_%(to_table)s_%(to_column)s")) if self.connection.features.connection_persists_old_columns: self.connection.close()
log = logging.getLogger(__name__)
if postgis: self.assertTrue(srs.wkt.startswith(sd['srtext'])) six.assertRegex(self, srs.proj4text, sd['proj4_re'])
ith_cluster_silhouette_values = \ sample_silhouette_values[cluster_labels == i]
instructor_dashboard_page = self.visit_instructor_dashboard() allowance_section = instructor_dashboard_page.select_special_exams().select_allowance_section()
tz = obj.tz.zone if hasattr( obj, 'tz') and hasattr(obj.tz, 'zone') else 'UTC' tz = StrSexpVector([tz]) utc_tz = StrSexpVector(['UTC'])
result = obj.rename_axis(arg, axis=axis) expected = obj.copy() setattr(expected, axis, list('abcd')) self._compare(result, expected)
sample_indices = _generate_sample_indices(random_state, n_samples) sample_counts = bincount(sample_indices, minlength=n_samples) unsampled_mask = sample_counts == 0 indices_range = np.arange(n_samples) unsampled_indices = indices_range[unsampled_mask] return unsampled_indices
elif existing['code'] == 404: ret['comment'] = 'A Profile with this name was not found.' else: ret = _load_result(existing, ret)
author_book_auto_m2m_intermediate = Author.books.through.objects.get(author=author, book=book) self.author_book_auto_m2m_intermediate_id = author_book_auto_m2m_intermediate.pk
if 'rules' in violation_thresholds: threshold_keys = sorted(violation_thresholds['rules'].keys()) for threshold_key in threshold_keys: if threshold_key not in safelint_counts['rules']: error_message += ( "\nNumber of {safelint_script} violations for {rule} could not be found in " "{safelint_report}." ).format( safelint_script=safelint_script, rule=threshold_key, safelint_report=safelint_report ) elif violation_thresholds['rules'][threshold_key] < safelint_counts['rules'][threshold_key]: error_message += \ "\nToo many {rule} violations ({count}).\nThe {rule} limit is {violations_limit}.".format( rule=threshold_key, count=safelint_counts['rules'][threshold_key], violations_limit=violation_thresholds['rules'][threshold_key], )
self.selenium.find_elements_by_link_text('Add another Related prepopulated')[1].click() self.selenium.find_element_by_id('id_relatedprepopulated_set-2-1-pubdate').send_keys('1981-08-22') self.get_select_option('#id_relatedprepopulated_set-2-1-status', 'option one').click() self.selenium.find_element_by_id('id_relatedprepopulated_set-2-1-name').send_keys( 'a tÃbűlaŘ inline with ignored ;"&*^\%$#@-/`~ characters' ) slug1 = self.selenium.find_element_by_id('id_relatedprepopulated_set-2-1-slug1').get_attribute('value') slug2 = self.selenium.find_element_by_id('id_relatedprepopulated_set-2-1-slug2').get_attribute('value') self.assertEqual(slug1, 'tabular-inline-ignored-characters-1981-08-22') self.assertEqual(slug2, 'option-one-tabular-inline-ignored-characters')
with self.assertRaises(Http404): self.get_form(expected_valid=False)
EmptyPromise( lambda: self.q(css='a.button.action-primary').present, 'Delete prompt is displayed' ).fulfill()
"utf8": u"✓", "req_bill_to_address_country": "US", "auth_avs_code": "X", "req_card_expiry_date": "01-2018", "bill_trans_ref_no": "85080648RYI23S6I", "req_bill_to_address_state": "MA", "signed_field_names": ",".join(signed_field_names), "req_payment_method": "card", "req_transaction_type": "sale", "auth_code": "888888", "req_locale": "en", "reason_code": "100", "req_bill_to_address_postal_code": "02139", "req_bill_to_address_line1": "123 Fake Street", "req_card_type": "001", "req_bill_to_address_city": "Boston", "signed_date_time": "2014-08-18T14:07:10Z", "req_currency": "usd", "auth_avs_code_raw": "I1", "transaction_id": "4083708299660176195663", "auth_time": "2014-08-18T140710Z", "message": "Request was processed successfully.", "auth_response": "100", "req_profile_id": "0000001", "req_transaction_uuid": "ddd9935b82dd403f9aa4ba6ecf021b1f", "auth_trans_ref_no": "85080648RYI23S6I", "req_bill_to_surname": "Doe", "req_bill_to_forename": first_name, "req_bill_to_email": "john@example.com", "req_override_custom_receipt_page": "http://localhost:8000/shoppingcart/postpay_callback/", "req_access_key": "abcd12345",
self._cache = {}
rexp = re.compile('(?m)^Conf '
course = CourseFactory.create() item = ItemFactory.create(parent_location=course.location) self.assertIsInstance(item, SequenceDescriptor)
assert isinstance(sls_data, dict) assert isinstance(yml_data, dict) assert sls_data == yml_data
['insert', 'update', 'delete', '_do_batched_write_command', '_do_batched_insert', ], max_sends if max_sends is not None else float("inf"), min_sends if min_sends is not None else 0,
if self.remote_field.through_fields is not None: if not (len(self.remote_field.through_fields) >= 2 and self.remote_field.through_fields[0] and self.remote_field.through_fields[1]): errors.append( checks.Error( "Field specifies 'through_fields' but does not provide " "the names of the two link fields that should be used " "for the relation through model '%s'." % qualified_model_name, hint="Make sure you specify 'through_fields' as through_fields=('field1', 'field2')", obj=self, id='fields.E337', ) )
self.assertEqual( list(User.objects.extra(select={"alpha": "%s"}, select_params=(-6,)) .filter(id=self.u.id).values_list('id', flat=True)), [self.u.id] )
super(NotLiveRedirectTest, self).setUp() CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'], start_date=datetime(year=2099, month=1, day=1) ).install() AutoAuthPage(self.browser, course_id=self.course_id).visit()
from salt import template
df = DataFrame({"a": ['R1', 'R2', np.nan, 'R4'], 'b': ["C1", "C2", "C3", "C4"], "c": [10, 15, np.nan, 20]}) result = df.set_index(['a', 'b'], drop=False) expected = DataFrame({"a": ['R1', 'R2', np.nan, 'R4'], 'b': ["C1", "C2", "C3", "C4"], "c": [10, 15, np.nan, 20]}, index=[Index(['R1', 'R2', np.nan, 'R4'], name='a'), Index(['C1', 'C2', 'C3', 'C4'], name='b')]) assert_frame_equal(result, expected)
if opstr in ['__div__', '__truediv__']: if _is_convertible_to_td(other): other = Timedelta(other) if isnull(other): raise NotImplementedError( "division by pd.NaT not implemented")
tag_links = self.q(css=self._bounded_selector(self.TAG_SELECTOR)) if len(tag_links) == 0: return None return[tag_link.text for tag_link in tag_links]
team.add_user(request.user) emit_team_event( 'edx.team.learner_added', course_key, { 'team_id': team.team_id, 'user_id': request.user.id, 'add_method': 'added_on_create' } )
import salt.ext.six as six
n_components, n_features = means.shape precisions_chol = np.empty((n_components, n_features, n_features)) for k in range(n_components): diff = X - means[k] covariance = np.dot(resp[:, k] * diff.T, diff) / nk[k] covariance.flat[::n_features + 1] += reg_covar try: cov_chol = linalg.cholesky(covariance, lower=True) except linalg.LinAlgError: raise ValueError(ESTIMATE_PRECISION_ERROR_MESSAGE) precisions_chol[k] = linalg.solve_triangular(cov_chol, np.eye(n_features), lower=True).T return precisions_chol
if always_check is False and did_install: ret['result'] = True ret['comment'] = 'Composer already installed this directory' return ret
result = self.get_json(self.LIST_URI) self.assertEqual(result["count"], 0) self.assertIsNone(result["next"]) self.assertIsNone(result["previous"]) self.assertEqual(result["results"], [])
if self.instance._state.adding: if kwargs.get('to_field') is not None: to_field = self.instance._meta.get_field(kwargs['to_field']) else: to_field = self.instance._meta.pk if to_field.has_default(): setattr(self.instance, to_field.attname, None)
self._tags[course_id][key] = value
PAIRWISE_DISTANCE_FUNCTIONS = { 'cityblock': manhattan_distances, 'cosine': cosine_distances, 'euclidean': euclidean_distances, 'l2': euclidean_distances, 'l1': manhattan_distances, 'manhattan': manhattan_distances,
result = idxr(s)[indexer] self.check(result, s, 3, getitem)
import pytz tz = pytz.timezone('Asia/Tokyo') expected_tuples = [(1.1, datetime.datetime(2011, 1, 1, tzinfo=tz)), (1.2, datetime.datetime(2011, 1, 2, tzinfo=tz)), (1.3, datetime.datetime(2011, 1, 3, tzinfo=tz))] expected = Index([1.1, 1.2, 1.3] + expected_tuples) self.assert_index_equal(result, expected)
self.check_models_ready() for model in self.models.values(): if model._meta.auto_created and not include_auto_created: continue if model._meta.swapped and not include_swapped: continue yield model
from __future__ import print_function
serialized_tab = tab.to_json() deserialized_tab = tab.from_json(serialized_tab) self.assertEquals(serialized_tab, deserialized_tab)
if self.data.minute == 0: return self.g() return '%s:%s' % (self.g(), self.i())
from __future__ import unicode_literals
temp = s.copy()
if not fnmatch.fnmatch(repo['url'], six.text_type(remote)): continue
res = idx.insert(1, 1.1) tm.assert_index_equal(res, pd.Index([1., 1.1, 2., 3., 4.])) self.assertEqual(res.dtype, np.float64)
return image.convert('RGB')
cp.__salt__ = {} cp.__opts__ = {} cp.__pillar__ = {} cp.__grains__ = {} cp.__context__ = {}
codes = c.codes
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010 13:30:00")
credit_course = self.add_credit_course() credit_course.enabled = False credit_course.save()
clf = self.factory(alpha=0.01, n_iter=20) clf.fit(X2, Y2, coef_init=np.zeros((3, 2)), intercept_init=np.zeros(3)) assert_equal(clf.coef_.shape, (3, 2)) assert_true(clf.intercept_.shape, (3,)) pred = clf.predict(T2) assert_array_equal(pred, true_result2)
def __init__(self, custom_optional_arg=None, model=None, query=None, using=None, hints=None): super(CustomInitQuerySet, self).__init__(model=model, query=query, using=using, hints=hints)
depths = [_find_longest_prefix_match(tree, tree_queries, MAX_HASH_SIZE, self._left_mask, self._right_mask) for tree, tree_queries in zip(self.trees_, np.rollaxis(bin_queries, 1))]
if LoginFailures.is_feature_enabled(): LoginFailures.clear_lockout_counter(user)
s1 = make_dtnat_arr(chunksize + 5) s2 = make_dtnat_arr(chunksize + 5, 0)
lti_params = response[LTI_PARAMS_KEY] return lti_params['oauth_consumer_key'] + ":" + lti_params['user_id']
with self.assertRaises(forms.ValidationError): f.clean('21.12.2010')
filedata = json.dumps(subs, indent=2) mime_type = 'application/json' filename = 'subs_{0}.srt.sjson'.format(subs_id) content_location = StaticContent.compute_location(self.course.id, filename) content = StaticContent(content_location, filename, mime_type, filedata) contentstore().save(content) del_cached_content(content_location) return content_location
return comdat['stderr']
if CourseMode.VERIFIED in modes and enrollment.mode in CourseMode.UPSELL_TO_VERIFIED_MODES: mode_info['show_upsell'] = True mode_info['verified_sku'] = modes['verified'].sku mode_info['verified_bulk_sku'] = modes['verified'].bulk_sku if modes['verified'].expiration_datetime: today = datetime.datetime.now(UTC).date() mode_info['days_for_upsell'] = (modes['verified'].expiration_datetime.date() - today).days
mock_is_user_payment_error.return_value = is_user_message_expected self._login() response = self.post_to_receipt_page({'decision': 'REJECT', 'reason_code': '99', 'signed_field_names': 'dummy'}) self.assertTrue(mock_is_user_payment_error.called) self.assertTrue(mock_is_user_payment_error.call_args[0][0], '99') user_message = "There was a problem with this transaction" system_message = "A system error occurred while processing your payment" self.assertRegexpMatches(response.content, user_message if is_user_message_expected else system_message) self.assertNotRegexpMatches(response.content, user_message if not is_user_message_expected else system_message)
return int(self.as_double())
from __future__ import absolute_import import os import sys import shutil import tempfile import stat
- cn=foo,ou=users,dc=example,dc=com: - delete_others: True - default: userPassword: changeme shadowLastChange: 0 sshPublicKey: [] - replace: cn: foo uid: foo uidNumber: 1000 gidNumber: 1000 gecos: Foo Bar givenName: Foo sn: Bar homeDirectory: /home/foo loginShell: /bin/bash objectClass: - inetOrgPerson - posixAccount - top - ldapPublicKey - shadowAccount
ii = iter_offset - 1
log = logging.getLogger(__name__)
if keyname(vm_) is None: raise SaltCloudSystemExit( 'The required \'keyname\' configuration setting is missing from the ' '\'ec2\' driver.' )
s = Series([1, 2, 3], index=list('abc')) result = s.at['a'] self.assertEqual(result, 1) self.assertRaises(ValueError, lambda: s.at[0])
settings = options.get("settings", "devstack") call_task("pavelib.servers.check_settings", args=[system, settings]) self.assertEquals( self.task_messages, [ "echo 'import {system}.envs.{settings}' " "| python manage.py {system} --settings={settings} shell --plain --pythonpath=.".format( system=system, settings=settings ), ] )
for X in [X_1row, X_1col, X_list_1row, X_list_1row]:
src_estate, src_yard, src_share = msg['route']['src'] salt.transport.jobber_estate_name = src_estate salt.transport.jobber_yard_name = src_yard
pass
if rerandomize == RANDOMIZATION.NEVER: self.assertEqual(seed, 1, msg="Seed should always be 1 when rerandomize='%s'" % rerandomize)
X_dense[3, :] = 0.0
time.sleep(5)
ds_list = ( TestDS( 'test_point', nfeat=5, nfld=3, geom='POINT', gtype=1, driver='ESRI Shapefile', fields={'dbl': OFTReal, 'int': OFTInteger, 'str': OFTString},
to_replace_res = [r'\s*(\.)\s*', r'(a|b)'] values = [r'\1\1', r'\1_crap'] res = dfmix.replace(to_replace_res, values, regex=True) expec = DataFrame({'a': mix['a'], 'b': ['a_crap', 'b_crap', '..', '..']})
expected = DatetimeIndex(['2015-06-19 05:33:20', '2015-05-27 22:33:20']) arr1 = [1.434692e+18, 1.432766e+18] arr2 = np.array(arr1).astype('int64') for errors in ['ignore', 'raise', 'coerce']: result = pd.to_datetime(arr1, errors=errors) tm.assert_index_equal(result, expected)
if len(image.shape) < 2 or len(image.shape) > 3: raise ValueError('image must have either 2 or 3 dimensions but its' ' shape is ' + str(image.shape))
self.q(css='div.problem .choicegroup input[value="' + choice_value + '"]').click() self.wait_for_ajax()
from __future__ import absolute_import
request = self.request_factory.get('/', {'date_registered__isnull': 'True'}) changelist = self.get_changelist(request, Book, modeladmin)
milestones_helpers.remove_course_content_user_milestones( course_key=usage_key.course_key, content_key=usage_key, user=student, relationship='fulfills' )
msg = "First argument to get_list_or_404() must be a Model, Manager, or QuerySet, not 'list'." with self.assertRaisesMessage(ValueError, msg): get_list_or_404([Article], title__icontains="Run")
pass
self.ccx_key = CCXLocator.from_course_locator(self._course.id, ccx.id) self.course = get_course_by_id(self.ccx_key, depth=None) setup_students_and_grades(self) self.client.login(username=coach.username, password="test") self.addCleanup(RequestCache.clear_request_cache)
staff_reset_time = timezone.now() + timedelta(days=1) with freeze_time(staff_reset_time): user = User.objects.get(email=staff_email) token = default_token_generator.make_token(user) uidb36 = int_to_base36(user.id)
related_objects = related_objects_graph[model._meta.concrete_model._meta] model._meta.__dict__['_relation_tree'] = related_objects
pass
for i, hyperparameter in enumerate(kernel.hyperparameters): theta[i] = np.log(42) kernel.theta = theta assert_almost_equal(getattr(kernel, hyperparameter.name), 42)
course = self.store.create_course('org_x', 'course_y', 'run_z', self.user_id) signal_handler.send.assert_called_with('course_published', course_key=course.id)
scaler_batch = MaxAbsScaler().fit(X)
return self.example_cert_set.course_key
if os.path.isfile(join(dirname(abspath(__file__)), 'private.py')):
pass
if isinstance(ff.widget, widgets.RelatedFieldWidgetWrapper): widget = ff.widget.widget else: widget = ff.widget
match_sets = ['src1 flag1', 'src2 flag2,flag3', ] self.assertEqual(iptables.build_rule(**{'match-set': match_sets}), '-m set --match-set src1 flag1 -m set --match-set src2 flag2,flag3')
self.assertIsInstance(Article.objects.iterator(), collections.Iterator)
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
data = { 'person1-first_name': 'John', 'person1-last_name': 'Lennon', 'person1-birthday': '1940-10-9', 'person2-first_name': 'Jim', 'person2-last_name': 'Morrison', 'person2-birthday': '1943-12-8' } p1 = Person(data, prefix='person1') self.assertTrue(p1.is_valid()) self.assertEqual(p1.cleaned_data['first_name'], 'John') self.assertEqual(p1.cleaned_data['last_name'], 'Lennon') self.assertEqual(p1.cleaned_data['birthday'], datetime.date(1940, 10, 9)) p2 = Person(data, prefix='person2') self.assertTrue(p2.is_valid()) self.assertEqual(p2.cleaned_data['first_name'], 'Jim') self.assertEqual(p2.cleaned_data['last_name'], 'Morrison') self.assertEqual(p2.cleaned_data['birthday'], datetime.date(1943, 12, 8))
wrappers.insert(0, wrap_with_license)
random = np.random.RandomState(seed=0) E = random.normal(size=(len(X), 2200))
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
n_samples = len(digits.data) data = digits.data / 16. data -= data.mean(axis=0)
if on_saltstack: html_search_template = 'googlesearch.html' else: html_search_template = 'searchbox.html'
tuple_args = tuple(list_args) _validate_ret(parallels._normalize_args(tuple_args))
MONGO_PORT_NUM = int(os.environ.get('EDXAPP_TEST_MONGO_PORT', '27017')) MONGO_HOST = os.environ.get('EDXAPP_TEST_MONGO_HOST', 'localhost')
err = SearchIndexingError mock_index_dictionary.return_value = err
for input, output in valid.items(): self.assertEqual(required.clean(input), output) self.assertEqual(optional.clean(input), output) for input, errors in invalid.items(): with self.assertRaises(ValidationError) as context_manager: required.clean(input) self.assertEqual(context_manager.exception.messages, errors)
return response
if value is None: return ret
return self.q(css='.create-user-button').present
event = salt.utils.event.get_event( 'master', __opts__['sock_dir'], __opts__['transport'], opts=__opts__, listen=False) event.fire_event(data, tagify(['roots', 'update'], prefix='fileserver'))
gnomedesktop.__grains__ = {} gnomedesktop.__salt__ = {} gnomedesktop.__context__ = {} gnomedesktop.__opts__ = {}
MULTI = 'multi' SINGLE = 'single' CURSOR = 'cursor' NO_RESULTS = 'no results'
qsets = ( ( Staff.objects.distinct().order_by('name'), ['<Staff: p1>', '<Staff: p1>', '<Staff: p2>', '<Staff: p3>'], ), ( Staff.objects.distinct('name').order_by('name'), ['<Staff: p1>', '<Staff: p2>', '<Staff: p3>'], ), ( Staff.objects.distinct('organisation').order_by('organisation', 'name'), ['<Staff: p1>', '<Staff: p1>'], ), ( Staff.objects.distinct('name', 'organisation').order_by('name', 'organisation'), ['<Staff: p1>', '<Staff: p1>', '<Staff: p2>', '<Staff: p3>'], ), ( Celebrity.objects.filter(fan__in=[self.fan1, self.fan2, self.fan3]).distinct('name').order_by('name'), ['<Celebrity: c1>', '<Celebrity: c2>'], ), ( (Celebrity.objects.filter(fan__in=[self.fan1, self.fan2]). distinct('name').order_by('name') | Celebrity.objects.filter(fan__in=[self.fan3]). distinct('name').order_by('name')), ['<Celebrity: c1>', '<Celebrity: c2>'], ), ( StaffTag.objects.distinct('staff', 'tag'), ['<StaffTag: t1 -> p1>'], ), ( Tag.objects.order_by('parent__pk', 'pk').distinct('parent'), ['<Tag: t2>', '<Tag: t4>', '<Tag: t1>'], ), ( StaffTag.objects.select_related('staff').distinct('staff__name').order_by('staff__name'), ['<StaffTag: t1 -> p1>'], ), ( (Staff.objects.distinct('id').order_by('id', 'coworkers__name'). values_list('id', 'coworkers__name')), [str_prefix("(1, %(_)s'p2')"), str_prefix("(2, %(_)s'p1')"), str_prefix("(3, %(_)s'p1')"), "(4, None)"] ), ) for qset, expected in qsets: self.assertQuerysetEqual(qset, expected) self.assertEqual(qset.count(), len(expected))
url(r'^api/courses/', include('course_api.urls')),
return self.table_name_converter(name)
if com.is_object_dtype(dtype): return lib.map_infer(self.values.ravel(), self._box_func).reshape(self.values.shape) return self.values
mock_get_id_token.return_value = "test_token" mock_anonymous_id_for_user.return_value = "anonymous_id" helpers.send_request( self.user, self.course.id, path="test", text="text", page=helpers.DEFAULT_PAGE, page_size=helpers.DEFAULT_PAGE_SIZE ) mock_get.assert_called_with( "http://example.com/test/", headers={ "x-annotator-auth-token": "test_token" }, params={ "user": "anonymous_id", "course_id": unicode(self.course.id), "text": "text", "highlight": True, 'page': 1, 'page_size': 25, }, timeout=(settings.EDXNOTES_CONNECT_TIMEOUT, settings.EDXNOTES_READ_TIMEOUT) )
third = e.count(':')
return False
meta = self.get_meta() old_login = meta.get('session_id', None) if old_login: SessionStore(session_key=old_login).delete() meta['session_id'] = session_id self.set_meta(meta) self.save()
return (field_object.usage_id.map_into_course(self.course_id), field_object.field_name)
from __future__ import absolute_import
cache_key = self._cache_key_for_kvs_key(kvs_key) if cache_key not in self._cache: raise KeyError(kvs_key.field_name) field_object = self._cache[cache_key] return json.loads(field_object.value)
N_REGIONS = 25
OrderedObjectAdmin.ordering = ['bool'] check_results_order()
if batch is None or (isinstance(batch, tuple) and len(batch) == 0): return True
if request.user.is_staff: return True user = get_object_or_404(User, username__iexact=url_username) if field_name in visible_fields(user.profile, user): return True raise Http404()
self._add_prerequisite_course() self.init_course_access() self._verify_unfulfilled_milestone_response()
module = CapaFactory.create() self.assertEqual(module.get_score()['score'], 0) other_module = CapaFactory.create(correct=True) self.assertEqual(other_module.get_score()['score'], 1)
return LoncapaProblem(xml, id='1', seed=seed, capa_system=capa_system or test_capa_system(), capa_module=mock_capa_module())
pipeline = Pipeline([ ('vect', TfidfVectorizer(min_df=3, max_df=0.95)), ('clf', LinearSVC(C=1000)), ])
from __future__ import absolute_import import os import time import logging
self.body.append(elem)
if subcommand in no_settings_commands: settings.configure()
usage_ids = set() for descriptor in descriptors: usage_ids.add(descriptor.scope_ids.usage_id) for aside_type in aside_types: usage_ids.add(AsideUsageKeyV1(descriptor.scope_ids.usage_id, aside_type)) return usage_ids
expected = Float64Index(arr) a = np.zeros(5, dtype='float64') result = fidx - a tm.assert_index_equal(result, expected)
step = str(Decimal('1') / 10 ** self.decimal_places).lower()
if sl_: tmpret = _get_template_texts(source_list=sl_, template=template, defaults=defaults, context=context) if not tmpret['result']: return tmpret text = tmpret['data']
metadata_to_inherit = self.cached_metadata.get(unicode(non_draft_loc), {}) inherit_metadata(module, metadata_to_inherit)
if i == 0: layer_below = self.dbm.visible_layer else: layer_below = self.dbm.hidden_layers[i-1] state_below = layer_to_state[layer_below] state_below = layer_below.upward_state(state_below)
con = _engine_builder(con) if not _is_sqlalchemy_connectable(con): raise NotImplementedError("read_sql_table only supported for " "SQLAlchemy connectable.") import sqlalchemy from sqlalchemy.schema import MetaData meta = MetaData(con, schema=schema) try: meta.reflect(only=[table_name], views=True) except sqlalchemy.exc.InvalidRequestError: raise ValueError("Table %s not found" % table_name) pandas_sql = SQLDatabase(con, meta=meta) table = pandas_sql.read_table( table_name, index_col=index_col, coerce_float=coerce_float, parse_dates=parse_dates, columns=columns, chunksize=chunksize) if table is not None: return table else: raise ValueError("Table %s not found" % table_name, con)
X = np.arange(200, dtype=np.float64).reshape(10, -1) X[2, :] = np.nan y = np.repeat([0, 1], X.shape[0] / 2) train_test_split(X, y, test_size=0.2, random_state=42)
return role
if self.opts.get('async', False): async_pub = self.async(self.opts['fun'], low, user=user) log.warning('Running in async mode. Results of this execution may ' 'be collected by attaching to the master event bus or ' 'by examing the master job cache, if configured. ' 'This execution is running under tag {tag}'.format(**async_pub))
gsn = GSN.new( layer_sizes=[ds.X.shape[1], 1000, ds.y.shape[1]], activation_funcs=["sigmoid", "tanh", rescaled_softmax], pre_corruptors=[GaussianCorruptor(0.5)] * 3, post_corruptors=[SaltPepperCorruptor(.3), None, SmoothOneHotCorruptor(.5)], layer_samplers=[BinomialSampler(), None, MultinomialSampler()], tied=False )
return aes_decrypt(base64.urlsafe_b64decode(encoded_data), key)
K = squareform(K) np.fill_diagonal(K, 1)
input_problem_answer(step, problem_type, correctness)
FEATURES['LICENSING'] = True
ravel = lambda i: i.ravel() if isinstance(i, np.ndarray) else i indexer = tuple(map(ravel, indexer))
def test_to_csv_engine_kw_deprecation(self): with tm.assert_produces_warning(FutureWarning): df = DataFrame({'col1': [1], 'col2': ['a'], 'col3': [10.1]}) df.to_csv(engine='python')
solution_rectangles = rectangles[aid].split(';') for solution_rectangle in solution_rectangles: sr_coords = re.match( r'[\(\[]([0-9]+),([0-9]+)[\)\]]-[\(\[]([0-9]+),([0-9]+)[\)\]]', solution_rectangle.strip().replace(' ', '')) if not sr_coords: msg = _('Error in problem specification! Cannot parse rectangle in {sr_coords}').format( sr_coords=etree.tostring(self.ielements[aid], pretty_print=True) ) raise Exception('[capamodule.capa.responsetypes.imageinput] ' + msg)
kwargs.pop('max_length', None) kwargs.pop('choices', None) help_text = kwargs.pop( 'help_text', _("The ISO 639-1 language code for this language."), ) super(LanguageField, self).__init__( max_length=16, choices=settings.ALL_LANGUAGES, help_text=help_text, *args, **kwargs )
self.assertChildren(block, self.children_for_user[user])
if not label and hint_node is not None: label = hint_node.get('label', None) if label is None: if correct: label = _(u'Correct') else: label = _(u'Incorrect')
if encrypted is not False: encrypted = postgres._DEFAULT_PASSWORDS_ENCRYPTION password = postgres._maybe_encrypt_password(name, password, encrypted=encrypted) db_args = { 'maintenance_db': maintenance_db, 'runas': user, 'host': db_host, 'user': db_user, 'port': db_port, 'password': db_password, }
operation = migrations.RunSQL(migrations.RunSQL.noop, migrations.RunSQL.noop) with connection.schema_editor() as editor: operation.database_forwards("test_runsql", editor, None, None) operation.database_backwards("test_runsql", editor, None, None)
regr_1 = DecisionTreeRegressor(max_depth=4)
download_url = xqueue_body.get('url') if download_url is None: rate_limiter.tick_bad_request_counter(request) log.warning(u"No download URL provided for example certificate with uuid '%s'.", uuid) return JsonResponseBadRequest( "Parameter 'download_url' is required for successfully generated certificates." ) else: cert.update_status(ExampleCertificate.STATUS_SUCCESS, download_url=download_url) log.info("Successfully updated example certificate with uuid '%s'.", uuid)
self.code = None answer = None try: answer = xml.xpath('//*[@id=$id]//answer', id=xml.get('id'))[0] except IndexError:
import logging
padding_len = AES.block_size - len(input_str) % AES.block_size return input_str + padding_len * chr(padding_len)
self.check_result('label', 'loc', 'c', 'ix', 'c', typs=['labels'], axes=0) self.check_result('label', 'loc', 'null', 'ix', 'null', typs=['mixed'], axes=0) self.check_result('label', 'loc', 8, 'ix', 8, typs=['mixed'], axes=0) self.check_result('label', 'loc', Timestamp('20130102'), 'ix', 1, typs=['ts'], axes=0) self.check_result('label', 'loc', 'c', 'ix', 'c', typs=['empty'], fails=KeyError)
if dtype.kind == result.dtype.kind: if (result.dtype.itemsize <= dtype.itemsize and np.prod(result.shape)): return result
self.assert_categorical_equal(cat, cat.shift(0))
self.assert_requirement_status(0.70, self.EXPIRED_DUE_DATE, 'failed')
node = _get_node(instance_id=vm_['instance_id']) ret.update(node)
clone = self._clone() if lookups == (None,): clone._prefetch_related_lookups = [] else: clone._prefetch_related_lookups.extend(lookups) return clone
rng = np.random.RandomState(0) n, p = 100, 5 X = rng.randn(n, p) * .1 X[:10] += np.array([3, 4, 5, 1, 2]) pca = PCA(n_components='mle', svd_solver='full').fit(X) assert_equal(pca.n_components, 'mle') assert_equal(pca.n_components_, 1)
self.assertNotIn('course', course_xml.attrib) self.assertNotIn('org', course_xml.attrib)
take_2d_axis1_%(name)s_%(dest)s_memview(values, indexer, out, fill_value=fill_value) return
return self.construct_scalar(node)
self.enrollment.can_refund = True self.assertTrue(self.enrollment.refundable())
'pidfile': '/var/run/salt-api.pid', 'logfile': '/var/log/salt/api', 'rest_timeout': 300,
names = sorted(str_repl, key=len, reverse=True)
self.set_many({kvs_key: value})
X, y = make_blobs(centers=n_centers, random_state=0, cluster_std=20) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0) n_centers = len(np.unique(y_train))
if suffix == '': mod = imp.load_module( '{0}.{1}.{2}.{3}'.format( self.loaded_base_name, self.mod_type_check(fpath), self.tag, name ), None, fpath, desc) if not self.initial_load: self._reload_submodules(mod) else: with salt.utils.fopen(fpath, desc[1]) as fn_: mod = imp.load_module( '{0}.{1}.{2}.{3}'.format( self.loaded_base_name, self.mod_type_check(fpath), self.tag, name ), fn_, fpath, desc)
if not objects_differ: for layer in state_data['schedule']['schedule_layers']: resource_layer = None for resource_layer in resource_object['schedule']['schedule_layers']: found = False if layer['name'] == resource_layer['name']: found = True break if not found: objects_differ = 'layer {0} missing'.format(layer['name']) break layer['id'] = resource_layer['id'] for k, v in layer.items(): if k == 'users': continue if k == 'start': continue if v != resource_layer[k]: objects_differ = 'layer {0} key {1} {2} != {3}'.format(layer['name'], k, v, resource_layer[k]) break if objects_differ: break if len(layer['users']) != len(resource_layer['users']): objects_differ = 'num users in layer {0} {1} != {2}'.format(layer['name'], len(layer['users']), len(resource_layer['users'])) break
enetcv_constrained = ElasticNetCV(n_alphas=3, eps=1e-1, max_iter=max_iter, cv=2, positive=True, n_jobs=1) enetcv_constrained.fit(X, y) assert_true(min(enetcv_constrained.coef_) >= 0)
return self(kind='kde', **kwds)
restart_django_servers()
version = Database.version_info if (version < (1, 2, 1) or ( version[:3] == (1, 2, 1) and (len(version) < 5 or version[3] != 'final' or version[4] < 2))): from django.core.exceptions import ImproperlyConfigured raise ImproperlyConfigured("MySQLdb-1.2.1p2 or newer is required; you have %s" % Database.__version__)
self.assertEqual(response.status_code, 403)
self.assertEqual(response.status_code, status_code) self.assertEqual(json.loads(response.content), {"error": error}) self.assertNotIn("partial_pipeline", self.client.session)
from __future__ import absolute_import import os import sys import copy import site import fnmatch import logging import datetime import traceback import re
self.assertEqual(req_status[1]["status"], None) self.assertEqual(req_status[1]["order"], 1)
self._assert_course_verification_status(VERIFY_STATUS_APPROVED) response2 = self.client.get(self.dashboard_url) self.assertContains(response2, attempt2.expiration_datetime.strftime("%m/%d/%Y")) self.assertEqual(response2.content.count(attempt2.expiration_datetime.strftime("%m/%d/%Y")), 2)
for system in ['lms', 'cms']: sh(django_cmd(system, DEFAULT_SETTINGS, 'compilejsi18n'))
import ioflo.base.deeding
if isinstance(index, slice): if any(elem is not None or not isinstance(elem, int) for elem in \ [index.start, index.step, index.stop]): raise TypeError("slice elements must be int or None--symbolic indexing not supported yet") return tuple_variable(self.component_variables[index]) if not isinstance(index, int): raise TypeError("index must be int or slice--symbolic indexing not supported yet") return self.component_variables[index]
interface = interface.split('\\') interface = ''.join(interface)
if mask is not None: m = ~mask.ravel().astype(bool, copy=False) if not m.all(): rows = rows[m]
algorithm, variety, raw_params, salt, data = bits version = 0x10
unsupported_functions = { 'Area', 'AsGeoJSON', 'AsGML', 'AsKML', 'AsSVG', 'BoundingCircle', 'Centroid', 'Difference', 'Distance', 'Envelope', 'ForceRHR', 'GeoHash', 'Intersection', 'IsValid', 'Length', 'MakeValid', 'MemSize', 'NumGeometries', 'NumPoints', 'Perimeter', 'PointOnSurface', 'Reverse', 'Scale', 'SnapToGrid', 'SymDifference', 'Transform', 'Translate', 'Union', }
mock_response = mock.Mock() mock_citc.return_value = mock_response course_id = u'course-v1:OrgFoo+CN199+CR-FALL01' tasks.update_ccxcon.delay(course_id) mock_citc.assert_called_once_with(CourseKey.from_string(course_id))
#--------------------------------------------------------------- epsx = Anorm * xnorm * eps epsr = Anorm * xnorm * rtol #Test for singular Hk (hence singular A) t1 = constantX(1) + relrnorm t2 = constantX(1) + relArnorml
try: pattern = to_replace.pattern except AttributeError: pattern = to_replace
xsupi = _cg(fhess_p, fgrad, maxiter=maxinner, tol=termcond)
raise Exception( "invalid combinate of [%s] on appending data [%s] vs " "current table [%s]" % (c, sv, ov))
face = downsampled_face i_h, i_w = face.shape p = 8 expected_n_patches = ((i_h - p + 1), (i_w - p + 1)) patches = extract_patches(face, patch_shape=p) assert_true(patches.shape == (expected_n_patches[0], expected_n_patches[1], p, p))
dim = row[0] srid = row[1] field_params = {} if srid != 4326: field_params['srid'] = srid if (isinstance(dim, six.string_types) and 'Z' in dim) or dim == 3: field_params['dim'] = 3
if ((if_none_match and (etag in etags or '*' in etags and etag)) and (not if_modified_since or (last_modified and if_modified_since and last_modified <= if_modified_since))): if request.method in ('GET', 'HEAD'): return _not_modified(request, response) else: return _precondition_failed(request) elif (if_match and ( (not etag and '*' in etags) or (etag and etag not in etags) or (last_modified and if_unmodified_since and last_modified > if_unmodified_since) )): return _precondition_failed(request) elif (not if_none_match and request.method in ('GET', 'HEAD') and last_modified and if_modified_since and last_modified <= if_modified_since): return _not_modified(request, response) elif (not if_match and last_modified and if_unmodified_since and last_modified > if_unmodified_since): return _precondition_failed(request)
pygments_style = 'sphinx'
self.course.tabs.append(CourseTab.load("notes")) self.course.advanced_modules = ["notes"]
try: import pythoncom import wmi import win32net import win32api import win32con import pywintypes from ctypes import windll HAS_WIN32NET_MODS = True except ImportError: HAS_WIN32NET_MODS = False
for item in mask.split(','): if item not in _MASK_TYPES: return 'Invalid mask type: {0}' . format(item)
return router
if not value: return '' try: return timeuntil(value, arg) except (ValueError, TypeError): return ''
course_id = CourseKeyField(max_length=255, db_index=True)
iris = datasets.load_iris()
actual = pd.read_excel(os.path.join(self.dirpath, 'testskiprows' + self.ext), 'skiprows_list', skiprows=[0, 2]) expected = DataFrame([[1, 2.5, pd.Timestamp('2015-01-01'), True], [2, 3.5, pd.Timestamp('2015-01-02'), False], [3, 4.5, pd.Timestamp('2015-01-03'), False], [4, 5.5, pd.Timestamp('2015-01-04'), True]], columns=['a', 'b', 'c', 'd']) tm.assert_frame_equal(actual, expected)
default_include = overrides.get( 'default_include', defaults['default_include'] ) overrides.update( salt.config.include_config(default_include, path, verbose=False) ) include = overrides.get('include', []) overrides.update( salt.config.include_config(include, path, verbose=True) )
return ''
if settings.COMPREHENSIVE_THEME_DIR: enable_comprehensive_theme(settings.COMPREHENSIVE_THEME_DIR)
self.assertRaises(ValueError, s.interpolate, method='linear', limit_direction='abc')
if os.path.isdir(pubfn_pend): log.info( 'New public key {id} is a directory'.format(**load) ) eload = {'result': False, 'id': load['id'], 'pub': load['pub']} self.event.fire_event(eload, salt.utils.event.tagify(prefix='auth')) return {'enc': 'clear', 'load': {'ret': False}}
context_template = [ self.engine.from_string("Wrong"), self.engine.from_string("1{% block first %}_{% endblock %}3{% block second %}_{% endblock %}"), ] output = self.engine.render_to_string('inheritance25', {'context_template': context_template}) self.assertEqual(output, '1234')
VISIBLE_CONTENT = [ ['class=&#34;problems-wrapper'], ['Some HTML for group 1'] ]
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.ext.six import string_types
introspect.__salt__ = {}
return self.make_call(reverse('topics_list'), expected_status, 'get', data, **kwargs)
if isinstance(coeff, str): coeff = float(coeff) assert isinstance(coeff, float) or hasattr(coeff, 'dtype') return coeff * T.sqr(self.W).sum()
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import CommandExecutionError, MinionError
response = self.session.post( url, data=self._encode_post_dict(details), headers=self.headers, )
if len(children) == 3: return LatexRendered( children[1].latex, parens=children[0].latex, tall=children[1].tall ) else: return children[0]
with remove_ccx(course_key) as (course_key, restore): return restore(self._modulestore.fill_in_run(course_key))
return RestrictedCourse.message_url_path(course_key, access_point)
#for dt in chain(*(np.sctypes[x] for x in ['uint', 'int', 'float'])): for dt in [np.float32, np.float64]: yield self.check_binop_typecasting, engine, parser, op, dt
if category == 'problem': for advanced_problem_type in ADVANCED_PROBLEM_TYPES: component = advanced_problem_type['component'] boilerplate_name = advanced_problem_type['boilerplate_name'] try: component_display_name = xblock_type_display_name(component) except PluginMissingError: log.warning('Unable to load xblock type %s to read display_name', component, exc_info=True) else: templates_for_category.append( create_template_dict(component_display_name, component, boilerplate_name, 'advanced') ) categories.add(component)
assertRaisesRegexp(ValueError, 'cannot insert', df.insert, 2, 'new_col', 4.) df.insert(2, 'new_col', 4., allow_duplicates=True) expected = DataFrame([[1, 1, 4., 5., 'bah', 3], [1, 2, 4., 5., 'bah', 3], [2, 3, 4., 5., 'bah', 3]], columns=['foo', 'foo', 'new_col', 'new_col', 'string', 'foo2']) check(df, expected)
from salt.states import mysql_user import salt
logger.info("Previous is {0}".format(self.prev)) cur = algorithm.scale_step if latest >= self.prev: logger.info("Looks like using {0} " "isn't working out so great for us.".format(cur)) cur *= self.scale if cur < self.giveup_after: logger.info("Guess we just have to give up.") self.continue_learning = False cur = self.giveup_after logger.info("Let's see how {0} does.".format(cur)) logger.info("Reloading saved params from last call") for p, v in safe_zip(model.get_params(), self.stored_values): p.set_value(v) latest = self.prev elif latest <= self.prev and self.scale_up != 1.: logger.info("Looks like we're making progress " "on the validation set, let's try speeding up") cur *= self.scale_up if cur > self.max_scale: cur = self.max_scale logger.info("New scale is {0}".format(cur)) algorithm.scale_step = cur self.monitor_channel.set_value(np.cast[config.floatX](cur)) self.prev = latest self.stored_values = [param.get_value() for param in model.get_params()]
rng = np.random.RandomState(0) X = rng.randn(10, 2) connectivity = kneighbors_graph(X, 5, include_self=False)
self.assertRaises(ValueError, df.dropna, axis=3)
qs = City.objects.all() with self.assertRaises(TypeError): qs.gml(field_name='name') ptown1 = City.objects.gml(field_name='point', precision=9).get(name='Pueblo') ptown2 = City.objects.gml(precision=9).get(name='Pueblo')
g.precisions_init = precisions_bad_shape[covar_type] assert_raise_message(ValueError, "The parameter '%s precision' should have " "the shape of" % covar_type, g.fit, X)
return security_group
if not change_password(username, password, uid): log.warning('unable to set user password') delete_user(username, uid) return False
EmptyPromise(lambda: self.q(css='.bookmark-button').visible, "Bookmark button visible").fulfill() return True
obj.delete() os.unlink(full_name)
chunk = self.data[self.cursor:(self.cursor + chunk_size)] self.cursor += chunk_size return chunk
for child in children: self._block_relations[child].parents.remove(usage_key)
self.assertEqual(2, self.model.objects.count()) with override_settings(SESSION_ENGINE=self.session_engine): management.call_command('clearsessions') self.assertEqual(1, self.model.objects.count())
from salt.modules import win_groupadd
self._check_is_fitted() X = _check_X(X, None, self.means_.shape[1]) _, _, log_resp = self._estimate_log_prob_resp(X) return np.exp(log_resp)
selector = RFE(estimator, step=5) sel = selector.fit(X, y) assert_equal(sel.support_.sum(), n_features // 2)
with mock.patch.dict('django.conf.settings.FEATURES', {'ENABLE_CREATOR_GROUP': True}): self.assert_created_course()
enable_course_home_improvements = BooleanField( default=False, verbose_name=_("Enable course home page improvements.") )
phi_aj = phi(a_j) derphi_aj = derphi(a_j)
parsed_referer = urlparse(referer) if parsed_referer.netloc in ['', domain] and parsed_referer.path == uri: return True
return generic.NDFrame._update_inplace(self, result, **kwargs)
print "Starting export" file_system = OSFS(root_dir) initial_course.runtime.export_fs = file_system.makeopendir(course_dir) root = lxml.etree.Element('root')
_make_books(10, base_date=datetime.date(2011, 12, 25)) res = self.client.get('/dates/books/2011/dec/') self.assertEqual(list(res.context['date_list']), list(sorted(res.context['date_list'])))
import salt.ext.six as six from salt.ext.six.moves import range
if settings.USE_TZ and value is not None and timezone.is_naive(value): current_timezone = timezone.get_current_timezone() try: return timezone.make_aware(value, current_timezone) except Exception: message = _( '%(datetime)s couldn\'t be interpreted ' 'in time zone %(current_timezone)s; it ' 'may be ambiguous or it may not exist.' ) params = {'datetime': value, 'current_timezone': current_timezone} six.reraise(ValidationError, ValidationError( message, code='ambiguous_timezone', params=params, ), sys.exc_info()[2]) return value
df = DataFrame(np.random.random_sample((20, 5)), columns=list('ABCDE')) expected = df
DataFrame._metadata = _metadata DataFrame.__finalize__ = _finalize
for i in xrange(n_splits): for j in xrange(n_splits): rows = slice(block_size * i, block_size * (i + 1)) cols = slice(block_size * j, block_size * (j + 1)) beta_ += np.sum(np.dot(X2.T[rows], X2[:, cols])) delta_ += np.sum(np.dot(X.T[rows], X[:, cols]) ** 2) rows = slice(block_size * i, block_size * (i + 1)) beta_ += np.sum(np.dot(X2.T[rows], X2[:, block_size * n_splits:])) delta_ += np.sum( np.dot(X.T[rows], X[:, block_size * n_splits:]) ** 2) for j in xrange(n_splits): cols = slice(block_size * j, block_size * (j + 1)) beta_ += np.sum(np.dot(X2.T[block_size * n_splits:], X2[:, cols])) delta_ += np.sum( np.dot(X.T[block_size * n_splits:], X[:, cols]) ** 2) delta_ += np.sum(np.dot(X.T[block_size * n_splits:], X[:, block_size * n_splits:]) ** 2) delta_ /= n_samples ** 2 beta_ += np.sum(np.dot(X2.T[block_size * n_splits:], X2[:, block_size * n_splits:])) beta = 1. / (n_features * n_samples) * (beta_ / n_samples - delta_) delta = delta_ - 2. * mu * emp_cov_trace.sum() + n_features * mu ** 2 delta /= n_features beta = min(beta, delta) shrinkage = 0 if beta == 0 else beta / delta return shrinkage
rng = np.random.RandomState(0) X = rng.random_sample((5, 4)) for kernel in (linear_kernel, polynomial_kernel, rbf_kernel, laplacian_kernel, sigmoid_kernel, cosine_similarity): K = kernel(X, X) assert_array_almost_equal(K, K.T, 15)
has_partial_answers = tree.xpath('responseparam[@partial_answers]') if has_partial_answers: partial_answers = has_partial_answers[0].get('partial_answers').split(',') for index, word in enumerate(partial_answers): partial_answers[index] = word.strip() partial_answers[index] = self.get_staff_ans(partial_answers[index])
zmq_version()
for block_key in block_structure.topological_traversal(): xblock = block_structure.get_xblock(block_key) parent_keys = block_structure.get_parents(block_key) merged_parent_access_list = [ block_structure.get_transformer_block_field(parent_key, cls, 'merged_group_access') for parent_key in parent_keys ] merged_group_access = _MergedGroupAccess(user_partitions, xblock, merged_parent_access_list) block_structure.set_transformer_block_field(block_key, cls, 'merged_group_access', merged_group_access)
ref = fromstr('MULTIPOLYGON(((12.4 44,12.5 44,12.5 43.9,12.4 43.9,12.4 44)))') self.assertTrue(ref.equals_exact(Country.objects.snap_to_grid(0.1).get(name='San Marino').snap_to_grid, tol))
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
delta1 = TT.constant(numpy.asarray(0.2, dtype=theano.config.floatX)) delta2 = TT.constant(numpy.asarray(0.1, dtype=theano.config.floatX)) phi_rec = phi0 a_rec = zero
mask = img.astype(bool)
result = self.data.pivot_table(values='D', index=['A', 'B'], columns='C', margins=True, aggfunc=np.mean) _check_output(result, 'D')
total_cost = 0 result = cls.objects.filter(course_id=course_key, status=status).aggregate( total=Sum( F('qty') * F('unit_cost'), output_field=models.DecimalField(decimal_places=2, max_digits=30) ) ) if result['total'] is not None: total_cost = result['total'] return total_cost
if not organizations_enabled(): return None from organizations import api as organizations_api return organizations_api.add_organization(organization_data=organization_data)
gecos_field = data.pw_gecos.split(',', 3) while len(gecos_field) < 4: gecos_field.append('') ret['fullname'] = gecos_field[0] ret['roomnumber'] = gecos_field[1] ret['workphone'] = gecos_field[2] ret['homephone'] = gecos_field[3]
self.p1.undergroundbar bar.place.name = 'foo' bar.place = None bar.save() self.p1.delete()
itemsize = max_len_string_array(com._ensure_object(column.values)) return chr(max(itemsize, 1))
subject = ''.join(subject.splitlines()) body = loader.render_to_string(email_template_name, context)
c(window=2) c(window=2, min_periods=1) c(window=2, min_periods=1, center=True) c(window=2, min_periods=1, center=False)
if self._build_requests_plus_30_for_minus_30(): if self._user_requested_plus_30_skip(): self.event[u'requested_skip_interval'] = -30
result = df1.loc[(slice(None), slice(None), slice('20130702', '20130709')), :] expected = df1.iloc[[1, 2, 6, 7, 12]] assert_frame_equal(result, expected)
join_idx = self.frame.index.join(other.index) diff_a = self.frame.index.difference(join_idx) diff_b = other.index.difference(join_idx) diff_a_vals = af.reindex(diff_a).values
return 'Looks like jail {0} has not been created'.format(name)
index_path = reverse('admin:index', current_app=self.name) return HttpResponseRedirect(index_path)
self.assertEqual(userreport['city'], "None") self.assertEqual(userreport['country'], "")
response = self.client.get(url) self.assertEquals(response.status_code, 403)
filepath = filename
with open(self.good_tar) as gtar: args = {"name": self.good_tar, "course-data": [gtar]} resp = self.client.post(self.url, args) self.assertEquals(resp.status_code, 200)
self.generated_operations = {}
val = StataMissingValue(struct.unpack('<f', b'\x00\x00\x00\x7f')[0]) self.assertTrue(val.string == '.') val = StataMissingValue(struct.unpack('<f', b'\x00\xd0\x00\x7f')[0]) self.assertTrue(val.string == '.z')
return get_branch_setting()
scsi_spec.device.sharedBus = vim.vm.device.VirtualSCSIController.Sharing.physicalSharing
signal.signal(signal.SIGINT, self.signal_handler)
if len(panels) == 0: return None elif len(panels) == 1: return panels[0] elif len(panels) == 2 and panels[0] == panels[1]: return panels[0] d = dict() minor, major, items = set(), set(), set() for panel in panels: items.update(panel.items) major.update(panel.major_axis) minor.update(panel.minor_axis) values = panel.values for item, item_index in panel.items.indexMap.items(): for minor_i, minor_index in panel.minor_axis.indexMap.items(): for major_i, major_index in panel.major_axis.indexMap.items(): try: d[(minor_i, major_i, item)] = values[item_index, major_index, minor_index] except: pass minor = sorted(list(minor)) major = sorted(list(major)) items = sorted(list(items)) data = np.dstack([np.asarray([np.asarray([d.get((minor_i, major_i, item), np.nan) for item in items]) for major_i in major]).transpose() for minor_i in minor]) return Panel(data, items, major, minor)
epoch_num = 15 termination_criterion = EpochCounter(epoch_num)
npz_data = serial.load(path + "dictionaries.npz") self._vocabulary = dict((word.lower(), word_index) for word_index, word in enumerate(npz_data['unique_' + data_mode]))
HTTPServer.shutdown(self)
unit_url = reverse_usage_url( 'container_handler', course.location.course_key.make_usage_key(unit.location.block_type, unit.location.name) ) usage_dict = {'label': u"{} / {}".format(unit.display_name, item.display_name), 'url': unit_url} if scheme_name == RANDOM_SCHEME: validation_summary = item.general_validation_message() usage_dict.update({'validation': validation_summary.to_json() if validation_summary else None}) usage_info[group_id].append(usage_dict) return usage_info
Eaten.objects.create(meal='m') q = Eaten.objects.none() with self.assertNumQueries(0): self.assertQuerysetEqual(q.all(), []) self.assertQuerysetEqual(q.filter(meal='m'), []) self.assertQuerysetEqual(q.exclude(meal='m'), []) self.assertQuerysetEqual(q.complex_filter({'pk': 1}), []) self.assertQuerysetEqual(q.select_related('food'), []) self.assertQuerysetEqual(q.annotate(Count('food')), []) self.assertQuerysetEqual(q.order_by('meal', 'food'), []) self.assertQuerysetEqual(q.distinct(), []) self.assertQuerysetEqual( q.extra(select={'foo': "1"}), [] ) q.query.low_mark = 1 with self.assertRaisesMessage(AssertionError, 'Cannot change a query once a slice has been taken'): q.extra(select={'foo': "1"}) self.assertQuerysetEqual(q.reverse(), []) self.assertQuerysetEqual(q.defer('meal'), []) self.assertQuerysetEqual(q.only('meal'), [])
fstr = '{0}.prep_jid'.format(self.opts['master_job_cache']) try: jid = self.mminion.returners[fstr](nocache=nocache, passed_jid=passed_jid) except (KeyError, TypeError): msg = ( 'Failed to allocate a jid. The requested returner \'{0}\' ' 'could not be loaded.'.format(fstr.split('.')[0]) ) log.error(msg) return {'error': msg} return jid
from_hex = BinConstructor('GEOSGeomFromHEX_buf') from_wkb = BinConstructor('GEOSGeomFromWKB_buf') from_wkt = GeomOutput('GEOSGeomFromWKT', [c_char_p])
if salt.utils.which_bin(['dmidecode', 'smbios']) is not None and not salt.utils.is_smartos(): grains = { 'biosversion': __salt__['smbios.get']('bios-version'), 'productname': __salt__['smbios.get']('system-product-name'), 'manufacturer': __salt__['smbios.get']('system-manufacturer'), 'biosreleasedate': __salt__['smbios.get']('bios-release-date'), 'uuid': __salt__['smbios.get']('system-uuid') } grains = dict([(key, val) for key, val in grains.items() if val is not None]) uuid = __salt__['smbios.get']('system-uuid') if uuid is not None: grains['uuid'] = uuid.lower() for serial in ('system-serial-number', 'chassis-serial-number', 'baseboard-serial-number'): serial = __salt__['smbios.get'](serial) if serial is not None: grains['serialnumber'] = serial break elif osdata['kernel'] == 'FreeBSD': kenv = salt.utils.which('kenv') if kenv: fbsd_hwdata = { 'biosversion': 'smbios.bios.version', 'manufacturer': 'smbios.system.maker', 'serialnumber': 'smbios.system.serial', 'productname': 'smbios.system.product', 'biosreleasedate': 'smbios.bios.reldate', 'uuid': 'smbios.system.uuid', } for key, val in six.iteritems(fbsd_hwdata): grains[key] = __salt__['cmd.run']('{0} {1}'.format(kenv, val)) elif osdata['kernel'] == 'OpenBSD': sysctl = salt.utils.which('sysctl') hwdata = {'biosversion': 'hw.version', 'manufacturer': 'hw.vendor', 'productname': 'hw.product', 'serialnumber': 'hw.serialno', 'uuid': 'hw.uuid'} for key, oid in six.iteritems(hwdata): value = __salt__['cmd.run']('{0} -n {1}'.format(sysctl, oid)) if not value.endswith(' value is not available'): grains[key] = value elif osdata['kernel'] == 'NetBSD': sysctl = salt.utils.which('sysctl') nbsd_hwdata = { 'biosversion': 'machdep.dmi.board-version', 'manufacturer': 'machdep.dmi.system-vendor', 'serialnumber': 'machdep.dmi.system-serial', 'productname': 'machdep.dmi.system-product', 'biosreleasedate': 'machdep.dmi.bios-date', 'uuid': 'machdep.dmi.system-uuid', } for key, oid in six.iteritems(nbsd_hwdata): result = __salt__['cmd.run_all']('{0} -n {1}'.format(sysctl, oid)) if result['retcode'] == 0: grains[key] = result['stdout'] elif osdata['kernel'] == 'Darwin': grains['manufacturer'] = 'Apple Inc.' sysctl = salt.utils.which('sysctl') hwdata = {'productname': 'hw.model'} for key, oid in hwdata.items(): value = __salt__['cmd.run']('{0} -b {1}'.format(sysctl, oid)) if not value.endswith(' is invalid'): grains[key] = value
try: m = missing.clean_fill_method(method) except: m = None
qs = Author.objects.annotate(Count('book')).filter( Q(name='Peter Norvig') | Q(age=F('book__count') + 33) ).order_by('name') self.assertQuerysetEqual( qs, ['Adrian Holovaty', 'Peter Norvig'], lambda b: b.name )
orphan_chapter = self.store.create_item(self.user.id, course.id, 'chapter', "OrphanChapter") self.store.publish(orphan_chapter.location, self.user.id)
response = self.client.get('/login_protected_method_view/') self.assertRedirects(response, '/accounts/login/?next=/login_protected_method_view/')
lexsorted_mi = MultiIndex.from_tuples( [('a', ''), ('b1', 'c1'), ('b2', 'c2')], names=['b', 'c']) lexsorted_df = DataFrame([[1, 3, 4]], columns=lexsorted_mi) self.assertTrue(lexsorted_df.columns.is_lexsorted())
upload_finish_time = datetime.utcnow().replace(microsecond=0, second=0)
values = Series([u('om'), NA, u('foo_nom'), u('nom'), u('bar_foo'), NA, u('foo')])
private_ips = [] public_ips = []
result = ols(y=y, x=x, window_type='rolling', window=10) print(result.beta)
return self.date_list_period
from __future__ import absolute_import
def __init__(self, location, content_type): self.location = location self.content_type = content_type
import salt.output from salt.ext.six import string_types from salt.utils import get_colors import salt.utils.locales
with extend_sys_path(dirname): import_module('test_only_new_module') filenames = set(autoreload.gen_filenames(only_new=True)) self.assertEqual(filenames, {npath(filename)})
result = store.select('wp', [Term( 'major_axis', '<', "20000108"), Term("minor_axis=['A', 'B']")]) expected = wp.truncate(after='20000108').reindex(minor=['A', 'B']) tm.assert_panel_equal(result, expected)
buggy = Buggy() buggy.a = 2 assert_raises(RuntimeError, clone, buggy)
block_structure = block_structure_cls(root_block_usage_key=0)
inp = [[1], [0], [2]] indicator_mat = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]]) mlb = MultiLabelBinarizer() assert_array_equal(mlb.fit_transform(inp), indicator_mat) assert_array_equal(mlb.inverse_transform(indicator_mat), inp)
return self.kernel.diag(X) ** self.exponent
from pandas.compat import range, zip from pandas import compat import itertools
self.assertEqual(result_dict['value'], "pong")
if not __opts__['test'] and changes is True: response = __salt__[esxi_cmd]('set_coredump_network_config', dump_ip=dump_ip, host_vnic=host_vnic, dump_port=dump_port).get(host) if response.get('success') is False: msg = response.get('stderr') if not msg: msg = response.get('stdout') ret['comment'] = 'Error: {0}'.format(msg) return ret
set_user_preference(self.user, ACCOUNT_VISIBILITY_PREF_KEY, preference_visibility) self.create_mock_profile(self.user) response = self.send_get(client)
self._is_valid_integer(key, axis)
for key, val in six.iteritems(repo_conf): setattr(self, key, val)
self.selection = Selection( self, where=where, start=start, stop=stop, **kwargs) coords = self.selection.select_coords() if self.selection.filter is not None: for field, op, filt in self.selection.filter.format(): data = self.read_column( field, start=coords.min(), stop=coords.max() + 1) coords = coords[ op(data.iloc[coords - coords.min()], filt).values]
del Person.houses.related_manager_cls._apply_rel_filters del Person.houses.related_manager_cls.get_queryset try: with warnings.catch_warnings(record=True) as warns: warnings.simplefilter('always') list(Person.objects.prefetch_related( Prefetch('houses', queryset=House.objects.filter(name='House 1')) )) finally: del Person.houses.related_manager_cls msg = ( 'The `django.db.models.fields.related_descriptors.ManyRelatedManager` class ' 'must implement a `_apply_rel_filters()` method that accepts a `QuerySet` as ' 'its single argument and returns an appropriately filtered version of it.' )
if 'freq' in kwargs: kwargs['freq'] = None self.values = Index(values, **kwargs)
def __init__(self, foo_param=0): self.foo_param = foo_param def fit(self, X, Y): assert_true(len(X) == len(Y)) return self def predict(self, T): return T.shape[0] predict_proba = predict decision_function = predict transform = predict def score(self, X=None, Y=None): if self.foo_param > 1: score = 1. else: score = 0. return score def get_params(self, deep=False): return {'foo_param': self.foo_param} def set_params(self, **params): self.foo_param = params['foo_param'] return self
schema = query_reply['schema']
self.assertContains(response, "<input", count=19) self.assertContains(response, "<select", count=4)
X = X.reshape(-1, 2 * numpy.prod(self.original_image_shape))
from scipy.stats import t result = [2 * t.sf(a, b) for a, b in zip(np.fabs(self._t_stat_raw), self._df_resid_raw)] return np.array(result)
from salt.modules import twilio_notify
models = serializers.sort_dependencies(app_list.items()) for model in models: if model in excluded_models: continue if model._meta.proxy and model._meta.proxy_for_model not in models: warnings.warn( "%s is a proxy model and won't be serialized." % model._meta.label, category=ProxyModelWarning, ) if not model._meta.proxy and router.allow_migrate_model(using, model): if use_base_manager: objects = model._base_manager else: objects = model._default_manager queryset = objects.using(using).order_by(model._meta.pk.name) if primary_keys: queryset = queryset.filter(pk__in=primary_keys) if count_only: yield queryset.order_by().count() else: for obj in queryset.iterator(): yield obj
giturl = "" data_dir = ""
jsonified = { "name": self.TEST_NAME, "description": self.TEST_DESCRIPTION, "parameters": self.TEST_PARAMETERS, "groups": [group.to_json() for group in self.TEST_GROUPS], "version": UserPartition.VERSION, "scheme": self.TEST_SCHEME_NAME, } with self.assertRaisesRegexp(TypeError, "missing value key 'id'"): UserPartition.from_json(jsonified)
return int(self.q(css=self.TOTAL_PAGES_CSS).text[0])
dive = Book.objects.using('other').create(title="Dive into Python", published=datetime.date(2009, 5, 4))
) cls.m1 = ModelWithStringPrimaryKey.objects.create(string_pk=cls.pk) content_type_pk = ContentType.objects.get_for_model(ModelWithStringPrimaryKey).pk user_pk = cls.superuser.pk LogEntry.objects.log_action(user_pk, content_type_pk, cls.pk, cls.pk, 2, change_message='Changed something')
def f(x, name=name, *args): x = self._shallow_copy(x) if isinstance(name, compat.string_types): return getattr(x, name)(*args, **kwargs) return x.apply(name, *args, **kwargs) return self._groupby.apply(f)
exec_cmd = ['prlctl', 'exec', 'macvm', 'uname'] exec_fcn = MagicMock() with patch.dict(parallels.__salt__, {'cmd.run': exec_fcn}): parallels.prlctl('exec', 'macvm uname', runas=runas) exec_fcn.assert_called_once_with(exec_cmd, runas=runas)
self._verify_unit_warning( self.UnitState(is_released=False, publish_state=self.PublishState.NEVER_PUBLISHED, is_locked=True), self.STAFF_ONLY_WARNING )
return self.asobject.values
clause, joins = query._add_q(self, reuse, allow_joins=allow_joins, split_subq=False) query.promote_joins(joins) return clause
self.panel['foo'] = lp2['ItemA'] assert_series_equal(self.panel['foo'].reindex(lp2.index), lp2['ItemA'], check_names=False)
from docutils import nodes, utils from docutils.parsers.rst import roles def ticket_role(name, rawtext, text, lineno, inliner, options=None, content=None): if options is None: options = {} try: num = int(text.replace('#', '')) except ValueError: msg = inliner.reporter.error( "ticket number must be... a number, got '%s'" % text) prb = inliner.problematic(rawtext, rawtext, msg) return [prb], [msg] url_pattern = inliner.document.settings.env.app.config.ticket_url if url_pattern is None: msg = inliner.reporter.warning( "ticket not configured: please configure ticket_url in conf.py") prb = inliner.problematic(rawtext, rawtext, msg) return [prb], [msg] url = url_pattern % num roles.set_classes(options) node = nodes.reference(rawtext, '#' + utils.unescape(text), refuri=url, **options) return [node], [] def setup(app): app.add_config_value('ticket_url', None, 'env') app.add_role('ticket', ticket_role) return {'parallel_read_safe': True}
from __future__ import absolute_import
programs_config = self.create_programs_config(cache_ttl=cache_ttl) self.assertEqual(programs_config.is_cache_enabled, is_cache_enabled)
def convert_extent(self, box, srid): raise NotImplementedError('Aggregate extent not implemented for this spatial backend.')
try: from hashlib import md5 except ImportError: from md5 import md5
df = DataFrame(['foo', 'foo', 'foo', 'barh', 'barh', 'barh'], columns=['A']) _maybe_remove(store, 'df') self.assertRaises(ValueError, store.append, 'df', df, min_itemsize={'foo': 20, 'foobar': 20})
CourseEnrollmentFactory( course_id=self.course.id, user=self.user, mode="verified" )
repo = kwargs.get('repo', '') if not fromrepo and repo: fromrepo = repo
X, y = make_blobs(centers=[[0, 0], [1, 0], [0, 1], [1, 1]], random_state=0, cluster_std=0.1, shuffle=False, n_samples=80) mask = np.ones(X.shape[0], dtype=np.bool) mask[np.where(y == 1)[0][::2]] = 0 mask[np.where(y == 2)[0][::2]] = 0 svm = SVC(kernel='linear') cv = [[mask, ~mask], [~mask, mask]] grid_search = GridSearchCV(svm, param_grid={'C': [1, 10]}, cv=cv) grid_search.fit(X, y) first = grid_search.grid_scores_[0] assert_equal(first.parameters['C'], 1) assert_array_almost_equal(first.cv_validation_scores, [1, 1. / 3.]) assert_almost_equal(first.mean_validation_score, 1 * 1. / 4. + 1. / 3. * 3. / 4.)
__virtualname__ = 'ntp'
self.create_role(role_name, [self.endorser]) serialized = self.serialize(self.make_cs_content(with_endorsement=True)) self.assertEqual(serialized["endorsed_by_label"], expected_label)
if data_home is None: data_home = environ.get('SCIKIT_LEARN_DATA', join('~', 'scikit_learn_data')) data_home = expanduser(data_home) if not exists(data_home): makedirs(data_home) return data_home
#'django.contrib.auth.middleware.AuthenticationMiddleware', 'cache_toolbox.middleware.CacheBackedAuthenticationMiddleware', 'django.contrib.auth.middleware.SessionAuthenticationMiddleware',
folders_list_in_path = folders_list_in_path[:-4]
if salt.utils.compare_versions(ver1=docker.__version__, oper='>=', ver2='0.5.0'): kwargs['insecure_registry'] = insecure_registry ret = client.pull(repo, **kwargs) if ret: image_logs, infos = _parse_image_multilogs_string(ret) if infos and infos.get('Id', None): repotag = repo if tag: repotag = '{0}:{1}'.format(repo, tag) _valid(status, out=image_logs if image_logs else ret, id_=infos['Id'], comment='Image {0} was pulled ({1})'.format( repotag, infos['Id']))
dest_structure['blocks'][new_parent_block_key].fields['children'] = new_children
from __future__ import unicode_literals
fake_data_api.add_course(self.COURSE_ID, course_modes=course_modes) result = api.add_enrollment(self.USERNAME, self.COURSE_ID, mode=mode) self.assertIsNotNone(result) self.assertEquals(result['student'], self.USERNAME) self.assertEquals(result['course']['course_id'], self.COURSE_ID) self.assertEquals(result['mode'], mode) self.assertTrue(result['is_active'])
x = Series(pd.PeriodIndex(['2015-11-01', '2015-12-01'], freq='D')) y = Series(pd.PeriodIndex(['2015-10-01', '2016-01-01'], freq='M')) expected = Series([x[0], x[1], y[0], y[1]], dtype='object') result = concat([x, y], ignore_index=True) tm.assert_series_equal(result, expected) self.assertEqual(result.dtype, 'object')
from pandas.core.series import Series
from __future__ import absolute_import
with override_settings(CREDIT_PROVIDER_SECRET_KEYS={}): response = self.post_credit_request(self.user.username, self.eligibility.course.course_key) self.assertEqual(response.status_code, 400)
timezone = ps_opts['d-i']['time']['zone']['argument'] sls[timezone] = {'timezone': ['system']} if ps_opts['d-i']['tzconfig']['gmt']['argument'] == 'true': sls[timezone]['timezone'].append('utc')
metadata = dict() metadata_file = os.path.join(dataset_path, 'metadata') if not os.path.exists(metadata_file): raise ValueError(dataset_path + ' is not a valid MLComp dataset') with open(metadata_file) as f: for line in f: if ":" in line: key, value = line.split(":", 1) metadata[key.strip()] = value.strip()
if course_overview and not hasattr(course_overview, 'image_set'): CourseOverviewImageSet.create_for_course(course_overview)
from __future__ import absolute_import
@no_oracle def test12a_count(self): "Testing `Count` aggregate on geo-fields." dallas = City.objects.get(name='Dallas')
from salt.utils import dictupdate
s = Series(pd.to_datetime(['1-1-1990', '2-1-1990', '3-1-1990', '4-1-1990', '5-1-1990'])) result = DatetimeIndex(s, freq='MS') expected = DatetimeIndex(['1-1-1990', '2-1-1990', '3-1-1990', '4-1-1990', '5-1-1990'], freq='MS') self.assert_index_equal(result, expected)
self.page.visit() self.page.q(css='.group-toggle').first.click() config.edit() config.groups[2].remove() config.save() self.page.q(css='.group-toggle').first.click() self._assert_fields(config, name="Name", description="Description", groups=["Group A", "Group B"]) self.browser.close() self.browser.switch_to_window(self.browser.window_handles[0])
TEMPLATES=[{ 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, }],
leftside = grouped.agg(aggf) rightside = getattr(self.series, op)(level=level, skipna=skipna) assert_series_equal(leftside, rightside)
if apps is not global_apps: global_apps.get_model('contenttypes', 'ContentType').objects.clear_cache()
TEST_DATA = None
return hashlib.md5(settings.PROFILE_IMAGE_SECRET_KEY + username).hexdigest()
return self.attempts > 0
return self.q(css=self.selector).present
if parse_dates is True or parse_dates is None or parse_dates is False: parse_dates = []
cherrypy._cpreqbody.process_urlencoded(entity) cherrypy.serving.request.unserialized_data = entity.params cherrypy.serving.request.raw_body = ''
with tm.assertRaisesRegexp(ValueError, msg): DataFrame({'A': {'a': 'a', 'b': 'b'}, 'B': ['a', 'b', 'c']})
if len(c1) == 2: tset = (5, 23) else: tset = (5, 23, 8) cs[i] = tset
self.q(css=self._bounded_selector(self.NAME_INPUT_SELECTOR)).results[0].send_keys(Keys.ENTER) self.wait_for_ajax()
if n_repeated > 0: n = n_informative + n_redundant indices = ((n - 1) * generator.rand(n_repeated) + 0.5).astype(np.intp) X[:, n:n + n_repeated] = X[:, indices]
cart = Order.get_cart_for_user(self.user) CertificateItem.add_to_order(cart, self.course_key, self.cost, 'honor') PaidCourseRegistration.add_to_order(self.cart, self.course_key) self.assertTrue(PaidCourseRegistration.contained_in_order(cart, self.course_key))
factory = OptionResponseXMLFactory() factory_args = {'question_text': 'The correct answer is {0}'.format(OPTION_2), 'options': [OPTION_1, OPTION_2], 'correct_option': OPTION_2, 'num_responses': 2} problem_xml = factory.build_xml(**factory_args) location = InstructorTaskTestCase.problem_location(problem_url_name) item = self.module_store.get_item(location) with self.module_store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, location.course_key): item.data = problem_xml self.module_store.update_item(item, self.user.id) self.module_store.publish(location, self.user.id)
raise ValueError( 'Second argument should be a filename, %s (type %s) was given' % (filename, type(filename)) )
os.dup2(stdout_child_fd, pty.STDIN_FILENO) os.dup2(stdout_child_fd, pty.STDOUT_FILENO) os.dup2(stderr_child_fd, pty.STDERR_FILENO)
from contextlib import contextmanager import logging
if errors: return errors ret = dict(list(disabled.items()) + list(self.call_chunks(chunks).items())) ret = self.call_listen(chunks, ret)
if not os.path.isfile(config): try: salt.utils.fopen(config, 'w+').close() except (IOError, OSError): msg = 'Could not create {0}' raise CommandExecutionError(msg.format(config))
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))
self.compare_dates( DateTest.date.from_json("2013-01-01"), DateTest.date.from_json("2012-12-31"), datetime.timedelta(days=1) ) self.compare_dates( DateTest.date.from_json("2013-01-01T00"), DateTest.date.from_json("2012-12-31T23"), datetime.timedelta(hours=1) ) self.compare_dates( DateTest.date.from_json("2013-01-01T00:00"), DateTest.date.from_json("2012-12-31T23:59"), datetime.timedelta(minutes=1) ) self.compare_dates( DateTest.date.from_json("2013-01-01T00:00:00"), DateTest.date.from_json("2012-12-31T23:59:59"), datetime.timedelta(seconds=1) ) self.compare_dates( DateTest.date.from_json("2013-01-01T00:00:00Z"), DateTest.date.from_json("2012-12-31T23:59:59Z"), datetime.timedelta(seconds=1) ) self.compare_dates( DateTest.date.from_json("2012-12-31T23:00:01-01:00"), DateTest.date.from_json("2013-01-01T00:00:00+01:00"), datetime.timedelta(hours=1, seconds=1) )
parent = self.get_a_block(usage_id="parent") parent.inherited = "Changed!" self.assertEqual(parent.inherited, "Changed!") for child_num in range(10): usage_id = "child_{}".format(child_num) child = self.get_a_block(usage_id=usage_id) child.parent = "parent" self.assertEqual(child.inherited, "Changed!")
return self.filter_by(data, "user", user)
TEMPLATE_NAME = 'formulaequationinput.html' def setUp(self): self.context = { 'id': 2, 'value': 'PREFILLED_VALUE', 'status': Status('unsubmitted'), 'label': 'test', 'previewer': 'file.js', 'reported_status': 'REPORTED_STATUS', 'trailing_text': None, } super(FormulaEquationInputTemplateTest, self).setUp() def test_no_size(self): xml = self.render_to_xml(self.context) self.assert_no_xpath(xml, "//input[@size]", self.context) def test_size(self): self.context['size'] = '40' xml = self.render_to_xml(self.context) self.assert_has_xpath(xml, "//input[@size='40']", self.context)
input_space = Conv2DSpace((3, 3), num_channels=3) filters_values = numpy.ones( (2, 3, 2, 2), dtype=theano.config.floatX ) filters = sharedX(filters_values) image = numpy.random.rand(1, 3, 3, 3).astype(theano.config.floatX) conv2d = Conv2D(filters, 1, input_space) f = theano.function([self.image_tensor], conv2d.lmul(self.image_tensor)) assert f(image).shape == (1, 2, 2, 2)
path = example_bin_lush_path + 'int_3tensor.lushbin' result = read_bin_lush_matrix(path) assert str(result.dtype) == 'int32' assert len(result.shape) == 3 if result.shape != (3, 2, 4): raise AssertionError( "ubyte_3tensor.lushbin stores a 3-tensor " "of shape (3,2,4), but read_bin_lush_matrix thinks it has " "shape " + str(result.shape) ) for i in xrange(1, result.shape[0]+1): for j in xrange(1, result.shape[1]+1): for k in xrange(1, result.shape[2]+1): assert (result[i - 1, j - 1, k - 1] == (i + 10000 ** j) * ((-2) ** k))
assert is_flat_specs(data_specs)
orig_names = list(columns) self.num_original_columns = len(columns) return line, orig_names, columns
student = request.user
match = not noqa and COMPARE_SINGLETON_REGEX.search(logical_line) if match: same = (match.group(1) == '==') singleton = match.group(2) msg = "'if cond is %s:'" % (('' if same else 'not ') + singleton) if singleton in ('None',): code = 'E711' else: code = 'E712' nonzero = ((singleton == 'True' and same) or (singleton == 'False' and not same)) msg += " or 'if %scond:'" % ('' if nonzero else 'not ') yield match.start(1), ("%s comparison to %s should be %s" % (code, singleton, msg))
self.assertFalse(self.cohort_management_page.is_category_selected())
log.info('Starting the Salt Puller on {0}'.format(pull_uri)) old_umask = os.umask(0o177) try: pull_sock.bind(pull_uri) finally: os.umask(old_umask)
self.q(css='.action-remove-member').first.click()
if width is None and height is None: with self.assertRaises(ValueError): getattr(field, 'width') with self.assertRaises(ValueError): getattr(field, 'height') else: self.assertEqual(field.width, width) self.assertEqual(field.height, height)
launchctl.__salt__ = {}
try: import keyring import keyring.backend
doc_topics, sstats_list = zip(*results) doc_topic_distr = np.vstack(doc_topics)
df = DataFrame([time(9, 0, 0), time(9, 1, 30)], columns=["a"]) df.to_sql('test_time', self.conn, index=False, flavor=self.flavor) res = read_sql_query('SELECT * FROM test_time', self.conn) if self.flavor == 'sqlite': expected = df.applymap(lambda _: _.strftime("%H:%M:%S.%f")) tm.assert_frame_equal(res, expected)
scores = cross_val_score(GaussianNB(), X, y, cv=10) assert_greater(scores.mean(), 0.77)
self.verify_tag(inclusion.inclusion_no_params, 'inclusion_no_params') self.verify_tag(inclusion.inclusion_one_param, 'inclusion_one_param') self.verify_tag(inclusion.inclusion_explicit_no_context, 'inclusion_explicit_no_context') self.verify_tag(inclusion.inclusion_no_params_with_context, 'inclusion_no_params_with_context') self.verify_tag(inclusion.inclusion_params_and_context, 'inclusion_params_and_context') self.verify_tag(inclusion.inclusion_two_params, 'inclusion_two_params') self.verify_tag(inclusion.inclusion_one_default, 'inclusion_one_default') self.verify_tag(inclusion.inclusion_unlimited_args, 'inclusion_unlimited_args') self.verify_tag(inclusion.inclusion_only_unlimited_args, 'inclusion_only_unlimited_args') self.verify_tag(inclusion.inclusion_tag_without_context_parameter, 'inclusion_tag_without_context_parameter') self.verify_tag(inclusion.inclusion_tag_use_l10n, 'inclusion_tag_use_l10n') self.verify_tag(inclusion.inclusion_unlimited_args_kwargs, 'inclusion_unlimited_args_kwargs')
if self.on is None and self.left_on is None and self.right_on is None:
ret = {}
from salttesting.helpers import ensure_in_syspath, expensiveTest
return str(self)
loffset = self.loffset if isinstance(loffset, compat.string_types): loffset = to_offset(self.loffset) needs_offset = ( isinstance(loffset, (DateOffset, timedelta)) and isinstance(result.index, DatetimeIndex) and len(result.index) > 0 ) if needs_offset: result.index = result.index + loffset return result
if mode == "login": return external_auth_login(request) elif mode == "register": return external_auth_register(request)
assert_frame_equal(df.div(row), df / row) assert_frame_equal(df.div(col, axis=0), (df.T / col).T)
response = self.client.get('/syndication/template/') doc = minidom.parseString(response.content) feed = doc.getElementsByTagName('rss')[0] chan = feed.getElementsByTagName('channel')[0] items = chan.getElementsByTagName('item') self.assertChildNodeContent(items[0], { 'title': 'Title in your templates: My first entry\n', 'description': 'Description in your templates: My first entry\n', 'link': 'http://example.com/blog/1/', })
try: Thread.commentable_id = commentable_id request = RequestFactory().post("dummy_url", {"body": text}) request.user = self.student request.view_name = "create_comment" response = views.create_comment( request, course_id=unicode(self.course.id), thread_id="dummy_thread_id" )
'milestones',
test_deviance = np.zeros((params['n_estimators'],), dtype=np.float64)
return ['nvmatrix.cuh', 'conv_util.cuh']
#if not sig_valid:
field_type, field_params, field_notes = self.get_field_type(connection, table_name, row) extra_params.update(field_params) comment_notes.extend(field_notes)
expected = Timestamp('3/11/2012 05:00', tz=self.tzstr('US/Eastern'))
world.browser.driver.get(url) assert_equal(world.css_text('body'), expected_text)
assert_raises_regexp(ValueError, "class_weight 'balanced' is not supported for " "partial_fit. In order to use 'balanced' weights, " "use compute_class_weight\('balanced', classes, y\). " "In place of y you can us a large enough sample " "of the full training set target to properly " "estimate the class frequency distributions. " "Pass the resulting weights as the class_weight " "parameter.", self.factory(class_weight='balanced').partial_fit, X, Y, classes=np.unique(Y))
dt = Timestamp('20130101 09:10:11') result = dt.floor('D') expected = Timestamp('20130101') self.assertEqual(result, expected)
DATETIME_INPUT_FORMATS = [
for i in ['_right_indicator', '_left_indicator', '_merge']: df_badcolumn = DataFrame({'col1': [1, 2], i: [2, 2]})
filter_ = dict() for supported_filter in self.supported_filters: if cleaned_data.get(supported_filter.param_name) is not None: filter_[supported_filter.field_name] = cleaned_data[supported_filter.param_name] cleaned_data['filter_'] = filter_ or None
ScopeIds(None, category, loc, loc), DictFieldData(data_content),
from __future__ import absolute_import
self.checker_class = kwargs.pop('checker_class', Checker) parse_argv = kwargs.pop('parse_argv', False) config_file = kwargs.pop('config_file', None) parser = kwargs.pop('parser', None) options_dict = dict(*args, **kwargs) arglist = None if parse_argv else options_dict.get('paths', None) options, self.paths = process_options( arglist, parse_argv, config_file, parser) if options_dict: options.__dict__.update(options_dict) if 'paths' in options_dict: self.paths = options_dict['paths']
self.q(css='a.nav-item').filter(text='All Topics')[0].click()
errstring = "Error: too few arguments" with self.assertRaisesRegexp(CommandError, errstring): call_command('export_olx')
return JsonResponse( CourseDetails.update_from_json(course_key, request.json, request.user), encoder=CourseSettingsEncoder )
score[i] += n_ranked_above / rank[label]
if field_object is None: field_object = self._create_object(kvs_key, serialized_value) field_object.save(force_insert=True) self._cache[cache_key] = field_object else: field_object.value = serialized_value field_object.save(force_update=True)
ALL_USERS_VISIBILITY = 'all_users'
end = len(text) state = self.state regexes = self.regexes toks = self.toks start = 0 while start < end: for match in regexes[state].finditer(text, start): name = match.lastgroup tok = toks[name] toktext = match.group(name) start += len(toktext) yield (tok.name, toktext) if tok.next: state = tok.next break self.state = state
self.assertListEqual(json.loads(response.content), expected)
if isinstance(source, (tuple, list)): source, = source
self.logout()
self.assertIn("ufeff", filedata) self.ufeff_srt_file.write(filedata) self.ufeff_srt_file.seek(0)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'softlayer', vm_['profile'], vm_=vm_) is False: return False
from __future__ import absolute_import import os import shutil import tempfile
logging.debug("Current state of '{}' element is '{}'".format(state_selector, current_state))
log = logging.getLogger(__name__) request_log = logging.getLogger('requests')
('BACKGROUND', (1, 2), (1, 2), '#EEEEEE'),
self.initialize_course( course_factory_kwargs={ 'user_partitions': [user_partition] } )
if set_request_user: self.request.user = self.user SafeSessionMiddleware.set_user_id_in_session(self.request, self.user) if set_session_cookie: self.client.response.cookies[settings.SESSION_COOKIE_NAME] = "some_session_id" response = SafeSessionMiddleware().process_response(self.request, self.client.response) self.assertEquals(response.status_code, 200)
kwargs['base_url'] = os.environ.get('DOCKER_HOST')
from salt.states import pyrax_queues
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("SaltRaetRoadStackSetup") self.addEnterDeed("StatsMasterTestSetup") act = self.addRecurDeed("SaltRaetStatsEventerMaster")
if isinstance(self.children[-1], six.string_types): if self.children[-1].isspace(): self.children.pop()
nbrs_D = neighbors.NearestNeighbors(n_neighbors=3, algorithm='brute', metric='precomputed') nbrs_D.fit(DXX) dist_D, ind_D = getattr(nbrs_D, method)(DYX) assert_array_almost_equal(dist_X, dist_D) assert_array_almost_equal(ind_X, ind_D)
lookup_class = targets[0].get_lookup('isnull') clause.add(lookup_class(targets[0].get_col(alias, sources[0]), False), AND)
chap_usage_key = self.response_usage_key(resp) new_obj = self.get_item_from_modulestore(chap_usage_key) self.assertEqual(new_obj.scope_ids.block_type, 'chapter') self.assertEqual(new_obj.display_name, display_name) self.assertEqual(new_obj.location.org, self.course.location.org) self.assertEqual(new_obj.location.course, self.course.location.course)
css_class = 'start-date' title = ugettext_lazy('Course Starts') @property def date(self): return self.course.start
if axis != 0: raise AssertionError('axis must be 0') return self.obj.reindex(keyarr, level=level)
old_ssd = n_past * var new_ssd = n_new * new_var total_ssd = (old_ssd + new_ssd + (n_past / float(n_new * n_total)) * (n_new * mu - n_new * new_mu) ** 2) total_var = total_ssd / n_total
kwargs.pop('freq', None) return MultiIndex.from_tuples(values, **kwargs)
basename = 'test_multisheet' expected_keys = [2, 'Charlie', 'Charlie'] dfs = self.get_exceldf(basename, sheetname=expected_keys) expected_keys = list(set(expected_keys)) tm.assert_contains_all(expected_keys, dfs.keys()) assert len(expected_keys) == len(dfs.keys())
cat_rev = pd.Categorical(["a", "b", "c"], categories=["c", "b", "a"], ordered=True) cat_rev_base = pd.Categorical( ["b", "b", "b"], categories=["c", "b", "a"], ordered=True) cat = pd.Categorical(["a", "b", "c"], ordered=True) cat_base = pd.Categorical(["b", "b", "b"], categories=cat.categories, ordered=True)
clf1 = LogisticRegressionCV(cv=precomputed_folds) target_copy = target.copy() target_copy[target_copy == 0] = 1 clf1.fit(train, target_copy)
re.compile(regex) opts['file_ignore_regex'].append(regex)
return self.tracker.backends['mem']
names_t_actual = sel.transform([feature_names]) assert_array_equal(feature_names_t, names_t_actual.ravel())
if result.ndim == 2: return DataFrame(result, index=self.index, columns=self.columns) else: return Series(result, index=self._get_agg_axis(axis))
django_settings.SOCIAL_AUTH_LOGIN_REDIRECT_URL = _SOCIAL_AUTH_LOGIN_REDIRECT_URL
bad_ptrs = (5, ctypes.c_char_p(b'foobar')) for bad_ptr in bad_ptrs: with self.assertRaises(TypeError): fg1._set_ptr(bad_ptr) with self.assertRaises(TypeError): fg2._set_ptr(bad_ptr)
self.assert_grade(problem, 'choice_3', 'incorrect')
if check_input or copy: X = check_array(X, accept_sparse='csr', dtype=np.float64) y = check_array(y, ensure_2d=False, copy=copy, dtype=None) check_consistent_length(X, y) _, n_features = X.shape classes = np.unique(y) random_state = check_random_state(random_state)
self.system.publish( self, 'grade', { 'value': scaled_score, 'max_value': max_score, 'user_id': user.id, }, ) self.module_score = scaled_score self.score_comment = comment
with tm.assert_produces_warning(FutureWarning): df.sort_index(by=['A', 'B'], ascending=[1, 0]) result = df.sort_values(by=['A', 'B'], ascending=[1, 0])
self.url = reverse( 'blocks_in_block_tree', kwargs={'usage_key_string': unicode(self.course_usage_key)} ) self.query_params = {'depth': 'all', 'username': self.user.username}
cert = GeneratedCertificate.eligible_certificates.get(user=self.student, course_id=self.course.id) self.assertEqual(cert.status, 'error') self.assertIn(self.ERROR_REASON, cert.error_reason)
expr = stripXML(self.mathml_start + expr + self.mathml_end) expected = stripXML(self.mathml_start + expected + self.mathml_end)
binary = salt.utils.which('at') if not binary: return '\'at.at\' is not available.'
raise NotImplementedError( "%s does not implement set_input_space yet" % str(type(self)))
assert_frame_equal(expected, df)
raise
msg = "Unable to fill values because Int64Index cannot contain NA" with tm.assertRaisesRegexp(ValueError, msg): idx.take(np.array([1, 0, -1]), fill_value=True)
import nose
if not overwrite and other_mask.all(): result[col] = this[col].copy() continue
combos = [{x[0]: x[1]} for x in zip(combo_keys, combo_values)]
FormSet = modelformset_factory(Poem, fields='__all__') john_milton = Poet(name="John Milton") john_milton.save() data = { 'form-TOTAL_FORMS': 1, 'form-INITIAL_FORMS': 0, 'form-MAX_NUM_FORMS': '', 'form-0-name': '', 'form-0-poet': str(john_milton.id), } formset = FormSet(initial=[{'poet': john_milton}], data=data) self.assertFalse(formset.extra_forms[0].has_changed())
toggles = { 'allow-nat': {'type': 'yes_no', 'value': allow_nat}, 'allow-snat': {'type': 'yes_no', 'value': allow_snat} }
for a in Article.objects.all(): a.delete() now = datetime.now() Article.objects.create(pub_date=now, headline='f') Article.objects.create(pub_date=now, headline='fo') Article.objects.create(pub_date=now, headline='foo') Article.objects.create(pub_date=now, headline='fooo') Article.objects.create(pub_date=now, headline='hey-Foo') Article.objects.create(pub_date=now, headline='bar') Article.objects.create(pub_date=now, headline='AbBa') Article.objects.create(pub_date=now, headline='baz') Article.objects.create(pub_date=now, headline='baxZ') self.assertQuerysetEqual( Article.objects.filter(headline__regex=r'fo*'), ['<Article: f>', '<Article: fo>', '<Article: foo>', '<Article: fooo>'] ) self.assertQuerysetEqual( Article.objects.filter(headline__iregex=r'fo*'), [ '<Article: f>', '<Article: fo>', '<Article: foo>', '<Article: fooo>', '<Article: hey-Foo>', ] ) self.assertQuerysetEqual( Article.objects.filter(headline__regex=r'fo+'), ['<Article: fo>', '<Article: foo>', '<Article: fooo>'] ) self.assertQuerysetEqual( Article.objects.filter(headline__regex=r'fooo?'), ['<Article: foo>', '<Article: fooo>'] ) self.assertQuerysetEqual( Article.objects.filter(headline__regex=r'^b'), ['<Article: bar>', '<Article: baxZ>', '<Article: baz>'] ) self.assertQuerysetEqual(Article.objects.filter(headline__iregex=r'^a'), ['<Article: AbBa>']) self.assertQuerysetEqual(Article.objects.filter(headline__regex=r'z$'), ['<Article: baz>']) self.assertQuerysetEqual( Article.objects.filter(headline__iregex=r'z$'), ['<Article: baxZ>', '<Article: baz>'] ) self.assertQuerysetEqual( Article.objects.filter(headline__regex=r'ba[rz]'), ['<Article: bar>', '<Article: baz>'] ) self.assertQuerysetEqual(Article.objects.filter(headline__regex=r'ba.[RxZ]'), ['<Article: baxZ>']) self.assertQuerysetEqual( Article.objects.filter(headline__iregex=r'ba[RxZ]'), ['<Article: bar>', '<Article: baxZ>', '<Article: baz>'] )
self.assertDeserializeEqual(False, 'false') self.assertDeserializeEqual(True, 'true')
repr(df)
import salt.ext.six as six
for c in Country.objects.num_geom(): self.assertEqual(2, c.num_geom)
self.assertEqual(Book.authors.through.objects.using('default').count(), 2) self.assertEqual(Book.authors.through.objects.using('other').count(), 0)
settings_module = os.environ.get(ENVIRONMENT_VARIABLE) if not settings_module: desc = ("setting %s" % name) if name else "settings" raise ImproperlyConfigured( "Requested %s, but settings are not configured. " "You must either define the environment variable %s " "or call settings.configure() before accessing settings." % (desc, ENVIRONMENT_VARIABLE)) self._wrapped = Settings(settings_module)
type = 'textbooks' priority = None view_name = 'book' @classmethod def is_enabled(cls, course, user=None): parent_is_enabled = super(TextbookTabs, cls).is_enabled(course, user) return settings.FEATURES.get('ENABLE_TEXTBOOK') and parent_is_enabled @classmethod def items(cls, course): for index, textbook in enumerate(course.textbooks): yield SingleTextbookTab( name=textbook.title, tab_id='textbook/{0}'.format(index), view_name=cls.view_name, index=index )
return self.q(css="body.discussion .forum-nav-sort-control").present
result = self.panel4d.reindex( major=self.panel4d.major_axis, copy=False) assert_panel4d_equal(result, self.panel4d) self.assertTrue(result is self.panel4d)
for method in ['POST', 'GET']: self.client.login(username=user.username, password='test') self.attempt_login(200, method) openid_setup = self.client.session['openid_setup'] self.assertIn('post_params', openid_setup) post_args = { 'email': user.email, 'password': 'bad_password', }
if complength == 9: result = { 'count': comps[0], 'hostname': comps[7], 'ip': comps[8], 'ms1': comps[1], 'ms2': comps[3], 'ms3': comps[5]} ret.append(result) elif complength == 8: result = { 'count': comps[0], 'hostname': None, 'ip': comps[7], 'ms1': comps[1], 'ms2': comps[3], 'ms3': comps[5]} ret.append(result) else: result = { 'count': comps[0], 'hostname': None, 'ip': None, 'ms1': None, 'ms2': None, 'ms3': None} ret.append(result)
return course_mode_tuple.slug in cls.VERIFIED_MODES
cc_rendered_selector = self.get_element_selector(CSS_CLASS_NAMES['closed_captions']) self.wait_for_element_visibility(cc_rendered_selector, 'Closed captions rendered')
self.assertRaises(TypeError, lambda: ind.view('i8'))
def __init__(y, p=1, type="none", season=None, exogen=None, lag_max=None, ic=None): pass
if not self._verify_auth(): self.redirect('/login') return
rval = (transform(raw_batch[0]),) + raw_batch[1:]
with filesystem.open('updates.html', 'r') as grading_policy: on_disk = grading_policy.read() self.assertEqual(on_disk, course_updates.data)
self.course_certs = get_completed_courses(self.user) progress = [] for program in self.engaged_programs: completed, in_progress, not_started = [], [], [] for course_code in program['course_codes']: name = course_code['display_name'] if self._is_complete(course_code): completed.append(name) elif self._is_in_progress(course_code): in_progress.append(name) else: not_started.append(name) progress.append({ 'id': program['id'], 'completed': completed, 'in_progress': in_progress, 'not_started': not_started, }) return progress
user = User() user.save() DarkLangConfig( released_languages=languages, changed_by=user, enabled=True ).save()
from salt.ext.six import string_types
if url_name is None or url_name == "": url_name = fallback_name()
super(TeamSignalsTest, self).setUp('lms.djangoapps.teams.utils.tracker') self.user = UserFactory.create(username="user") self.moderator = UserFactory.create(username="moderator") self.team = CourseTeamFactory(discussion_topic_id=self.DISCUSSION_TOPIC_ID) self.team_membership = CourseTeamMembershipFactory(user=self.user, team=self.team)
try: from cassandra.cluster import Cluster from cassandra.cluster import NoHostAvailable from cassandra.connection import ConnectionException, ConnectionShutdown from cassandra.auth import PlainTextAuthProvider from cassandra.query import dict_factory HAS_CASSANDRA_DRIVER = True except ImportError as e: HAS_CASSANDRA_DRIVER = False
EDX_XML_PARSER = XMLParser(dtd_validation=False, load_dtd=False, remove_comments=True, remove_blank_text=True, encoding='utf-8')
current_name = None
expected_0_2 = date_range(start='2000-01-04', periods=7, freq='D', name='idx') expected_7_9 = date_range(start='2000-01-01', periods=7, freq='D', name='idx')
from __future__ import absolute_import, print_function import os import sys import pprint
y = 0.5 * X.ravel() \ + np.random.randn(n_samples, 1).ravel()
warnings.warn("BoltzmannIsingHidden.get_weights_topo returns the " + "BOLTZMANN weights, is that what we want?") if not isinstance(self.input_space, Conv2DSpace): raise NotImplementedError() W = self.W W = W.T W = W.reshape((self.detector_layer_dim, self.input_space.shape[0], self.input_space.shape[1], self.input_space.nchannels)) W = Conv2DSpace.convert(W, self.input_space.axes, ('b', 0, 1, 'c')) return function([], W)()
try: old = __salt__['ip.get_interface'](name) new = __salt__['ip.build_interface'](name, type, enabled, **kwargs) if kwargs['test']: if old == new: pass if not old and new: ret['result'] = None ret['comment'] = 'Interface {0} is set to be ' \ 'added.'.format(name) elif old != new: diff = difflib.unified_diff(old, new, lineterm='') ret['result'] = None ret['comment'] = 'Interface {0} is set to be ' \ 'updated:\n{1}'.format(name, '\n'.join(diff)) else: if not old and new: ret['comment'] = 'Interface {0} ' \ 'added.'.format(name) ret['changes']['interface'] = 'Added network interface.' apply_ranged_setting = True elif old != new: diff = difflib.unified_diff(old, new, lineterm='') ret['comment'] = 'Interface {0} ' \ 'updated.'.format(name) ret['changes']['interface'] = '\n'.join(diff) apply_ranged_setting = True except AttributeError as error: ret['result'] = False ret['comment'] = str(error) return ret
if isinstance(data, dict): log_data = data.copy() if isinstance(hide_fields, list): for item in data: for field in hide_fields: if item == field: log_data[item] = 'XXXXXXXXXX' log.trace('Request POST Data: {0}'.format(pprint.pformat(log_data))) else: log.trace('Request POST Data: {0}'.format(pprint.pformat(data)))
raise cls.MyException()
from __future__ import unicode_literals
for vm_ in list_active_vms(): info[vm_] = _info(vm_)
self.courseware_page.visit() annotation_component_page = AnnotationComponentPage(self.browser) self.assertEqual( annotation_component_page.component_name, 'Test Annotation Module'.format() ) return annotation_component_page
GDAL_INTEGER_TYPES = [1, 2, 3, 4, 5]
EmptyPromise( lambda: cohort_name == self.cohort_management_page.get_selected_cohort(), "Waiting for new cohort to appear" ).fulfill() self.assertEqual(0, self.cohort_management_page.get_selected_cohort_count()) _assignment_type = assignment_type or 'manual' msg = "Waiting for currently selected cohort assignment type" EmptyPromise( lambda: _assignment_type == self.cohort_management_page.get_cohort_associated_assignment_type(), msg ).fulfill() self.cohort_management_page.select_manage_settings() self.cohort_management_page.add_students_to_selected_cohort([self.instructor_name]) EmptyPromise( lambda: 1 == self.cohort_management_page.get_selected_cohort_count(), 'Waiting for student to be added' ).fulfill() self.assertFalse(self.cohort_management_page.is_assignment_settings_disabled) self.assertEqual('', self.cohort_management_page.assignment_settings_message) self.assertEqual( self.event_collection.find({ "name": "edx.cohort.created", "time": {"$gt": start_time}, "event.cohort_name": cohort_name, }).count(), 1 ) self.assertEqual( self.event_collection.find({ "name": "edx.cohort.creation_requested", "time": {"$gt": start_time}, "event.cohort_name": cohort_name, }).count(), 1 )
if isinstance(freq, DateOffset): freq = freq.rule_code else: freq = frequencies.get_base_alias(freq)
with open(self.TEST_INDEX_FILENAME, "w+") as index_file: json.dump({}, index_file)
html_use_smartypants = True
pure_X = X[inliers_mask] pure_location = pure_X.mean(0) pure_emp_cov = EmpiricalCovariance().fit(pure_X) err_loc_emp_pure[i, j] = np.sum(pure_location ** 2) err_cov_emp_pure[i, j] = pure_emp_cov.error_norm(np.eye(n_features))
if host_name: host_ref = search_index.FindByDnsName(dnsName=host_name, vmSearch=False) else: host_ref = search_index.FindByDnsName(dnsName=host, vmSearch=False)
def __getstate__(self): return {}
from django.db.models import Model if objs: new_ids = set() for obj in objs: if isinstance(obj, self.model): if not router.allow_relation(obj, self.instance): raise ValueError( 'Cannot add "%r": instance is on database "%s", value is on database "%s"' % (obj, self.instance._state.db, obj._state.db) ) fk_val = self.through._meta.get_field( target_field_name).get_foreign_related_value(obj)[0] if fk_val is None: raise ValueError( 'Cannot add "%r": the value for field "%s" is None' % (obj, target_field_name) ) new_ids.add(fk_val) elif isinstance(obj, Model): raise TypeError( "'%s' instance expected, got %r" % (self.model._meta.object_name, obj) ) else: new_ids.add(obj)
self._create_video_component() self.edit_component() self.open_advanced_tab() self.video.upload_translation('chinese_transcripts.srt', 'zh') self.save_unit_settings() self.assertTrue(self.video.is_captions_visible()) unicode_text = "好 各位同学".decode('utf-8') self.assertIn(unicode_text, self.video.captions_text) self.edit_component() self.open_advanced_tab() self.assertEqual(self.video.translations(), ['zh']) self.video.upload_translation('uk_transcripts.srt', 'uk') self.save_unit_settings() self.assertTrue(self.video.is_captions_visible()) self.assertIn(unicode_text, self.video.captions_text) self.assertEqual(self.video.caption_languages.keys(), ['zh', 'uk'])
int_ts = Series(np.ones(10, dtype=int), index=lrange(10)) self.assertAlmostEqual(np.median(int_ts), int_ts.median())
assert_equal(X_pred.shape[0], n_samples)
with self.settings(DEBUG=True): self.verify_unsafe_response(non_sensitive_view, check_for_vars=False) with self.settings(DEBUG=False): self.verify_unsafe_response(non_sensitive_view, check_for_vars=False)
class Meta(AbstractUser.Meta): swappable = 'AUTH_USER_MODEL'
tasks = instructor_task.api.get_instructor_task_history(course_id, module_state_key)
p = Place(name='Zombie Cats', address='Not sure') p.save() self.r1.place = p self.r1.save() with self.assertNumQueries(0): self.assertEqual(p.restaurant, self.r1)
year = self.year month = self.month day = self.day testarr = month < 3 year[testarr] -= 1 month[testarr] += 12 return Float64Index(day + np.fix((153 * month - 457) / 5) + 365 * year + np.floor(year / 4) - np.floor(year / 100) + np.floor(year / 400) + 1721118.5 + (self.hour + self.minute / 60.0 + self.second / 3600.0 + self.microsecond / 3600.0 / 1e+6 + self.nanosecond / 3600.0 / 1e+9 ) / 24.0)
with ensure_clean_path(self.path) as path: dfq = DataFrame(np.random.randn(10, 4), columns=list( 'ABCD'), index=date_range('20130101', periods=10)) dfq.to_hdf(path, 'dfq', format='table')
for embedded in (x for x in data if isinstance(x, dict)): try: data = embedded[each] embed_match = True break except KeyError: pass if not embed_match: return default
p2 = Parent.objects.create(name="Parent 2") c.parent = p2 self.assertIs(c.parent, p2)
max_forms_input = ( '<input id="id_binarytree_set-MAX_NUM_FORMS" ' 'name="binarytree_set-MAX_NUM_FORMS" type="hidden" value="%d" />' ) total_forms_hidden = ( '<input id="id_binarytree_set-TOTAL_FORMS" ' 'name="binarytree_set-TOTAL_FORMS" type="hidden" value="2" />' ) response = self.client.get(reverse('admin:admin_inlines_binarytree_add')) self.assertContains(response, max_forms_input % 3) self.assertContains(response, total_forms_hidden)
'pillar_roots': dict,
image = image * 255. image = np.cast['uint8'](image)
self.assertEqual( deploy_dir_path, default_config['deploy_scripts_search_path'][0] )
CONFIG_PREFIX = SERVICE_VARIANT + "." if SERVICE_VARIANT else ""
import salt.utils import salt.utils.xmlutil as xml import salt.utils.args import salt.loader import salt.config import salt.version from salt._compat import ElementTree as ET from salt.template import compile_template from salt import syspaths
if position is not None: try: position = int(position) except (ValueError, TypeError): log.exception('Non-integer %r passed as position.', position) position = None
fig, ax = plt.subplots(1, 3, figsize=(12, 4)) ax[0].scatter(X_train[:, 0], X_train[:, 1], color=np.where(Y_train > 0, 'r', 'b')) ax[1].scatter(Xtr_s[:, 0], Xtr_s[:, 1], color=np.where(Y_train > 0, 'r', 'b')) ax[2].scatter(Xtr_r[:, 0], Xtr_r[:, 1], color=np.where(Y_train > 0, 'r', 'b')) ax[0].set_title("Unscaled data") ax[1].set_title("After standard scaling (zoomed in)") ax[2].set_title("After robust scaling (zoomed in)") for a in ax[1:]: a.set_xlim(-3, 3) a.set_ylim(-3, 3) plt.tight_layout() plt.show()
survey = self._create_test_survey() self.assertIsNotNone(survey) new_survey = SurveyForm.get(self.test_survey_name) self.assertIsNotNone(new_survey) self.assertEqual(new_survey.form, self.test_form)
n_classes = 3 X = np.array([[1.1, 2.2], [2.2, -4.4], [3.3, -2.2], [1.1, 1.1]]) y = np.array([0, 1, 2, 0]) lbin = LabelBinarizer() Y_bin = lbin.fit_transform(y)
problem.grade_answers({'1_2_1': 'test'})
with self.assertRaisesRegexp(CommandError, unicode(GitExportError.BAD_COURSE)): call_command( 'git_export', 'foo/bar:baz', 'silly' ) with self.assertRaisesRegexp(CommandError, unicode(GitExportError.URL_BAD)): call_command( 'git_export', 'foo/bar/baz', 'silly' )
if not raw: block_fields = self._serialize_fields(category, block_fields) if not asides: asides = [] document = { 'block_type': category, 'definition': definition_id, 'fields': block_fields, 'asides': asides, 'edit_info': { 'edited_on': datetime.datetime.now(UTC), 'edited_by': user_id, 'previous_version': None, 'update_version': new_id } } if block_defaults: document['defaults'] = block_defaults return BlockData(**document)
admins = win32security.ConvertStringSidToSid('S-1-5-32-544') user = win32security.ConvertStringSidToSid('S-1-5-32-545') system = win32security.ConvertStringSidToSid('S-1-5-18') owner = win32security.ConvertStringSidToSid('S-1-3-4')
changes = 'Container created' try: cid = out['out']['info']['id'] except Exception as e: log.debug(str(e)) else: changes = 'Container {0} created'.format(cid) out['comment'] = changes ret = _ret_status(out, name, changes=changes) return ret
for option in self._find_options(inputfield): if option['choice'] == choice: return option
if self._is_in_bulk_operation(course_key, ignore_case): return self._get_bulk_ops_record(course_key, ignore_case).index else: return self.db_connection.get_course_index(course_key, ignore_case)
if messages is None: break if messages: self._used_storages.add(storage) all_messages.extend(messages) if all_retrieved: break
template_name_suffix = '_form'
if getattr(self, '_cache', None) is None: return if key is None: self._cache.clear() else: self._cache.pop(key, None)
rval = shared(obj.get_value()) obj.__getstate__ = None
sdist_class = cmdclass['sdist']
del self.bias_from_marginals
mappings = getattr(settings, 'HOSTNAME_MODULESTORE_DEFAULT_MAPPINGS', None)
class Channel(models.Model): name = models.CharField(max_length=255)
dm['A'] = 'bar' self.assertEqual('bar', dm['A'][0])
curl -sSik https://localhost:8000 \\ -d client=runner \\ -d fun='jobs.lookup_jid' \\ -d jid='20150129182456704682' \\ -d outputter=highstate
return np.dot(A, B)
n_samples = int(X.shape[0]) rs = check_random_state(self.random_state)
result = p.select(lambda x: x in ('D', 'A'), axis=3) expected = p.reindex(minor=['A', 'D']) self.assert_panel4d_equal(result, expected)
try: return self.render(context) except Exception as e: if context.template.engine.debug and not hasattr(e, 'template_debug'): e.template_debug = context.template.get_exception_info(e, self.token) raise
return microsite.get_template_path(relative_path, **kwargs)
log = logging.getLogger(__name__)
def stop(self): pass
ensemble = BaggingClassifier(base_estimator=Perceptron(), n_estimators=0) iris = load_iris() assert_raise_message(ValueError, "n_estimators must be greater than zero, got 0.", ensemble.fit, iris.data, iris.target)
raise NotImplementedError(str(type(self)) + " does not implement " "do_inpainting.")
CohortFactory(course_id=course.id, name="ManualCohort") CohortFactory(course_id=course.id, name="ManualCohort2")
self.cohort_management_page.select_cohort(self.cohort_name) self.assertIsNone(self.cohort_management_page.get_cohort_associated_content_group()) self.assertIsNone(self.cohort_management_page.get_cohort_related_content_group_message()) self.assertEquals(["Apples", "Bananas"], self.cohort_management_page.get_all_content_groups())
self.assertIsNone(dti2.freq)
from salt.states import aws_sqs
score_subset = base_estimator.score(X_inlier_subset, y_inlier_subset)
codes = self.codes if codes.ndim > 1: raise NotImplementedError("Categorical with ndim > 1.") if np.prod(codes.shape) and (periods != 0): codes = np.roll(codes, com._ensure_platform_int(periods), axis=0) if periods > 0: codes[:periods] = -1 else: codes[periods:] = -1
has_enrolled_professional = (CourseMode.is_professional_slug(enrollment_mode) and is_active) if CourseMode.has_professional_mode(modes) and not has_enrolled_professional: redirect_url = reverse('verify_student_start_flow', kwargs={'course_id': unicode(course_key)}) if ecommerce_service.is_enabled(request.user): professional_mode = modes.get(CourseMode.NO_ID_PROFESSIONAL_MODE) or modes.get(CourseMode.PROFESSIONAL) if professional_mode.sku: redirect_url = ecommerce_service.checkout_page_url(professional_mode.sku)
raise NotImplementedError(_("Problem's definition does not support rescoring."))
for p in user_partitions: has_selected = any(g["selected"] for g in p["groups"]) has_selected_groups = has_selected_groups or has_selected
shp_path = os.path.realpath(os.path.join(os.path.dirname(upath(__file__)), '..', 'data')) co_shp = os.path.join(shp_path, 'counties', 'counties.shp') co_mapping = {'name': 'Name', 'state': 'State', 'mpoly': 'MULTIPOLYGON', }
df = DataFrame({'A': np.random.randn(10), 'B': ci.values}) idf = df.set_index('B') str(idf) tm.assert_index_equal(idf.index, ci, check_names=False) self.assertEqual(idf.index.name, 'B')
v = np.empty(len(index), dtype=object)
if 'snapshot_ids' in kwargs: kwargs['snapshot_id'] = kwargs['snapshot_ids']
return _isnull(obj)
try: api.regenerate_user_certificates(params["user"], params["course_key"], course=course)
if item in ['constraint']: if not isinstance(extra_args, (list, tuple)) or '--full' not in extra_args: cmd += ['--full']
if len(document.children) == 1: if not isinstance(document.children[0], six.string_types): document = document.children[0] return document
from __future__ import unicode_literals
with self.assertRaises(TypeError): cntry_g.country_code(17) with self.assertRaises(TypeError): cntry_g.country_name(GeoIP2)
with skip_signal( post_save, receiver=course_team_post_save_callback, sender=CourseTeam, dispatch_uid='teams.signals.course_team_post_save_callback' ): solar_team = self.test_team_name_id_map[u'Sólar team'] solar_team.last_activity_at = datetime.utcnow().replace(tzinfo=pytz.utc) solar_team.save()
if self._middleware_chain is None: self.load_middleware()
X_trans = pipeline.fit(X).transform(X) X_trans2 = pipeline.fit_transform(X) X_trans3 = pca.fit_transform(X) assert_array_almost_equal(X_trans, X_trans2) assert_array_almost_equal(X_trans, X_trans3)
self.assertEqual(NaT.isoformat(), 'NaT')
__array_priority__ = 1000
try: salt.utils.files.copyfile(sfn, real_name, __salt__['config.backup_mode'](backup), __opts__['cachedir']) except IOError as io_error: __clean_tmp(sfn) return _error( ret, 'Failed to commit change: {0}'.format(io_error))
AT = self.A.T AT_xlT = dot(AT, AT.transpose_right(self.xl, T=True)) AT_xlt_shape = dot_shape_from_shape(AT, AT.transpose_right_shape(self.xl.shape, T=True)) assert AT_xlT.shape == AT_xlt_shape, (AT_xlT.shape, AT_xlt_shape)
self.setGrains({'a': 'aval'}) ret = grains.present( name='foo:is:nested', value='bar') self.assertEqual(ret['result'], True) self.assertEqual(ret['changes'], {'foo': {'is': {'nested': 'bar'}}}) self.assertEqual( grains.__grains__, {'a': 'aval', 'foo': {'is': {'nested': 'bar'}}}) self.assertGrainFileContent("a: aval\n" + "foo:\n" + " is:\n" + " nested: bar\n" )
if version is None: version = self.version new_key = self.key_func(key, self.key_prefix, version) return new_key
fig.subplots_adjust(wspace=0, hspace=0)
from __future__ import unicode_literals
try: DumbCategory.objects.create() except TypeError: self.fail( "Creation of an instance of a model with only the PK field " "shouldn't error out after bulk insert refactoring (#17056)" )
return []
raise NotImplementedError(str(self.__class__) + " does not implement " "_get_default_output_layer")
r.streaming_content = iter(['abc', 'def']) r.streaming_content = (chunk.upper() for chunk in r.streaming_content) self.assertEqual(list(r), [b'ABC', b'DEF'])
result = get_email_params(self.course, False)
break
mock_snap_numb = MagicMock(return_value=snap_id) with patch.object(parallels, 'snapshot_name_to_id', mock_snap_numb): self.assertEqual(parallels._validate_snap_name(name, 3.14159), snap_id) mock_snap_numb.assert_called_once_with(name, u'3.14159', strict=True, runas=None)
index = self.get_course_index(course_key)
clf = svm.SVC(kernel='linear', C=0.1).fit(X_train, y_train) msg = "change the shape of the decision function" dec = assert_warns_message(ChangedBehaviorWarning, msg, clf.decision_function, X_train) assert_equal(dec.shape, (len(X_train), 10))
from pandas import tslib df.ix['b', 'timestamp'] = tslib.iNaT self.assertTrue(com.isnull(df.ix['b', 'timestamp']))
return self.q(css="div.problem div.capa_inputtype.textline div.correct span.status").is_present()
problem = self.build_problem( choice_type='checkbox', choices=[False, False, True, True], credit_type='edc' )
node = node.split(':') if node[0] == 'UPID': ret['node'] = str(node[1]) ret['pid'] = str(node[2]) ret['pstart'] = str(node[3]) ret['starttime'] = str(node[4]) ret['type'] = str(node[5]) ret['vmid'] = str(node[6]) ret['user'] = str(node[7]) ret['upid'] = str(upid)
json_resp = json.loads(resp.content) self.assertEqual(json_resp.get('total_cost'), self.cart.total_cost)
rsmemjson = GDALRaster(JSON_RASTER) bandmemjson = rsmemjson.bands[0] if numpy: numpy.testing.assert_equal( bandmemjson.data(), numpy.array(range(25)).reshape(5, 5) ) else: self.assertEqual(bandmemjson.data(), list(range(25)))
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21")
self.n_classes = 1
if not permissions.can_access_others_blocks(requesting_user, course_key): raise PermissionDenied( "'{requesting_username}' does not have permission to access view for '{requested_username}'." .format(requesting_username=requesting_user.username, requested_username=requested_username) )
assert isinstance(location, (NoneType, basestring, UsageKey)) if location == '': return None if isinstance(location, basestring): location = super(UsageKeyField, self).to_python(location) return Location.from_deprecated_string(location) else: return location
context['user_id'] = 12345 context['course_id'] = "course-v1:edx+100+1" return context
def setUp(self): super(TestFileSystemFinder, self).setUp() self.finder = finders.FileSystemFinder() test_file_path = os.path.join(TEST_ROOT, 'project', 'documents', 'test', 'file.txt') self.find_first = (os.path.join('test', 'file.txt'), test_file_path) self.find_all = (os.path.join('test', 'file.txt'), [test_file_path])
status['task_id'] = instructor_task.task_id status['task_state'] = instructor_task.task_state status['in_progress'] = instructor_task.task_state not in READY_STATES if instructor_task.task_output is not None: status['task_progress'] = json.loads(instructor_task.task_output)
pip_version = version(pip_bin)
try: return super(MultiValueDict, self).__getitem__(key) except KeyError: if default is None: return [] return default
self.assertRaises(ValueError, df_rev.reindex, df.index, method='pad') self.assertRaises(ValueError, df_rev.reindex, df.index, method='ffill') self.assertRaises(ValueError, df_rev.reindex, df.index, method='bfill') self.assertRaises(ValueError, df_rev.reindex, df.index, method='nearest')
response = self.client.get(reverse('admin:admin_views_article_changelist')) self.assertContains(response, 'bodyclass_consistency_check ')
assert_series_equal(nat_series_dtype_timestamp + NaT, nat_series_dtype_timestamp) assert_series_equal(NaT + nat_series_dtype_timestamp, nat_series_dtype_timestamp)
app_config = apps.get_app_config('admin') self.assertEqual(app_config.name, 'django.contrib.admin') app_config = apps.get_app_config('staticfiles') self.assertEqual(app_config.name, 'django.contrib.staticfiles') with self.assertRaises(LookupError): apps.get_app_config('admindocs') msg = "No installed app with label 'django.contrib.auth'. Did you mean 'myauth'" with self.assertRaisesMessage(LookupError, msg): apps.get_app_config('django.contrib.auth')
mock_request.return_value.status_code = 200 self._set_mock_request_data(mock_request, { "closed": False, "commentable_id": 'test_commentable_id', 'thread_id': 'test_thread_id', }) request = RequestFactory().post("dummy_url", {"body": "Test comment", 'auto_subscribe': True}) request.user = self.student request.view_name = "create_comment" views.create_comment(request, course_id=unicode(self.course.id), thread_id='test_thread_id') event_name, event = mock_emit.call_args[0] self.assertEqual(event_name, 'edx.forum.response.created') self.assertEqual(event['body'], "Test comment") self.assertEqual(event['commentable_id'], 'test_commentable_id') self.assertEqual(event['user_forums_roles'], ['Student']) self.assertEqual(event['user_course_roles'], ['Wizard']) self.assertEqual(event['discussion']['id'], 'test_thread_id') self.assertEqual(event['options']['followed'], True)
name = "splunk_search.list_all get defaults" try: client.saved_searches.delete(name) except Exception: pass search = client.saved_searches.create(name, search="nothing") defaults = dict(search.content) client.saved_searches.delete(name)
for backend in auth.get_backends(): if not hasattr(backend, 'has_module_perms'): continue try: if backend.has_module_perms(user, app_label): return True except PermissionDenied: return False return False
CourseEnrollmentFactory(user=self.moderator, course_id=self.course.id) self.moderator.roles.add(Role.objects.get(name="Moderator", course_id=self.course.id))
texinfo_documents = [ ( 'index', 'getting_started', u'getting_started Documentation', u'EdX Doc Team', 'getting_started', 'One line description of project.', 'Miscellaneous', ), ]
feature_importance = clf.feature_importances_ feature_importance = 100.0 * (feature_importance / feature_importance.max()) sorted_idx = np.argsort(feature_importance) pos = np.arange(sorted_idx.shape[0]) + .5 plt.subplot(1, 2, 2) plt.barh(pos, feature_importance[sorted_idx], align='center') plt.yticks(pos, boston.feature_names[sorted_idx]) plt.xlabel('Relative Importance') plt.title('Variable Importance') plt.show()
_mock_counts.return_value = {} with self.assertRaises(SystemExit): call_task('pavelib.quality.run_safelint')
result = self.panel4d.reindex() assert_panel4d_equal(result, self.panel4d) self.assertFalse(result is self.panel4d)
if issparse(X): if not isinstance(X, csr_matrix): X = csr_matrix(X) norms = csr_row_norms(X) else: norms = np.einsum('ij,ij->i', X, X) if not squared: np.sqrt(norms, norms) return norms
assert False, "student_view should produce valid html"
self.assertEqual( list( TestObject.objects .extra(select=OrderedDict((('foo', 'first'), ('bar', 'second'), ('whiz', 'third')))) .values_list() ), [('first', 'second', 'third', obj.pk, 'first', 'second', 'third')] )
if not request.user.is_staff: raise Http404 c = {} c['code'] = '' c['results'] = None if request.method == 'POST': py_code = c['code'] = request.POST.get('code') g = {} try: safe_exec(py_code, g) except Exception as e: c['results'] = traceback.format_exc() else: c['results'] = pprint.pformat(g) return render_to_response("debug/run_python_form.html", c)
if not self.q(css="input.practice_exam").present: return False
y_pred = estimator.predict(X) if sample_weight is not None: return self._sign * self._score_func(y_true, y_pred, sample_weight=sample_weight, **self._kwargs) else: return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
self.explicit = True return optparse.Option.take_action(self, action, dest, *args, **kwargs)
df = DataFrame({'DATE': pd.to_datetime( ['10-Oct-2013', '10-Oct-2013', '10-Oct-2013', '11-Oct-2013', '11-Oct-2013', '11-Oct-2013']), 'label': ['foo', 'foo', 'bar', 'foo', 'foo', 'bar'], 'VAL': [1, 2, 3, 4, 5, 6]})
if subtask_status.get_retry_count() == 0: to_list, num_optout = _filter_optouts_from_recipients(to_list, course_email.course_id) subtask_status.increment(skipped=num_optout)
self._timeout_future(tag, future) if future in self.timeout_map: tornado.ioloop.IOLoop.current().remove_timeout(self.timeout_map[future]) del self.timeout_map[future]
argspec = inspect.getargspec(func) argspec = inspect.formatargspec(*argspec) argspec = argspec.replace('*','\*') signature = '%s%s' % (func_name, argspec)
for hint_node in response.findall('./stringequalhint'): if self.match_hint_node(hint_node, student_answer, False, self.case_insensitive): new_cmap[self.answer_id]['msg'] += self.make_hint_div( hint_node, False, [student_answer], self.tags[0] ) return
self.assertSetEqual( set(response.data['blocks'].iterkeys()), self.non_orphaned_block_usage_keys, )
list_escalation_policies = salt.utils.alias_function(list_policies, 'list_escalation_policies')
if hasattr(self, 'extensions'): self.kwargstruct = KwargsStruct(**self.kwargs) for extension in self.extensions: extension.run_hooks('__post_parse_args__', self.kwargstruct) self.kwargs = self.kwargstruct.__dict__
cost = SumOfCosts([SumOfParams(), (0., DummyCost())])
self.assertEqual( my_function('POST', {'username': 'adrian', 'password1': 'secret', 'password2': 'secret'}), str_prefix( "VALID: [('password1', %(_)s'secret'), ('password2', %(_)s'secret'), ('username', %(_)s'adrian')]" ) )
X = rng.normal(size=(n_samples, n_features)) X = np.dot(X, corr) X[:n_relevant_features] /= np.abs( linalg.svdvals(X[:n_relevant_features])).max() X = StandardScaler().fit_transform(X.copy())
if name == 'self': continue if param.kind == inspect.Parameter.VAR_POSITIONAL: name = '*' + name elif param.kind == inspect.Parameter.VAR_KEYWORD: name = '**' + name if param.default != inspect.Parameter.empty: args.append((name, param.default)) else: args.append((name,))
return self.submit_question_answer( self.problem_display_name, {'2_1': response1, '2_2': response2} )
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
ax[0, 0].hist(X[:, 0], bins=bins, fc='#AAAAFF', normed=True) ax[0, 0].text(-3.5, 0.31, "Histogram")
value = self.literal
#html_use_index = True
def setUp(self): super(SendCompositeOutcomeTest, self).setUp() self.descriptor = MagicMock() self.descriptor.location = BlockUsageLocator( course_key=self.course_key, block_type='problem', block_id='problem', ) self.weighted_scores = MagicMock() self.weighted_scores_mock = self.setup_patch( 'lti_provider.tasks.get_weighted_scores', self.weighted_scores ) self.module_store = MagicMock() self.module_store.get_item = MagicMock(return_value=self.descriptor) self.check_result_mock = self.setup_patch( 'lti_provider.tasks.modulestore', self.module_store ) @ddt.data( (2.0, 2.0, 1.0), (2.0, 0.0, 0.0), (1, 2, 0.5), ) @ddt.unpack def test_outcome_with_score_score(self, earned, possible, expected): self.weighted_scores.score_for_module = MagicMock(return_value=(earned, possible)) tasks.send_composite_outcome( self.user.id, unicode(self.course_key), self.assignment.id, 1 ) self.send_score_update_mock.assert_called_once_with(self.assignment, expected) def test_outcome_with_outdated_version(self): self.assignment.version_number = 2 self.assignment.save() tasks.send_composite_outcome( self.user.id, unicode(self.course_key), self.assignment.id, 1 ) self.assertEqual(self.weighted_scores_mock.call_count, 0)
from salt.states import artifactory
self.clear.mminion.returners = {'.prep_jid': lambda x: 1}
tm._skip_if_no_pytz() from pytz import timezone as timezone
file_groups = {} for translatable in file_list: file_group = file_groups.setdefault(translatable.locale_dir, []) file_group.append(translatable) for locale_dir, files in file_groups.items(): self.process_locale_dir(locale_dir, files)
assert_raises(ValueError, cross_val_score, svm, linear_kernel.tolist(), y)
self.problem_page.click_choice('choice_choice_1') self.problem_page.click_check() self.problem_page.wait_for_expected_status('label.choicegroup_incorrect', 'incorrect')
'openedx.core.djangoapps.api_admin',
OLD_LOGGING = { 'version': 1, 'disable_existing_loggers': False, 'handlers': { 'mail_admins': { 'level': 'ERROR', 'class': 'django.utils.log.AdminEmailHandler' } }, 'loggers': { 'django.request': { 'handlers': ['mail_admins'], 'level': 'ERROR', 'propagate': True, }, } }
X = 5 * rng.rand(10000, 1) y = np.sin(X).ravel() y[::5] += 3 * (0.5 - rng.rand(X.shape[0]/5)) sizes = np.logspace(1, 4, 7) for name, estimator in {"KRR": KernelRidge(kernel='rbf', alpha=0.1, gamma=10), "SVR": SVR(kernel='rbf', C=1e1, gamma=10)}.items(): train_time = [] test_time = [] for train_test_size in sizes: t0 = time.time() estimator.fit(X[:train_test_size], y[:train_test_size]) train_time.append(time.time() - t0)
result = self.read_csv(StringIO(data), index_col='time', squeeze=True) self.assertFalse(result._is_view)
course_key = CourseKey.from_string('org/course/run') asset_key = course_key.make_asset_key(asset_type(), filename()) (curr_version, prev_version) = versions() return AssetMetadata( asset_key, pathname=pathname(), internal_name=str([filename() for __ in xrange(10)]), locked=locked(), contenttype=contenttype(), thumbnail=filename(), fields=fields(), curr_version=curr_version, prev_version=prev_version, edited_by=user_id(), edited_by_email='staff@edx.org', edited_on=date_and_time(), created_by=user_id(), created_by_email='staff@edx.org', created_on=date_and_time(), )
with transaction.atomic(): with transaction.atomic(): Reporter.objects.create(id=1, first_name="Tintin")
self._create_course_unit_with_handout('textbook.pdf') self.assertEqual(self.video.download_handout('application/pdf'), (True, True)) self.edit_component() self.open_advanced_tab() self.video.clear_handout() self.save_unit_settings() self.assertFalse(self.video.is_handout_button_visible)
self.q(css='.dropdown-menu li a').nth(2).click()
if intercept_init is not None: intercept_init = np.asarray(intercept_init, dtype=np.float64) if intercept_init.shape != (1,) and intercept_init.shape != (): raise ValueError("Provided intercept_init " "does not match dataset.") self.intercept_ = intercept_init.reshape(1,) else: self.intercept_ = np.zeros(1, dtype=np.float64, order="C")
_ignore_names = ["get", "join", "translate"]
STATIC_URL = "/static/" STATICFILES_FINDERS = ( 'django.contrib.staticfiles.finders.FileSystemFinder', ) STATICFILES_DIRS = ( (TEST_ROOT / "staticfiles" / "lms").abspath(), )
response2 = self.client.get('/template/cached/bar/') self.assertEqual(response2.status_code, 200)
return self.display_name_with_default
for n_samples, n_features in ((6, 5), (5, 10)): y = rng.randn(n_samples) X = rng.randn(n_samples, n_features) sample_weight = 1.0 + rng.rand(n_samples)
num_ccx = 3 for _ in xrange(num_ccx): self.make_ccx() all_ccx = CustomCourseForEdX.objects.all() all_ccx = all_ccx.order_by('id') self.assertEqual(len(all_ccx), num_ccx) title_str = 'Title CCX {0}' for num, ccx in enumerate(all_ccx): ccx.display_name = title_str.format(string.ascii_lowercase[-(num + 1)]) ccx.save()
_change_access(course, user, level, 'allow', send_email)
ForestClassifier = FOREST_CLASSIFIERS[name]
site = Site.objects.get_current() self.assertEqual("example.com", site.name) s2 = Site.objects.get(id=settings.SITE_ID) s2.name = "Example site" s2.save() site = Site.objects.get_current() self.assertEqual("Example site", site.name)
super(ExpressionRuleViolation, self).__init__(rule) self.expression = expression self.start_line = 0 self.start_column = 0 self.end_line = 0 self.end_column = 0 self.lines = [] self.is_disabled = False
if deleted['code'] == 200: ret['result'] = True ret['comment'] = 'Profile was successfully deleted.' ret['changes']['old'] = existing['content'] ret['changes']['new'] = {} else: ret = _load_result(deleted, ret)
datap = os.path.join(cdir, minion, 'data.p') try: with salt.utils.fopen(datap, 'rb') as fp_: miniondata = serial.load(fp_) except (IOError, OSError): return minion, None, None grains = miniondata.get('grains') pillar = miniondata.get('pillar') return minion, grains, pillar
module_path = dirname(__file__) data = np.loadtxt(join(module_path, 'data', 'digits.csv.gz'), delimiter=',') with open(join(module_path, 'descr', 'digits.rst')) as f: descr = f.read() target = data[:, -1] flat_data = data[:, :-1] images = flat_data.view() images.shape = (-1, 8, 8) if n_class < 10: idx = target < n_class flat_data, target = flat_data[idx], target[idx] images = images[idx] return Bunch(data=flat_data, target=target.astype(np.int), target_names=np.arange(10), images=images, DESCR=descr)
for course in store.get_courses(**kwargs): course_id = self._clean_locator_for_mapping(course.id) if course_id not in courses: courses[course_id] = course
(['honor', 'verified', 'audit'], ['1', '2', '3']),
self.assertFalse('<td>Verified</td>' in response.content) self.assertFalse('<td>Audit</td>' in response.content) self.assertFalse('<td>Honor</td>' in response.content) self.assertFalse('<td>Professional</td>' in response.content)
self.assertTrue(has_access(user, action, course)) self.assertTrue(has_access(user, action, CourseOverview.get_from_id(course.id)))
for b, sb in zip(blocks, self.blocks): b.mgr_locs = sb.mgr_locs
url = STUDIO_BASE_URL + '/textbooks/' + self._course_key for book in self._textbooks: payload = json.dumps(book) response = self.session.post(url, headers=self.headers, data=payload) if not response.ok: raise FixtureError( "Could not add book to course: {0} with {1}. Status was {2}".format( book, url, response.status_code))
expected = self.course actual = self.ccx.course self.assertEqual(expected, actual)
activation_key = create_account(self.USERNAME, self.PASSWORD, self.EMAIL) user = User.objects.get(username=self.USERNAME)
self.assertEqual(mail.outbox[-1].to[0], 'finance@example.com')
df = DataFrame([[1, 2], [3, 4]], columns=[u'\xe9', u'b']) df.to_sql('test_unicode', self.conn, index=False)
self.log_dir.makedirs_p() self.har_dir.makedirs_p() self.report_dir.makedirs_p()
params, response = _validate_post_params(request.POST) if response is not None: return response
keyarr = _asarray_tuplesafe(key)
p = FieldOverridePost.objects.create(title="Test Post", content="Test Content") response = self.client.get(reverse('admin:admin_views_fieldoverridepost_change', args=(p.pk,))) self.assertContains(response, '<p class="help">Overridden help text for the date</p>') self.assertContains(response, '<label for="id_public">Overridden public label:</label>', html=True) self.assertNotContains(response, "Some help text for the date (with unicode ŠĐĆŽćžšđ)")
X, y, labels = indexable(X, y, labels) n_samples = _num_samples(X) if self.n_folds > n_samples: raise ValueError( ("Cannot have number of folds n_folds={0} greater" " than the number of samples: {1}.").format(self.n_folds, n_samples)) for train, test in super(_BaseKFold, self).split(X, y, labels): yield train, test
@patch.dict(settings.FEATURES, {'AUTOMATIC_VERIFY_STUDENT_IDENTITY_FOR_TESTING': False}) @override_settings(VERIFY_STUDENT={ "SOFTWARE_SECURE": { "API_URL": "https://verify.example.com/submit/", "API_ACCESS_KEY": "dcf291b5572942f99adaab4c2090c006", "API_SECRET_KEY": "c392efdcc0354c5f922dc39844ec0dc7", "FACE_IMAGE_AES_KEY": "f82400259e3b4f88821cd89838758292", "RSA_PUBLIC_KEY": ( "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDkgtz3fQdiXshy/RfOHkoHlhx/" "SSPZ+nNyE9JZXtwhlzsXjnu+e9GOuJzgh4kUqo73ePIG5FxVU+mnacvufq2cu1SOx" "lRYGyBK7qDf9Ym67I5gmmcNhbzdKcluAuDCPmQ4ecKpICQQldrDQ9HWDxwjbbcqpVB" "PYWkE1KrtypGThmcehLmabf6SPq1CTAGlXsHgUtbWCwV6mqR8yScV0nRLln0djLDm9d" "L8tIVFFVpAfBaYYh2Cm5EExQZjxyfjWd8P5H+8/l0pmK2jP7Hc0wuXJemIZbsdm+DSD" "FhCGY3AILGkMwr068dGRxfBtBy/U9U5W+nStvkDdMrSgQezS5+V test@example.com" ), "AWS_ACCESS_KEY": "c987c7efe35c403caa821f7328febfa1", "AWS_SECRET_KEY": "fc595fc657c04437bb23495d8fe64881", "S3_BUCKET": "test.example.com", }, "DAYS_GOOD_FOR": 10, }) @httpretty.activate @moto.mock_s3 def test_submit_photos_for_reverification(self): conn = boto.connect_s3() conn.create_bucket("test.example.com")
return self._folds[k]
with self.assertRaises(TypeError): class ProxyModel(SwappableModel):
setattr(self, hyperparameter.name, np.exp(theta[i:i + hyperparameter.n_elements])) i += hyperparameter.n_elements
etcd_mod.__opts__ = {} etcd_mod.__utils__ = {}
td[2] = np.nan result = td.bfill() expected = td.fillna(0) expected[2] = timedelta(days=1, seconds=9 * 3600 + 60 + 1) assert_series_equal(result, expected)
test_args = {'which_set': 'test'} for key in self.args: if key in ['which_set', 'restrict_instances', 'self', 'start', 'stop']: continue test_args[key] = self.args[key] return FoveatedNORB(**test_args)
if opts.fields is None and opts.exclude is None: raise ImproperlyConfigured( "Creating a ModelForm without either the 'fields' attribute " "or the 'exclude' attribute is prohibited; form %s " "needs updating." % name )
import salt.output import salt.utils.http
DOC_STORE_CONFIG = { 'host': MONGO_HOST, 'db': 'test_xmodule', 'port': MONGO_PORT_NUM, 'collection': 'modulestore{0}'.format(uuid.uuid4().hex[:5]), } modulestore_options = { 'default_class': 'xmodule.raw_module.RawDescriptor', 'fs_root': tempdir.mkdtemp_clean(), 'xblock_mixins': (InheritanceMixin, XModuleMixin, EditInfoMixin) }
XQUEUE_INTERFACE['url'] = 'http://localhost:8040'
for fixture_label in fixture_labels: if self.find_fixtures(fixture_label): break else: return
Xc, Yc, x_mean, y_mean, x_std, y_std =\ pls_._center_scale_xy(X.copy(), Y.copy(), scale=True) assert_array_almost_equal(Xc, np.dot(T, P.T), err_msg="X != TP'") assert_array_almost_equal(Yc, np.dot(U, Q.T), err_msg="Y != UQ'")
self.attributes["commentable_id"] = self.attributes["id"] self.retrieved = True return self
del df['foo'] expected = DataFrame([[4., 5., 'bah', 3], [4., 5., 'bah', 3], [4., 5., 'bah', 3]], columns=['new_col', 'new_col', 'string', 'foo2']) assert_frame_equal(df, expected)
return urllib.unquote(urllib.unquote(response_str))
from salt.modules import cmdmod from salt.exceptions import CommandExecutionError, SaltInvocationError import salt.utils import salt.utils.odict
return dec
var_x_plus_y = var(x + y) var_y = var(y) assert_equal(cov_x_y, 0.5 * (var_x_plus_y - var_x - var_y))
POST_AUTH_PARAMS = ('course_id', 'enrollment_action', 'course_mode', 'email_opt_in')
result = store.select_as_multiple(['df1', 'df2'], where=[Term( 'index>df2.index[4]')], selector='df2') expected = concat([df1, df2], axis=1) expected = expected[5:] tm.assert_frame_equal(result, expected)
from __future__ import absolute_import import os import logging
class ReferencedByGenRel(models.Model): content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE) object_id = models.PositiveIntegerField() content_object = GenericForeignKey('content_type', 'object_id')
cmd_prefix.extend(['apt-get', '-q', '-y']) if kwargs.get('force_yes', False): cmd_prefix.append('--force-yes') if 'force_conf_new' in kwargs and kwargs['force_conf_new']: cmd_prefix += ['-o', 'DPkg::Options::=--force-confnew'] else: cmd_prefix += ['-o', 'DPkg::Options::=--force-confold'] cmd_prefix += ['-o', 'DPkg::Options::=--force-confdef'] if 'install_recommends' in kwargs and not kwargs['install_recommends']: cmd_prefix.append('--no-install-recommends') if 'only_upgrade' in kwargs and kwargs['only_upgrade']: cmd_prefix.append('--only-upgrade') if skip_verify: cmd_prefix.append('--allow-unauthenticated') if fromrepo: cmd_prefix.extend(['-t', fromrepo]) cmd_prefix.append('install')
self._upload_file('auto_reg_enrollment.csv')
self.password = 'test' self.user = UserFactory.create(password=self.password) self.staff = UserFactory.create(password=self.password, is_staff=True)
result = df.drop_duplicates('C') expected = df.iloc[[0, 2]] tm.assert_frame_equal(result, expected) result = df.drop_duplicates('C', keep='last') expected = df.iloc[[-2, -1]] tm.assert_frame_equal(result, expected)
return etree.Element("choiceresponse")
this = self for other in others: if not isinstance(this, DatetimeIndex): this = Index.union(this, other) continue if not isinstance(other, DatetimeIndex): try: other = DatetimeIndex(other) except TypeError: pass this, other = this._maybe_utc_convert(other) if this._can_fast_union(other): this = this._fast_union(other) else: tz = this.tz this = Index.union(this, other) if isinstance(this, DatetimeIndex): this.tz = tz if this.freq is None: this.offset = to_offset(this.inferred_freq) return this
s = Series(["b", "b", "b"]) self.assertRaises(TypeError, lambda: cat > s) self.assertRaises(TypeError, lambda: cat_rev > s) self.assertRaises(TypeError, lambda: s < cat) self.assertRaises(TypeError, lambda: s < cat_rev)
train_scores, test_scores = [], [] partitions = zip(train_sizes, np.split(train, train_sizes)[:-1]) for n_train_samples, partial_train in partitions: train_subset = train[:n_train_samples] X_train, y_train = _safe_split(estimator, X, y, train_subset) X_partial_train, y_partial_train = _safe_split(estimator, X, y, partial_train) X_test, y_test = _safe_split(estimator, X, y, test, train_subset) if y_partial_train is None: estimator.partial_fit(X_partial_train, classes=classes) else: estimator.partial_fit(X_partial_train, y_partial_train, classes=classes) train_scores.append(_score(estimator, X_train, y_train, scorer)) test_scores.append(_score(estimator, X_test, y_test, scorer)) return np.array((train_scores, test_scores)).T
return token.application
try: evaluator(dict(), dict(), answer) return True except (StudentInputError, UndefinedVariable): return False
kwargs['timeout'] = timeout
return ""
__virtualname__ = 'lvm'
self.assertEqual([], p('en-gb;q=1.0000')) self.assertEqual([], p('en;q=0.1234')) self.assertEqual([], p('en;q=.2')) self.assertEqual([], p('abcdefghi-au')) self.assertEqual([], p('**')) self.assertEqual([], p('en,,gb')) self.assertEqual([], p('en-au;q=0.1.0')) self.assertEqual( [], p('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXZ,en') ) self.assertEqual([], p('da, en-gb;q=0.8, en;q=0.7,#')) self.assertEqual([], p('de;q=2.0')) self.assertEqual([], p('de;q=0.a')) self.assertEqual([], p('12-345')) self.assertEqual([], p('')) self.assertEqual([], p('en; q=1,'))
else: return OuterAtomic(using, savepoint, read_committed)
self._assets.extend(asset_name)
from salt.modules import gentoo_service
for solver in ('lbfgs', 'newton-cg', 'liblinear', 'sag'): Cs = [1e3] coefs, Cs, _ = f(logistic_regression_path)( X, y, Cs=Cs, fit_intercept=True, tol=1e-6, solver=solver, intercept_scaling=10000., random_state=0) lr = LogisticRegression(C=Cs[0], fit_intercept=True, tol=1e-4, intercept_scaling=10000., random_state=0) lr.fit(X, y) lr_coef = np.concatenate([lr.coef_.ravel(), lr.intercept_]) assert_array_almost_equal(lr_coef, coefs[0], decimal=4, err_msg="with solver = %s" % solver)
s = OrderedSet() self.assertFalse(s) s.add(1) self.assertTrue(s)
self.assertFalse(linode._validate_name('foo_'))
linklocal_network = IPv6Network('fe80::/10') return self in linklocal_network
class User(models.Model): name = models.CharField(max_length=30) friends = models.ManyToManyField(auth.User)
with self.assertRaisesMessage(CommandError, "No fixture named 'db_fixture_3' found."): management.call_command('loaddata', 'db_fixture_3', verbosity=0) with self.assertRaisesMessage(CommandError, "No fixture named 'db_fixture_3' found."): management.call_command('loaddata', 'db_fixture_3', verbosity=0, using='default') self.assertQuerysetEqual(Article.objects.all(), [])
env['PATH_INFO'] = path.decode(ISO_8859_1) if six.PY3 else path
self.assertRaises(ValueError, self.read_csv, StringIO(data), names=['a', 'b'], usecols=[1], header=None)
field_errors = {}
PIPELINE_JS_COMPRESSOR = None
#html_show_sphinx = True
CONFIG_DIR = os.path.join(ROOT_DIR, 'etc') CACHE_DIR = os.path.join(ROOT_DIR, 'var', 'cache', 'salt') SOCK_DIR = os.path.join(ROOT_DIR, 'var', 'run', 'salt') SRV_ROOT_DIR = os.path.join(ROOT_DIR, 'srv') BASE_FILE_ROOTS_DIR = os.path.join(SRV_ROOT_DIR, 'salt') BASE_PILLAR_ROOTS_DIR = os.path.join(SRV_ROOT_DIR, 'pillar') BASE_THORIUM_ROOTS_DIR = os.path.join(SRV_ROOT_DIR, 'thorium') BASE_MASTER_ROOTS_DIR = os.path.join(SRV_ROOT_DIR, 'salt-master') LOGS_DIR = os.path.join(ROOT_DIR, 'var', 'log', 'salt') PIDFILE_DIR = os.path.join(ROOT_DIR, 'var', 'run') SPM_FORMULA_PATH = os.path.join(ROOT_DIR, 'spm', 'salt') SPM_PILLAR_PATH = os.path.join(ROOT_DIR, 'spm', 'pillar') SPM_REACTOR_PATH = os.path.join(ROOT_DIR, 'spm', 'reactor')
cached_pkg = installer
#pylint: disable=E0602
_config_filename_ = 'proxy' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'proxy')
value = _sanitize_index(value, self.index, copy=False) if not isinstance(value, (np.ndarray, Index)): if isinstance(value, list) and len(value) > 0: value = com._possibly_convert_platform(value) else: value = com._asarray_tuplesafe(value) elif value.ndim == 2: value = value.copy().T else: value = value.copy()
#ret = self.run_function('state.sls', mods='requisites.prereq_recursion_error') #self.assertEqual( #)
self.assertRaises(TypeError, lambda: dt_tz - ts) self.assertRaises(TypeError, lambda: dt_tz - dt) self.assertRaises(TypeError, lambda: dt_tz - ts_tz2) self.assertRaises(TypeError, lambda: dt - dt_tz) self.assertRaises(TypeError, lambda: ts - dt_tz) self.assertRaises(TypeError, lambda: ts_tz2 - ts) self.assertRaises(TypeError, lambda: ts_tz2 - dt) self.assertRaises(TypeError, lambda: ts_tz - ts_tz2)
query_features = [ 'code', 'redeem_code_url', 'course_id', 'company_name', 'created_by', 'redeemed_by', 'invoice_id', 'purchaser', 'customer_reference_number', 'internal_reference', 'is_valid' ]
import salt.utils import salt.utils.itertools import salt.utils.url import salt.fileserver from salt.utils.process import os_is_running as pid_exists from salt.exceptions import FileserverConfigError, GitLockError, get_error_message from salt.utils.event import tagify
super(TestFooter, self).setUp() cache.clear()
if len(cur_state_below) == 1: cur_state_below, = cur_state_below
s = Series(['Wes McKinney', 'Travis Oliphant']) result = s.str.split() expected = ['Travis', 'Oliphant'] self.assertEqual(result[1], expected) result = s.str.rsplit() self.assertEqual(result[1], expected)
if callable(field_name): if field_name.__name__ == '<lambda>': return 'lambda' + str(field_index) else: return field_name.__name__ return field_name
octal = 0 vals['permissions'] = {} if 'r' in comps[2]: octal += 4 vals['permissions']['read'] = True else: vals['permissions']['read'] = False if 'w' in comps[2]: octal += 2 vals['permissions']['write'] = True else: vals['permissions']['write'] = False if 'x' in comps[2]: octal += 1 vals['permissions']['execute'] = True else: vals['permissions']['execute'] = False vals['octal'] = octal
return Response(status=400, data=form_errors)
idx = PeriodIndex(['2011-01', '2011-02', '2011-03', '2011-04'], freq='M', name='idx')
self.assertNotIn('url_name', course_xml.attrib)
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
try: os.makedirs(self._temp_folder) os.chmod(self._temp_folder, FOLDER_PERMISSIONS) except OSError as e: if e.errno != errno.EEXIST: raise e
bookmark_data = self.get_bookmark_data(self.vertical_4) bookmark, __ = Bookmark.create(bookmark_data) bookmark_data['display_name'] = self.vertical_4.display_name_with_default self.assert_bookmark_model_is_valid(bookmark, bookmark_data)
return u'%b %d, %Y'
mock_get.return_value = Mock(status_code=200, text=response_success, content=response_success) transcript_name = transcripts_utils.youtube_video_transcript_name(youtube_text_api) self.assertIsNone(transcript_name)
dbkey['run'] = _id_field['run']
df = DataFrame(np.random.randn(1, 100010), columns=None, index=None) with ensure_clean() as filename: df.to_csv(filename, header=False, index=False) rs = read_csv(filename, header=None) assert_frame_equal(rs, df)
for (key, value) in data.items(): if is_file(value): lines.extend(encode_file(boundary, key, value)) elif not isinstance(value, six.string_types) and is_iterable(value): for item in value: if is_file(item): lines.extend(encode_file(boundary, key, item)) else: lines.extend(to_bytes(val) for val in [ '--%s' % boundary, 'Content-Disposition: form-data; name="%s"' % key, '', item ]) else: lines.extend(to_bytes(val) for val in [ '--%s' % boundary, 'Content-Disposition: form-data; name="%s"' % key, '', value ])
self.functions = salt.loader.minion_mods(self.opts) self.returners = salt.loader.returners(self.opts, self.functions)
self.assertEqual(1, len(nullqs)) self.assertEqual('Puerto Rico', nullqs[0].name)
if is_jail(name): return 'Looks like there was an issue deleteing jail \ {0}'.format(name)
version_guids, id_version_map = self.collect_ids_from_matching_indexes(branch, **kwargs) if not version_guids: return for entry in self.find_course_blocks_by_id(version_guids): for course_index in id_version_map[entry['_id']]: yield entry, course_index
if row_max[-1] == 0: y_i_all_argmax = np.append(y_i_all_argmax, [len(y.data)])
data_scores = [(6311132704823138710, 273), (2685045978526272070, 23), (8921811264899370420, 45), (long(17019687244989530680), 270), (long(9930107427299601010), 273)] dtype = [('uid', 'u8'), ('score', 'u8')] data = np.zeros((len(data_scores),), dtype=dtype) data[:] = data_scores df_crawls = DataFrame(data) self.assertEqual(df_crawls['uid'].dtype, object)
else: columns_all = np.hsplit(X.data, X.indptr[1:-1]) mask_missing_values = _get_mask(X.data, missing_values) mask_valids = np.hsplit(np.logical_not(mask_missing_values), X.indptr[1:-1])
try: cls.connect() except cls.driver.err.OperationalError: raise nose.SkipTest( "{0} - can't connect to MySQL server".format(cls))
self.q(css=self.content_groups_css + " .new-button").first.click()
return subnet
block = self.store.get_item(block_location) self.assertEqual(block.merged_group_access, expected_dict)
self.assertIsNone(res) self.assert_numpy_array_equal(cat.__array__(), np.array([1, 2, 3, 1], dtype=np.int64)) self.assert_index_equal(cat.categories, Index([1, 2, 3]))
import salt.loader from salt.template import compile_template from salt.ext.six import string_types from salt.roster import get_roster_file
return course_key.to_deprecated_string()
c[4] = "a" exp = np.array([0, 1, 2, 0, 0], dtype='int8') self.assert_numpy_array_equal(c.codes, exp) c._codes[4] = 2 exp = np.array([0, 1, 2, 0, 2], dtype='int8') self.assert_numpy_array_equal(c.codes, exp)
if oper in ('=', ''): oper = '==' return oper, verstr
test_content_field = String( help="A content field that will be explicitly set", scope=Scope.content, default="default value" ) test_settings_field = String( help="A settings field that will be explicitly set", scope=Scope.settings, default="default value" )
ret = self.run_run_plus(fun='fileserver.file_list', args=['backend="roots"']) self.assertIsInstance(ret['fun'], list)
return dict((_maybe_box_datetimelike(key), value) for key, value in iteritems(d))
if (is_categorical_dtype(getattr(values, 'dtype', None)) or is_categorical_dtype(dtype)):
if 'width' not in ds_input or 'height' not in ds_input: raise GDALException('Specify width and height attributes for JSON or dict input.')
opts['utils_dirs'] = ( opts.get('utils_dirs') or [os.path.join(opts['extension_modules'], 'utils')] )
self.assertRaises(Exception, store.select_as_multiple, None, where=['A>0', 'B>0'], selector='df1')
self.assertIn("basic", project_state.real_apps)
X, Y = data assert Y is not None batch_size = model.batch_size drop_mask_X = sharedX( model.get_input_space().get_origin_batch(batch_size)) drop_mask_X.name = 'drop_mask' X_space = model.get_input_space() updates = OrderedDict() rval = FixedVarDescr() inputs = [X, Y] if not self.supervised: update_X = self.mask_gen(X, X_space=X_space) else: drop_mask_Y = sharedX(np.ones(batch_size,)) drop_mask_Y.name = 'drop_mask_Y' update_X, update_Y = self.mask_gen(X, Y, X_space) updates[drop_mask_Y] = update_Y rval.fixed_vars['drop_mask_Y'] = drop_mask_Y if self.mask_gen.sync_channels: n = update_X.ndim assert n == drop_mask_X.ndim - 1 update_X.name = 'raw_update_X' zeros_like_X = T.zeros_like(X) zeros_like_X.name = 'zeros_like_X' update_X = zeros_like_X + update_X.dimshuffle(0, 1, 2, 'x') update_X.name = 'update_X' updates[drop_mask_X] = update_X rval.fixed_vars['drop_mask'] = drop_mask_X if hasattr(model.inference_procedure, 'V_dropout'): include_prob = model.inference_procedure.include_prob include_prob_V = model.inference_procedure.include_prob_V include_prob_Y = model.inference_procedure.include_prob_Y theano_rng = make_theano_rng(None, 2012 + 10 + 20, which_method="binomial") for elem in flatten([model.inference_procedure.V_dropout]): updates[elem] = theano_rng.binomial(p=include_prob_V, size=elem.shape, dtype=elem.dtype, n=1) / include_prob_V if "Softmax" in str(type(model.hidden_layers[-1])): hid = model.inference_procedure.H_dropout[:-1] y = model.inference_procedure.H_dropout[-1] updates[y] = theano_rng.binomial(p=include_prob_Y, size=y.shape, dtype=y.dtype, n=1) / include_prob_Y else: hid = model.inference_procedure.H_dropout for elem in flatten(hid): updates[elem] = theano_rng.binomial(p=include_prob, size=elem.shape, dtype=elem.dtype, n=1) / include_prob rval.on_load_batch = [utils.function(inputs, updates=updates)] return rval
with self.assertRaises(NotImplementedError): self.client.ajax_post( self.url, data={'invalid_request': None}, )
__virtualname__ = 'memcache'
filterspec = changelist.get_filters(request)[0][3] self.assertEqual(force_text(filterspec.title), 'is best seller') choice = select_by(filterspec.choices(changelist), "display", "Unknown") self.assertEqual(choice['selected'], True) self.assertEqual(choice['query_string'], '?is_best_seller__isnull=True')
course_key = CourseKey.from_string(course_id) task = instructor_task.api.generate_certificates_for_students(request, course_key) message = _('Certificate generation task for all students of this course has been started. ' 'You can view the status of the generation task in the "Pending Tasks" section.') response_payload = { 'message': message, 'task_id': task.task_id } return JsonResponse(response_payload)
('no_overrides', 1, True, False): (48, 1, 6, 13), ('no_overrides', 2, True, False): (120, 16, 6, 84), ('no_overrides', 3, True, False): (400, 81, 6, 335), ('ccx', 1, True, False): (48, 1, 6, 13), ('ccx', 2, True, False): (120, 16, 6, 84), ('ccx', 3, True, False): (400, 81, 6, 335), ('ccx', 1, True, True): (47, 1, 6, 13), ('ccx', 2, True, True): (119, 16, 6, 84), ('ccx', 3, True, True): (399, 81, 6, 335), ('no_overrides', 1, False, False): (48, 1, 6, 13), ('no_overrides', 2, False, False): (120, 16, 6, 84), ('no_overrides', 3, False, False): (400, 81, 6, 335), ('ccx', 1, False, False): (48, 1, 6, 13), ('ccx', 2, False, False): (120, 16, 6, 84), ('ccx', 3, False, False): (400, 81, 6, 335), ('ccx', 1, False, True): (47, 1, 6, 13), ('ccx', 2, False, True): (119, 16, 6, 84), ('ccx', 3, False, True): (399, 81, 6, 335),
while token.contents.startswith('elif'): bits = token.split_contents()[1:] condition = TemplateIfParser(parser, bits).parse() nodelist = parser.parse(('elif', 'else', 'endif')) conditions_nodelists.append((condition, nodelist)) token = parser.next_token()
try: if check_user(self.config['user']): pr = activate_profile(profiling_enabled) try: ret = runner.run() if isinstance(ret, dict) and 'retcode' in ret.get('data', {}): self.exit(ret['data']['retcode']) finally: output_profile( pr, stats_path=self.options.profiling_path, stop=True)
self._create_video_component() self.edit_component() self.open_advanced_tab() self.video.upload_translation('uk_transcripts.srt', 'uk') self.video.upload_translation('chinese_transcripts.srt', 'zh') self.save_unit_settings() self.assertTrue(self.video.is_captions_visible()) unicode_text = "Привіт, edX вітає вас.".decode('utf-8') self.assertIn(unicode_text, self.video.captions_text) self.assertEqual(self.video.caption_languages.keys(), ['zh', 'uk']) self.edit_component() self.open_advanced_tab() self.assertEqual(self.video.translations(), ['zh', 'uk']) self.video.remove_translation('uk') self.save_unit_settings() self.assertTrue(self.video.is_captions_visible()) unicode_text = "好 各位同学".decode('utf-8') self.assertIn(unicode_text, self.video.captions_text) self.edit_component() self.open_advanced_tab() self.assertEqual(self.video.translations(), ['zh']) self.video.remove_translation('zh') self.save_unit_settings() self.assertFalse(self.video.is_captions_visible())
_options['checktype'] = '1'
for k, v in result_json.items(): if isinstance(v, list): result_json[k] += next_page_results[k]
else:
has_dimension_fields = self.width_field or self.height_field if not has_dimension_fields: return
self.assertEqual(iptables.build_rule(jump='REDIRECT', **{'to-port': 8080}), '--jump REDIRECT --to-port 8080')
self.read_csv(self.csv1, memory_map=True)
signal_handler.reset_mock() with self.store.bulk_operations(course_key): categories = DIRECT_ONLY_CATEGORIES for block_type in categories: log.debug('Testing with block type %s', block_type) block = self.store.create_item(self.user_id, course_key, block_type) signal_handler.send.assert_not_called()
self.assertOLXIsDraftOnly(block_list_to_revert) self.publish(block_list_to_revert) self.assertOLXIsPublishedOnly(block_list_to_revert) self.revert_to_published(block_list_to_revert) self.assertOLXIsPublishedOnly(block_list_to_revert)
sl = self.frame[:20] tm.assert_sp_frame_equal(sl, self.frame.reindex(self.frame.index[:20]))
pattern_esc = r"REPLACE(REPLACE(REPLACE({}, '\', '\\'), '%%', '\%%'), '_', '\_')" _pattern_ops = { 'contains': "'%%' || {} || '%%'", 'icontains': "'%%' || UPPER({}) || '%%'", 'startswith': "{} || '%%'", 'istartswith': "UPPER({}) || '%%'", 'endswith': "'%%' || {}", 'iendswith': "'%%' || UPPER({})", }
for name in FOREST_CLASSIFIERS: yield check_gridsearch, name
if isinstance(obj, cls): err_msg = "{0}Input must not be type {1}" raise AssertionError(err_msg.format(msg, cls))
sa_4326 = 'POINT (-98.493183 29.424170)'
field = lambda fn: self.q(css='.wrapper-create-course #new-course-{}'.format(fn)) field('name').fill(display_name) field('org').fill(org) field('number').fill(number) field('run').fill(run)
client_kwargs['version'] = 'auto'
td = timedelta(minutes=5, seconds=3) resulta = df['A'] + td resultb = resulta - td assert_series_equal(df['A'], resultb) self.assertEqual(resultb.dtype, 'M8[ns]')
assert_raises(ValueError, auc, [0.0, 0.5, 1.0], [0.1, 0.2])
if sorted(os.getgroups()) != sorted(supgroups): try: os.setgroups(supgroups) except OSError as err: raise CommandExecutionError( 'Failed to set supplemental groups to {0}. Error: {1}'.format( supgroups, err ) )
result._data = result._data.downcast()
self._create_video_component() self.edit_component() self.video.set_url_field('t_not_exist.mp4', 1) self.assertEqual(self.video.message('status'), 'No Timed Transcript') self.video.upload_transcript('chinese_transcripts.srt') self.assertEqual(self.video.message('status'), 'Timed Transcript Uploaded Successfully') self.save_unit_settings() unicode_text = "好 各位同学".decode('utf-8') self.assertIn(unicode_text, self.video.captions_text) self.edit_component() self.open_advanced_tab() self.video.set_field_value('Default Timed Transcript', '') self.open_basic_tab() self.assertEqual(self.video.message('status'), 'No Timed Transcript') self.save_unit_settings() self.assertFalse(self.video.is_captions_visible()) self.edit_component() self.assertEqual(self.video.message('status'), 'No Timed Transcript') self.assertTrue(self.video.verify_field_value('Default Timed Transcript', ''))
rng = date_range(START, END, freq=datetools.bmonthEnd)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'joyent', vm_['profile'], vm_=vm_) is False: return False
return [('127.0.0.1:11211 (1)', {})]
from salttesting import skipIf, TestCase from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
actions = self.get_actions(request) if actions: list_display = ['action_checkbox'] + list(list_display)
import logging import re
(('custom',), {}, '<label for="id_field">custom:</label>'),
store.open('r') self.assertTrue(store.is_open) self.assertEqual(len(store), 1) self.assertEqual(store._mode, 'r') store.close() self.assertFalse(store.is_open)
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../') import integration
try: ret = self.run_function( 'state.sls', mods='issue-2068-template-str', timeout=120 ) self.assertSaltTrueReturn(ret) finally: if os.path.isdir(venv_dir): shutil.rmtree(venv_dir)
@property def info(self): "Return information about the GeoIP library and databases in use." meta = self._reader.metadata() return 'GeoIP Library:\n\t%s.%s\n' % (meta.binary_format_major_version, meta.binary_format_minor_version)
ipca = IncrementalPCA(n_components=2, batch_size=10).fit(X) Y = ipca.transform(X) Y_inverse = ipca.inverse_transform(Y) assert_almost_equal(X, Y_inverse, decimal=3)
xml = ''.join(line.strip() for line in xml.split('\n')) factory = self.capa_factory_for_problem_xml(xml) module = factory.create()
log.error( 'Unable to resolve %s from %s remote \'%s\' ' 'to either an annotated or non-annotated tag', tag_ref, self.role, self.id ) return None
wait = None
model = models.IntegerField()
V_name = 'V' if hasattr(V, 'name') and V.name is not None: V_name = V.name rval = T.nnet.sigmoid( \ ( \ self.bias_hid + \ self.transformer.lmul(V) \ ) / T.sqr(self.sigma) \ ) rval.name = 'mean_H_given_V( %s )' % V_name return rval
with tm.assertRaises(TypeError): Timestamp(year=2000, month=1) with tm.assertRaises(ValueError): Timestamp(year=2000, month=0, day=1) with tm.assertRaises(ValueError): Timestamp(year=2000, month=13, day=1) with tm.assertRaises(ValueError): Timestamp(year=2000, month=1, day=0) with tm.assertRaises(ValueError): Timestamp(year=2000, month=1, day=32)
AutoAuthPage(self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id, staff=True).visit()
from __future__ import unicode_literals
@property def _constructor(self): return TimedeltaIndexResampler
self.X_memmap_info = None self.y_memmap_info = None
html = etree.tostring(xpath_node).strip() inner_html = re.sub('(?ms)<%s[^>]*>(.*)</%s>' % (xpath_node.tag, xpath_node.tag), '\\1', html) return inner_html.strip()
('defunct-cart', 'defunct-cart'),
mean_dist = np.mean(pairwise_distances(query, X, metric='cosine')) neighbors = lshf.radius_neighbors(query, radius=mean_dist, return_distance=False)
__BACKUP_ATTRIBUTE_NAME = '__monkey_patch'
emit_field_changed_events( user, user, sender._meta.db_table, excluded_fields=['last_login', 'first_name', 'last_name'], hidden_fields=['password'] )
result = ts - ts expected = Timedelta('0 days') _check(result, expected)
Site.objects.clear_cache()
self.assertEqual(marty._state.db, 'default') self.assertEqual(pro._state.db, 'default') self.assertEqual(dive._state.db, 'other') self.assertEqual(mark._state.db, 'other')
test_organization_data = { 'name': 'test organization', 'short_name': 'test_organization', 'description': 'Test Organization Description', 'active': True, 'logo': '/logo_test1.png/' } test_org = organizations_api.add_organization(organization_data=test_organization_data) organizations_api.add_organization_course(organization_data=test_org, course_id=unicode(self.course.id)) self._add_course_certificates(count=1, signatory_count=1, is_active=True) test_url = get_certificate_url( user_id=self.user.id, course_id=unicode(self.course.id) ) response = self.client.get(test_url) self.assertIn( 'a course of study offered by test_organization, an online learning initiative of test organization', response.content ) self.assertNotIn( 'a course of study offered by testorg', response.content ) self.assertIn( '<title>test_organization {} Certificate |'.format(self.course.number, ), response.content ) self.assertIn('logo_test1.png', response.content)
method = '.' + method if how is not None else ''
query = "SELECT * FROM raw_query_author WHERE first_name = %s" author = Author.objects.all()[2] params = [author.first_name] qset = Author.objects.raw(query, params=params) results = list(qset) self.assertProcessed(Author, results, [author]) self.assertNoAnnotations(results) self.assertEqual(len(results), 1) self.assertIsInstance(repr(qset), str)
return self._topology(capi.geos_linemerge(self.ptr))
alert = get_modal_alert(self.student_admin_section.browser) alert.dismiss()
self.assertFalse(self._has_changes(parent.location)) self.assertFalse(self._has_changes(child.location))
self.assertTrue(Article.objects.filter(headline=old_headline)) self.assertFalse(Article.objects.filter(headline=new_headline))
def dumps(self, obj): return pickle.dumps(obj, pickle.HIGHEST_PROTOCOL) def loads(self, data): return pickle.loads(data)
fit_params = fit_params if fit_params is not None else {} fit_params = dict([(k, _index_param_value(X, v, train)) for k, v in fit_params.items()])
y_type = type_of_target(y) if y_type not in ['binary', 'multiclass', 'multiclass-multioutput', 'multilabel-indicator', 'multilabel-sequences']: raise ValueError("Unknown label type: %r" % y_type)
ts[datetime(2000, 1, 6)] = 0 self.assertEqual(ts[datetime(2000, 1, 6)], 0)
qs = BaseA.objects.filter(a__f1='foo') self.assertEqual(str(qs.query).count('INNER JOIN'), 1) qs = qs.filter(Q(b__f1='foo') | Q(b__f2='foo')) self.assertEqual(str(qs.query).count('INNER JOIN'), 2) qs = BaseA.objects.filter(Q(b__f1='foo') | Q(b__f2='foo')) self.assertEqual(str(qs.query).count('INNER JOIN'), 1) qs = qs.filter(a__f1='foo') self.assertEqual(str(qs.query).count('INNER JOIN'), 2)
return self._div_for_xblock_id(xblock_id)[0].find_element_by_css_selector( '.header-actions .{action}-button.action-button'.format(action=action) )
clone_feature = voidptr_output(lgdal.OGR_F_Clone, [c_void_p]) destroy_feature = void_output(lgdal.OGR_F_Destroy, [c_void_p], errcheck=False) feature_equal = int_output(lgdal.OGR_F_Equal, [c_void_p, c_void_p]) get_feat_geom_ref = geom_output(lgdal.OGR_F_GetGeometryRef, [c_void_p]) get_feat_field_count = int_output(lgdal.OGR_F_GetFieldCount, [c_void_p]) get_feat_field_defn = voidptr_output(lgdal.OGR_F_GetFieldDefnRef, [c_void_p, c_int]) get_fid = int_output(lgdal.OGR_F_GetFID, [c_void_p]) get_field_as_datetime = int_output( lgdal.OGR_F_GetFieldAsDateTime, [c_void_p, c_int, c_int_p, c_int_p, c_int_p, c_int_p, c_int_p, c_int_p] ) get_field_as_double = double_output(lgdal.OGR_F_GetFieldAsDouble, [c_void_p, c_int]) get_field_as_integer = int_output(lgdal.OGR_F_GetFieldAsInteger, [c_void_p, c_int]) if GDAL_VERSION >= (2, 0): get_field_as_integer64 = int64_output(lgdal.OGR_F_GetFieldAsInteger64, [c_void_p, c_int]) get_field_as_string = const_string_output(lgdal.OGR_F_GetFieldAsString, [c_void_p, c_int]) get_field_index = int_output(lgdal.OGR_F_GetFieldIndex, [c_void_p, c_char_p])
user=user, course_id=course.location.course_key, status=CertificateStatuses.downloadable
with self.assertRaises(InvalidSessionKey): self.backend()._key_to_file("a/b/c")
if existing['code'] == 200:
from __future__ import unicode_literals
plt.subplot(2, 1, 2) plt.plot(coef, label='True coef') plt.plot(coef_, label='Estimated coef') plt.legend() plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.26) plt.show()
self.update_structure(parent_usage_key.course_key, new_structure)
from __future__ import absolute_import
session_user_id = SafeSessionMiddleware.get_user_id_from_session(request)
self.assertTrue(self.cohort_management_page.is_category_selected())
for column in six.iteritems(self.get_indexes(cursor, table_name)): if column[1]['primary_key']: return column[0] return None
err_msg = None
if hasattr(obj, '__name__'): return getattr(obj, '__name__') if isinstance(obj, partial): return _get_callable_name(obj.func) if hasattr(obj, '__call__'): return obj.__class__.__name__ return None
print("Computing regularization path using the coordinate descent lasso...") t1 = time.time() model = LassoCV(cv=20).fit(X, y) t_lasso_cv = time.time() - t1
import glob import shutil import logging import os
def dispatcher(self, obj): reduced = reduce_func(obj) self.save_reduce(obj=obj, *reduced) self.dispatch[type] = dispatcher
PROFILE_IMAGE_BACKEND = { 'class': 'storages.backends.overwrite.OverwriteStorage', 'options': { 'location': os.path.join(MEDIA_ROOT, 'profile-images/'), 'base_url': os.path.join(MEDIA_URL, 'profile-images/'), }, }
rng = np.random.RandomState(42) sample_weight = rng.randint(0, 5, X.shape[0]) core1, label1 = dbscan(X, sample_weight=sample_weight) assert_equal(len(label1), len(X))
for tz in ['US/Eastern', 'Asia/Tokyo']: idx = pd.DatetimeIndex(['2011-01-01 09:00', pd.NaT, '2011-01-01 11:00'])
with store.bulk_operations(xblock.location.course_key):
return xblock._edit_info.get('published_by')
items = list(items) return (items[i:i + chunk_size] for i in xrange(0, len(items), chunk_size))
print(metrics.classification_report(y_test, y_predicted, target_names=dataset.target_names))
for iface in [a for a in new]: ndata = new[iface] nmac = ndata.get('lxc.network.hwaddr', '') ntype = ndata.get('lxc.network.type', '') omac, otype = '', '' if iface in old: odata = old[iface] omac = odata.get('lxc.network.hwaddr', '') otype = odata.get('lxc.network.type', '') if otype and not ntype: ntype = otype if not ntype: ntype = 'veth' new[iface]['lxc.network.type'] = ntype if omac and not nmac: new[iface]['lxc.network.hwaddr'] = omac
FEATURES['AUTOMATIC_VERIFY_STUDENT_IDENTITY_FOR_TESTING'] = True
row_items = [E.TD(x) for x in items] self.table.append(E.TR(*row_items))
return self._fit(X, compute_sources=True)
if tabs is None: tabs = [] course = collections.namedtuple('MockCourse', ['tabs']) if isinstance(tabs, basestring): course.tabs = self.get_tab_type_dicts(tabs) else: course.tabs = tabs return course
world.wait_for_visible('#data-student-profiles-table')
from __future__ import unicode_literals
conf_list = [] conf_dict = {} for line in full_conf.splitlines(): if not line.strip() or line.strip().startswith('#'): conf_list.append(line) continue comps = line.strip().split() conf_line = { 'service': comps[0], 'conn_type': comps[1], 'private': comps[2], 'unpriv': comps[3], 'chroot': comps[4], 'wakeup': comps[5], 'maxproc': comps[6], 'command': ' '.join(comps[7:]), } dict_key = '{0} {1}'.format(comps[0], comps[1]) conf_list.append(conf_line) conf_dict[dict_key] = conf_line
X2 = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0], [1.0, 1.0], [1.0, 0.0]]) y2 = [1, 1, 1, -1, -1]
return Packer(default=default, encoding=encoding, unicode_errors=unicode_errors, use_single_float=use_single_float, autoreset=autoreset, use_bin_type=use_bin_type).pack(o)
subobj.main_loop(time_budget=time_budget)
if ac_only is not None: task_definition.Settings.DisallowStartIfOnBatteries = ac_only if stop_if_on_batteries is not None: task_definition.Settings.StopIfGoingOnBatteries = stop_if_on_batteries if wake_to_run is not None: task_definition.Settings.WakeToRun = wake_to_run
self.q(css=self._bounded_selector('a.action-view')).first.click()
class Meta(object): list_serializer_class = BulkTeamCountTopicListSerializer
data = self.frame.unstack()
return tuple(itertools.chain([int(a[0]) for a in self.non_index_axes], [int(a.axis) for a in self.index_axes]))
n_samples = 200
self.client.login(username=self.global_staff.username, password='test') url = reverse('start_certificate_regeneration', kwargs={'course_id': unicode(self.course.id)}) response = self.client.post(url)
continue
title_re = re.compile(sixu('^\\s*[#*=]{4,}\\n[a-z0-9 -]+\\n[#*=]{4,}\\s*'), re.I|re.S) lines[:] = title_re.sub(sixu(''), sixu("\n").join(lines)).split(sixu("\n"))
var_x = var(x) std_x = std(x) self.assertFalse((var_x < 0).any().any()) self.assertFalse((std_x < 0).any().any()) if cov: cov_x_x = cov(x, x) self.assertFalse((cov_x_x < 0).any().any())
for parent in self.model.mro()[1:]: if hasattr(parent, '_meta'): default_manager_name = parent._meta.default_manager_name break
rnn = RNN(input_space=SequenceSpace(VectorSpace(dim=1)), layers=[Recurrent(dim=1, layer_name='recurrent', irange=0, nonlinearity=lambda x: x), Linear(dim=1, layer_name='linear', irange=0)]) X_data, X_mask = rnn.get_input_space().make_theano_batch() y_data, y_mask = rnn.get_output_space().make_theano_batch() default_cost = Default() cost = default_cost.expr(rnn, ((X_data, X_mask), (y_data, y_mask))) tensor.grad(cost, rnn.get_params(), disconnected_inputs='ignore')
def db_for_read(self, model, instance=None, **hints): if instance: return instance._state.db or 'other' return 'other' def db_for_write(self, model, **hints): return DEFAULT_DB_ALIAS def allow_relation(self, obj1, obj2, **hints): return obj1._state.db in ('default', 'other') and obj2._state.db in ('default', 'other') def allow_migrate(self, db, app_label, **hints): return True
axes = _check_plot_works( df.hist, column='height', by='classroom', layout=(2, 2)) self._check_axes_shape(axes, axes_num=3, layout=(2, 2))
try: return self.modules[usage_key.course_key][usage_key] except KeyError: raise ItemNotFoundError(usage_key)
self.key = key self.value = value self.time = time self.min_compress_len = min_compress_len return True
score = scores_client.get(problem_descriptor.location) cached_max_score = max_scores_cache.get(problem_descriptor.location) if score and score.total is not None: correct = score.correct if score.correct is not None else 0.0 total = score.total elif cached_max_score is not None and settings.FEATURES.get("ENABLE_MAX_SCORE_CACHE"): correct = 0.0 total = cached_max_score else: problem = module_creator(problem_descriptor) if problem is None: return (None, None)
with tm.assert_produces_warning(None): for idx1, val in [(fidx1, 3), (didx1, datetime(2014, 3, 1))]: result = idx1 < val expected = np.array([True, False, False, False, False, False]) self.assert_numpy_array_equal(result, expected) result = idx1 > val expected = np.array([False, False, False, False, True, True]) self.assert_numpy_array_equal(result, expected)
_random_seed = os.environ.get('SKLEARN_SEED', None) if _random_seed is None: _random_seed = np.random.uniform() * (2 ** 31 - 1) _random_seed = int(_random_seed) print("I: Seeding RNGs with %r" % _random_seed) np.random.seed(_random_seed) random.seed(_random_seed)
ProxyCategory.objects.create() qs = ProxyCategory.objects.all() self.assertEqual(qs.count(), 1) str(qs.query) self.assertEqual(qs.count(), 1)
features_in_first_mlp = 5 features_in_second_mlp = 10 targets_in_first_mlp = 2 targets_in_second_mlp = 2
random_state = check_random_state(0) y_true = random_state.randint(0, 2, size=(n_samples, )) y_pred = random_state.randint(0, 2, size=(n_samples, )) y_score = random_state.random_sample(size=(n_samples,)) for name in ALL_METRICS: if (name in METRICS_WITHOUT_SAMPLE_WEIGHT or name in METRIC_UNDEFINED_BINARY): continue metric = ALL_METRICS[name] if name in THRESHOLDED_METRICS: yield check_sample_weight_invariance, name, metric, y_true, y_score else: yield check_sample_weight_invariance, name, metric, y_true, y_pred
with patch('sys.argv', ['salt-call']): ret = utils.daemonize_if({}) self.assertEqual(None, ret)
qedec_date = Period(freq="Q-DEC", year=2007, quarter=1) qejan_date = Period(freq="Q-JAN", year=2007, quarter=1) qejun_date = Period(freq="Q-JUN", year=2007, quarter=1) for x in range(3): for qd in (qedec_date, qejan_date, qejun_date): self.assertEqual((qd + x).qyear, 2007) self.assertEqual((qd + x).quarter, x + 1)
from __future__ import unicode_literals
from salt.utils.async import SyncWrapper
provider = generics.get_object_or_404(CreditProvider, provider_id=provider_id)
new_io = six.StringIO() call_command( "createsuperuser", interactive=False, email="joe@somewhere.org", date_of_birth="1976-04-01", stdout=new_io, ) command_output = new_io.getvalue().strip() self.assertEqual(command_output, 'Superuser created successfully.') u = CustomUser._default_manager.get(email="joe@somewhere.org") self.assertEqual(u.date_of_birth, date(1976, 4, 1))
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath, MockWraps from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch ensure_in_syspath('../')
self.assertTrue(len(ax.get_children()) > 0)
from __future__ import absolute_import
gmm = mixture.GaussianMixture(n_components=10, covariance_type='full', max_iter=100).fit(X) plot_results(X, gmm.predict(X), gmm.means_, gmm.covariances_, 0, 'Expectation-maximization')
self.user = UserFactory.build(username='test', email='test@edx.org') self.user.set_password('test_password') self.user.save()
input_dict = {'1_2_1': '0'} correct_map = problem.grade_answers(input_dict)
paramvalues['binddn'] = _render_template(paramvalues['binddn'], username) paramvalues['binddn'] = ldap.filter.escape_filter_chars(paramvalues['binddn'])
for url_name in self.EXPECTED_URL_NAMES: self.assertContains(response, reverse(url_name))
nv.validate_window_func('var', args, kwargs) def f(arg): return algos.ewmcov(arg, arg, self.com, int(self.adjust), int(self.ignore_na), int(self.min_periods), int(bias)) return self._apply(f, **kwargs)
if id_ not in minion_timeouts: minion_timeouts[id_] = time.time() + timeout
resp = self.client.post( self.postback_url, self.student_answers ) self.assertEquals(resp.status_code, 200) data = json.loads(resp.content) self.assertIn('redirect_url', data) answers = self.survey.get_answers(self.student) self.assertEquals(answers[self.student.id], self.student_answers)
def __init__(self, *args): self.args = args
data = self._check_column_names(data)
message = self.textbook_page.get_element_text('.wrapper-content .no-textbook-content') self.assertIn("You haven't added any textbooks", message)
if user.is_anonymous(): return None
def add_fields(self, form, index): super(BaseCustomDeleteFormSet, self).add_fields(form, index) self.can_delete = True if DELETION_FIELD_NAME in form.fields: del form.fields[DELETION_FIELD_NAME] def _should_delete_form(self, form): return hasattr(form, 'should_delete') and form.should_delete()
for conn in connections.all(): conn.close()
return self.modulestore.get_library(self.courselike_key, depth=None, lazy=False)
from sklearn.cluster.k_means_ import _init_centroids X_norms = np.sum(X**2, axis=1) precompute = _init_centroids( X, 3, "k-means++", random_state=0, x_squared_norms=X_norms) assert_array_equal( precompute, _init_centroids(X, 3, "k-means++", random_state=0))
return 'For analytics about your course, go to <a href="http://example.com/courses/{}" ' \ 'target="_blank">Example</a>.'.format(unicode(self.course.id))
try: return self[key] except KeyError: return default
self.assertTrue(timezone.is_naive(dt))
self.user = User.objects.get(pk=self.user.pk)
self.assertSetEqual({p.ewkt for p in ref_u1}, {p.ewkt for p in u1}) self.assertSetEqual({p.ewkt for p in ref_u2}, {p.ewkt for p in u2}) self.assertSetEqual({p.ewkt for p in ref_u1}, {p.ewkt for p in u3})
self.session.delete()
vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max() for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()): ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin, vmax=.5 * vmax) ax.set_xticks(()) ax.set_yticks(())
queryset = self.get_queryset(request) model = queryset.model field = model._meta.pk if from_field is None else model._meta.get_field(from_field) try: object_id = field.to_python(object_id) return queryset.get(**{field.name: object_id}) except (model.DoesNotExist, ValidationError, ValueError): return None
return store.create_item(user.id, usage_key.course_key, usage_key.block_type, block_id=usage_key.block_id)
return super(FunctionDocumenter, self).format_name()
self.obj._consolidate_inplace() self.obj._data = self.obj._data.setitem(indexer=indexer, value=value) self.obj._maybe_update_cacher(clear=True)
ret = grains.present( name='foo,is,nested', value=['l1', 'l2'], delimiter=',') self.assertEqual(ret['result'], False) self.assertEqual(ret['changes'], {}) self.assertEqual(ret['comment'], 'The key \'foo:is:nested\' exists and the ' + 'given value is a dict or a list. ' + 'Use \'force=True\' to overwrite.') self.assertEqual( grains.__grains__, {'a': 'aval', 'foo': {'is': {'nested': 'bar'}}})
project_state = ProjectState() project_state.add_model(ModelState.from_model(Magazine)) msg = ( "The field migrations.Magazine.authors was declared with a lazy reference " "to 'migrations.author\', but app 'migrations' doesn't provide model 'author'.\n" "The field migrations.Magazine_authors.author was declared with a lazy reference " "to \'migrations.author\', but app 'migrations' doesn't provide model 'author'." ) with self.assertRaisesMessage(ValueError, msg): project_state.apps
os.remove(CONFIG)
problem = new_loncapa_problem(xml_str)
module_class = SplitTestModule
('svc', SVC(kernel='linear')),
S = func([[1]], metric='precomputed') assert_true(isinstance(S, np.ndarray))
func_ret = __salt__['service.stop'](name) if not func_ret: ret['result'] = False ret['comment'] = 'Service {0} failed to die'.format(name) if enable is True: ret.update(_enable(name, True, result=False, **kwargs)) elif enable is False: ret.update(_disable(name, True, result=False, **kwargs)) else: ret['comment'] = 'Service {0} was killed'.format(name) if enable is True: ret.update(_enable(name, False, **kwargs)) elif enable is False: ret.update(_disable(name, False, **kwargs)) after_toggle_status = __salt__['service.status'](name) if 'service.enabled' in __salt__: after_toggle_enable_status = __salt__['service.enabled'](name) else: after_toggle_enable_status = True if ( (before_toggle_enable_status != after_toggle_enable_status) or (before_toggle_status != after_toggle_status) ) and not ret.get('changes', {}): ret['changes'][name] = func_ret return ret
self.q(css=self.EXPAND_COLLAPSE_CSS).click()
username = user.get('user', user.get('name')) if username: if username == name: return True else: log.warning('Could not find username in user: %s', user)
bandwidth = cluster.estimate_bandwidth(X, quantile=0.3)
s = Series([None, pd.NaT, '2013-08-05 15:30:00.000001']) self.assertEqual(s.dtype, 'datetime64[ns]') s = Series([np.nan, pd.NaT, '2013-08-05 15:30:00.000001']) self.assertEqual(s.dtype, 'datetime64[ns]') s = Series([pd.NaT, None, '2013-08-05 15:30:00.000001']) self.assertEqual(s.dtype, 'datetime64[ns]') s = Series([pd.NaT, np.nan, '2013-08-05 15:30:00.000001']) self.assertEqual(s.dtype, 'datetime64[ns]')
act.actor.lane_stack.value.server.close() testStack = self.store.fetch('.salt.test.lane.stack') if testStack: testStack.value.server.close()
if 'transport' in opts: ttype = opts['transport'] elif 'transport' in opts.get('pillar', {}).get('master', {}): ttype = opts['pillar']['master']['transport']
CountryAccessRule.objects.create( restricted_course=self.restricted_course, country=self.countries['US'], rule_type=CountryAccessRule.WHITELIST_RULE )
query = 'objects:' pos = searchindex.find(query) if pos < 0: raise ValueError('"objects:" not found in search index')
return GeoIP_record_by_addr(self._city, c_char_p(enc_query))
transformers = BlockStructureTransformers() if user is not None: transformers += COURSE_BLOCK_ACCESS_TRANSFORMERS + [ProctoredExamTransformer()] transformers += [ BlocksAPITransformer( block_counts, student_view_data, depth, nav_depth ) ]
self.assertUgettext('Date/time', 'Datum/Zeit')
context['show_homepage_promo_video'] = microsite.get_value('show_homepage_promo_video', False)
spmatrix = spmatrix.astype(dtype)
with assert_raises(ValueError): course = self.process_xml(CourseFactory.build(policy={'days_early_for_beta': 'null'}))
from __future__ import absolute_import
STATIC_URL = '/static/' STATIC_ROOT = ENV_ROOT / "staticfiles"
old_model_name = self.renamed_models.get((app_label, model_name), model_name) old_field_name = self.renamed_fields.get((app_label, model_name, field_name), field_name) old_field = self.old_apps.get_model(app_label, old_model_name)._meta.get_field(old_field_name) new_field = self.new_apps.get_model(app_label, model_name)._meta.get_field(field_name) if hasattr(new_field, "remote_field") and getattr(new_field.remote_field, "model", None): rename_key = ( new_field.remote_field.model._meta.app_label, new_field.remote_field.model._meta.model_name, ) if rename_key in self.renamed_models: new_field.remote_field.model = old_field.remote_field.model if hasattr(new_field, "remote_field") and getattr(new_field.remote_field, "through", None): rename_key = ( new_field.remote_field.through._meta.app_label, new_field.remote_field.through._meta.model_name, ) if rename_key in self.renamed_models: new_field.remote_field.through = old_field.remote_field.through old_field_dec = self.deep_deconstruct(old_field) new_field_dec = self.deep_deconstruct(new_field) if old_field_dec != new_field_dec: both_m2m = old_field.many_to_many and new_field.many_to_many neither_m2m = not old_field.many_to_many and not new_field.many_to_many if both_m2m or neither_m2m: preserve_default = True if (old_field.null and not new_field.null and not new_field.has_default() and not new_field.many_to_many): field = new_field.clone() new_default = self.questioner.ask_not_null_alteration(field_name, model_name) if new_default is not models.NOT_PROVIDED: field.default = new_default preserve_default = False else: field = new_field self.add_operation( app_label, operations.AlterField( model_name=model_name, name=field_name, field=field, preserve_default=preserve_default, ) ) else: self._generate_removed_field(app_label, model_name, field_name) self._generate_added_field(app_label, model_name, field_name)
if not isnull(self.fill_value): shifted = self.to_dense().shift(periods, freq=freq, axis=axis) return shifted.to_sparse(fill_value=self.fill_value, kind=self.kind)
DATE_FORMAT = 'j E Y г.' TIME_FORMAT = 'G:i' DATETIME_FORMAT = 'j E Y г. G:i' YEAR_MONTH_FORMAT = 'F Y г.' MONTH_DAY_FORMAT = 'j F' SHORT_DATE_FORMAT = 'd.m.Y' SHORT_DATETIME_FORMAT = 'd.m.Y H:i'
lis_outcome_service_url = models.CharField(max_length=255, unique=True) lti_consumer = models.ForeignKey(LtiConsumer)
assert_index_equal(left.columns, right.columns, exact=check_column_type, check_names=check_names, check_less_precise=check_less_precise, check_exact=check_exact, check_categorical=check_categorical, obj='{0}.columns'.format(obj))
ml_cost = (model.free_energy(pos_v).mean() - model.free_energy(neg_v).mean())
self.check_result('neg int', 'iloc', -1, 'ix', {0: 6, 1: 9, 2: 12}, typs=['ints']) self.check_result('neg int', 'iloc', -1, 'indexer', -1, typs=['labels', 'mixed', 'ts', 'floats', 'empty'], fails=IndexError)
q_obj = ( Q(d__name='foo') | Q(b__name='foo') | Q(b__c__name='foo') ) qset = ModelA.objects.filter(q_obj) self.assertEqual(list(qset), [self.a1]) self.assertEqual(str(qset.query).count('INNER JOIN'), 1)
df = pd.DataFrame({'i': [0] * 3, 'b': [False] * 3}) vc = df.i.value_counts() result = vc.get(99, default='Missing') self.assertEqual(result, 'Missing')
if not os.path.isfile(path): raise SaltInvocationError('File not found: {0}'.format(path))
instructor = self.make_instructor() self.assertTrue(CourseInstructorRole(self.course.id).has_user(instructor))
warnings.warn("Overriding setting %s can lead to unexpected behavior." % kwargs['setting'], stacklevel=5)
from . import signals
reverification_service = ReverificationService() checkpoint_location = u'i4x://{org}/{course}/edx-reverification-block/{checkpoint}'.format( org=self.course_id.org, course=self.course_id.course, checkpoint=checkpoint_name ) expected_url = ( '/verify_student/reverify' '/{course_key}' '/{checkpoint_location}/' ).format(course_key=unicode(self.course_id), checkpoint_location=checkpoint_location) self.assertEqual( reverification_service.start_verification(unicode(self.course_id), checkpoint_location), expected_url )
certs_api.set_cert_generation_enabled(self.course.id, True)
from __future__ import unicode_literals
true_importances = np.zeros(n_features)
return certificate
self.wait_for_ajax() self._find_within(ancestor_selector + " .action-more").click() EmptyPromise( lambda: self._is_element_visible(ancestor_selector + " .actions-dropdown"), "Secondary action menu opened" ).fulfill() yield if self._is_element_visible(ancestor_selector + " .actions-dropdown"): self._find_within(ancestor_selector + " .action-more").click() EmptyPromise( lambda: not self._is_element_visible(ancestor_selector + " .actions-dropdown"), "Secondary action menu closed" ).fulfill()
pass
store = self._get_modulestore_for_courselike(course_key) if not hasattr(store, 'fill_in_run'): return course_key return store.fill_in_run(course_key)
many_days = datetime.timedelta(days=60)
rng = make_np_rng(rng, self._default_seed, which_method='uniform') X = rng.uniform(-1, 1, size=(num_examples, 2)) y = _four_regions_labels(X) super(FourRegions, self).__init__(X=X, y=y, y_labels=4)
train_scores, test_scores = [], [] partitions = zip(train_sizes, np.split(train, train_sizes)[:-1]) for n_train_samples, partial_train in partitions: train_subset = train[:n_train_samples] X_train, y_train = _safe_split(estimator, X, y, train_subset) X_partial_train, y_partial_train = _safe_split(estimator, X, y, partial_train) X_test, y_test = _safe_split(estimator, X, y, test, train_subset) if y_partial_train is None: estimator.partial_fit(X_partial_train, classes=classes) else: estimator.partial_fit(X_partial_train, y_partial_train, classes=classes) train_scores.append(_score(estimator, X_train, y_train, scorer)) test_scores.append(_score(estimator, X_test, y_test, scorer)) return np.array((train_scores, test_scores)).T
if stream_stdout is True: self.stream_stdout = sys.stdout elif stream_stdout is False: self.stream_stdout = None elif stream_stdout is not None: if not hasattr(stream_stdout, 'write') or \ not hasattr(stream_stdout, 'flush') or \ not hasattr(stream_stdout, 'close'): raise TerminalException( '\'stream_stdout\' needs to have at least 3 methods, ' '\'write()\', \'flush()\' and \'close()\'.' ) self.stream_stdout = stream_stdout else: raise TerminalException( 'Don\'t know how to handle \'{0}\' as the VT\'s ' '\'stream_stdout\' parameter.'.format(stream_stdout) )
self._func_path = '.'.join([func.__module__, func.__name__])
from __future__ import absolute_import
#add_function_parentheses = True
aws = {} if hasattr(e, 'status'): aws['status'] = e.status if hasattr(e, 'reason'): aws['reason'] = e.reason if hasattr(e, 'message') and e.message != '': aws['message'] = e.message if hasattr(e, 'error_code') and e.error_code is not None: aws['code'] = e.error_code
def test_noname_file_default_name(self): self.assertEqual(File(BytesIO(b'A file with no name')).name, None) def test_noname_file_get_size(self): self.assertEqual(File(BytesIO(b'A file with no name')).size, 19)
if prefixed_path in self.copied_files: return self.log("Skipping '%s' (already copied earlier)" % path) if not self.delete_file(path, prefixed_path, source_storage): return source_path = source_storage.path(path) if self.dry_run: self.log("Pretending to copy '%s'" % source_path, level=1) else: self.log("Copying '%s'" % source_path, level=1) with source_storage.open(path) as source_file: self.storage.save(prefixed_path, source_file) self.copied_files.append(prefixed_path)
def __repr__(self): className = getattr(self, '_outputName', self.__class__.__name__)
ccx_id = getattr(course_id, 'ccx', None) ccx = None if ccx_id: ccx = CustomCourseForEdX.objects.filter(id=ccx_id) if not ccx: log.warning( "CCX does not exist for course with id %s", course_id ) return None return ccx[0]
resp = self.client.post( reverse('login_post'), {'email': email, 'password': password} ) self.assertEqual(resp.status_code, 200) return resp
exam_review_policy = get_review_policy_by_exam_id(exam['id']) self.assertEqual(exam_review_policy['review_policy'], sequence.exam_review_rules)
for val in value: if not self.valid_value(val): raise ValidationError( self.error_messages['invalid_choice'], code='invalid_choice', params={'value': val}, )
result = s.sum() self.assertEqual(result, v.sum(dtype=dtype)) result = s.min() self.assertTrue(np.allclose(float(result), 0.0)) result = s.max() self.assertTrue(np.allclose(float(result), v[-1]))
corruptor = BinomialCorruptor(corruption_level=0.5) model = HigherOrderContractiveAutoencoder( corruptor=corruptor, num_corruptions=2, nvis=5, nhid=7, act_enc='sigmoid', act_dec='sigmoid') X = tensor.matrix() data = np.random.randn(10, 5).astype(config.floatX) ff = theano.function([X], model.higher_order_penalty(X)) assert type(ff(data)) == np.ndarray
clf = LinearDiscriminantAnalysis(priors=[0.5, 0.5]) clf.fit(X, y)
return self._values.ravel(order=order)
X = T.matrix() y_hat = self.mlp.fprop(X) theano_rng = make_theano_rng(None, 2013+11+20, which_method="multinomial") if self.stochastic: a = theano_rng.multinomial(pvals=y_hat, dtype='float32') else: mx = T.max(y_hat, axis=1).dimshuffle(0, 'x') a = T.eq(y_hat, mx) if self.epsilon is not None: a = theano_rng.multinomial(pvals = (1. - self.epsilon) * a + self.epsilon * T.ones_like(y_hat) / y_hat.shape[1], dtype = 'float32') if self.epsilon_stochastic is not None: a = theano_rng.multinomial(pvals = (1. - self.epsilon_stochastic) * a + self.epsilon_stochastic * y_hat, dtype = 'float32') logger.info("Compiling classifier agent learning function") t1 = time.time() f = function([X], a) t2 = time.time() logger.info("...done, took {0}".format(t2 - t1)) return f
self._assert_receipt_contains("tax purposes") self._assert_receipt_contains(self.course.display_name)
assert all([layer in layer_to_updated for layer in layer_to_state]) assert all([layer in layer_to_state for layer in layer_to_updated]) assert all([(layer_to_state[layer] is layer_to_updated[layer]) == layer_to_clamp[layer] for layer in layer_to_state])
LogoutPage(self.browser).visit() self._auto_auth("STAFF_TESTER", "staff101@example.com", True) self.course_outline.visit()
prepend_root_dirs = [ 'pki_dir', 'cachedir', 'pidfile', 'sock_dir', 'extension_modules', 'autosign_file', 'autoreject_file', 'token_dir', 'syndic_dir', 'sqlite_queue_dir' ]
self._create_test_file( 'test_include.xml', '<test>Test include</test>' )
if serialized.startswith("="): serialized = serialized[1:].strip()
dataset = fetch_olivetti_faces(shuffle=True, random_state=rng) faces = dataset.data
plt.figure(figsize=(4, 3)) plt.axes([.2, .15, .75, .7]) plt.plot(model.cv_alphas_, np.mean(model.grid_scores, axis=1), 'o-') plt.axvline(model.alpha_, color='.5') plt.title('Model selection') plt.ylabel('Cross-validation score') plt.xlabel('alpha')
ret = dict() for pkg_data in reversed(sorted(_ret, cmp=lambda a_vrs, b_vrs: version_cmp(a_vrs['edition'], b_vrs['edition']))): pkg_name = pkg_data.pop('name') if pkg_name not in ret: ret[pkg_name] = pkg_data.copy() del ret[pkg_name]['edition']
expected = 'You can now <a href="' + reverse('login') + '">login</a>.' self.assertIn(expected, resp.content)
first_name_label = _(u"First Name")
dataset_sources="sources.lst" dataset_web="http://www.stevenpigeon.org/secret" dataset_conf_path="" dataset_data_path="" root_conf_path=None root_data_path=None user_conf_path=None user_data_path=None super_powers=False
if new_user is not None: AUDIT_LOG.info(u"Login success on new account creation - {0}".format(new_user.username))
__virtualname__ = 'etcd'
self.assertIsInstance( list(SimpleItem.objects.annotate(Count('feature')).defer('name')), list) self.assertIsInstance( list(SimpleItem.objects.annotate(Count('feature')).only('name')), list)
[postgres._EXTENSION_NOT_INSTALLED], [postgres._EXTENSION_INSTALLED], [postgres._EXTENSION_NOT_INSTALLED], [postgres._EXTENSION_INSTALLED], [postgres._EXTENSION_NOT_INSTALLED], [postgres._EXTENSION_NOT_INSTALLED], [postgres._EXTENSION_TO_MOVE, postgres._EXTENSION_TO_UPGRADE, postgres._EXTENSION_INSTALLED], [postgres._EXTENSION_INSTALLED], [postgres._EXTENSION_TO_MOVE, postgres._EXTENSION_INSTALLED], [postgres._EXTENSION_INSTALLED], [postgres._EXTENSION_TO_UPGRADE, postgres._EXTENSION_INSTALLED], [postgres._EXTENSION_INSTALLED], [postgres._EXTENSION_TO_UPGRADE, postgres._EXTENSION_INSTALLED], [postgres._EXTENSION_TO_UPGRADE, postgres._EXTENSION_INSTALLED], [postgres._EXTENSION_TO_MOVE, postgres._EXTENSION_INSTALLED], [postgres._EXTENSION_TO_MOVE, postgres._EXTENSION_INSTALLED],
n_topics, X = _build_sparse_mtx() lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=10, random_state=0) distr = lda.fit_transform(X) perplexity_1 = lda.perplexity(X, distr, sub_sampling=False)
super(ForgotPasswordPageTest, self).setUp() self.user_info = self._create_user() self.reset_password_page = ResetPasswordPage(self.browser)
orig_session, _ = cherrypy.session.cache.get(auth_token, ({}, None)) salt_token = orig_session.get('token', auth_token)
return True
multioutput_y = np.column_stack([y2, y2[::-1]]) scores = cross_val_score(clf, X_sparse, multioutput_y) assert_array_equal(scores, clf.score(X_sparse, multioutput_y))
choices = [(0, 0), (1, 1)] model_field = models.Field(choices=choices) form_field = model_field.formfield(show_hidden_initial=True) self.assertTrue(form_field.show_hidden_initial) form_field = model_field.formfield(show_hidden_initial=False) self.assertFalse(form_field.show_hidden_initial)
if not isinstance(cls.REQUIRED_FIELDS, (list, tuple)): errors.append( checks.Error( "'REQUIRED_FIELDS' must be a list or tuple.", obj=cls, id='auth.E001', ) )
for tz, utc_offsets in self.timezone_utc_offsets.items(): hrs_pre = utc_offsets['utc_offset_daylight'] hrs_post = utc_offsets['utc_offset_standard'] self._test_all_offsets( n=3, tstart=self._make_timestamp(self.ts_pre_fallback, hrs_pre, tz), expected_utc_offset=hrs_post)
login = self.client.post(login_url, self.super_login) self.assertRedirects(login, self.index_url) self.assertFalse(login.context) self.client.get(reverse('admin:logout'))
def render(self, name, value, attrs=None): return super(ViewersWidget, self).render(name, ', '.join(value), attrs)
return np.vstack([kernel.diag(X) for kernel in self.kernels]).T
from __future__ import absolute_import
expected = Series([0.0, 1.0, 2.0, 3.0, 4.0], index=[datetime(1975, 1, i, 0) for i in range(1, 6)]) with tm.assert_produces_warning(FutureWarning, check_stacklevel=False): r = series.rolling(window=1, freq='D') tm.assert_series_equal(expected, r.min())
return not self.q(css='div.ui-loading').visible
return self.get(key, default, version=version)
return self.check_root()
s = Series(['A', 'B', 'C', 'a', 'B', 'B', 'A', 'C']) with tm.assertRaises(TypeError): s.isin('a')
user, created = User.objects.get_or_create(username=USERNAME, email=EMAIL) if created: user.set_unusable_password() user.save()
current_attrs[attr_id] = __salt__['xattr.read'](name, attr_id, hex=True).replace(" ", "").replace("\n", "") attr_val = attr_val[2:].replace(" ", "")
exp_fancy = df.iloc[[2]]
result = s[list(mask)] expected = s[mask] assert_series_equal(result, expected) self.assert_index_equal(result.index, s.index[mask])
if after and before and after < before: raise ValueError('after < before') i, j = self.levels[0].slice_locs(before, after) left, right = self.slice_locs(before, after) new_levels = list(self.levels) new_levels[0] = new_levels[0][i:j] new_labels = [lab[left:right] for lab in self.labels] new_labels[0] = new_labels[0] - i return MultiIndex(levels=new_levels, labels=new_labels, verify_integrity=False)
import salt.config import salt.minion import salt.utils import salt.utils.event from salt.utils.network import host_to_ip as _host_to_ip from salt.utils.network import remote_port_tcp as _remote_port_tcp from salt.ext.six.moves import zip from salt.utils.decorators import with_deprecated from salt.exceptions import CommandExecutionError
if not dummy_na and len(levels) == 0: return get_empty_Frame(data, sparse)
import salt.ext.six as six
if self.covariance_type == 'full': return self.covars_ elif self.covariance_type == 'diag': return [np.diag(cov) for cov in self.covars_] elif self.covariance_type == 'tied': return [self.covars_] * self.n_components elif self.covariance_type == 'spherical': return [np.diag(cov) for cov in self.covars_]
SplitTestTransformer().transform(usage_info, block_structure) user_partitions = block_structure.get_transformer_data(self, 'user_partitions') if not user_partitions: return user_groups = _get_user_partition_groups( usage_info.course_key, user_partitions, usage_info.user ) block_structure.remove_block_if( lambda block_key: not block_structure.get_transformer_block_field( block_key, self, 'merged_group_access' ).check_group_access(user_groups) )
import salt.utils.compat
if not (dtype is None or com.is_int64_dtype(dtype)): raise TypeError('Invalid to pass a non-int64 dtype to RangeIndex')
raise RemoteDataError("No data fetched using " "{0!r}".format(method.__name__))
mocked_course = Mock(advanced_modules=['split_test']) mocked_modulestore = Mock() mocked_modulestore.get_course.return_value = mocked_course self.split_test_module.system.modulestore = mocked_modulestore self.split_test_module.user_partitions = [ UserPartition(0, 'first_partition', 'First Partition', [Group("0", 'alpha'), Group("1", 'beta')]) ] expected_url = '/group_configurations/edX/xml_test_course/101#0' self.assertEqual(expected_url, self.split_test_module.group_configuration_url)
NEVER = lambda x: False ALWAYS = lambda x: True
df = DataFrame([time(9, 0, 0), time(9, 1, 30)], columns=["a"]) df.to_sql('test_time', self.conn, index=False) res = read_sql_table('test_time', self.conn) tm.assert_frame_equal(res, df)
params, response = self._validate_parameters(request, bool(initial_verification)) if response is not None: return response
user = User.objects.create_user('forms_test2', 'tesT@EXAMple.com', 'test') self.assertEqual(user.email, 'tesT@example.com') user = User.objects.create_user('forms_test3', 'tesT', 'test') self.assertEqual(user.email, 'tesT')
self.assertEqual(self.bseries.shape, (20, )) self.assertEqual(self.btseries.shape, (20, )) self.assertEqual(self.iseries.shape, (20, ))
return self._get_groups(self.experiment_groups_css)
for kind in ['integer', 'block']: values = np.array([np.nan, 1, 2, 0, np.nan, 0, 1, 2, 1, np.nan]) rvalues = np.array([np.nan, 2, 3, 4, np.nan, 0, 1, 3, 2, np.nan])
r = StreamingHttpResponse(['abc', 'def']) self.assertEqual(list(r), [b'abc', b'def']) self.assertEqual(list(r), [])
if resp.context: self.assertEqual(resp.context['course'], self.course)
payment_info = json.loads(response.content) self.assertEqual(payment_info["payment_url"], "/shoppingcart/payment_fake")
import yaml
max_categories = (10 if get_option("display.max_categories") == 0 else get_option("display.max_categories")) attrs = [ ('categories', ibase.default_pprint(self.categories, max_seq_items=max_categories)), ('ordered', self.ordered)] if self.name is not None: attrs.append(('name', ibase.default_pprint(self.name))) attrs.append(('dtype', "'%s'" % self.dtype)) max_seq_items = get_option('display.max_seq_items') or len(self) if len(self) > max_seq_items: attrs.append(('length', len(self))) return attrs
email_body_plain = render_to_string('credit_notifications/credit_eligibility_email.txt', context) msg_alternative.attach(SafeMIMEText(email_body_plain, _subtype='plain', _charset='utf-8'))
response = self.client.get(self.url) self.assert_no_xss(response, '<script>alert("XSS")</script>')
ret = self.run_function( 'mysql.db_check', name=dbname, connection_user=self.user, connection_pass=self.password ) expected = [] for tablename, engine in sorted(six.iteritems(tablenames)): if engine is 'MEMORY': expected.append([{ 'Table': dbname+'.'+tablename, 'Msg_text': ("The storage engine for the table doesn't" " support check"), 'Msg_type': 'note', 'Op': 'check' }]) else: expected.append([{ 'Table': dbname+'.'+tablename, 'Msg_text': 'OK', 'Msg_type': 'status', 'Op': 'check' }]) self.assertEqual(ret, expected)
_kwargs.update(string_kwarg)
from random import choice from string import letters, digits, punctuation RANDOM_SHARED_SECRET = ''.join( choice(letters + digits + punctuation) for x in range(250) )
self.assertIsNone(getattr(self.request, 'session', None))
if not _initial_defencoding: _initial_defencoding = sys.getdefaultencoding()
prev_points = 100 mean = 0. var = 1. x_empty = np.empty((0, X.shape[1])) tmean, tvar = GaussianNB._update_mean_variance(prev_points, mean, var, x_empty) assert_equal(tmean, mean) assert_equal(tvar, var)
mpoly = OGRGeometry(mp.wkt, srs=None) mpoly.srs = mpoly.srs mpoly.srid = mpoly.srid
current_year = datetime.datetime.now().year self.set_year_of_birth(current_year - 10) self.assertFalse(self.profile.requires_parental_consent())
response = requests.get(self.url + "?test_param=") self.assertEqual(response.status_code, 400)
from salt.ext.six import binary_type, string_types, text_type from salt.ext.six.moves import cStringIO, StringIO
import salt.transport.client import salt.transport.frame import salt.ext.six as six
super(MongoModuleStore, self)._drop_database(database, collections, connections)
X, y = make_blobs(n_samples=100) brc = Birch(n_clusters=3) brc.fit(X) brc_partial = Birch(n_clusters=None) brc_partial.partial_fit(X[:50]) brc_partial.partial_fit(X[50:]) assert_array_equal(brc_partial.subcluster_centers_, brc.subcluster_centers_)
with open(filename, 'w') as cython_hash_file: for key, value in hashes.items(): cython_hash_file.write("%s %s %s %s\n" % (key, value[0], value[1], value[2]))
Number.objects.filter(pk=self.n.pk).update(integer=640 / F('integer'), float=42.7 / F('float'))
import logging
_get_file_from_s3(metadata, saltenv, fnd['bucket'], path, cached_file_path)
shutil.move(bare_repo, '{0}/not_bare.git'.format(settings.TEST_ROOT)) try: git_import.switch_branch('master', rdir) except GitImportError: self.assertIn('Unable to fetch remote', output.getvalue()) shutil.move('{0}/not_bare.git'.format(settings.TEST_ROOT), bare_repo) output.truncate(0)
return super(PythonSerializer, self).getvalue()
transform = AdditiveChi2Sampler(sample_steps=sample_steps) assert_equal(transform.sample_interval, None)
assert_array_equal(X, Xdigits)
tm.assert_index_equal(s.index, ds.index)
from __future__ import absolute_import
test = read_csv(path, index_col=0) datetime_frame_str = datetime_frame.applymap( lambda x: x.strftime('%Y-%m-%d')) datetime_frame_str.index = datetime_frame_str.index.map( lambda x: x.strftime('%Y-%m-%d'))
result = module.get_demand_hint(0) self.assertEqual(result['contents'], u'Hint (1 of 2): Demand 1') self.assertEqual(result['hint_index'], 0) result = module.get_demand_hint(1) self.assertEqual(result['contents'], u'Hint (2 of 2): Demand 2') self.assertEqual(result['hint_index'], 1)
a, b = first, second assert a is not b, "%s: %r is %r" % (msg.format(a, b), a, b)
if self.__no_lock: self.__no_lock = not self.__refresh
self.through._default_manager.using(db).bulk_create([ self.through(**{ '%s_id' % source_field_name: self.related_val[0], '%s_id' % target_field_name: obj_id, }) for obj_id in new_ids ])
self.input_space.validate(state_below) if iter_name is None: iter_name = 'anon' if state_above is not None: assert layer_above is not None msg = layer_above.downward_message(state_above) msg.name = 'msg_from_'+layer_above.layer_name+'_to_'+self.layer_name+'['+iter_name+']' else: msg = None if not hasattr(state_below, 'ndim'): raise TypeError("state_below should be a TensorType, got " + str(state_below) + " of type " + str(type(state_below))) if state_below.ndim != 4: raise ValueError("state_below should have ndim 4, has "+str(state_below.ndim)) if double_weights: state_below = 2. * state_below state_below.name = self.layer_name + '_'+iter_name + '_2state' z = self.transformer.lmul(state_below) + self.broadcasted_bias() if self.layer_name is not None and iter_name is not None: z.name = self.layer_name + '_' + iter_name + '_z' p,h = self.max_pool(z, (self.pool_rows, self.pool_cols), msg) p.name = self.layer_name + '_p_' + iter_name h.name = self.layer_name + '_h_' + iter_name return p, h
return reduce(np.result_type, arrays_and_dtypes)
assert_equal(distances.shape, (n_queries,)) assert_equal(distances.dtype, object) assert_equal(neighbors.shape, (n_queries,)) assert_equal(neighbors.dtype, object)
beta = 5 eps = 1e-6 graph.data = np.exp(-beta * graph.data / graph.data.std()) + eps
new_index, row_indexer = self.index.reindex(axes['index']) new_columns, col_indexer = self.columns.reindex(axes['columns']) if row_indexer is not None and col_indexer is not None: indexer = row_indexer, col_indexer new_values = algos.take_2d_multi(self.values, indexer, fill_value=fill_value) return self._constructor(new_values, index=new_index, columns=new_columns) else: return self._reindex_with_indexers({0: [new_index, row_indexer], 1: [new_columns, col_indexer]}, copy=copy, fill_value=fill_value)
from __future__ import absolute_import
try: cur.executemany(cmd, newitems) except psycopg2.IntegrityError as esc: return('One or more items already exists in this queue. ' 'sqlite error: {0}'.format(esc))
result = idx.delete(len(idx))
from unit.modules.boto_s3_bucket_test import BotoS3BucketTestCaseMixin
for key in list(__context__): try: if key.startswith('systemd._systemctl_status.') \ or key in ('systemd.systemd_services',): __context__.pop(key) except AttributeError: continue
parent_key = course_key.make_usage_key(parent_type, parent_id) self.assertElementAttrsSubset(element, { 'parent_url': re.escape(unicode(parent_key)), 'index_in_children_list': re.escape(str(index_in_children_list)), })
MEDIA_URL = ''
cauth.email_enabled = False cauth.save() self.assertFalse(BulkEmailFlag.feature_enabled(course_id)) self.assertEquals( cauth.__unicode__(), "Course 'abc/123/doremi': Instructor Email Not Enabled" )
report_dir.makedirs_p() msg = colorize('green', "Combining coverage reports") print msg sh("coverage combine --rcfile={}".format(coveragerc)) msg = colorize('green', "Generating coverage reports") print msg sh("coverage html --rcfile={}".format(coveragerc)) sh("coverage xml --rcfile={}".format(coveragerc)) sh("coverage report --rcfile={}".format(coveragerc))
ref_kml_regex = re.compile(r'^<Point><coordinates>-95.363\d+,29.763\d+,18</coordinates></Point>$') self.assertTrue(ref_kml_regex.match(h.kml))
resp_asset = post_asset_update(False, course) self.assertFalse(resp_asset['locked']) verify_asset_locked_state(False)
middleware = CacheMiddleware()
with timezone.override(timezone.get_fixed_timezone(-300)): now = timezone.now() self.assertFalse(self.storage.exists('test.file.tz.on'))
delete_comment(request, comment_id) return Response(status=204)
enrollment.activate() self.assertTrue(CourseEnrollment.is_enrolled(user, course_id)) self.assert_no_events_were_emitted()
self.course_enrollment.change_mode(self.mode) self.course_enrollment.activate()
alpha_init = numpy.zeros(self.nslab) + alpha0 if alpha_irange > 0: alpha_init += (2 * rng.rand(self.nslab) - 1) * alpha_irange self.log_alpha = sharedX(numpy.log(alpha_init), name='log_alpha') self.alpha = tensor.exp(self.log_alpha) self.alpha.name = 'alpha'
import salt.ext.six as six
'USE_MICROSITES': False,
with io.open(create_path('valid_urls.txt'), encoding='utf8') as f: for url in f: TEST_DATA.append((URLValidator(), url.strip(), None)) with io.open(create_path('invalid_urls.txt'), encoding='utf8') as f: for url in f: TEST_DATA.append((URLValidator(), url.strip(), ValidationError))
if name in list_tasks(location):
course_key = course_key or self.course_key credit_course = self.add_credit_course(course_key=course_key) requirement = CreditRequirement.objects.create( course=credit_course, namespace="grade", name="grade", active=True ) status = CreditRequirementStatus.objects.create( username=self.USER_INFO["username"], requirement=requirement, ) status.status = "satisfied" status.reason = {"final_grade": self.FINAL_GRADE} status.save() CreditEligibility.objects.create( username=self.USER_INFO['username'], course=CreditCourse.objects.get(course_key=course_key) )
return dict((_instantiate(k, bindings), _instantiate(v, bindings)) for k, v in six.iteritems(proxy))
init_params = init_params[1:]
data = {'opts': opts_pkg, 'grains': opts_pkg['grains'], 'pillar': pillar_data} if data_cache: with salt.utils.fopen(datap, 'w+b') as fp_: fp_.write( self.serial.dumps(data) )
feed_elem = doc.getElementsByTagName('rss') self.assertEqual(len(feed_elem), 1) feed = feed_elem[0] self.assertEqual(feed.getAttribute('version'), '0.91')
PaymentFakeView.PAYMENT_STATUS_RESPONSE = new_status return HttpResponse()
proc.start()
self.fake_payment_page.submit_payment()
aggs = City.objects.aggregate(Union('location__point'))
] TIME_INPUT_FORMATS = [
if value: params.append(template % value)
if i is not None: k = res_index[i] e.args = e.args + ('occurred at index %s' % pprint_thing(k), )
from salt.modules import dpkg
return _check_range_and_return('horizontal shift', label, -5, 5)
random_state = check_random_state(0) methods = ['barnes_hut', 'exact'] for method in methods: for dt in [np.float32, np.float64]: X = random_state.randn(100, 2).astype(dt) tsne = TSNE(n_components=2, perplexity=2, learning_rate=100.0, random_state=0, method=method) tsne.fit_transform(X)
from __future__ import unicode_literals
return self.get_selector('#disabled-btn-start-generating-certificates')
space, source = self.get_monitoring_data_specs() space.validate(data) rval = OrderedDict() X = data epsilon_shape = (X.shape[0], self.nhid) epsilon = self.sample_from_epsilon(shape=epsilon_shape) phi = self.encode_phi(X) z = self.sample_from_q_z_given_x(epsilon=epsilon, phi=phi) theta = self.decode_theta(z) X_r = self.means_from_theta(theta) rval["reconstruction_mse"] = T.sqr(X - X_r).mean() posterior_channels = \ self.posterior.monitoring_channels_from_conditional_params(phi) safe_update(rval, posterior_channels) conditional_channels = \ self.conditional.monitoring_channels_from_conditional_params(theta) safe_update(rval, conditional_channels) prior_channels = self.prior.monitoring_channels_from_prior_params() safe_update(rval, prior_channels) return rval
from __future__ import unicode_literals
update = True
else:
self.assertContains(response, 'Base view for admindocs views.')
try: val = self.verify_geom(feat.geom, model_field) except GDALException: raise LayerMapError('Could not retrieve geometry from feature.')
try: for line in value.splitlines(): ret += '{0}: {1}\n'.format(key, line) except AttributeError: ret += '{0}: {1}\n'.format(key, value)
return False
cmd_args = ' '.join(args) cmd_kwargs = ''.join([ ' --{0} {1}'.format(k, v) for k, v in six.iteritems(kwargs) if not k.startswith('__') ]) cmd_exec = '{0}{1}'.format(cmd_args, cmd_kwargs) log.debug('Chef command: {0}'.format(cmd_exec))
v = s[g == 1].iloc[0] self.assertEqual(expected.iloc[0], v) self.assertEqual(expected2.iloc[0], v)
def wrap_f_init(*args): data = f_init(*args) length = len(data) / 2 return data[:length], data[length:] return wrap_f_init
return [ CourseKey.from_string(course_overview['id']) for course_overview in CourseOverview.objects.values('id') ]
child = sp.Popen(cmd, shell=True, stdout=sp.PIPE, stderr=sp.STDOUT) output = child.communicate()[0] rc = child.returncode return output, rc
X = np.arange(5).reshape(-1, 1) X_embedded = np.array([[0], [2], [4], [1], [3]]) assert_almost_equal(trustworthiness(X, X_embedded, n_neighbors=1), 0.2)
if is_active and (enrollment_mode in CourseMode.VERIFIED_MODES + [CourseMode.NO_ID_PROFESSIONAL_MODE]): return redirect(reverse('dashboard'))
if dt: return JSONEncoder().default(dt) return None
return binascii.hexlify(struct.pack('<' + structure, *data)).upper()
descriptor_class = WordCloudDescriptor raw_field_data = { 'all_words': {'cat': 10, 'dog': 5, 'mom': 1, 'dad': 2}, 'top_words': {'cat': 10, 'dog': 5, 'dad': 2}, 'submitted': False } def test_bad_ajax_request(self): "Make sure that answer for incorrect request is error json" response = self.ajax_request('bad_dispatch', {}) self.assertDictEqual(response, { 'status': 'fail', 'error': 'Unknown Command!' }) def test_good_ajax_request(self): "Make sure that ajax request works correctly" post_data = MultiDict(('student_words[]', word) for word in ['cat', 'cat', 'dog', 'sun']) response = self.ajax_request('submit', post_data) self.assertEqual(response['status'], 'success') self.assertEqual(response['submitted'], True) self.assertEqual(response['total_count'], 22) self.assertDictEqual( response['student_words'], {'sun': 1, 'dog': 6, 'cat': 12} ) self.assertListEqual( response['top_words'], [{'text': 'dad', 'size': 2, 'percent': 9.0}, {'text': 'sun', 'size': 1, 'percent': 5.0}, {'text': 'dog', 'size': 6, 'percent': 27.0}, {'text': 'mom', 'size': 1, 'percent': 5.0}, {'text': 'cat', 'size': 12, 'percent': 54.0}] ) self.assertEqual( 100.0, sum(i['percent'] for i in response['top_words']))
_check_all_orients(self.series) self.assertEqual(self.series.to_json(), self.series.to_json(orient="index"))
with self.assertRaises(TemplateSyntaxError): self.engine.get_template('exception03')
self.assertTrue( isinstance(widget, widgetclass), "Wrong widget for %s.%s: expected %s, got %s" % ( model.__class__.__name__, fieldname, widgetclass, type(widget), ) )
msg = "LinearSVC does not support sample_weight." assert_warns_message( UserWarning, msg, calibrated_clf.fit, X_train, y_train, sample_weight=sw_train) probs_with_sw = calibrated_clf.predict_proba(X_test)
check_is_fitted(self, 'mean_') X = check_array(X) Xr = X - self.mean_ n_features = X.shape[1] log_like = np.zeros(X.shape[0]) precision = self.get_precision() log_like = -.5 * (Xr * (np.dot(Xr, precision))).sum(axis=1) log_like -= .5 * (n_features * log(2. * np.pi) - fast_logdet(precision)) return log_like
X, _ = make_blobs(n_samples=n_samples + n_queries, n_features=n_features, centers=10, random_state=0) X_index = X[:n_samples] X_query = X[n_samples:] nbrs = NearestNeighbors(n_neighbors=1, algorithm='brute', metric='cosine').fit(X_index) neighbors_exact = nbrs.kneighbors(X_query, return_distance=False)
def two_pass_var(X): mean = X.mean(axis=0) Y = X.copy() return np.mean((Y - mean)**2, axis=0)
response = self.fetch('/login', method='POST', body=yaml.dump(self.auth_creds_dict), headers={'Content-Type': self.content_type_map['yaml']})
X, y = make_blobs(n_samples=100000, centers=n_centres, random_state=0)
tm._skip_if_no_scipy() result = df.interpolate(axis=1, method='values') assert_frame_equal(result, expected)
xblock = modulestore().get_item(location) xblock.visible_to_staff_only = True self.store.update_item(xblock, self.user.id)
clf = svm.LinearSVC(random_state=0).fit(X, Y) sp_clf = svm.LinearSVC(random_state=0).fit(X_sp, Y)
from __future__ import absolute_import import multiprocessing import logging
rolling_f_result = rolling_f(x, pairwise=True) expected = Panel(items=x.index, major_axis=x.columns, minor_axis=x.columns) for i, _ in enumerate(x.columns): for j, _ in enumerate(x.columns): expected.iloc[:, i, j] = ( getattr( x.iloc[:, i] .rolling(window=window, min_periods=min_periods, center=center), name)(x.iloc[:, j])) tm.assert_panel_equal(rolling_f_result, expected)
certificates = self.course.certificates['certificates'] self.assertEqual(len(certificates), 1) self.assertEqual(certificates[0].get('name'), 'Name 0') self.assertEqual(certificates[0].get('description'), 'Description 0')
import salt.crypt import salt.client import salt.config import salt.utils import salt.utils.event from salt import syspaths from salt.utils import vt from salt.utils.nb_popen import NonBlockingPopen from salt.utils.yamldumper import SafeOrderedDumper from salt.utils.validate.path import is_writeable
X, _, _, _, _ = _preprocess_data(X, y, True, True)
indexers = ['major_axis', 'labels', 'minor_axis'] _maybe_remove(store, 'p4d') store.append('p4d', p4d.ix[:, :, :10, :], axes=indexers) store.append('p4d', p4d.ix[:, :, 10:, :]) assert_panel4d_equal(store['p4d'], p4d) check_indexers('p4d', indexers)
if transformer.VERSION == 0: raise TransformerException('VERSION attribute is not set on transformer {0}.', transformer.name()) self.set_transformer_data(transformer, TRANSFORMER_VERSION_KEY, transformer.VERSION)
if depth is not None: depth -= 1
return
instructor_task = self._create_success_entry() instructor_task.task_output = "{}" succeeded, message = get_task_completion_info(instructor_task) self.assertFalse(succeeded) self.assertEquals(message, "No progress status information available")
return 'f_table_name'
provider_register_url = self._check_register_page() try_login_response = self.client.get(provider_register_url) self.assertEqual(try_login_response.status_code, 302) provider_response = self.do_provider_login(try_login_response['Location']) self.assertEqual(provider_response.status_code, 302) self.assertEqual(provider_response['Location'], self.url_prefix + self.register_page_url) register_response = self.client.get(self.register_page_url) tpa_context = register_response.context["data"]["third_party_auth"] self.assertEqual(tpa_context["errorMessage"], None) self.assertEqual(tpa_context["currentProvider"], self.PROVIDER_NAME) form_data = register_response.context['data']['registration_form_desc'] form_fields = {field['name']: field for field in form_data['fields']} self.assertEqual(form_fields['email']['defaultValue'], self.USER_EMAIL) self.assertEqual(form_fields['name']['defaultValue'], self.USER_NAME) self.assertEqual(form_fields['username']['defaultValue'], self.USER_USERNAME) ajax_register_response = self.client.post( reverse('user_api_registration'), { 'email': 'email-edited@tpa-test.none', 'name': 'My Customized Name', 'username': 'new_username', 'honor_code': True, } ) self.assertEqual(ajax_register_response.status_code, 200) continue_response = self.client.get(tpa_context["finishAuthUrl"]) self.assertEqual(continue_response.status_code, 302) self.assertEqual(continue_response['Location'], self.url_prefix + reverse('dashboard'))
if self.is_categorical is not None: categories = self.is_categorical.categories ordered = self.is_categorical.ordered values = [Categorical.from_array(values[:, i], categories=categories, ordered=ordered) for i in range(values.shape[-1])]
composer.__grains__ = {} composer.__salt__ = {} composer.__context__ = {} composer.__opts__ = {}
self.assertEqual(hasher.encode.call_count, 1)
if underspecifies_dtypes(from_space, to_type): try: from_space.get_origin_batch(batch_size, dtype=to_type) except TypeError as ex: assert dtype_is_none_msg in str(ex) except Exception as unexpected_ex: print("Expected an exception of type TypeError with message " "%s, got a %s instead with message %s." % (dtype_is_none_msg, type(unexpected_ex), str(unexpected_ex))) raise unexpected_ex finally: return
fmt = formats.pop(formats.index(time_format)) formats.insert(0, fmt) format_found = True
self.add_child()
try: return json.loads(stdout, object_hook=salt.utils.decode_dict) except Exception as e: log.error("JSON Render failed for: {0}\n{1}".format(stdout, stderr)) log.error(str(e))
Y.tag.test_value[2][3] = 1.1 np.testing.assert_raises(ValueError, elemwise_kl, Y, Y_hat) Y.tag.test_value[2][3] = -0.1 np.testing.assert_raises(ValueError, elemwise_kl, Y, Y_hat)
import logging
return name
plt.subplot(1, 2, i + 1) if basemap: print(" - plot coastlines using basemap") m = Basemap(projection='cyl', llcrnrlat=Y.min(), urcrnrlat=Y.max(), llcrnrlon=X.min(), urcrnrlon=X.max(), resolution='c') m.drawcoastlines() m.drawcountries() else: print(" - plot coastlines from coverage") plt.contour(X, Y, land_reference, levels=[-9999], colors="k", linestyles="solid") plt.xticks([]) plt.yticks([])
res = s.fillna(1) tm.assert_series_equal(res, pd.Series([1 + 1j, 1, 3 + 3j, 4 + 4j])) self.assertEqual(res.dtype, np.complex128)
total_amount = models.FloatField()
all_assets.extend(course_assets.setdefault(asset_key.block_type, [])) idx = all_assets.find(asset_key)
nr_tx_name = "{}.{}".format(instance.__class__.__name__, handler) nr_tx_name += "/{}".format(suffix) if (suffix and handler == "xmodule_handler") else "" newrelic.agent.set_transaction_name(nr_tx_name, group="Python/XBlock/Handler")
import salt.loader import salt.payload import salt.utils import salt.utils.templates import salt.utils.url from salt.utils.locales import sdecode from salt.exceptions import CommandExecutionError
self.get_data_specs(model)[0].validate(data) return OrderedDict()
pass
update_wrapper(self, fn)
blocks_stack.extend(children)
result = df.iloc[:, slice(4, 8)] expected = df.ix[:, 8:14] assert_frame_equal(result, expected)
recons = T.dot(self.W,h) diffs = recons - v rval = T.dot(diffs,diffs) / N.cast[floatX](self.nvis) return rval
updates = [self.momentum * velocity - self.learning_rate * grad for velocity, grad in zip(self.velocities, grads)] self.velocities = updates if self.nesterov: updates = [self.momentum * velocity - self.learning_rate * grad for velocity, grad in zip(self.velocities, grads)] return updates
import salt.ext.six as six
if covariance_type == 'spherical': cv = np.tile(tied_cv.mean() * np.ones(tied_cv.shape[1]), (n_components, 1)) elif covariance_type == 'tied': cv = tied_cv elif covariance_type == 'diag': cv = np.tile(np.diag(tied_cv), (n_components, 1)) elif covariance_type == 'full': cv = np.tile(tied_cv, (n_components, 1, 1)) else: raise ValueError("covariance_type must be one of " + "'spherical', 'tied', 'diag', 'full'") return cv
patches = np.empty(patches_shape) for ii, image in enumerate(X): patches[ii * n_patches:(ii + 1) * n_patches] = extract_patches_2d( image, patch_size, self.max_patches, self.random_state) return patches
procedure_args['geo_col'] = self._geocol_select(geo_field, field_name)
with self.assertRaises(VersionConflictError): _fail = modulestore().create_child( user, new_course.location, 'chapter', fields={'display_name': 'chapter 3'}, )
asset_dir = root_courselike_dir + '/' + AssetMetadata.EXPORTED_ASSET_DIR + '/' if not os.path.isdir(asset_dir): os.makedirs(asset_dir) asset_root = lxml.etree.Element(AssetMetadata.ALL_ASSETS_XML_TAG) course_assets = self.modulestore.get_all_asset_metadata(self.courselike_key, None) for asset_md in course_assets: asset = lxml.etree.SubElement(asset_root, AssetMetadata.ASSET_XML_TAG) asset_md.to_xml(asset) with OSFS(asset_dir).open(AssetMetadata.EXPORTED_ASSET_FILENAME, 'w') as asset_xml_file: lxml.etree.ElementTree(asset_root).write(asset_xml_file)
check_estimators_unfitted("estimator", CorrectNotFittedErrorClassifier)
ndim = set([b.ndim for b in blocks])
X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]], random_state=0, n_features=2, cluster_std=0.1) X -= X.min() y = multioutput_estimator_convert_y_2d(name, y) estimator = Estimator() set_testing_parameters(estimator) set_random_state(estimator) pipeline = make_pipeline(estimator) estimator.fit(X, y) pipeline.fit(X, y)
if fastpath: self.axes = [axis] if isinstance(block, list):
logged_in = False session_id = None for sess in sess_list: if win32ts.WTSQuerySessionInformation(None, sess['SessionId'], win32ts.WTSUserName) == name: session_id = sess['SessionId'] logged_in = True
with self.assertRaisesMessage(CommandError, "No fixture named 'this_fixture_doesnt_exist' found."): management.call_command('loaddata', 'this_fixture_doesnt_exist', verbosity=0) disable_constraint_checking.assert_not_called() enable_constraint_checking.assert_not_called()
dense_classifier = dense_results = AdaBoostRegressor( base_estimator=CustomSVR(), random_state=1 ).fit(X_train, y_train)
if self.done and self.rerandomize == RANDOMIZATION.ALWAYS: event_info['failure'] = 'done' self.track_function_unmask('save_problem_fail', event_info) return { 'success': False, 'msg': _("Problem needs to be reset prior to save.") }
df['foo'] = np.ones((4, 2)).tolist()
hasher.iterations = 1 encoded = make_password('letmein') algo, iterations, salt, hash = encoded.split('$', 3) self.assertEqual(iterations, '1')
user = self.create_user_and_enroll('verified') self.add_verification_status(user, VerificationStatus.DENIED_STATUS)
arr = [1370745748 + t for t in range(20)] + [iNaT] idx = DatetimeIndex(arr * 3) tm.assert_index_equal(idx.unique(), DatetimeIndex(arr)) self.assertEqual(idx.nunique(), 20) self.assertEqual(idx.nunique(dropna=False), 21)
return True
continue
return [lib for lib in modulestore().get_libraries() if has_studio_read_access(user, lib.location.library_key)]
course_id = SlashSeparatedCourseKey.from_deprecated_string(course_id) problem_to_reset = strip_if_string(request.GET.get('problem_to_reset')) student_identifier = request.GET.get('unique_student_identifier', None) student = None if student_identifier is not None: student = get_student_from_identifier(student_identifier) all_students = request.GET.get('all_students') in ['true', 'True', True] if not (problem_to_reset and (all_students or student)): return HttpResponseBadRequest("Missing query parameters.") if all_students and student: return HttpResponseBadRequest( "Cannot rescore with all_students and unique_student_identifier." ) try: module_state_key = course_id.make_usage_key_from_deprecated_string(problem_to_reset) except InvalidKeyError: return HttpResponseBadRequest("Unable to parse problem id") response_payload = {} response_payload['problem_to_reset'] = problem_to_reset if student: response_payload['student'] = student_identifier instructor_task.api.submit_rescore_problem_for_student(request, module_state_key, student) response_payload['task'] = 'created' elif all_students: instructor_task.api.submit_rescore_problem_for_all_students(request, module_state_key) response_payload['task'] = 'created' else: return HttpResponseBadRequest() return JsonResponse(response_payload)
d = g.city(query) self.assertEqual('US', d['country_code']) self.assertEqual('Houston', d['city']) self.assertEqual('TX', d['region'])
migration_loader = MigrationLoader(connection) self.assertEqual( migration_loader.graph.forwards_plan(("migrations", "0002_second")), [ ("migrations", "0001_initial"), ("migrations", "0003_third"), ("migrations", "0002_second"), ], )
log = logging.getLogger(__name__)
df = DataFrame(columns=['A', 'B', 'A', 'C'], data=[range(4), range(2, 6), range(0, 8, 2)])
from __future__ import absolute_import
if self.q(css=".register-button").visible: return "register" elif self.q(css=".login-button").visible: return "login" elif self.q(css=".js-reset").visible: return "password-reset" elif self.q(css=".proceed-button").visible: return "hinted-login"
if not encoding or 'ascii' in encoding.lower(): try: encoding = locale.getpreferredencoding() except Exception: pass
min_score = 100 requirements = milestone.get('requirements') if requirements: try: min_score = int(requirements.get('min_score')) except (ValueError, TypeError): log.warning( 'Failed to find minimum score for gating milestone %s, defaulting to 100', json.dumps(milestone) )
if self.request_cache is not None: if 'metadata_inheritance' not in self.request_cache.data: self.request_cache.data['metadata_inheritance'] = {} self.request_cache.data['metadata_inheritance'][unicode(course_id)] = tree
__opts__['test'] = orig_test return ret
if event['data']['jid'] not in self.jid_forward_cache: jdict['__load__'].update( self.mminion.returners[fstr](event['data']['jid']) ) self.jid_forward_cache.add(event['data']['jid']) if len(self.jid_forward_cache) > self.opts['syndic_jid_forward_cache_hwm']: tmp = sorted(list(self.jid_forward_cache)) tmp.pop(0) self.jid_forward_cache = set(tmp)
lines.append(line) continue
MY_NAME = 'test_ext_pillar_opts'
if not self.exists(table_id): raise NotFoundException("Table does not exist") try: self.service.tables().delete( datasetId=self.dataset_id, projectId=self.project_id, tableId=table_id).execute() except self.http_error as ex: self.process_http_error(ex)
p3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width, color='blue') p4 = ax.bar(ind + width, [0, 0, 0, class2_1[-1]], width, color='steelblue')
current_language = get_language() return [code for code, name in settings.LANGUAGES if not code == current_language][0]
X = diabetes.data alphas, _, lasso_path = linear_model.lars_path(X, y, method='lasso', positive=True) lasso_cd = linear_model.Lasso(fit_intercept=False, normalize=True, tol=1e-8, positive=True)
self.assertEqual(response.data['id'], '%s,%s' % (self.user.username, unicode(self.vertical_3.location))) self.assertEqual(response.data['course_id'], self.course_id) self.assertEqual(response.data['usage_id'], unicode(self.vertical_3.location)) self.assertIsNotNone(response.data['created']) self.assertEqual(len(response.data['path']), 2) self.assertEqual(response.data['display_name'], self.vertical_3.display_name)
CourseEnrollment.unenroll(user, course_id) self.assertFalse(CourseEnrollment.is_enrolled(user, course_id)) self.assertFalse(CourseEnrollment.is_enrolled_by_partial(user, course_id_partial)) self.assert_no_events_were_emitted()
faces, target, target_names = load_func( data_folder_path, resize=resize, min_faces_per_person=min_faces_per_person, color=color, slice_=slice_)
if len(set(ml)) != 1: raise ValueError("Composite space is empty or containing " "incompatible index spaces") return ml[0]
call = [call for call in mock_request.call_args_list if call[0][1].endswith(self.cs_endpoint)][0] if call[0][0] == "get": return call[1]["params"] elif call[0][0] == "post": return call[1]["data"]
login_request = self.request_factory.get('/course_specific_login/MITx/999/Robot_Super_Course' + '?course_id=MITx/999/Robot_Super_Course' + '&enrollment_action=enroll') _reg_request = self.request_factory.get('/course_specific_register/MITx/999/Robot_Super_Course' + '?course_id=MITx/999/course/Robot_Super_Course' + '&enrollment_action=enroll')
modules = [m for m in modules] locations = [ BlockUsageLocator( course_key=course.id, block_type=module.location.block_type, block_id=module.location.block_id ) if isinstance(module.location, BlockUsageLocator) and module.location.version else module.location for module in modules ]
draft_store_items = self.store.get_items( self.course.id, revision=ModuleStoreEnum.RevisionOption.draft_only ) items_from_draft_store = [item for item in draft_store_items if item.location == self.problem.location] self.assertEqual(len(items_from_draft_store), 1) self.assertTrue(getattr(items_from_draft_store[0], 'is_draft', False))
s = Series([Timestamp('20130101'), Timestamp('20130101'), Timestamp( '20130102'), Timestamp('20130103 9:01:01')]) td = s.diff()
return "SELECT cache_key FROM %s ORDER BY cache_key LIMIT 1 OFFSET %%s"
text_repr, real_count, msg_prefix = self._assert_contains( response, text, status_code, msg_prefix, html) self.assertEqual(real_count, 0, msg_prefix + "Response should not contain %s" % text_repr)
coffee_file_path = os.path.dirname(__file__) + "/test_files/js/*.coffee" os.system("node_modules/.bin/coffee -c %s" % (coffee_file_path))
invalid_obj = invalid_obj or model_admin admin_obj = model_admin(model, AdminSite()) errors = admin_obj.check() self.assertEqual(len(errors), 1) error = errors[0] self.assertEqual(error.hint, hint) self.assertEqual(error.obj, invalid_obj) self.assertEqual(error.id, id) six.assertRegex(self, error.msg, msg)
if generate_translation: old_langs = set(old_metadata.get('transcripts', {})) if old_metadata else set() new_langs = set(item.transcripts)
self.maxpoints = dict() for inputfield in self.inputfields: maxpoints = inputfield.get('points', '1') self.maxpoints.update({inputfield.get('id'): int(maxpoints)})
for err in self.geometries.errors: with self.assertRaises((GEOSException, ValueError)): fromstr(err.wkt)
self.assertEqual(col[1], Timestamp('2000-06-01 07:00:00'))
for i in xrange(30): resp = self._login(self.email, 'wrong_password{0}'.format(i)) self.assertEqual(resp.status_code, 200) resp = self._login(self.email, 'wrong_password') self.assertEqual(resp.status_code, 200) data = parse_json(resp) self.assertFalse(data['success']) self.assertIn('Too many failed login attempts.', data['value'])
iris = datasets.load_iris()
path = urlparse.urlparse(url).path resolver = resolve(path) ccx_key = resolver.kwargs['course_id']
ret['changes']['diff'] = ( ''.join(difflib.unified_diff(slines, nlines)) )
import collections
for metric in ('euclidean', 'cosine'): clf = NearestCentroid(metric=metric).fit(iris.data, iris.target) score = np.mean(clf.predict(iris.data) == iris.target) assert score > 0.9, "Failed with score = " + str(score)
_layout_type = 'single' def __init__(self, data, x, y, **kwargs): MPLPlot.__init__(self, data, **kwargs) if x is None or y is None: raise ValueError(self._kind + ' requires and x and y column') if com.is_integer(x) and not self.data.columns.holds_integer(): x = self.data.columns[x] if com.is_integer(y) and not self.data.columns.holds_integer(): y = self.data.columns[y] self.x = x self.y = y @property def nseries(self): return 1 def _post_plot_logic(self, ax, data): x, y = self.x, self.y ax.set_ylabel(pprint_thing(y)) ax.set_xlabel(pprint_thing(x))
ip_int = 0 for i in range(parts_hi): ip_int <<= 16 ip_int |= self._parse_hextet(parts[i]) ip_int <<= 16 * parts_skipped for i in range(-parts_lo, 0): ip_int <<= 16 ip_int |= self._parse_hextet(parts[i]) return ip_int
return {"result": "inclusion_one_default - Expected result: %s, %s" % (one, two)}
v4_ordered = [self.v4addr, self.v4net, self.v4intf] v6_ordered = [self.v6addr, self.v6net, self.v6intf] self.assertEqual(v4_ordered, sorted(self.v4_objects, key=ipaddress.get_mixed_type_key)) self.assertEqual(v6_ordered, sorted(self.v6_objects, key=ipaddress.get_mixed_type_key)) self.assertEqual(v4_ordered + v6_ordered, sorted(self.objects, key=ipaddress.get_mixed_type_key)) self.assertEqual(NotImplemented, ipaddress.get_mixed_type_key(object))
timezone.get_default_timezone.cache_clear()
with expected_store.branch_setting(ModuleStoreEnum.Branch.published_only, expected_course_key): with actual_store.branch_setting(ModuleStoreEnum.Branch.published_only, actual_course_key): expected_items = expected_store.get_items(expected_course_key, revision=ModuleStoreEnum.RevisionOption.published_only) actual_items = actual_store.get_items(actual_course_key, revision=ModuleStoreEnum.RevisionOption.published_only) self.assertGreater(len(expected_items), 0) self._assertCoursesEqual(expected_items, actual_items, actual_course_key)
value = value.rstrip('-')
for l in [slice(3.0, 4), slice(3, 4.0), slice(3.0, 4.0)]:
return response.set_cookie_wrapped_func( key, value, max_age=max_age, expires=expires, path=path, domain=domain, secure=secure, httponly=httponly )
extensions = [ 'sphinx.ext.autodoc', 'sphinx.ext.doctest', 'sphinx.ext.intersphinx', 'sphinx.ext.todo', 'sphinx.ext.coverage', 'sphinx.ext.pngmath', 'sphinx.ext.mathjax', 'sphinx.ext.viewcode', 'sphinxcontrib.napoleon']
class PreparedPredicate(GEOSFuncFactory): argtypes = [PREPGEOM_PTR, GEOM_PTR] restype = c_char errcheck = staticmethod(check_predicate)
if not metric_base.startswith('virt.'): metric_base += '.' + ret['id'].replace('.', '_')
from __future__ import absolute_import
with self.assertNumQueries(2, using='other'): ages = ", ".join(str(a.authorwithage.age) for a in A.prefetch_related('authorwithage'))
return obj.user.email
x_p_nrm, y_p_nrm = norm(x_p), norm(y_p) x_n_nrm, y_n_nrm = norm(x_n), norm(y_n)
revoke_access(course_ccx, staff, 'staff')
class TestObject:
idx = 0 start_idx = end_idx for rd in rings: start_idx = downsample_ring(img, idx, rd, output, start_idx) idx += rd
y = zca_dataset.adjust_for_viewer(x.T).T z = x/np.abs(x).max(axis=0) assert_allclose(z, y)
try:
EDX_DOMAIN_FRAGMENTS = [ "Thank you for signing up for {platform}".format(platform=settings.PLATFORM_NAME), "http://edx.org/activate/", "https://www.edx.org/contact-us", "This email was automatically sent by edx.org" ]
return Index(self.group_info[0]).is_monotonic
task_progress = json.loads(entry.task_output) start_time = task_progress['start_time'] prev_duration = task_progress['duration_ms'] new_duration = int((time() - start_time) * 1000) task_progress['duration_ms'] = max(prev_duration, new_duration)
__virtualname__ = 'pkg'
if not HAS_BOTO: return (False, 'The boto_lambda module could not be loaded: ' 'boto libraries not found') elif _LooseVersion(boto.__version__) < _LooseVersion(required_boto_version): return (False, 'The boto_lambda module could not be loaded: ' 'boto version {0} or later must be installed.'.format(required_boto_version)) elif _LooseVersion(boto3.__version__) < _LooseVersion(required_boto3_version): return (False, 'The boto_lambda module could not be loaded: ' 'boto version {0} or later must be installed.'.format(required_boto3_version)) else: return True
class OtherModel(models.Model): pass class Model(models.Model): m2m = models.ManyToManyField('OtherModel') field = Model._meta.get_field('m2m') self.assertEqual(field.check(from_model=Model), [])
app_config = apps.get_containing_app_config(module)
ret['comment'] = 'Event module not available. Schedule enable job failed.'
@ignore_warnings(module="django.core.cache.backends.base") def test_load_overlong_key(self): self.session._session_key = (string.ascii_letters + string.digits) * 20 self.assertEqual(self.session.load(), {})
header_fields = ['Iter', 'Train Loss'] verbose_fmt = ['{iter:>10d}', '{train_score:>16.4f}'] if est.subsample < 1: header_fields.append('OOB Improve') verbose_fmt.append('{oob_impr:>16.4f}') header_fields.append('Remaining Time') verbose_fmt.append('{remaining_time:>16s}')
@patch('salt.utils.which', lambda exe: exe) def test_existing_binary_in_linux(self): self.assertTrue(salt.utils.which('this-binary-exists-under-linux'))
return self.q(css='.course-number-override .certificate-value').first.text[0]
outside_tar = self.unsafe_common_dir / "unsafe_file.tar.gz" with tarfile.open(outside_tar, "w:gz") as tar: tar.addfile(tarfile.TarInfo(str(self.unsafe_common_dir / "../a_file"))) return outside_tar
ROOT_EXTRA_FIELDS = 'root_extra_fields'
s = self.ts.copy() result = getattr(s, op)(s) self.assertEqual(result.name, self.ts.name)
ncnt = 0 while True: part = next(queue) if part is None: time.sleep(0.01) ncnt += 1 if ncnt > 5: break continue if self.opts.get('raw'): parts.update({part['id']: part}) if part['id'] in minion_tracker[queue]['minions']: minion_tracker[queue]['minions'].remove(part['id']) else: print_cli('minion {0} was already deleted from tracker, probably a duplicate key'.format(part['id'])) else: parts.update(part) for id in part.keys(): if id in minion_tracker[queue]['minions']: minion_tracker[queue]['minions'].remove(id) else: print_cli('minion {0} was already deleted from tracker, probably a duplicate key'.format(id))
child = self.store.get_item(locations['child']) child.display_name = 'Changed Display Name' self.store.update_item(child, self.user_id)
df = DataFrame({u'あああああ': [1, 222, 33333, 4], 'b': [u'あ', u'いいい', u'¡¡', u'ええええええ']}, index=['a', 'bb', 'c', '¡¡¡']) expected = (u" b あああああ\na あ 1\n" u"bb いいい 222\nc ¡¡ 33333\n" u"¡¡¡ ええええええ 4") self.assertEqual(_rep(df), expected)
if type(mdata1) != type(mdata2): self.fail(self._formatMessage(msg, u"{} is not same type as {}".format(mdata1, mdata2))) for attr in mdata1.ATTRS_ALLOWED_TO_UPDATE: self.assertEqual(getattr(mdata1, attr), getattr(mdata2, attr), msg)
charlie = User(pk=51, username='charlie', email='charlie@example.com') charlie.set_unusable_password()
if num_format_str is None and style_dict is None: return None
clf = self.factory() clf.fit(X2, Y2)
return response
self.cart = Order.get_cart_for_user(self.user) CertificateItem.add_to_order( self.cart, self.course_key, self.COST, 'verified' ) self.cart.start_purchase()
resource = {"id": self.resource_id_second, 'reason': ''} if test_case['handler'] == 'remove_resource': self.check_event_response_by_http_status(test_case['handler'], resource, test_case['status']) else: self.check_event_response_by_key(test_case['handler'], resource, test_case['key'], test_case['val'])
arr = np.array([1] * 5, dtype='int64') result = to_timedelta(arr, unit='s') expected = TimedeltaIndex([np.timedelta64(1, 's')] * 5) tm.assert_index_equal(result, expected)
request = Mock() request.user = User.objects.get(username=requester_username) request.get_host = Mock(return_value="testhost") request.META = {'REMOTE_ADDR': '0:0:0:0', 'SERVER_NAME': 'testhost'} request.is_secure = Mock(return_value=False) return request
values = range(55109) data = pd.DataFrame.from_dict({'a': values, 'b': values, 'c': values, 'd': values}) grouped = data.groupby(['a', 'b', 'c', 'd']) self.assertEqual(len(grouped), len(values))
Ensure myasg exists: boto_asg.present: - name: myasg - launch_config_name: mylc - availability_zones: - us-east-1a - us-east-1b - min_size: 1 - max_size: 1 - desired_capacity: 1 - load_balancers: - myelb - profile: keyid: GKTADJGHEIQSXMKKRBJ08H key: askdjghsdfjkghWupUjasdflkdfklgjsdfjajkghs region: us-east-1
student_module = StudentModule.objects.get( course_id=self.course.id, student=self.student_user ) for val in ('Correct', True, False, 0, 0.0, 1, 1.0, None): state = json.loads(student_module.state) state["student_answers"]['{}_2_1'.format(self.p1_html_id)] = val student_module.state = json.dumps(state) student_module.save()
youtube_str = '0.75_BAD!!!,1.0:AXdE34_U,1.25:KLHF9K_Y,1.5:VO3SxfeD,' output = VideoDescriptor._parse_youtube(youtube_str) self.assertEqual(output, {'0.75': '', '1.00': 'AXdE34_U', '1.25': 'KLHF9K_Y', '1.50': 'VO3SxfeD'})
return np.object_
SCORE_CHANGED.send( sender=None, points_possible=event['max_value'], points_earned=event['value'], user_id=user_id, course_id=unicode(course_id), usage_id=unicode(descriptor.location) )
lhs = DataFrame(randint(5, size=(5, 2))) expect = -lhs result = pd.eval(expr, engine=self.engine, parser=self.parser) assert_frame_equal(expect, result)
response['X-XRDS-Location'] = get_xrds_url('identity', request) return response
if SALT_CRON_IDENTIFIER in comment_line: parts = comment_line.split(SALT_CRON_IDENTIFIER) comment_line = parts[0].rstrip() if len(parts[1]) > 1: identifier = parts[1][1:]
filedata = json.dumps(subs, indent=2) mime_type = 'application/json' filename = 'subs_{0}.srt.sjson'.format(subs_id) content_location = StaticContent.compute_location(self.course.id, filename) content = StaticContent(content_location, filename, mime_type, filedata) contentstore().save(content) del_cached_content(content_location) return content_location
theano.config.floatX = self.orig_floatX
def __init__(self, loss="squared_loss", penalty="l2", alpha=0.0001, l1_ratio=0.15, fit_intercept=True, n_iter=5, shuffle=True, verbose=0, epsilon=DEFAULT_EPSILON, random_state=None, learning_rate="invscaling", eta0=0.01, power_t=0.25, warm_start=False, average=False): super(SGDRegressor, self).__init__(loss=loss, penalty=penalty, alpha=alpha, l1_ratio=l1_ratio, fit_intercept=fit_intercept, n_iter=n_iter, shuffle=shuffle, verbose=verbose, epsilon=epsilon, random_state=random_state, learning_rate=learning_rate, eta0=eta0, power_t=power_t, warm_start=warm_start, average=average)
unfiltered_response = self.verify_response(params={'username': self.staff_user.username}) for org in [self.course.org, alternate_course.org]: self.assertTrue(
split_rerun_id = CourseLocator(org=org, course=course_number, run="2012_Q2") CourseRerunState.objects.initiated(course.id, split_rerun_id, self.user, fields['display_name']) result = rerun_course.delay( unicode(course.id), unicode(split_rerun_id), self.user.id, json.dumps(fields, cls=EdxJSONEncoder) )
source = utils.find_staff_lock_source(item) self.assertEqual(source.location, expected_source.location) self.assertTrue(source.visible_to_staff_only)
response = searcher.search( doc_type=CoursewareSearchIndexer.DOCUMENT_TYPE, field_dictionary={"course": unicode(self.course.id)} ) self.assertEqual(response["total"], 3)
if subscribe: UserPreference.objects.get_or_create(user=user, key=NOTIFICATION_PREF_KEY, defaults={ "value": UsernameCipher.encrypt(user.username) }) return render_to_response("resubscribe.html", {'token': token}) else: UserPreference.objects.filter(user=user, key=NOTIFICATION_PREF_KEY).delete() return render_to_response("unsubscribe.html", {'token': token})
if len(self) == 0: return self._constructor(dtype=self.dtype, index=self.index).__finalize__(self) if kwds or args and not isinstance(func, np.ufunc): f = lambda x: func(x, *args, **kwds) else: f = func if isinstance(f, np.ufunc): return f(self) if is_extension_type(self.dtype): mapped = self._values.map(f) else: values = self.asobject mapped = lib.map_infer(values, f, convert=convert_dtype) if len(mapped) and isinstance(mapped[0], Series): from pandas.core.frame import DataFrame return DataFrame(mapped.tolist(), index=self.index) else: return self._constructor(mapped, index=self.index).__finalize__(self)
pass
if getattr(xblock, 'is_draft', False): return True elif xblock.has_children: if len(xblock.children) > len(xblock.get_children()): return True return any([self.has_changes(child) for child in xblock.get_children()]) else: return False
response = self.client.get(reverse('admin:admin_views_article_changelist'), {'o': '-3'}) self.assertContentBefore( response, 'Newest content', 'Middle content', "Results of sorting on Model method are out of order." ) self.assertContentBefore( response, 'Middle content', 'Oldest content', "Results of sorting on Model method are out of order." )
callable = callable
self.criteria = criteria[_REQUIRES_PATH] + \ criteria[_REQUIRES_STAT] + \ criteria[_REQUIRES_CONTENTS]
import salt.utils from salt.ext.six import string_types from salt.exceptions import SaltInvocationError, CommandExecutionError import salt.ext.six as six
charged_amt = Decimal(params['ccAuthReply_amount'])
email_placeholder = _(u"username@domain.com")
return transformer.transform(X) * transformer_weights[name]
self.elapsed = time.time() - self._start_time
return ( self.q(css="#login-anchor").is_present() and self.q(css="#register-anchor").is_present() and self.current_form is not None )
] DATETIME_INPUT_FORMATS = [
#latex_font_size = '10pt'
NO_BOTO_MODULE = True BOTO_NOT_CONFIGURED = True try: import boto NO_BOTO_MODULE = False try: boto.connect_iam() BOTO_NOT_CONFIGURED = False except boto.exception.NoAuthHandlerFound: pass except ImportError: pass
option2 = main2
n_samples_per_label = np.bincount(labels)
return block_structure.get_transformer_block_field( block_key, cls, cls.BLOCK_DEPTH, )
self.assert_enrollment_status(username='fake-user', expected_status=status.HTTP_404_NOT_FOUND, as_server=False) self.assert_enrollment_status(username='fake-user', expected_status=status.HTTP_406_NOT_ACCEPTABLE, as_server=True)
if isinstance(account_ou, str): account_ou = account_ou.split('\\') account_ou = ''.join(account_ou)
name_is_eq = (other.get('name') is None or self.name == other['name'])
return self.connectable.execute(*args, **kwargs)
except NotFoundError: log.exception("Module indicating to user that request doesn't exist") raise Http404
CourseEnrollment.enroll(self.first_audit_user, self.course_key, "audit") CourseEnrollment.enroll(self.second_audit_user, self.course_key, "audit") CourseEnrollment.enroll(self.third_audit_user, self.course_key, "audit")
if not db_exists(name, **connection_args): log.info('DB \'{0}\' does not exist'.format(name)) return False
media = Media() for field in self.fields.values(): media = media + field.widget.media return media
return offset.days * 86400 + offset.seconds
aid = self.answer_ids[-1] new_cmap.set_hint_and_mode(aid, hint_text, hintmode)
def _check_cast(df, v): self.assertEqual( list(set([s.dtype.name for _, s in compat.iteritems(df)]))[0], v)
from datetime import datetime import sys import os import nose import numpy as np
return self.q(css='input[name=problem-grade-report]')
mlp.fit(X, y)
pass
AUTH_ENTRY_CUSTOM = getattr(settings, 'THIRD_PARTY_AUTH_CUSTOM_AUTH_FORMS', {})
frame = DataFrame(mat, columns=['A', 'B', 'C'], index=[1, 2], dtype=np.float64) self.assertEqual(frame.values.dtype, np.float64)
diff = np.zeros(len(out), dtype='bool') for lab in labels[:-1]: diff |= np.r_[True, lab[1:] != lab[:-1]]
if issubclass(inds.dtype.type, np.datetime64): inds = inds.view(np.int64) if inds.dtype == np.object_: inds = lib.maybe_convert_objects(inds)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
key = self.cache_key(template_name, template_dirs, skip) cached = self.get_template_cache.get(key) if cached: if isinstance(cached, type) and issubclass(cached, TemplateDoesNotExist): raise cached(template_name) elif isinstance(cached, TemplateDoesNotExist): raise copy_exception(cached) return cached try: template = super(Loader, self).get_template( template_name, template_dirs, skip, ) except TemplateDoesNotExist as e: self.get_template_cache[key] = copy_exception(e) if self.engine.debug else TemplateDoesNotExist raise else: self.get_template_cache[key] = template return template
vmconfig['changed'][prop] = vmconfig['state'][prop]
brc_partial.set_params(n_clusters=3) brc_partial.partial_fit(None) assert_array_equal(brc_partial.subcluster_labels_, brc.subcluster_labels_)
import salt.utils from salt.exceptions import CommandExecutionError
caches['default'] response = default_view(request, '11') self.assertEqual(response.content, b'Hello World 1')
return (retrieve_all, kwargs)
library_content_metadata = { 'source_library_id': unicode(self.library_key), 'mode': 'random', 'max_count': 1, 'has_score': False } course_fixture.add_children( XBlockFixtureDesc('chapter', SECTION_NAME).add_children( XBlockFixtureDesc('sequential', SUBSECTION_NAME).add_children( XBlockFixtureDesc('vertical', UNIT_NAME).add_children( XBlockFixtureDesc('library_content', "Library Content", metadata=library_content_metadata) ) ) ) )
mock_inf_db_client = MagicMock(return_value=MockInfluxDBClient()) with patch.object(influx, '_client', mock_inf_db_client): self.assertEqual(influx.db_list(user='root', password='root', host='localhost', port=8086), DB_LIST)
cmap = CorrectMap() for answer_id in answer_ids: cmap.update(CorrectMap(answer_id=answer_id, queuestate=None)) self.problem.correct_map.update(cmap)
msg = 'Account not yet activated: please look for link in your email' return default_render_failure(request, msg)
super(TestReindexCourse, self).setUp() self.store = modulestore() self.first_lib = LibraryFactory.create( org="test", library="lib1", display_name="run1", default_store=ModuleStoreEnum.Type.split ) self.second_lib = LibraryFactory.create( org="test", library="lib2", display_name="run2", default_store=ModuleStoreEnum.Type.split ) self.first_course = CourseFactory.create( org="test", course="course1", display_name="run1" ) self.second_course = CourseFactory.create( org="test", course="course2", display_name="run1" )
return (user, None)
if paid_course_reg_item is not None: coupon_redemption = CouponRedemption.objects.select_related('coupon').filter( order_id=paid_course_reg_item.order_id) coupon_codes = [redemption.coupon.code for redemption in coupon_redemption] coupon_codes = ", ".join(coupon_codes) registration_code_used = 'N/A'
Foo.objects.filter(d__gte=100000000000)
import yaml import msgpack import salt.ext.six as six if salt.utils.is_windows(): import win32api
if not self or not other: return False
sfn = '{0}/{1}/schedule.conf'.format(__opts__['config_dir'], os.path.dirname(__opts__['default_include'])) if os.path.isfile(sfn): with salt.utils.fopen(sfn, 'rb') as fp_: try: schedule = yaml.safe_load(fp_.read()) except yaml.YAMLError as exc: ret['comment'].append('Unable to read existing schedule file: {0}'.format(exc))
if key_betas is not None: betas = numpy.hstack((betas, key_betas)) betas.sort()
csv_content = "nonenrolled@test.com,dummy_notes" data = self.upload_file(csv_content=csv_content) self.assertEquals(len(data['row_errors']['user_not_enrolled']), 1) self.assertEquals(len(data['general_errors']), 0) self.assertEquals(len(data['success']), 0)
clf = Pipeline([("svc", SVC())]) assert_raises(ValueError, clf.set_params, svc__stupid_param=True) assert_raises(ValueError, clf.set_params, svm__stupid_param=True)
thread = self.setup_thread() self.team_page.visit() self.assertEqual(self.team_page.discussion_id, self.teams[0]['discussion_topic_id']) discussion = self.team_page.discussion_page self.assertTrue(discussion.is_browser_on_page()) self.assertTrue(discussion.is_discussion_expanded()) self.assertEqual(discussion.get_num_displayed_threads(), 1) self.assertTrue(discussion.has_thread(thread['id'])) assertion = self.assertTrue if should_have_permission else self.assertFalse assertion(discussion.q(css='.post-header-actions').present) assertion(discussion.q(css='.add-response').present) assertion(discussion.q(css='.new-post-btn').present)
cm = metrics.confusion_matrix(y_test, y_predicted) print(cm)
elif existing['code'] == 404: ret['result'] = True ret['comment'] = 'This Monitor already does not exist. No changes made.' ret['changes']['old'] = {} ret['changes']['new'] = {} else: ret = _load_result(existing, ret)
vote_count = 0 if current_vote_status: self.register_get_user_response(self.user, upvoted_ids=["test_comment"]) vote_count = 1 self.register_comment_votes_response("test_comment") self.register_comment(overrides={"votes": {"up_count": vote_count}}) data = {"voted": new_vote_status} result = update_comment(self.request, "test_comment", data) self.assertEqual(result["vote_count"], 1 if new_vote_status else 0) self.assertEqual(result["voted"], new_vote_status) last_request_path = urlparse(httpretty.last_request().path).path votes_url = "/api/v1/comments/test_comment/votes" if current_vote_status == new_vote_status: self.assertNotEqual(last_request_path, votes_url) else: self.assertEqual(last_request_path, votes_url) self.assertEqual( httpretty.last_request().method, "PUT" if new_vote_status else "DELETE" ) actual_request_data = ( httpretty.last_request().parsed_body if new_vote_status else parse_qs(urlparse(httpretty.last_request().path).query) ) actual_request_data.pop("request_id", None) expected_request_data = {"user_id": [str(self.user.id)]} if new_vote_status: expected_request_data["value"] = ["up"] self.assertEqual(actual_request_data, expected_request_data) event_name, event_data = mock_emit.call_args[0] self.assertEqual(event_name, "edx.forum.response.voted") self.assertEqual( event_data, { 'undo_vote': not new_vote_status, 'url': '', 'target_username': self.user.username, 'vote_value': 'up', 'user_forums_roles': [FORUM_ROLE_STUDENT], 'user_course_roles': [], 'commentable_id': 'dummy', 'id': 'test_comment' } )
msg['return']['ret']['minions'] = self._availablize(msg['return']['ret']['minions']) if msg.get('__worker_verify') == self.worker_verify.value: self.publish.value.append(msg)
result = self.frame.add(self.frame[:0]) assert_frame_equal(result, self.frame * np.nan)
module = CapaFactory.create(max_attempts=None, rerandomize=RANDOMIZATION.NEVER, done=False) self.assertFalse(module.should_show_save_button())
from __future__ import absolute_import, print_function import json import logging
CommitOnSuccessManager.ENABLED = False OuterAtomic.ALLOW_NESTED = True if not hasattr(OuterAtomic, 'atomic_for_testcase_calls'): OuterAtomic.atomic_for_testcase_calls = 0 OuterAtomic.atomic_for_testcase_calls += 1 return wrapped_func(*args, **kwargs)
envs = __opts__.get('azurefs_envs', []) for env in envs: storage_conn = azure.get_storage_conn(opts=envs[env]) result = azure.list_blobs( storage_conn=storage_conn, container=env, )
Wi = (Vi - 2 * np.outer(np.dot(Vi, h), h) + (1 - alpha_i) * w_reg[i, :, None])
self.assertRedirects(response, '/some_view/')
self.assertTrue(self._search_for_content(self.EDITED_SEARCH_STRING))
X = np.arange(100).reshape(-1, 1) X_embedded = X.copy() random_state.shuffle(X_embedded) assert_less(trustworthiness(X, X_embedded), 0.6)
certificate_html_view_configuration_model = apps.get_model("certificates", "CertificateHtmlViewConfiguration") certificate_html_view_configuration_model.objects.all().delete()
import integration import salt.utils
result = prefix_middleware.process_request(request) self.assertIsNone(result)
self._validate_estimator()
shutil.move(src_filename, dst_filename)
alphas.append(0) grid_scores.append(cross_val_score(EmpiricalCovariance(), X, cv=cv, n_jobs=self.n_jobs, verbose=inner_verbose)) self.grid_scores = np.array(grid_scores) best_alpha = alphas[best_index] self.alpha_ = best_alpha self.cv_alphas_ = alphas
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
assert_array_equal(reg.predict([[0.2, -1.0]]), np.array([-1]))
assert_raises(ValueError, sel.inverse_transform, np.array([[1], [2]]))
result = f.clean('21-12-2010') self.assertEqual(result, date(2010, 12, 21))
first_addend = random.randint(-100, 100) second_addend = 10 - first_addend
self.assertURLEqual( '{}?_changelist_filters=is_staff__exact%3D0%26is_superuser__exact%3D0'.format(change_user_url), '{}?_changelist_filters=is_superuser__exact%3D0%26is_staff__exact%3D0'.format(change_user_url) )
return False
#----------------------------------------------------------------- dbar = dbarn epln = eplnn dlta = cs * dbar + sn * alpha gbar = sn * dbar - cs * alpha
from __future__ import unicode_literals
predictions = cross_val_predict(clf, X_sparse, multioutput_y) assert_equal(predictions.shape, (150, 2))
return " ".join(str(arg) for arg in args if arg)
__virtualname__ = 'win_iis'
factory_kwargs = {'timeout': timeout, 'safe': safe} if getattr(self, 'io_loop', None):
if not hasattr(self, 'W_lr_scale'): self.W_lr_scale = None if not hasattr(self, 'b_lr_scale'): self.b_lr_scale = None rval = OrderedDict() if self.W_lr_scale is not None: W, = self.transformer.get_params() rval[W] = self.W_lr_scale if self.b_lr_scale is not None: rval[self.b] = self.b_lr_scale return rval
self.john = Employee.objects.create(name='John Blue', department=self.dev) self.jack = Employee.objects.create(name='Jack Red', department=self.design)
gating_api.add_prerequisite(self.course.id, self.seq1.location)
if dispatch == 'preview_chemcalc': return self.preview_chemcalc(data) return {}
simplefilter('ignore')
mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh') self._mock_sh = mock_sh.start()
for index in xrange(3): self.course_fixture.create_xblock( self.course_fixture.get_nested_xblocks(category="vertical")[index].locator, XBlockFixtureDesc('html', 'Unpublished HTML Component ' + str(index)), )
if not self.onOffset(dt): businesshours = self._get_business_hours_by_sec() if self.n >= 0: dt = self._prev_opening_time( dt) + timedelta(seconds=businesshours) else: dt = self._next_opening_time( dt) + timedelta(seconds=businesshours) return dt
logger.info("These packages are available:") for this_package in packages_sources.values(): if this_package.name in installed_packages_list: state="u" if installed_packages_list[this_package.name].timestamp<this_package.timestamp else 'i'; else: state="-" package_time = time.strftime("%a, %d %b %Y %H:%M:%S", time.gmtime(this_package.timestamp)) logger.info("{0} {1:<20} {2:<8} " "{3:<30} {4}".format(state, this_package.name, this_package.readable_size, package_time, this_package.source))
if not self.theme_location: return False
with self.assertRaisesMessage(Exception, "Oops"):
self.client.login(username=self.student.username, password=self.PASSWORD) response = self.client.get(reverse("dashboard")) self.assertNotContains(response, "donate-container")
assert_raises(ValueError, svm.SVC(C=-1).fit, X, Y)
salt '*' pkg.install sources='[{"<pkg name>": "http://packages.server.com/<pkg filename>"}]' salt '*' pkg.install sources='[{"SMClgcc346": "http://packages.server.com/gcc-3.4.6-sol10-sparc-local.pkg"}]'
sources = _consolidate_repo_sources(sources)
self.q(css='.wrapper-create-course .new-course-save').first.click() self.wait_for_ajax()
elif typ == u'block_index': return globals()[obj[u'klass']](obj[u'length'], obj[u'blocs'], obj[u'blengths']) elif typ == u'int_index': return globals()[obj[u'klass']](obj[u'length'], obj[u'indices']) elif typ == u'ndarray': return unconvert(obj[u'data'], np.typeDict[obj[u'dtype']], obj.get(u'compress')).reshape(obj[u'shape']) elif typ == u'np_scalar': if obj.get(u'sub_typ') == u'np_complex': return c2f(obj[u'real'], obj[u'imag'], obj[u'dtype']) else: dtype = dtype_for(obj[u'dtype']) try: return dtype(obj[u'data']) except: return dtype.type(obj[u'data']) elif typ == u'np_complex': return complex(obj[u'real'] + u'+' + obj[u'imag'] + u'j') elif isinstance(obj, (dict, list, set)): return obj else: return obj
return VectorSpace(dim=self._total)
model.threshold = 1.0 assert_greater(X_transform.shape[1], model.transform(data).shape[1])
keys1 = list(dict1.keys()) keys2 = list(dict2.keys()) result = {} for key1 in keys1: result[key1] = dict1[key1] for key2 in keys2: result[key2] = dict2[key2] return result
centers = _init_centroids(X, n_clusters, init, random_state=random_state, x_squared_norms=x_squared_norms) centers = np.ascontiguousarray(centers) if verbose: print('Initialization complete') centers, labels, n_iter = k_means_elkan(X, n_clusters, centers, tol=tol, max_iter=max_iter, verbose=verbose) inertia = np.sum((X - centers[labels]) ** 2) return labels, inertia, centers, n_iter
styles = list('rgcby') axes = df.plot(style=styles, subplots=True) for ax, c in zip(axes, styles): self._check_colors(ax.get_lines(), linecolors=[c]) tm.close()
if resolv_alias: if hasattr(func, 'func_globals') and name in func.func_globals: if not func.func_globals[name] is func: name = '%s-alias' % name if inspect.ismethod(func): if hasattr(func, 'im_class'): klass = func.im_class module.append(klass.__name__) if os.name == 'nt' and win_characters: name = _clean_win_chars(name) module = [_clean_win_chars(s) for s in module] return module, name
'SHOW_LANGUAGE_SELECTOR': False,
client = self.login_client(api_client, user) self.send_patch( client, { "string_pref": "updated_value", "new_pref": "new_value", "extra_pref": None, }, expected_status=403 if user == "staff_user" else 404 )
cache.set('expire1', 'very quickly', 1) cache.set('expire2', 'very quickly', 1) cache.set('expire3', 'very quickly', 1)
from __future__ import unicode_literals
return __MP_LOGGING_QUEUE
def __init__(self): self.modules = {'A': MockContext()}
tm.assert_numpy_array_equal(Float64Index([1.0, np.nan]).isin([np.nan]), np.array([False, True])) tm.assert_numpy_array_equal( Float64Index([1.0, np.nan]).isin([float('nan')]), np.array([False, True])) tm.assert_numpy_array_equal(Float64Index([1.0, np.nan]).isin([pd.NaT]), np.array([False, True]))
name = com._get_callable_name(a) or a keys.append(name)
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
foo = 'bar' def __eq__(self, other): return self.foo == other.foo
self.q(css=".subsection-header-actions .configure-button").first.click() self.q(css="#start_date").fill("01/01/2030") self.q(css=".action-save").first.click() self.wait_for_ajax()
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
with connection.schema_editor() as editor: editor.create_model(Author) columns = self.column_classes(Author) self.assertEqual(columns['name'][0], "CharField") with connection.schema_editor() as editor: editor.alter_db_table(Author, "schema_author", "schema_otherauthor") Author._meta.db_table = "schema_otherauthor" columns = self.column_classes(Author) self.assertEqual(columns['name'][0], "CharField") with connection.schema_editor() as editor: editor.alter_db_table(Author, "schema_otherauthor", "schema_author") Author._meta.db_table = "schema_author" columns = self.column_classes(Author) self.assertEqual(columns['name'][0], "CharField")
n_samples, n_features = X.shape n_components, _ = means.shape log_prob = np.empty((n_samples, n_components)) for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)): log_det = -2. * np.sum(np.log(np.diagonal(prec_chol))) y = np.dot(X - mu, prec_chol) log_prob[:, k] = -.5 * (n_features * np.log(2. * np.pi) + log_det + np.sum(np.square(y), axis=1)) return log_prob
np.random.seed(0) X[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1)) y[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)
UNICODETEST_WITH_SIGNS = 'Testing Unicode \N{COPYRIGHT SIGN},\N{TRADE MARK SIGN},\N{REGISTERED SIGN} '+TIMESTR UNICODETEST_WITHOUT_SIGNS = 'Testing Unicode'+TIMESTR UNICODE_TEST_KEY = 'UnicodeKey \N{TRADE MARK SIGN} '+TIME_INT_UNICODE UNICODE_TEST_KEY_DEL = 'Delete Me \N{TRADE MARK SIGN} '+TIME_INT_UNICODE
self.assertRaises(KeyError, s.__getitem__, (2000, 3, 4))
toggles = { 'address-status': {'type': 'yes_no', 'value': address_status}, 'cmp-enabled': {'type': 'yes_no', 'value': cmp_enabled}, 'dhcp-relay': {'type': 'true_false', 'value': dhcp_relay}, 'reject': {'type': 'true_false', 'value': reject}, '12-forward': {'type': 'true_false', 'value': twelve_forward}, 'internal': {'type': 'true_false', 'value': internal}, 'ip-forward': {'type': 'true_false', 'value': ip_forward} }
students = self._create_students(10)
return hash(self.__class__) ^ hash(self.varname)
#pylint: disable=E0602
self.assertEqual(response.status_code, 400)
messages.pop()
__virtualname__ = 'service'
accept_header = self.content_type_map['real-accept-header-json'] response = self.fetch('/', headers={'Accept': accept_header}) self.assertEqual(response.headers['Content-Type'], self.content_type_map['json']) self.assertEqual(type(json.loads(response.body)), dict)
centers = np.array([ [0.0, 5.0, 0.0, 0.0, 0.0], [1.0, 1.0, 4.0, 0.0, 0.0], [1.0, 0.0, 0.0, 5.0, 1.0], ]) n_samples = 100 n_clusters, n_features = centers.shape X, true_labels = make_blobs(n_samples=n_samples, centers=centers, cluster_std=1., random_state=42) X_csr = sp.csr_matrix(X)
raise TypeError('Improper geometry input type: %s' % str(type(geo_input)))
with patch.dict('django.conf.settings.FEATURES', {'ENABLE_MKTG_SITE': True}): self.assertTrue(is_marketing_link_set('ABOUT')) self.assertFalse(is_marketing_link_set('NOT_CONFIGURED')) with patch.dict('django.conf.settings.FEATURES', {'ENABLE_MKTG_SITE': False}): self.assertTrue(is_marketing_link_set('ABOUT')) self.assertFalse(is_marketing_link_set('NOT_CONFIGURED'))
from __future__ import absolute_import import logging import re
if len(idx) == 0: continue else: with tm.assertRaises(Exception): func(idx)
return name + '_valid'
@override_settings(TEMPLATES=[{ 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'OPTIONS': { 'loaders': [ ('django.template.loaders.locmem.Loader', { '403.html': 'This is a test template for a 403 error ({{ exception }}).', }), ], }, }]) def test_403_template(self): response = self.client.get('/raises403/') self.assertContains(response, 'test template', status_code=403) self.assertContains(response, '(Insufficient Permissions).', status_code=403)
from __future__ import absolute_import
if logging.getLoggerClass() is not SaltLoggingClass:
import salt.utils.event from salt.ext import six
course = ItemFactory.create( parent_location=self.course.location, category="course", display_name="Test course", )
settings = options.get("settings", None) asset_settings = options.get("asset_settings", None) is_optimized = options.get("optimized", False) is_fast = options.get("fast", False) self.reset_task_messages() call_task("pavelib.servers.run_all_servers", options=options) expected_settings = settings if settings else "devstack" expected_asset_settings = asset_settings if asset_settings else expected_settings if is_optimized: expected_settings = "devstack_optimized" expected_asset_settings = "test_static_optimized" expected_collect_static = not is_fast and expected_settings != "devstack" expected_messages = [] if not is_fast: expected_messages.append(EXPECTED_PREPROCESS_ASSETS_COMMAND.format( system="lms", asset_settings=expected_asset_settings )) expected_messages.append(EXPECTED_PREPROCESS_ASSETS_COMMAND.format( system="cms", asset_settings=expected_asset_settings )) expected_messages.append(u"xmodule_assets common/static/xmodule") expected_messages.append(u"install npm_assets") expected_messages.append(EXPECTED_COFFEE_COMMAND.format(platform_root=self.platform_root)) expected_messages.extend(self.expected_sass_commands()) if expected_collect_static: expected_messages.append(EXPECTED_COLLECT_STATIC_COMMAND.format( system="lms", asset_settings=expected_asset_settings )) expected_messages.append(EXPECTED_COLLECT_STATIC_COMMAND.format( system="cms", asset_settings=expected_asset_settings )) expected_messages.append( EXPECTED_RUN_SERVER_COMMAND.format( system="lms", settings=expected_settings, port=8000, ) ) expected_messages.append( EXPECTED_RUN_SERVER_COMMAND.format( system="cms", settings=expected_settings, port=8001, ) ) expected_messages.append(EXPECTED_CELERY_COMMAND.format(settings="dev_with_worker")) self.assertEquals(self.task_messages, expected_messages)
raise
cmd = ( '{0} -NoProfile -ExecutionPolicy unrestricted ' '-Command "iex ((new-object net.webclient).' 'DownloadString(\'https://chocolatey.org/install.ps1\'))" ' '&& SET PATH=%PATH%;%systemdrive%\\chocolatey\\bin' .format(ps_path) ) result = __salt__['cmd.run_all'](cmd, python_shell=True)
dr = pd.date_range('2016-01-01', '2016-01-03', freq='12H') abc = ['a', 'b', 'c'] ix = pd.MultiIndex.from_product([dr, abc]) df = pd.DataFrame({'c1': range(0, 15)}, index=ix) idx = pd.IndexSlice
MAX_ATTEMPTS = 30 for i in range(MAX_ATTEMPTS): if exists( table_name, region, key, keyid, profile ): return True else:
sz = np.sum(z, axis=0) self.gamma_.T[1] = 1. + sz self.gamma_.T[2].fill(0) for i in range(self.n_components - 2, -1, -1): self.gamma_[i, 2] = self.gamma_[i + 1, 2] + sz[i] self.gamma_.T[2] += self.alpha
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
PIPELINE_JS_COMPRESSOR = None
with patch('salt.utils.which', mock_true): ret = parallels.__virtual__() self.assertTrue(ret) self.assertEqual(ret, 'parallels')
try: os.remove(trans_tar) except (OSError, IOError): pass
obj = self.clone() obj.add_annotation(Count('*'), alias='__count', is_summary=True) number = obj.get_aggregation(using, ['__count'])['__count'] if number is None: number = 0 return number
self.socket.sendall(bytes_)
raise Exception('Area on geodetic coordinate systems not supported.')
label = self._maybe_cast_slice_bound(label, side, kind)
if not _acquire_subtask_lock(current_task_id): format_str = "Unexpected task_id '{}': already being executed - for subtask of instructor task '{}'" msg = format_str.format(current_task_id, entry) TASK_LOG.warning(msg) dog_stats_api.increment('instructor_task.subtask.duplicate.locked', tags=[entry.course_id]) raise DuplicateTaskException(msg)
from pandas.io import clipboard clipboard.to_clipboard(self, excel=excel, sep=sep, **kwargs)
pear_group = self.notes_page.tag_groups[group_index] self.assertEqual(tag_name + " (3)", pear_group.title) self.assertTrue(pear_group.scrolled_to_top(group_index))
df = df_orig.copy() df.loc[(slice(None), 1), (slice(None), ['foo'])] = 100 expected = df_orig.copy() expected.iloc[[0, 3], [1, 3]] = 100 assert_frame_equal(df, expected)
other = RangeIndex(1, 6) result = self.index.intersection(other) expected = Index(np.sort(np.intersect1d(self.index.values, other.values))) self.assert_index_equal(result, expected)
i = 200 while i > 0: module = CapaFactory.create(rerandomize=rerandomize) assert 0 <= module.seed < 1000 i -= 1
site_configuration_history = SiteConfigurationHistory.objects.filter( site=site_configuration.site, ).all()
gb = df.groupby("A") exp_idx = pd.CategoricalIndex(['a', 'b', 'z'], name='A', ordered=True) expected = DataFrame({'values': Series([3, 7, np.nan], index=exp_idx)}) result = gb.sum() tm.assert_frame_equal(result, expected)
self.service.set_credit_requirement_status( self.user.id, self.course.id, 'grade', 'grade' )
if not hasattr(self, 'sampling_procedure') or \ self.sampling_procedure is None: self.sampling_procedure = GibbsEvenOdd() self.sampling_procedure.set_dbm(self)
df5 = df.set_index('a') ax = df5.plot(y='b') self._check_legend_labels(ax, labels=['b']) ax = df5.plot(y='b', label='LABEL_b') self._check_legend_labels(ax, labels=['LABEL_b']) self._check_text_labels(ax.xaxis.get_label(), 'a') ax = df5.plot(y='c', label='LABEL_c', ax=ax) self._check_legend_labels(ax, labels=['LABEL_b', 'LABEL_c']) self.assertTrue(df5.columns.tolist() == ['b', 'c'])
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
patcher = mock.patch.object(linter_class, '_is_valid_directory', return_value=True) patch_start = patcher.start() self.addCleanup(patcher.stop) return patch_start
idx = PeriodIndex(['2016-05-16', 'NaT', NaT, np.NaN], freq='D')
columns = MultiIndex.from_tuples([('a', ''), ('c', 'c1')]) df2 = DataFrame(columns=columns, data=[[1, 33], [0, 44]])
if isinstance(data, dict): data = [data]
result = self.index.difference(self.index.sortlevel(1)[0]) self.assertEqual(len(result), 0)
self.colors = [np.asarray([1, 1, 0]), np.asarray([1, 0, 1]), np.asarray([0, 1, 0])]
self.assertQuerysetEqual( qs & qs2, [('Second Revision', 'First Revision')], transform=lambda r: (r.title, r.base.title), ordered=False )
X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
import salt.utils.boto3 import salt.utils.compat import salt.utils
try: from pandas.core.index import MultiIndex except ImportError: pass
from pandas.core.window import RollingGroupby return RollingGroupby(self, *args, **kwargs)
import os
def _check_vlan(): tag = __salt__['openvswitch.port_get_tag'](name) interfaces = __salt__['network.interfaces']() if not 0 <= id <= 4095: ret['result'] = False ret['comment'] = comment_vlan_invalid_id elif name not in interfaces: ret['result'] = False ret['comment'] = comment_vlan_invalid_name elif tag and name in port_list: try: if int(tag[0]) == id: ret['result'] = True ret['comment'] = comment_vlan_port_exists except (ValueError, KeyError): pass
class AuthorsInline(admin.TabularInline): model = Book.authors.through class BookAdmin(admin.ModelAdmin): inlines = [AuthorsInline] errors = BookAdmin(Book, AdminSite()).check() self.assertEqual(errors, [])
CSRF_COOKIE_AGE = 60 * 60 * 24 * 7 * 52 CSRF_COOKIE_SECURE = False
field_errors = {} serializer = self.get_serializer(self.get_object_or_none(), data=patch, partial=True) fields = self.get_serializer().get_fields() for key in patch: if key in fields and fields[key].read_only: field_errors[key] = { 'developer_message': "This field is not editable", 'user_message': _("This field is not editable"), } add_serializer_errors(serializer, patch, field_errors) return field_errors
if strategy == "median": median = np.empty(len(columns)) for i, column in enumerate(columns): median[i] = _get_median(column, n_zeros_axis[i])
if by is not None: warnings.warn("by argument to sort_index is deprecated, pls use " ".sort_values(by=...)", FutureWarning, stacklevel=2) if level is not None: raise ValueError("unable to simultaneously sort by and level") return self.sort_values(by, axis=axis, ascending=ascending, inplace=inplace)
assert_array_equal(X_transformed_sparse.toarray(), X_transformed_dense)
_ret = _eni_attribute( r['result'], 'source_dest_check', source_dest_check, region, key, keyid, profile ) ret['changes'] = dictupdate.update(ret['changes'], _ret['changes']) ret['comment'] = ' '.join([ret['comment'], _ret['comment']]) if not _ret['result']: ret['result'] = _ret['result'] return ret if allocate_eip: if 'allocationId' not in r['result']: if __opts__['test']: ret['comment'] = ' '.join([ret['comment'], 'An EIP is set to be allocated and assocaited to the ENI.']) else: eip_alloc = __salt__['boto_ec2.allocate_eip_address'](domain=None, region=region, key=key, keyid=keyid, profile=profile) if eip_alloc: _ret = __salt__['boto_ec2.associate_eip_address'](instance_id=None, instance_name=None, public_ip=None, allocation_id=eip_alloc['allocation_id'], network_interface_id=r['result']['id'], private_ip_address=None, allow_reassociation=False, region=region, key=key, keyid=keyid, profile=profile) if not _ret: _ret = __salt__['boto_ec2.release_eip_address'](public_ip=None, allocation_id=eip_alloc['allocation_id'], region=region, key=key, keyid=keyid, profile=profile) ret['result'] = False msg = 'Failed to assocaite the allocated EIP address with the ENI. The EIP {0}'.format('was successfully released.' if _ret else 'was NOT RELEASED.') ret['comment'] = ' '.join([ret['comment'], msg]) return ret else: ret['result'] = False ret['comment'] = ' '.join([ret['comment'], 'Failed to allocate an EIP address']) return ret else: ret['comment'] = ' '.join([ret['comment'], 'An EIP is already allocated/assocaited to the ENI']) if arecords: for arecord in arecords: if 'name' not in arecord: msg = 'The arecord must contain a "name" property.' raise SaltInvocationError(msg) log.debug('processing arecord {0}'.format(arecord)) _ret = None dns_provider = 'boto_route53' arecord['record_type'] = 'A' public_ip_arecord = False if 'public' in arecord: public_ip_arecord = arecord.pop('public') if public_ip_arecord: if 'publicIp' in r['result']: arecord['value'] = r['result']['publicIp'] elif 'public_ip' in eip_alloc: arecord['value'] = eip_alloc['public_ip'] else: msg = 'Unable to add an A record for the public IP address, a public IP address does not seem to be allocated to this ENI.' raise CommandExecutionError(msg) else: arecord['value'] = r['result']['private_ip_address'] if 'provider' in arecord: dns_provider = arecord.pop('provider') if dns_provider == 'boto_route53': if 'profile' not in arecord: arecord['profile'] = profile if 'key' not in arecord: arecord['key'] = key if 'keyid' not in arecord: arecord['keyid'] = keyid if 'region' not in arecord: arecord['region'] = region _ret = __states__['.'.join([dns_provider, 'present'])](**arecord) log.debug('ret from dns_provider.present = {0}'.format(_ret)) ret['changes'] = dictupdate.update(ret['changes'], _ret['changes']) ret['comment'] = ' '.join([ret['comment'], _ret['comment']]) if not _ret['result']: ret['result'] = _ret['result'] if ret['result'] is False: return ret return ret
data = super(PaidCourseRegistration, self).analytics_data() sku = data['sku'] if self.course_id != CourseKeyField.Empty: data['name'] = unicode(self.course_id) data['category'] = unicode(self.course_id.org) if self.mode: data['sku'] = sku + u'.' + unicode(self.mode) return data
from __future__ import absolute_import
try: rel_obj = getattr(instance, self.cache_name) except AttributeError: val = self.field.get_local_related_value(instance) if None in val: rel_obj = None else: qs = self.get_queryset(instance=instance) qs = qs.filter(self.field.get_reverse_related_filter(instance)) rel_obj = qs.get() if not self.field.remote_field.multiple: setattr(rel_obj, self.field.remote_field.get_cache_name(), instance) setattr(instance, self.cache_name, rel_obj)
string = ParseString(argument, 0, len(argument)) if string.string == argument and "<" not in argument: return True
all_filters = { "Images": ['image/png', 'image/jpeg', 'image/jpg', 'image/gif', 'image/tiff', 'image/tif', 'image/x-icon'], "Documents": [ 'application/pdf', 'text/plain', 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'application/vnd.openxmlformats-officedocument.wordprocessingml.template', 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'application/vnd.openxmlformats-officedocument.presentationml.slideshow', 'application/vnd.openxmlformats-officedocument.presentationml.template', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', 'application/vnd.openxmlformats-officedocument.spreadsheetml.template', 'application/msword', 'application/vnd.ms-excel', 'application/vnd.ms-powerpoint', ], } requested_file_types = all_filters.get(requested_filter, None) where = ["JSON.stringify(this.contentType).toUpperCase() == JSON.stringify('{}').toUpperCase()".format( req_filter) for req_filter in requested_file_types] filter_params = { "$where": ' || '.join(where), } return filter_params
df = DataFrame({'zero': {'a': 0.0, 'b': 1}, 'one': {'a': 2.0, 'b': 0}}) s = Series({'zero': 0.0, 'one': 2.0}) result = df.replace(s, {'zero': 0.5, 'one': 1.0}) expected = DataFrame( {'zero': {'a': 0.5, 'b': 1}, 'one': {'a': 1.0, 'b': 0.0}}) assert_frame_equal(result, expected)
modulestore().delete_course(destination_course_key, user_id)
self._auto_auth(self.USERNAME, self.EMAIL, False) self.dashboard.visit()
StudentViewTransformer.collect(block_structure) BlockCountsTransformer.collect(block_structure) BlockDepthTransformer.collect(block_structure) BlockNavigationTransformer.collect(block_structure)
self.assertEqual(request.get_port(), '8080')
alpha_grid, scores_path = lasso_stability_path(X, y, random_state=42, eps=0.05)
from __future__ import absolute_import
cases = [klass(second.values) for klass in [np.array, Series, list]] for case in cases: if isinstance(idx, PeriodIndex): msg = "can only call with other PeriodIndex-ed objects" with tm.assertRaisesRegexp(ValueError, msg): result = first.intersection(case) elif isinstance(idx, CategoricalIndex): pass else: result = first.intersection(case) self.assertTrue(tm.equalContents(result, second))
pass
if ':' not in ip_str: return False
ioloop.install()
dec = self._decision_function(X) if self.decision_function_shape is None and len(self.classes_) > 2: warnings.warn("The decision_function_shape default value will " "change from 'ovo' to 'ovr' in 0.18. This will change " "the shape of the decision function returned by " "SVC.", ChangedBehaviorWarning) if self.decision_function_shape == 'ovr' and len(self.classes_) > 2: return _ovr_decision_function(dec < 0, dec, len(self.classes_)) return dec
s = Series(self.arr) self.assertRaises(ValueError, s.ewm) self.assertRaises(ValueError, s.ewm, com=10.0, alpha=0.5) self.assertRaises(ValueError, s.ewm, span=10.0, alpha=0.5) self.assertRaises(ValueError, s.ewm, halflife=10.0, alpha=0.5)
return self._num_batches
ret['result'] = True if __opts__['test'] else __salt__['vmadm.start'](name, key='hostname') if not isinstance(ret['result'], bool) and ret['result'].get('Error'): ret['result'] = False ret['comment'] = 'failed to start {0}'.format(name) else: ret['changes'][name] = 'running' ret['comment'] = 'vm {0} started'.format(name)
self._validate_hyperparameters() if np.any(np.array(hidden_layer_sizes) <= 0): raise ValueError("hidden_layer_sizes must be > 0, got %s." % hidden_layer_sizes)
def f(): Series([timedelta(days=1), 'foo'], dtype='m8[ns]')
X_pred = rng.random_sample((2, 4)) K_pred = np.dot(X_pred, X_fit.T) X_pred_centered = scaler.transform(X_pred) K_pred_centered = np.dot(X_pred_centered, X_fit_centered.T) K_pred_centered2 = centerer.transform(K_pred) assert_array_almost_equal(K_pred_centered, K_pred_centered2)
ttime1 = time.time() out1 = f(spfilt, biasvals, img1d) ttot += time.time() - ttime1 temp = refout - out1 assert (temp < 1e-10).all() vis = T.grad(output, input, output) downprop = function([kerns,output], vis) temp1 = time.time() for zz in range(100): visval = downprop(spfilt,out1) indices, indptr, spmat_shape, sptype, outshp, kmap = \ sp.convolution_indices.sparse_eval(imshp,kshp,nkern,ss,conv_mode) spmat = sparse.csc_matrix((spfilt[kmap],indices,indptr),spmat_shape) visref = N.dot(out1,spmat.todense()) assert N.all(visref==visval)
test_db_allows_multiple_connections = True
'ENABLE_THIRD_PARTY_AUTH': False,
self.data = (self.data * 200) def time_read_csv_roundtrip_converter(self): read_csv(StringIO(self.data), sep=',', header=None, float_precision='round_trip') class read_csv_thou_vb(object): goal_time = 0.2 def setup(self): self.N = 10000 self.K = 8 self.format = (lambda x: '{:,}'.format(x)) self.df = DataFrame((np.random.randn(self.N, self.K) * np.random.randint(100, 10000, (self.N, self.K)))) self.df = self.df.applymap(self.format) self.df.to_csv('test.csv', sep='|') def time_read_csv_thou_vb(self): read_csv('test.csv', sep='|', thousands=',') def teardown(self): os.remove('test.csv') class read_csv_vb(object): goal_time = 0.2 def setup(self): self.N = 10000 self.K = 8 self.df = DataFrame((np.random.randn(self.N, self.K) * np.random.randint(100, 10000, (self.N, self.K)))) self.df.to_csv('test.csv', sep='|') def time_read_csv_vb(self): read_csv('test.csv', sep='|') def teardown(self): os.remove('test.csv') class read_table_multiple_date(object): goal_time = 0.2 def setup(self): self.N = 10000 self.K = 8 self.data = 'KORD,19990127, 19:00:00, 18:56:00, 0.8100, 2.8100, 7.2000, 0.0000, 280.0000\n KORD,19990127, 20:00:00, 19:56:00, 0.0100, 2.2100, 7.2000, 0.0000, 260.0000\n KORD,19990127, 21:00:00, 20:56:00, -0.5900, 2.2100, 5.7000, 0.0000, 280.0000\n KORD,19990127, 21:00:00, 21:18:00, -0.9900, 2.0100, 3.6000, 0.0000, 270.0000\n KORD,19990127, 22:00:00, 21:56:00, -0.5900, 1.7100, 5.1000, 0.0000, 290.0000\n ' self.data = (self.data * 200) def time_read_table_multiple_date(self): read_table(StringIO(self.data), sep=',', header=None, parse_dates=[[1, 2], [1, 3]]) class read_table_multiple_date_baseline(object): goal_time = 0.2 def setup(self): self.N = 10000 self.K = 8 self.data = 'KORD,19990127 19:00:00, 18:56:00, 0.8100, 2.8100, 7.2000, 0.0000, 280.0000\n KORD,19990127 20:00:00, 19:56:00, 0.0100, 2.2100, 7.2000, 0.0000, 260.0000\n KORD,19990127 21:00:00, 20:56:00, -0.5900, 2.2100, 5.7000, 0.0000, 280.0000\n KORD,19990127 21:00:00, 21:18:00, -0.9900, 2.0100, 3.6000, 0.0000, 270.0000\n KORD,19990127 22:00:00, 21:56:00, -0.5900, 1.7100, 5.1000, 0.0000, 290.0000\n ' self.data = (self.data * 200) def time_read_table_multiple_date_baseline(self): read_table(StringIO(self.data), sep=',', header=None, parse_dates=[1]) class read_csv_default_converter_python_engine(object): goal_time = 0.2 def setup(self): self.data = '0.1213700904466425978256438611,0.0525708283766902484401839501,0.4174092731488769913994474336\n 0.4096341697147408700274695547,0.1587830198973579909349496119,0.1292545832485494372576795285\n 0.8323255650024565799327547210,0.9694902427379478160318626578,0.6295047811546814475747169126\n 0.4679375305798131323697930383,0.2963942381834381301075609371,0.5268936082160610157032465394\n 0.6685382761849776311890991564,0.6721207066140679753374342908,0.6519975277021627935170045020\n ' self.data = (self.data * 200) def time_read_csv_default_converter(self): read_csv(StringIO(self.data), sep=',', header=None, float_precision=None, engine='python') class read_csv_default_converter_with_decimal_python_engine(object): goal_time = 0.2 def setup(self): self.data = '0,1213700904466425978256438611;0,0525708283766902484401839501;0,4174092731488769913994474336\n 0,4096341697147408700274695547;0,1587830198973579909349496119;0,1292545832485494372576795285\n 0,8323255650024565799327547210;0,9694902427379478160318626578;0,6295047811546814475747169126\n 0,4679375305798131323697930383;0,2963942381834381301075609371;0,5268936082160610157032465394\n 0,6685382761849776311890991564;0,6721207066140679753374342908;0,6519975277021627935170045020\n ' self.data = (self.data * 200) def time_read_csv_default_converter_with_decimal(self): read_csv(StringIO(self.data), sep=';', header=None, float_precision=None, decimal=',', engine='python')
errors = to_list(errors)
return tuple(len(self._get_axis(a)) for a in self._AXIS_ORDERS)
before = self.make_project_state([]) after = self.make_project_state([self.author_unmanaged_default_pk, self.book]) autodetector = MigrationAutodetector(before, after) changes = autodetector._detect_changes() self.assertEqual(changes['otherapp'][0].operations[0].fields[2][1].remote_field.field_name, 'id')
self.html_unit1 = ItemFactory.create( parent_location=self.vertical.location, category="html", display_name="Html Content 1", modulestore=self.store, publish_item=True, ) self.html_unit1.parent = self.vertical
from nose.tools import assert_equal from nose.tools import assert_not_equal from nose.tools import assert_true from nose.tools import assert_false from nose.tools import assert_raises from nose.tools import raises from nose import SkipTest from nose import with_setup
col_names = [str(descr[0]) for descr in matlab_dict['mldata_descr_ordering'][0]]
template_id = 'multiplechoice.yaml' resp = self.create_xblock(parent_usage_key=self.seq_usage_key, category='problem', boilerplate=template_id) self.problem_usage_key = self.response_usage_key(resp) self.problem_update_url = reverse_usage_url("xblock_handler", self.problem_usage_key)
__func_alias__ = { 'list_': 'list' }
try: self.storage.path('') except NotImplementedError: pass else: if self.storage.location not in searched_locations: searched_locations.append(self.storage.location) if self.storage.exists(path): match = self.storage.path(path) if all: match = [match] return match return []
cases = [0.5, 'xxx'] methods = [idx.intersection, idx.union, idx.difference, idx.symmetric_difference]
if not self.user_can_access_course(user, course): raise Http404
panel = tm.makePanel()
if hasattr(w, "toarray"): d = float(w.nnz) / (w.shape[0] * w.shape[1]) else: d = 0 if w is None else float((w != 0).sum()) / w.size return d
import pandas as pd import unittest import nose import numpy as np import sys from pandas import Series, DataFrame import pandas.util.testing as tm from pandas.util.testing import (assert_almost_equal, assertRaisesRegexp, raise_with_traceback, assert_index_equal, assert_series_equal, assert_frame_equal, assert_numpy_array_equal, RNGContext, assertRaises, skip_if_no_package_deco) from pandas.compat import is_platform_windows
from __future__ import absolute_import
self.assertEqual(len(ctx.dicts), 4)
sys.path.insert(0, os.path.abspath('sphinxext'))
s2 = Series(date_range('2000-01-01 09:00:00', periods=5, tz='CET'), name='foo') self.assertRaises(ValueError, lambda: s - s2)
self.encoder = encoder self.dataset = dataset self.path = path self.batch_size = batch_size self.topo = topo
for app in ENV_TOKENS.get('ADDL_INSTALLED_APPS', []): INSTALLED_APPS += (app,)
@patch('edxmako.LOOKUP', {}) def test_with_package(self): add_lookup('test', 'management', __name__) dirs = LOOKUP['test'].directories self.assertEqual(len(dirs), 1) self.assertTrue(dirs[0].endswith('management'))
if self.dim != 2: kwargs['dim'] = self.dim if self.geography is not False: kwargs['geography'] = self.geography return name, path, args, kwargs
_check_all_orients(self.frame) self.assertEqual(self.frame.to_json(), self.frame.to_json(orient="columns"))
result = df.loc(axis=1)[:, 'foo'] expected = df.loc[:, (slice(None), 'foo')] assert_frame_equal(result, expected)
payload = client.FakePayload() payload.write( '\r\n'.join([ '--' + client.BOUNDARY, 'Content-Disposition: form-data; name*=UTF-8\'\'file_unicode; filename*=UTF-8\'\'%s' % urlquote( UNICODE_FILENAME ), 'Content-Type: application/octet-stream', '', 'You got pwnd.\r\n', '\r\n--' + client.BOUNDARY + '--\r\n' ]) ) r = { 'CONTENT_LENGTH': len(payload), 'CONTENT_TYPE': client.MULTIPART_CONTENT, 'PATH_INFO': "/unicode_name/", 'REQUEST_METHOD': 'POST', 'wsgi.input': payload, } response = self.client.request(**r) self.assertEqual(response.status_code, 200)
validator = kwarg validation_arg = ()
import logging import os import re
mock_create.return_value = None
df = DataFrame({'x': [1.1, 2.1, 3.1, 4.1], 'y': [5.1, 6.1, 7.1, 8.1]}, index=[0, 1, 2, 3]) df.insert(2, 'z', np.nan)
sfn = __salt__['cp.cache_file'](source, saltenv) if not sfn: return _error( ret, 'Source file \'{0}\' not found'.format(source)) htype = source_sum.get('hash_type', __opts__.get('hash_type', 'md5')) source_sum = { 'hash_type': htype, 'hsum': get_hash(sfn, form=htype) }
one_class = np.array([0, 0, 0, 0]) lb = LabelBinarizer().fit(one_class)
quotechar = None
cls.mooc_start = start = datetime.datetime( 2010, 5, 12, 2, 42, tzinfo=pytz.UTC ) chapter = ItemFactory.create( start=start, parent=course, category='sequential' ) cls.sections = sections = [ ItemFactory.create( parent=chapter, category="sequential", metadata={'graded': True, 'format': 'Homework'}) for _ in xrange(4) ] cls.problems = [ [ ItemFactory.create( parent=section, category="problem", data=StringResponseXMLFactory().build_xml(answer='foo'), metadata={'rerandomize': 'always'} ) for _ in xrange(4) ] for section in sections ]
source = GDALRaster({ 'datatype': 1, 'driver': 'tif', 'name': rstfile.name, 'width': 5, 'height': 5, 'nr_of_bands': 1, 'srid': 4326, 'origin': (-5, 5), 'scale': (2, -2), 'skew': (0, 0), 'bands': [{ 'data': range(25), 'nodata_value': ndv, }], })
self.assertTrue(tpl.render(ctx).startswith("2011"))
regr.fit(diabetes_X_train, diabetes_y_train)
for i in asg.instances: if lifecycle_state is not None and i.lifecycle_state != lifecycle_state: continue if health_status is not None and i.health_status != health_status: continue instance_ids.append(i.instance_id) instances = ec2_conn.get_only_instances(instance_ids=instance_ids) if attributes: return [[getattr(instance, attr).encode("ascii") for attr in attributes] for instance in instances] else: return [getattr(instance, attribute).encode("ascii") for instance in instances if getattr(instance, attribute)] return [getattr(instance, attribute).encode("ascii") for instance in instances]
for item in items: self.assertChildNodes(item, ['title', 'link', 'description', 'guid', 'georss:point'])
return course.id.org
with tm.assertRaisesRegexp(ValueError, 'Length of names'): self.index.set_names(names[0], level=[0, 1])
from salt.modules import sysmod
response = requests.get(self.url, params={"test_param": 2}) self.assertEqual(response.status_code, 200)
partition = lambda x: float(x) / 100.0 * num_minions try: if '%' in batch: res = partition(float(batch.strip('%'))) if res < 1: return int(math.ceil(res)) else: return int(res) else: return int(batch) except ValueError: print(('Invalid batch data sent: {0}\nData must be in the form' 'of %10, 10% or 3').format(batch))
from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.modules.dockerng import ( CLIENT_TIMEOUT, STOP_TIMEOUT, VALID_CREATE_OPTS, _validate_input, _get_repo_tag ) import salt.utils import salt.ext.six as six
resp = self._change_enrollment('unenroll') self.assertEqual(resp.status_code, 400)
s = Series([u'あ', u'いい', u'ううう', u'ええええ'], index=[u'ああ', u'いいいい', u'う', u'えええ'], name=u'おおおおおおお') expected = (u"ああ あ\nいいいい いい\nう ううう\n" u"えええ ええええ\nName: おおおおおおお, dtype: object") self.assertEqual(_rep(s), expected)
def __init__(self, **kwargs): super(Softplus, self).__init__(**kwargs) @wraps(Layer.fprop) def fprop(self, state_below): p = self._linear_part(state_below) p = T.nnet.softplus(p) return p @wraps(Layer.cost) def cost(self, *args, **kwargs): raise NotImplementedError()
linehandles = [x for x in handles if not isinstance(x, PolyCollection)] self._check_colors(linehandles, linecolors=custom_colors) for h in handles: self.assertTrue(h.get_alpha() is None) tm.close()
class MediaForm(ModelForm): class Meta: model = Media exclude = ['url'] class MediaInline(GenericTabularInline): readonly_fields = ['description'] form = MediaForm model = Media class EpisodeAdmin(admin.ModelAdmin): inlines = [ MediaInline ] ma = EpisodeAdmin(Episode, self.site) self.assertEqual( list(list(ma.get_formsets_with_inlines(request))[0][0]().forms[0].fields), ['keywords', 'id', 'DELETE'])
_np_version = np.version.short_version _nlv = LooseVersion(_np_version) _np_version_under1p8 = _nlv < '1.8' _np_version_under1p9 = _nlv < '1.9' _np_version_under1p10 = _nlv < '1.10' _np_version_under1p11 = _nlv < '1.11' _np_version_under1p12 = _nlv < '1.12'
module = CapaFactory.create(attempts=1, done=True)
unavailable_certificates = \ [cert for cert in generated_certificates if cert.status == CertificateStatuses.unavailable and cert.grade == default_grade]
loss = sgd_fast.ModifiedHuber() cases = [ (1.0, 1.0, 0.0), (-1.0, -1.0, 0.0), (2.0, 1.0, 0.0), (0.0, 1.0, -2.0), (-1.0, 1.0, -4.0), (0.5, -1.0, 3.0), (0.5, -1.0, 3.0), (-2.0, 1.0, -4.0), (-3.0, 1.0, -4.0) ] _test_gradient_common(loss, cases)
AccessTestData(partition_groups={1: 1, 2: 2}, merged_parents_list=[{1: {3}}, {1: {1}}], expected_access=True),
for a in self.non_index_axes: obj = _reindex_axis(obj, a[0], a[1])
self.cursor().execute('SET CONSTRAINTS ALL IMMEDIATE') self.cursor().execute('SET CONSTRAINTS ALL DEFERRED')
return render_to_response('student_account/account_settings.html', account_settings_context(request))
response = self.client.get( reverse('admin:generic_inline_admin_episode_change', args=(self.episode_pk,)) ) self.assertEqual(response.status_code, 200)
self.train_set.adjust_for_viewer(self.train_set.X)
contents.update({'student_response': ''}) (error, msg) = qinterface.send_to_queue(header=xheader, body=json.dumps(contents), files_to_upload=submission)
if data_type == cx_Oracle.NUMBER: precision, scale = description[4:6] if scale == 0: if precision > 11: return 'BigIntegerField' elif precision == 1: return 'BooleanField' else: return 'IntegerField' elif scale == -127: return 'FloatField'
y_subsample = y[indices, k] classes_subsample = np.unique(y_subsample)
self.skipTest('This test fails when using tests/runtests.py. salt-runtests will be available soon.')
forms = list(formset) self.assertEqual(forms, formset.forms) self.assertEqual(len(formset), len(forms))
self.http_client.fetch(self.get_url('/jobs'), self.stop, method='GET', headers={saltnado.AUTH_TOKEN_HEADER: self.token['token']}, follow_redirects=False, ) response = self.wait(timeout=30) response_obj = json.loads(response.body)['return'][0] try: for jid, ret in six.iteritems(response_obj): self.assertIn('Function', ret) self.assertIn('Target', ret) self.assertIn('Target-type', ret) self.assertIn('User', ret) self.assertIn('StartTime', ret) self.assertIn('Arguments', ret) except AttributeError as attribute_error: print(json.loads(response.body)) raise
start = non_capturing_groups.pop() inner = NonCapture(result[start:]) result = result[:start] + [inner]
mdict = copy.deepcopy(self.dict1) mdict['A'] = [1, 2] res = dictupdate.update(copy.deepcopy(mdict), {'A': [3, 4]}, merge_lists=True) mdict['A'] = [1, 2, 3, 4] self.assertEqual(res, mdict)
if commit: obj.delete()
if kernel32.WaitForSingleObject(process_info.hProcess, INFINITE) == \ win32con.WAIT_OBJECT_0: exitcode = wintypes.DWORD() kernel32.GetExitCodeProcess(process_info.hProcess, ctypes.byref(exitcode)) ret['retcode'] = exitcode.value
UserCourseTagFactory( user=self.student, course_id=self.course.id, key='xblock.partition_service.partition_{0}'.format(self.partition.id), value=str(user_tag) )
temp = s.copy() temp[1] = True tm.assert_series_equal(temp, pd.Series([1, 1, 3, 4])) self.assertEqual(temp.dtype, np.int64)
X, y = check_X_y(X, y, ['csr', 'csc', 'coo']) args = [X[safe_mask(X, y == k)] for k in np.unique(y)] return f_oneway(*args)
val = '\n'.join([v for v in val.split('\n') if not v.startswith('#')])
self.xml = SimplerXMLGenerator(self.stream, self.options.get("encoding", settings.DEFAULT_CHARSET)) self.xml.startDocument() self.xml.startElement("django-objects", {"version": "1.0"})
class Note(models.Model): content_type = models.ForeignKey(ContentType, models.CASCADE) object_id = models.PositiveIntegerField() content_object = GenericForeignKey() note = models.TextField()
with transaction.atomic(): with six.assertRaisesRegex(self, TypeError, "'Article' instance expected, got <Reporter.*"): self.r.article_set.add(self.r2) self.assertQuerysetEqual( self.r.article_set.all(), ["<Article: John's second story>", "<Article: This is a test>"] )
self.pipe = None
self.assertEqual(len(details['course_modes']), 3) self.assertEqual(details, cached_details)
self.vw.default_parts.set([self.wheelset, self.doors, self.engine], clear=True) expected_messages.append({ 'instance': self.vw, 'action': 'pre_clear', 'reverse': False, 'model': Part, }) expected_messages.append({ 'instance': self.vw, 'action': 'post_clear', 'reverse': False, 'model': Part, }) expected_messages.append({ 'instance': self.vw, 'action': 'pre_add', 'reverse': False, 'model': Part, 'objects': [self.doors, self.engine, self.wheelset], }) expected_messages.append({ 'instance': self.vw, 'action': 'post_add', 'reverse': False, 'model': Part, 'objects': [self.doors, self.engine, self.wheelset], }) self.assertEqual(self.m2m_changed_messages, expected_messages)
self._test_page("/logout", 302)
specs = self.course_groups.strip() if not specs: return {} specs = [line.split(',', 1) for line in specs.splitlines()] return { slug.strip().lower(): [CourseKey.from_string(key.strip()) for key in keys.strip().split(',')] for slug, keys in specs }
if s3_meta: files = __get_pillar_files_from_s3_meta(s3_meta) environments = __get_pillar_environments(files)
return True
resources = ['ASCII text', u'❄ I am a special snowflake.', "❄ So am I, but I didn't tell you."] self.assertEqual(hash_resource(resources), 'a76e27c8e80ca3efd7ce743093aa59e0')
output = np.empty(len(self), dtype=self.dtype) int_index = self.sp_index.to_int_index() output.fill(self.fill_value) output.put(int_index.indices, self) return output
if not fragment.js_init_fn: fragment.initialize_js('XBlockToXModuleShim') fragment.json_init_args = {'xmodule-type': block.js_module_name}
for r in range(len(res.index)): for c in range(len(res.columns)): self.assertTrue(res.ix[r, c] is not np.nan)
try: conf = self.get_conf() conf.get(('remote', 'origin'), 'url') except KeyError: try: conf.set('http', 'sslVerify', self.ssl_verify) conf.set( 'remote "origin"', 'fetch', '+refs/heads/*:refs/remotes/origin/*' ) conf.set('remote "origin"', 'url', self.url) conf.set('remote "origin"', 'pushurl', self.url) conf.write_to_path() except os.error: pass else: new = True except os.error: pass return new
p = T.switch(p > 0., p, self.left_slope * p) return p
problem_scores = {unicode(score.module_id): score for score in gradeset['raw_scores'] if score.graded} earned_possible_values = list() for problem_id in problems: try: problem_score = problem_scores[problem_id] earned_possible_values.append([problem_score.earned, problem_score.possible]) except KeyError: earned_possible_values.append(['N/A', 'N/A']) rows.append(student_fields + [final_grade] + list(chain.from_iterable(earned_possible_values)))
privilege_enabled = False try: privilege_enabled = _enable_privilege(win32security.SE_RESTORE_NAME) win32security.SetNamedSecurityInfo( path, win32security.SE_FILE_OBJECT, win32security.GROUP_SECURITY_INFORMATION, None, groupSID, None, None ) finally: if privilege_enabled: _disable_privilege(win32security.SE_RESTORE_NAME)
module = CapaFactory.create(due=self.yesterday_str, done=True) self.assertFalse(module.should_show_reset_button())
if len(name.split()) > 1: cmd_kwargs.update({'args': name.split(' ', 1)[1]})
fn = None paths = ('/etc/yum/yum.conf', '/etc/yum.conf') for path in paths: if os.path.exists(path): fn = path break
reset_time = datetime.now(UTC) + timedelta(seconds=300) with freeze_time(reset_time): response = self.client.post(url) self.assertEquals(response.status_code, 404)
outside_tar = self.unsafe_common_dir / "unsafe_file.tar.gz" with tarfile.open(outside_tar, "w:gz") as tar: tar.addfile(tarfile.TarInfo(os.path.join(os.path.abspath("."), "a_file"))) return outside_tar
graph = MigrationGraph() graph.add_node(("testapp", "0001_initial"), None) graph.add_node(("testapp", "0002_foobar"), None) graph.add_node(("otherapp", "0001_initial"), None) graph.add_dependency("testapp.0002_foobar", ("testapp", "0002_foobar"), ("testapp", "0001_initial"))
b = [True, False, True, False, ] self.check_result('bool', 'iloc', b, 'ix', b, typs=['ints']) self.check_result('bool', 'iloc', b, 'ix', b, typs=['labels', 'mixed', 'ts', 'floats', 'empty'], fails=IndexError)
X, y = [[0, 1], [-1e-20, 1]], [0, 1] for X in (X, np.array(X), csr_matrix(X)): assert_raises(ValueError, chi2, X, y)
with salt.utils.fopen(requirements_file_path, 'w') as fhw: fhw.write('zope.interface==4.0.1\n')
'address': __ipv4_quad, 'netmask': __ipv4_netmask, 'broadcast': __ipv4_quad, 'metric': __int,
with override('de'): response = self.client.get('/old_jsoni18n/') data = json.loads(response.content.decode('utf-8')) self.assertIn('catalog', data) self.assertIn('formats', data) self.assertIn('plural', data) self.assertEqual(data['catalog']['month name\x04May'], 'Mai') self.assertIn('DATETIME_FORMAT', data['formats']) self.assertEqual(data['plural'], '(n != 1)')
pass
result = frame.pivot(columns='columns')
rs = ser.replace([np.nan, 'foo', 'bar'], -1)
if api != 2: log.error('Foreman API v2 is supported only, please specify' 'version 2 in your Salt master config') raise Exception
self._first_chunk = False
self.assertEqual(len(self._get_enrollments()), 1)
self.assertItemsEqual( data.keys(), ['company_about_url', 'company_privacy_url', 'company_tos_url'] )
raise NotImplementedError("Need to update for C01B") if state_above is not None: msg = layer_above.downward_message(state_above) try: self.output_space.validate(msg) except TypeError as e: reraise_as(TypeError(str(type(layer_above))+".downward_message gave something that was not the right type: "+str(e))) else: msg = None z = self.transformer.lmul(state_below) + self.broadcasted_bias() p, h, p_sample, h_sample = self.max_pool(z, (self.pool_rows, self.pool_cols), msg, theano_rng) return p_sample, h_sample
raw = [ ('s1', ('s1_db', ['bravo', 'alpha'])) ]
shp = (2, 2) strd = (3, 3) im_shp = (6, 4) pool_it = max_pool(X_sym, pool_shape=shp, pool_stride=strd, image_shape=im_shp, try_dnn=False) pool_dnn = max_pool(X_sym, pool_shape=shp, pool_stride=strd, image_shape=im_shp) assert pool_it.owner.op != pool_dnn.owner.op
alphas = np.linspace(lars_cv.alphas_[0], .1 * lars_cv.alphas_[0], 6) clf = RandomizedLasso(alpha=alphas, random_state=42).fit(X, y) trees = ExtraTreesRegressor(100).fit(X, y) F, _ = f_regression(X, y)
response_data = { "id": "test_comment", "thread_id": "test_thread", "parent_id": None, "author": self.user.username, "author_label": None, "created_at": "1970-01-01T00:00:00Z", "updated_at": "1970-01-01T00:00:00Z", "raw_body": "Original body", "rendered_body": "<p>Original body</p>", "endorsed": False, "endorsed_by": None, "endorsed_by_label": None, "endorsed_at": None, "abuse_flagged": False, "voted": False, "vote_count": 0, "children": [], "editable_fields": [], "child_count": 0, } response_data.update(overrides or {}) return response_data
del good
dset = fetch_mldata(dataname, target_name='y', data_name='z', data_home=tmpdir) for n in ["COL_NAMES", "DESCR", "target", "data", "x"]: assert_in(n, dset) assert_not_in("y", dset) assert_not_in("z", dset)
new_image = Image.new('RGB', dimensions, color) new_buf = StringIO() new_image.save(new_buf, format='png') new_buf.seek(0) new_name = name.format(prefix) new_key = StaticContent.compute_location(cls.courses[prefix].id, new_name) new_content = StaticContent(new_key, new_name, 'image/png', new_buf.getvalue(), locked=locked) contentstore().save(new_content) return new_content
pred_entropies = stats.distributions.entropy(lp_model.label_distributions_.T)
__, problem = self._create_problem_with_content_group(0, 1) problem.group_access = { 0: [0], 1: [1], } self.store.update_item(problem, ModuleStoreEnum.UserID.test)
st_mode = st.st_mode read_all = stat.S_IRUSR read_all |= stat.S_IRGRP read_all |= stat.S_IROTH
assert zca_dataset.has_targets()
raise AssertionError('Error performing exclusion: ' 's1: %s s2: %s other: %s' % (s1, s2, other))
self.verify_pdf_certificate()
from jnpr.junos import Device from jnpr.junos.utils.sw import SW from jnpr.junos.utils.scp import SCP import jnpr.junos.utils import jnpr.junos.cfg HAS_JUNOS = True
self.course.user_partitions = [ UserPartition( id=0, name='Cohort user partition', scheme=UserPartition.get_scheme('cohort'), description='Cohorted user partition', groups=[ Group(id=0, name="Group A"), Group(id=1, name="Group B"), ], ), UserPartition( id=1, name='Random user partition', scheme=UserPartition.get_scheme('random'), description='Random user partition', groups=[ Group(id=0, name="Group A"), Group(id=1, name="Group B"), ], ), ] self.store.update_item(self.course, ModuleStoreEnum.UserID.test)
self.assert_enrollment_activation(False, selected_mode)
salt '*' vsphere.get_firewall_status my.esxi.host root bad-password
self.assertQuerysetEqual( Item.objects.order_by('note__note', 'name'), ['<Item: two>', '<Item: four>', '<Item: one>', '<Item: three>'] )
from salt.states import nftables
assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 2) assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 3, 2)
res = __salt__['cmd.run_all'](cmd) if res['retcode'] == 0: for ds in [l for l in res['stdout'].splitlines()]: ds = ds.split("\t") ds_data = {}
def _visit(obj): res = [] for child in obj.get('children', []): res.append((child['id'], child)) if 'children' in child: res += _visit(child) return res return dict(_visit(self.thread))
print("Compute unstructured hierarchical clustering...") st = time.time() ward = AgglomerativeClustering(n_clusters=6, linkage='ward').fit(X) elapsed_time = time.time() - st label = ward.labels_ print("Elapsed time: %.2fs" % elapsed_time) print("Number of points: %i" % label.size)
for from_space in composite_spaces: for to_dtype in composite_dtypes: test_get_origin_batch(from_space, to_dtype) test_make_shared_batch(from_space, to_dtype) test_make_theano_batch(from_space, to_dtype) test_dtype_setter(from_space, to_dtype)
all_variables, all_functions = add_defaults(variables, functions, case_sensitive)
user2 = UserFactory.create() problems = StudentModule.objects.filter( course_id=self.course.id, student=self.student_user ) for problem in problems: problem.student_id = user2.id problem.save()
log = logging.getLogger(__name__)
if not self.preserve_default: field = self.field.clone() field.default = NOT_PROVIDED else: field = self.field state.models[app_label, self.model_name_lower].fields.append((self.name, field)) state.reload_model(app_label, self.model_name_lower)
xp = DataFrame(index=['a']) rs = xp.apply(lambda x: x['a'], axis=1) assert_frame_equal(xp, rs)
single.shell.send( trans_tar, '{0}/salt_state.tgz'.format(__opts__['thin_dir']))
current_bindings = list_bindings(site)
request = api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO['username'])
if self._match_pattern(handlers_list): return else: self.send_response(404, content="404 Not Found")
from __future__ import absolute_import
mlp = MLP(layers=[Linear(layer_name='h', dim=5, irange=0.01, max_col_norm=0.01)]) conditional = BernoulliVector(mlp=mlp, name='conditional') vae = DummyVAE() conditional.set_vae(vae) input_space = VectorSpace(dim=5) conditional.initialize_parameters(input_space=input_space, ndim=5)
for group in filtered_groups: if group.vpc_id is None: return group if len(filtered_groups) > 1: raise Exception('Security group belongs to more VPCs, specify the VPC ID!') elif len(filtered_groups) == 1: return filtered_groups[0] return None
cs, sn, rho = _sym_ortho(rhobar1, beta)
if not clone_type and power: task = new_vm_ref.PowerOn() salt.utils.vmware.wait_for_task(task, vm_name, 'power', 5, 'info')
message = self.q(css='div.wrapper-msg') if message.present: return message.text[0] return None
salt '*' win_update.download_updates
return s
"form-2-id": str(fd3.id), "form-2-reference": "789", "form-2-driver": "bill", "form-2-restaurant": "thai",
try: return self.encrypt_session_key(request.session.session_key) except AttributeError: return ''
try: import redis HAS_REDIS = True except ImportError: HAS_REDIS = False
raise NotImplementedError(str(type(self))+" does not implement get_stdev_rewards")
return "ROLLBACK TO SAVEPOINT %s" % self.quote_name(sid)
self.q(css='.new-library-button').first.click() self.wait_for_ajax()
get_connection(using).rollback()
drbd.__grains__ = {} drbd.__salt__ = {} drbd.__context__ = {}
time_tuple = (years, months, days, hours, minutes, seconds, 0)
cohorts_status = {}
df3 = df1.set_index(['text'], append=True) df2 = df1.set_index(['text'], append=True) self.assertTrue(df3.equals(df2))
if hasattr(other, 'name'): return self.name == other.name return self.name == other
_default_logging_level_ = 'warning' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'master') _loglevel_config_setting_name_ = 'cli_salt_log_file' try: os.getcwd() except OSError: sys.exit("Cannot access current working directory. Exiting!")
'context_processors.doc_url',
correct = self.context['correct'] messages = self.context['messages'] overall_message = self.clean_message_html(self.context['overall_message']) grade_decimals = self.context.get('grade_decimals')
if theano.config.compute_test_value == 'raise': dummy_v.tag.test_value = np.zeros( (x.tag.test_value.shape[0], self.input_space.num_channels, self.input_space.shape[0], self.input_space.shape[1]), dtype=dummy_v.dtype )
self.upload_handlers = ImmutableList( self.upload_handlers, warning="You cannot alter upload handlers after the upload has been processed." ) parser = MultiPartParser(META, post_data, self.upload_handlers, self.encoding) return parser.parse()
altered_fields = fields.copy() altered_fields[empty_field] = "" resp = self._build_and_run_request(user, altered_fields) self._assert_bad_request(resp, empty_field, zendesk_mock_class, datadog_mock)
saltenv = env
self.current_view.close() self.current_view = self.MAPPING["recent"](self.browser)
for i, j in product([0, 1], repeat=2): metric([i], [j])
if existing['code'] == 200:
try: os.kill(int(open(pidfile).read().strip()), 0) sys.exit(1) except Exception as ex: pass
item.video_bumper["transcripts"][lang] = filename
import logging log = logging.getLogger(__file__)
korean_1 = '한국어 시험' korean_utf8_1 = ('\xed\x95\x9c\xea\xb5\xad\xec\x96\xb4' ' \xec\x8b\x9c\xed\x97\x98') korean_unicode_1 = u'\ud55c\uad6d\uc5b4 \uc2dc\ud5d8' korean_2 = '첫 번째 행' korean_utf8_2 = '\xec\xb2\xab \xeb\xb2\x88\xec\xa7\xb8 \xed\x96\x89' korean_unicode_2 = u'\uccab \ubc88\uc9f8 \ud589' korean_3 = '마지막 행' korean_utf8_3 = '\xeb\xa7\x88\xec\xa7\x80\xeb\xa7\x89 \xed\x96\x89' korean_unicode_3 = u'\ub9c8\uc9c0\ub9c9 \ud589' test_file = os.path.join(integration.TMP, 'salt_utf8_tests/'+korean_utf8_1+'.txt' ) template_path = os.path.join(integration.TMP_STATE_TREE, 'issue-8947.sls') template_lines = [
rng = np.random.RandomState([2012, 7, 19]) batch_size = 5 rows = 32 cols = 30 channels = 3 pool_rows = 2 pool_cols = 3 zv = rng.randn(batch_size, rows, cols, channels).astype(config.floatX) * 1. - 1.5 top_down_v = rng.randn(batch_size, rows / pool_rows, cols / pool_cols, channels).astype(config.floatX) p_np, h_np = max_pool_python(zv, (pool_rows, pool_cols), top_down_v) z_th = T.TensorType(broadcastable=(False, False, False, False), dtype = config.floatX)() z_th.name = 'z_th' zr = z_th.dimshuffle(0, 3, 1, 2) top_down_th = T.TensorType(broadcastable=(False, False, False, False), dtype = config.floatX)() top_down_th.name = 'top_down_th' top_down_r = top_down_th.dimshuffle(0, 3, 1, 2) p_th, h_th = f(zr, (pool_rows, pool_cols), top_down_r) func = function([z_th, top_down_th], [p_th.dimshuffle(0, 2, 3, 1), h_th.dimshuffle(0, 2, 3, 1)]) pv, hv = func(zv, top_down_v) assert p_np.shape == pv.shape assert h_np.shape == hv.shape if not np.allclose(h_np, hv): print((h_np.min(), h_np.max())) print((hv.min(), hv.max())) assert False if not np.allclose(p_np, pv): diff = abs(p_np - pv) print('max diff ', diff.max()) print('min diff ', diff.min()) print('ave diff ', diff.mean()) assert False
with connection.schema_editor() as editor: editor.create_model(Tag) self.assertTrue( self.get_indexes(Tag._meta.db_table)['id']['primary_key'], ) id_field = Tag._meta.get_field("id") old_field = Tag._meta.get_field("slug") new_field = SlugField(primary_key=True) new_field.set_attributes_from_name("slug") new_field.model = Tag with connection.schema_editor() as editor: editor.remove_field(Tag, id_field) editor.alter_field(Tag, old_field, new_field) self.assertNotIn( 'id', self.get_indexes(Tag._meta.db_table), ) self.assertTrue( self.get_indexes(Tag._meta.db_table)['slug']['primary_key'], )
used_joins = [] if len(lookups) == 0: lookups = ['exact'] if value is None: if lookups[-1] not in ('exact', 'iexact'): raise ValueError("Cannot use None as a query value") lookups[-1] = 'isnull' value = True elif hasattr(value, 'resolve_expression'): pre_joins = self.alias_refcount.copy() value = value.resolve_expression(self, reuse=can_reuse, allow_joins=allow_joins) used_joins = [k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)] if hasattr(value, 'query') and hasattr(value.query, 'bump_prefix'): value = value._clone() value.query.bump_prefix(self) if hasattr(value, 'bump_prefix'): value = value.clone() value.bump_prefix(self) if (connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and lookups[-1] == 'exact' and value == ''): value = True lookups[-1] = 'isnull' return value, lookups, used_joins
assert unmasked.ndim == 2 assert hasattr(unmasked.owner.op, 'scalar_op') if drop_mask is not None: masked_mean = unmasked * drop_mask else: masked_mean = unmasked if not hasattr(self, 'learn_init_inpainting_state'): self.learn_init_inpainting_state = 0 if not self.learn_init_inpainting_state: masked_mean = block_gradient(masked_mean) masked_mean.name = 'masked_mean'
result = self.client.get_html(self._url()) self.assertNotIn('Test certificate', result.content)
rng = numpy.random.RandomState(seed=42) data = rng.normal(size=(1000, 10)) self.y = numpy.random.binomial(1, 0.5, (1000, 1)) super(ToyDataset, self).__init__(X=data, y=self.y, y_labels=2)
super(MultinomialSampler, self).__init__(0, *args, **kwargs)
p = self.PersonModel(name="Joan") p.mugshot.save("shot", self.file1) p = self.PersonModel.objects.get(name="Joan") path = p.mugshot.path shutil.move(path, path + '.moved') self.PersonModel.objects.get(name="Joan")
head, tail = reordered_frame[:10].copy(), reordered_frame head['A'] = 1
return (type(self)==type(other) and self.props() == other.props())
compat_keys = { 'from': 'start_time', 'to': 'end_time' } sources = xml.findall('source') if sources: field_data['html5_sources'] = [ele.get('src') for ele in sources]
key = 'a' * length
t1, t2, t3 = orig.clone(), orig.clone(), orig.clone() t1.transform(trans.srid) t2.transform(gdal.SpatialReference('EPSG:2774')) ct = gdal.CoordTransform(gdal.SpatialReference('WGS84'), gdal.SpatialReference(2774)) t3.transform(ct)
cols = ['b', 'a'] _check_df(df, cols)
same_index = self.bseries.reindex(self.bseries.index) tm.assert_sp_series_equal(self.bseries, same_index) self.assertIsNot(same_index, self.bseries)
self.video.show_captions()
self.connection.cursor().execute("SELECT 1")
tmp_theme = path(mkdtemp_clean()) template_dir = tmp_theme / "lms/templates" template_dir.makedirs() with open(template_dir / "footer.html", "w") as footer: footer.write("<footer>TEMPORARY THEME</footer>")
score += _loglikelihood(doc_topic_prior, doc_topic_distr, dirichlet_doc_topic, self.n_topics)
if not os.path.isabs(dest): raise SaltInvocationError('Destination path must be absolute') if os.path.isdir(dest): dest = os.path.join(dest, os.path.basename(source)) dest_dir = dest else: dest_dir = os.path.split(dest)[0] if not os.path.isdir(dest_dir): if makedirs: try: os.makedirs(dest_dir) except OSError as exc: raise CommandExecutionError( 'Unable to make destination directory {0}: {1}' .format(dest_dir, exc) ) else: raise SaltInvocationError( 'Directory {0} does not exist'.format(dest_dir) ) if not overwrite and os.path.exists(dest): raise CommandExecutionError( 'Destination path {0} already exists. Use overwrite=True to ' 'overwrite it'.format(dest) )
topo_X = np.random.randn(1, 20, 25, 5) rings = [5] output = encode(topo_X, rings) reconstruct = decode(output, (20, 25, 5), rings) np.testing.assert_allclose(encode(reconstruct, rings), output)
y = f(X).ravel()
super(OverrideFieldDataTests, cls).setUpClass() cls.course = CourseFactory.create(enable_ccx=True)
parent_links = {} for base in reversed([new_class] + parents): if not hasattr(base, '_meta'): continue if base != new_class and not base._meta.abstract: continue for field in base._meta.local_fields: if isinstance(field, OneToOneField): related = resolve_relation(new_class, field.remote_field.model) parent_links[make_model_tuple(related)] = field
self.assertEqual( self.run_function('shadow.set_maxdays', [NO_USER, 7]), 'ERROR: User not found: {0}'.format(NO_USER)) self.assertEqual( self.run_function('shadow.get_maxdays', [NO_USER]), 'ERROR: User not found: {0}'.format(NO_USER))
pieces["short"] = mo.group(3)
assert_method = getattr(self, expected_result) assert_method(block_list)
fred, created = book.authors.get_or_create(name="Fred") self.assertTrue(created)
assert_page_correct( page=1, page_size=4, expected_start=0, expected_stop=4, expected_next=2, expected_prev=None )
win32security.SetNamedSecurityInfo( path, win32security.SE_FILE_OBJECT, win32security.OWNER_SECURITY_INFORMATION + win32security.GROUP_SECURITY_INFORMATION, userSID, groupSID, None, None )
RestrictedCourse.invalidate_cache_for_course(instance.course_key) CountryAccessRule.invalidate_cache_for_course(instance.course_key)
__virtualname__ = 'sentry'
cohort = CourseUserGroup.objects.create( name="TestCohort", course_id=self.course_key, group_type=CourseUserGroup.COHORT ) mock_tracker.emit.assert_called_with( "edx.cohort.created", {"cohort_id": cohort.id, "cohort_name": cohort.name} ) mock_tracker.reset_mock()
try: latest = cls.objects.filter(course_key=course_key).latest() except cls.DoesNotExist: return False else: return latest.enabled
socket.setdefaulttimeout(10)
section = ItemFactory.create(parent=self.course, category='chapter', display_name='Test Section') subsection = ItemFactory.create(parent=section, category='sequential', display_name='Test Subsection') vertical = ItemFactory.create(parent=subsection, category='vertical', display_name='Test Unit')
self.assertTrue(user(email).is_active)
n_samples, n_features = X.shape n_targets = y.shape[1]
lines = docstring.expandtabs().splitlines() indent = min(len(line) - len(line.lstrip()) for line in lines if line.lstrip()) trimmed = [lines[0].lstrip()] + [line[indent:].rstrip() for line in lines[1:]] return "\n".join(trimmed).strip()
pca = PCA(n_components=50) pca.fit(X) pca_test = PCA(n_components=50, svd_solver='full') pca_test.fit(X) assert_array_almost_equal(pca.components_, pca_test.components_)
'tools.lowdata_fmt.on': True,
from __future__ import absolute_import import os
os.environ['REQUIRE_BUILD_PROFILE_OPTIMIZE'] = 'none'
for field_name in remove_fields: self.fields.pop(field_name)
_metadata = DataFrame._metadata _finalize = DataFrame.__finalize__
with self.modulestore.branch_setting(ModuleStoreEnum.Branch.published_only, self.courselike_key): courselike = self.get_courselike() export_fs = courselike.runtime.export_fs = fsm.makeopendir(self.target_dir)
message = "fk_name 'title' is not a ForeignKey to 'model_formsets.Author'." with self.assertRaisesMessage(ValueError, message): inlineformset_factory(Author, Book, fields="__all__", fk_name='title')
table = self.data.pivot_table(index=['A', 'B'], columns='C', margins=True, aggfunc=np.mean) for value_col in table.columns.levels[0]: _check_output(table[value_col], value_col)
location = '//%s' % self.get_full_path()
urlpatterns = ( '',
df = DataFrame({'a': [1]}).dropna() self.assertIsNone(df.is_copy) df['a'] += 1
REQUIRE_ENVIRONMENT = "node"
from __future__ import absolute_import import logging
try: import MySQLdb HAS_MYSQL = True except ImportError: HAS_MYSQL = False
if isinstance(arg, compat.string_types): return getattr(self, arg)(*args, **kwargs) return self._aggregate_generic(arg, *args, **kwargs)
self.original_usage = edit_info.get('original_usage', None) self.original_usage_version = edit_info.get('original_usage_version', None)
shim.remove_shim_context(event)
with mock.patch('commerce.signals.refund_seat') as mock_refund_seat: self.send_signal() self.assertFalse(mock_refund_seat.called)
self.cohort_management_page.select_cohort(self.manual_cohort_name) self.assertIsNone(self.cohort_management_page.get_cohort_associated_content_group()) self.assertEqual( "Warning:\nNo content groups exist. Create a content group", self.cohort_management_page.get_cohort_related_content_group_message() ) self.assertFalse(self.cohort_management_page.select_content_group_radio_button()) self.cohort_management_page.select_studio_group_settings() group_settings_page = GroupConfigurationsPage( self.browser, self.course_info['org'], self.course_info['number'], self.course_info['run'] ) group_settings_page.wait_for_page()
bulk_ops_record.nest()
for error_set in response.context['inline_admin_formset'].formset.errors: self.assertEqual(['Children must share a family name with their parents in this contrived test case'], error_set.get('__all__'))
from __future__ import absolute_import import os
LOCATION='createcachetable_dry_run_mode'
teams = self.get_teams_list(data=data, expected_status=status, **kwargs) if names is not None and 200 <= status < 300: results = teams['results'] self.assertEqual(names, [team['name'] for team in results])
if isinstance(x, LinearTransform): return dot_shape_from_shape(x, tuple(y.shape)) elif isinstance(y, LinearTransform): return dot_shape_from_shape(tuple(x.shape), y) else: raise TypeError('One of x or y should be a LinearTransform')
for db_name in reversed(cls._databases_names()): transaction.set_rollback(True, using=db_name) atomics[db_name].__exit__(None, None, None)
linkedin = 'LinkedIn' facebook = 'Facebook' twitter = 'Twitter'
if not upstream and desired_upstream: actions.append( 'Tracking branch would be set to {0}'.format( desired_upstream ) ) elif upstream and desired_upstream is False: actions.append( 'Tracking branch would be unset' ) elif desired_upstream and upstream != desired_upstream: actions.append( 'Tracking branch would be ' 'updated to {0}'.format(desired_upstream) ) if ret['changes']: return _neutral_test(ret, _format_comments(actions)) else: formatted_actions = _format_comments(actions) if not revs_match \ and not update_head \ and formatted_actions: ret['comment'] = formatted_actions return ret return _uptodate(ret, target, _format_comments(actions))
io = StringIO()
result.append('*') render_kw_only_separator = False
self.assert_enrollment_status(mode=CourseMode.VERIFIED, expected_status=status.HTTP_403_FORBIDDEN) course_mode, is_active = CourseEnrollment.enrollment_mode_for_user(self.user, self.course.id) self.assertTrue(is_active) self.assertEqual(course_mode, CourseMode.DEFAULT_MODE_SLUG)
assert hasattr(Y_hat, 'owner') owner = Y_hat.owner assert owner is not None op = owner.op if isinstance(op, Print): assert len(owner.inputs) == 1 Y_hat, = owner.inputs owner = Y_hat.owner op = owner.op if not isinstance(op, T.nnet.Softmax): raise ValueError("Expected Y_hat to be the output of a softmax, " "but it appears to be the output of " + str(op) + " of type " + str(type(op))) z, = owner.inputs assert z.ndim == 2 return z
self.user.profile.mailing_address = None self.user.profile.save()
random_state = check_random_state(0) n_components = 2 methods = ['exact', 'barnes_hut'] X = random_state.randn(100, n_components).astype(np.float32) for init in ('random', 'pca'): for method in methods: tsne = TSNE(n_components=n_components, perplexity=50, learning_rate=100.0, init=init, random_state=0, method=method) X_embedded = tsne.fit_transform(X) T = trustworthiness(X, X_embedded, n_neighbors=1) assert_almost_equal(T, 1.0, decimal=1)
section = Section.objects.create(name='<a>evil</a>') response = self.client.get(reverse('admin:admin_views_section_change', args=(section.pk,))) self.assertNotContains(response, "<a>evil</a>", status_code=200) self.assertContains(response, "&lt;a&gt;evil&lt;/a&gt;", status_code=200)
log.info(u"SHIB creds returned: %r", shib)
dates = lrange(20111201, 20111205) ids = 'abcde' idx = MultiIndex.from_tuples([x for x in cart_product(dates, ids)]) idx.names = ['date', 'secid'] df = DataFrame(np.random.randn(len(idx), 3), idx, ['X', 'Y', 'Z']) rs = df.xs(20111201, level='date') xp = df.ix[20111201, :] assert_frame_equal(rs, xp)
f = self.mixed_frame.copy() piece = DataFrame([[1., 2.], [3., 4.]], index=f.index[0:2], columns=['A', 'B']) key = (slice(None, 2), ['A', 'B']) f.ix[key] = piece assert_almost_equal(f.ix[0:2, ['A', 'B']].values, piece.values)
assert_true(abs(len(y_test_unique) - round(test_size * len(y_unique))) <= 1) assert_true(abs(len(y_train_unique) - round((1.0 - test_size) * len(y_unique))) <= 1)
return self.slug_field
if not widget: widget = self.field.widget if self.field.localize: widget.is_localized = True attrs = attrs or {} if not widget.is_hidden and self.field.required and self.form.use_required_attribute: attrs['required'] = True if self.field.disabled: attrs['disabled'] = True auto_id = self.auto_id if auto_id and 'id' not in attrs and 'id' not in widget.attrs: if not only_initial: attrs['id'] = auto_id else: attrs['id'] = self.html_initial_id if not only_initial: name = self.html_name else: name = self.html_initial_name return force_text(widget.render(name, self.value(), attrs=attrs))
_mock_pep8_violations = MagicMock( return_value=(1, ['lms/envs/common.py:32:2: E225 missing whitespace around operator']) ) with patch('pavelib.quality._get_pep8_violations', _mock_pep8_violations): with self.assertRaises(SystemExit): pavelib.quality.run_quality("")
args = [self.system, '--settings=acceptance'] if self.fasttest: args.append('--skip-collect') call_task('pavelib.assets.update_assets', args=args)
response = self._generate()
df = DataFrame(randn(4, 3), index=list('ABCD')) expected = df.ix[['E']]
before = self.make_project_state([self.author_name_deconstructible_dict_1]) after = self.make_project_state([self.author_name_deconstructible_dict_3]) autodetector = MigrationAutodetector(before, after) changes = autodetector._detect_changes() self.assertEqual(len(changes), 1)
LETTUCE_SERVER_PORT = 8003 XQUEUE_PORT = 8040 YOUTUBE_PORT = 8031 LTI_PORT = 8765 VIDEO_SOURCE_PORT = 8777
if '%.4g' % 1.7e8 == '1.7e+008': expected = (' a\n' '0 1.500000e+000\n' '1 1.000000e-017\n' '2 -5.500000e-007') else: expected = (' a\n' '0 1.500000e+00\n' '1 1.000000e-17\n' '2 -5.500000e-07') self.assertEqual(result, expected)
from __future__ import absolute_import import salt.utils from datetime import datetime import logging import time
df = pd.DataFrame({'x': np.arange(8), 'y': np.arange(8) // 4, 'z': np.arange(8) % 2})
profile_page.privacy = privacy
from __future__ import unicode_literals
for item in ret: for struct in [__opts__, __grains__, __pillar__]: for config_key in __valid_configs[item]: value = salt.utils.traverse_dict_and_list(struct, config_key, None) if value: ret[item] = value break return ret['user'], ret['passwd']
from dateutil.relativedelta import relativedelta, weekday from dateutil.easter import easter import pandas.tslib as tslib from pandas.tslib import Timestamp, OutOfBoundsDatetime, Timedelta
usage_key = usage_key.replace(course_key=store.fill_in_run(usage_key.course_key))
print("Feature ranking:")
return instr.encode('mbcs')
libraries = ['m'] if not is_platform_windows() else []
import re import logging
import dateutil if dateutil.__version__ >= LooseVersion( '2.3') and dateutil.__version__ <= LooseVersion('2.4.0'): timezones = ['UTC', 'Asia/Tokyo', 'US/Eastern', 'dateutil/US/Pacific'] else: timezones = ['UTC', 'Asia/Tokyo', 'US/Eastern', 'dateutil/America/Los_Angeles']
futures[0].set_result('foo') self.wait()
for ext in extensions: if ext.sources[0].endswith(('.c','.cpp')): root, _ = os.path.splitext(ext.sources[0]) ext.sources[0] = root + suffix
self.check_caches("mstc_cache_test_key")
response = self.client.get(self.do_redirect_url) self.assertEqual(response.status_code, 200)
self.initdb(default_ms) self._create_block_hierarchy() with self.assertRaises(InvalidVersionError): self.store.revert_to_published(self.vertical_x1a, self.user_id)
mask = mask.reshape(values.shape) result_shape = tuple([values.shape[0]] + [-1] * (self.ndim - 1)) values = _block_shape(values[~mask], ndim=self.ndim) if self.ndim > 1: values = values.reshape(result_shape)
random_state = check_random_state(0) y_true = random_state.randint(0, 2, size=(20, )) y_pred = random_state.randint(0, 2, size=(20, ))
return tslib.ints_to_pydatetime(self.asi8, tz=self.tz)
current_info = info(name) if not current_info: raise CommandExecutionError('User \'{0}\' does not exist'.format(name))
if 'host' not in payload: payload.update({"host": self.host})
s = Series(['a,b', 'c,d'], name='xxx') res = s.str.split(',') exp = Series([['a', 'b'], ['c', 'd']], name='xxx') tm.assert_series_equal(res, exp)
if swapped and swapped == to_string: return model._meta.swappable if model._meta.swappable and model._meta.label == to_string: return model._meta.swappable
from salt.modules import schedule from salt.utils.event import SaltEvent
HAS_IMPORTLIB = False
self.assertQuerysetEqual( Article.objects.filter(headline__regex=r'b.*az'), [ '<Article: barfoobaz>', '<Article: baz>', '<Article: bazbaRFOO>', '<Article: foobarbaz>', '<Article: foobaz>', ] ) self.assertQuerysetEqual( Article.objects.filter(headline__iregex=r'b.*ar'), [ '<Article: bar>', '<Article: barfoobaz>', '<Article: bazbaRFOO>', '<Article: foobar>', '<Article: foobarbaz>', ] )
if (old_field.db_index or old_field.unique) and not (new_field.db_index or new_field.unique): index_to_remove = self._create_index_name(model, [old_field.column], suffix='_like') index_names = self._constraint_names(model, [old_field.column], index=True) for index_name in index_names: if index_name == index_to_remove: self.execute(self._delete_constraint_sql(self.sql_delete_index, model, index_name))
DATE_FORMAT = 'j. F Y' TIME_FORMAT = 'H:i' DATETIME_FORMAT = 'j. F Y H:i' YEAR_MONTH_FORMAT = 'F Y' MONTH_DAY_FORMAT = 'j. F' SHORT_DATE_FORMAT = 'd.m.Y' SHORT_DATETIME_FORMAT = 'd.m.Y H:i'
return self.sm_ols.df_resid
kwargs_str = get_kwargs_str(field_name)
with tm.assert_produces_warning(FutureWarning): s2 = Series(Categorical(["a", "b", np.nan, "a"], categories=["a", "b", np.nan]))
draft = 'draft' published = None
srid = models.IntegerField(primary_key=True) auth_name = models.CharField(max_length=256) auth_srid = models.IntegerField() srtext = models.CharField(max_length=2048) proj4text = models.CharField(max_length=2048) class Meta: app_label = 'gis' db_table = 'spatial_ref_sys' managed = False @property def wkt(self): return self.srtext
self._assertOLXBase(block_list, draft=False, published=True)
with ensure_clean_store(self.path) as store:
sorted_grid_scores = list(sorted(search.grid_scores_, key=lambda x: x.mean_validation_score)) best_score = sorted_grid_scores[-1].mean_validation_score assert_equal(search.best_score_, best_score)
regr_1 = DecisionTreeRegressor(max_depth=2) regr_2 = DecisionTreeRegressor(max_depth=5) regr_3 = DecisionTreeRegressor(max_depth=8) regr_1.fit(X, y) regr_2.fit(X, y) regr_3.fit(X, y)
if 'local_state' in opts: if opts['local_state']: return opts mopts = self.client.master_opts() if not isinstance(mopts, dict): opts['renderer'] = 'yaml_jinja' opts['failhard'] = False opts['state_top'] = salt.utils.url.create('top.sls') opts['nodegroups'] = {} opts['file_roots'] = {'base': [syspaths.BASE_FILE_ROOTS_DIR]} else: opts['renderer'] = mopts['renderer'] opts['failhard'] = mopts.get('failhard', False) if mopts['state_top'].startswith('salt://'): opts['state_top'] = mopts['state_top'] elif mopts['state_top'].startswith('/'): opts['state_top'] = salt.utils.url.create(mopts['state_top'][1:]) else: opts['state_top'] = salt.utils.url.create(mopts['state_top']) opts['state_top_saltenv'] = mopts.get('state_top_saltenv', None) opts['nodegroups'] = mopts.get('nodegroups', {}) opts['state_auto_order'] = mopts.get( 'state_auto_order', opts['state_auto_order']) opts['file_roots'] = mopts['file_roots'] opts['top_file_merging_strategy'] = mopts.get('top_file_merging_strategy', opts.get('top_file_merging_strategy')) opts['env_order'] = mopts.get('env_order', opts.get('env_order', [])) opts['default_top'] = mopts.get('default_top', opts.get('default_top')) opts['state_events'] = mopts.get('state_events') opts['state_aggregate'] = mopts.get('state_aggregate', opts.get('state_aggregate', False)) opts['jinja_lstrip_blocks'] = mopts.get('jinja_lstrip_blocks', False) opts['jinja_trim_blocks'] = mopts.get('jinja_trim_blocks', False) return opts
wp = tm.makePanel() lp = wp.to_frame() df = lp.reset_index() assert_frame_equal(df.pivot('major', 'minor'), lp.unstack())
CourseGradingModel.update_section_grader_type(self.course, 'Homework', self.user) descriptor = modulestore().get_item(self.course.location) section_grader_type = CourseGradingModel.get_section_grader_type(self.course.location)
if not (request.user.is_staff or CourseInstructorRole(course.id).has_user(request.user) or CourseStaffRole(course.id).has_user(request.user)): raise Http404 log.debug('course_id=%s', course_id) cilset = CourseImportLog.objects.filter( course_id=course_id ).order_by('-created') log.debug('cilset length=%s', len(cilset))
CREDIT_PROVIDER_TIMESTAMP_EXPIRATION = 15 * 60
xqueue = XQueueCertInterface() if insecure: xqueue.use_https = False generate_pdf = not has_html_certificates_enabled(course_key, course) return xqueue.regen_cert( student, course_key, course=course, forced_grade=forced_grade, template_file=template_file, generate_pdf=generate_pdf )
X, Y = _generate_center_coordinates(l_x) angles = np.linspace(0, np.pi, n_dir, endpoint=False) data_inds, weights, camera_inds = [], [], [] data_unravel_indices = np.arange(l_x ** 2) data_unravel_indices = np.hstack((data_unravel_indices, data_unravel_indices)) for i, angle in enumerate(angles): Xrot = np.cos(angle) * X - np.sin(angle) * Y inds, w = _weights(Xrot, dx=1, orig=X.min()) mask = np.logical_and(inds >= 0, inds < l_x) weights += list(w[mask]) camera_inds += list(inds[mask] + i * l_x) data_inds += list(data_unravel_indices[mask]) proj_operator = sparse.coo_matrix((weights, (camera_inds, data_inds))) return proj_operator
management.call_command('loaddata', 'fixture6.json', verbosity=0) self.assertQuerysetEqual(Tag.objects.all(), [ '<Tag: <Article: Copyright is fine the way it is> tagged "copyright">', '<Tag: <Article: Copyright is fine the way it is> tagged "law">', ], ordered=False)
self.q(css='div.problem button.check').click() self.wait_for_ajax()
self._effective_batch_size = batch_size = old_batch_size // 2 if self.verbose >= 10: self._print("Batch computation too slow (%.2fs.) " "Setting batch_size=%d.", ( batch_duration, batch_size))
self._create_course_modes(course_modes) data.create_course_enrollment(self.user.username, unicode(self.course.id), enrollment_mode, True) enrollment_attributes = [ { "namespace": "credit", "name": "provider_id", "value": "hogwarts", } ]
if db_alias != 'default': return objects = BadgeImageConfiguration.objects.using(db_alias) if not objects.exists(): for mode in ['honor', 'verified', 'professional']: conf = objects.create(mode=mode) file_name = '{0}{1}'.format(mode, '.png') conf.icon.save( 'badges/{}'.format(file_name), File(open(settings.PROJECT_ROOT / 'static' / 'images' / 'default-badges' / file_name)) )
return True
pkg, ver = re.split('[; ]', line, 1)[0].rsplit('-', 1)
place1.name = "Guido's All New House of Pasta" place1.save_base(raw=True)
self.connection.settings_dict['NAME'] = primary_settings_dict['NAME']
self.course.enable_ccx = False self.mstore.update_item(self.course, self.instructor.id) self.assertIsNone(ccxconapi.course_info_to_ccxcon(self.course_key)) self.assertEqual(mock_post.call_count, 0)
random_state = check_random_state(0) distances = random_state.randn(50, 2).astype(np.float32) distances = np.abs(distances.dot(distances.T)) np.fill_diagonal(distances, 0.0) desired_perplexity = 25.0 P = _binary_search_perplexity(distances, None, desired_perplexity, verbose=0) P = np.maximum(P, np.finfo(np.double).eps) mean_perplexity = np.mean([np.exp(-np.sum(P[i] * np.log(P[i]))) for i in range(P.shape[0])]) assert_almost_equal(mean_perplexity, desired_perplexity, decimal=3)
self.assertRaises(TypeError, self.frame.reindex, columns=['A'], level=1)
s = (y == 2) + (y == 4) X = X[s, :] y = y[s] y = (y != 2).astype(int)
pickle.dumps(x)
return
import jinja2
)
link_html = '<a href=\"https://www.test.com/tos\">Terms of Service</a>' self._assert_reg_field( {"honor_code": "required", "terms_of_service": "required"}, { "label": "I agree to the {platform_name} {link_html}.".format( platform_name=settings.PLATFORM_NAME, link_html=link_html ), "name": "terms_of_service", "defaultValue": False, "type": "checkbox", "required": True, "errorMessages": { "required": "You must agree to the {platform_name} {link_html}.".format( platform_name=settings.PLATFORM_NAME, link_html=link_html ) } } )
den = np.max(np.abs(safe_sparse_dot(Y, X))) if fit_intercept: bias = intercept_scaling * np.ones((np.size(y), 1)) den = max(den, abs(np.dot(Y, bias)).max())
self.url = xml.get('url', None)
response, __ = self._get_update_response_and_expected_data(None, None) self.assertEqual(response.status_code, 200)
if (not hasattr(settings, 'SOCIAL_SHARING_SETTINGS') or not getattr(settings, 'SOCIAL_SHARING_SETTINGS', {}).get("CUSTOM_COURSE_URLS")): filtered_list.append('social_sharing_url')
log.exception('Unable to gather submission metadata, it will not be included in the event.')
self.assertEqual(timesince(self.t, self.t + self.onemicrosecond), '0\xa0minutes') self.assertEqual(timesince(self.t, self.t + self.onesecond), '0\xa0minutes')
cert = GeneratedCertificate.eligible_certificates.get(user=self.student, course_id=self.course.id) self.assertEqual(cert.status, CertificateStatuses.downloadable)
from salttesting.unit import skipIf, TestCase from salttesting.case import ModuleCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
expected = response.get('answer').strip() if self.check_string([expected], student_answer): hint_node = response.find('./correcthint') if hint_node is not None: new_cmap[self.answer_id]['msg'] += self.make_hint_div( hint_node, True, [student_answer], self.tags[0] ) return
h_mean = self.mean_h_given_v(v) h_mean_shape = (batch_size, self.nhid) h_sample = rng.binomial(size=h_mean_shape, n = 1, p = h_mean, dtype = h_mean.dtype)
'tools.salt_token.on': True, 'tools.salt_auth.on': False,
cp = s.copy() cp.ix[3:11] = 0 self.assertTrue((cp.ix[3:11] == 0).values.all())
self.set_group_access(block_specified, None) self.check_access(self.red_cat, block_accessed, True) self.check_access(self.blue_dog, block_accessed, True) self.check_access(self.white_mouse, block_accessed, True) self.check_access(self.gray_worm, block_accessed, True) self.ensure_staff_access(block_accessed)
old = list_pkgs(saltenv=saltenv)
return _algos.arrmap_object(rvalues, lambda x: op(lvalues, x))
active_children = [] for group in user_partition.groups: group_id = unicode(group.id) child_location = self.group_id_to_child.get(group_id, None) child = get_child_descriptor(child_location) if child: active_children.append(child)
def __init__(self, n_estimators=10, criterion="mse", max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0., max_features="auto", max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False): super(RandomForestRegressor, self).__init__( base_estimator=DecisionTreeRegressor(), n_estimators=n_estimators, estimator_params=("criterion", "max_depth", "min_samples_split", "min_samples_leaf", "min_weight_fraction_leaf", "max_features", "max_leaf_nodes", "random_state"), bootstrap=bootstrap, oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, verbose=verbose, warm_start=warm_start) self.criterion = criterion self.max_depth = max_depth self.min_samples_split = min_samples_split self.min_samples_leaf = min_samples_leaf self.min_weight_fraction_leaf = min_weight_fraction_leaf self.max_features = max_features self.max_leaf_nodes = max_leaf_nodes
energy = dbm.energy(V = v_state, hidden = [y_state]) unnormalized_prob = T.exp(-energy) assert unnormalized_prob.ndim == 1 unnormalized_prob = unnormalized_prob[0] unnormalized_prob = function([], unnormalized_prob)
self.check_result('int slice2', 'loc', slice(2, 4), 'ix', [2, 4], typs=['ints'], axes=0) self.check_result('int slice2', 'loc', slice(3, 6), 'ix', [3, 6], typs=['ints'], axes=1) self.check_result('int slice2', 'loc', slice(4, 8), 'ix', [4, 8], typs=['ints'], axes=2)
resp = self.client.post(modx_url, { get_input_id(u'{}_1').format(index): response for index, response in enumerate(responses, 2) }) return resp
httpretty.register_uri( httpretty.POST, "{}/baskets/".format(TEST_API_URL), body=json.dumps({'payment_data': expected_payment_data}), content_type="application/json", )
adapters[adapter]['data']['inet']['addrfam'] = 'inet' adapters[adapter]['data']['inet']['proto'] = 'manual' adapters[adapter]['data']['inet']['master'] = adapters[adapter]['master']
course_key = SlashSeparatedCourseKey.from_deprecated_string(course_id) course = get_course_with_access(request.user, 'load', course_key) staff_access = bool(has_access(request.user, 'staff', course)) return render_to_response('courseware/syllabus.html', { 'course': course, 'staff_access': staff_access, })
return usage_key in self.modules[usage_key.course_key]
return len(self._find_within(".discussion-response"))
with self.assertRaisesMessage(management.CommandError, "Unknown model in excludes: fixtures.FooModel"): self._dumpdata_assert(['fixtures', 'sites'], '', exclude_list=['fixtures.FooModel'])
if instance in vmconfig['state'] and vmconfig['state'][instance] is not None: for state_cfg in vmconfig['state'][instance]: add_instance = True
EmptyResultSet = sql.EmptyResultSet
try: return index.get_level_values(index.names[i]) except KeyError: return index.get_level_values(i)
with open(pickle_fname, 'rb') as fid: example_code_obj = pickle.load(fid) fid.close() str_repl = {} for name, cobj in example_code_obj.items(): this_module = cobj['module'].split('.')[0]
out = StringIO() call_command( 'inspectdb', table_name_filter=lambda tn: tn == 'inspectapp_allogrfields', stdout=out ) output = out.getvalue() if connection.features.supports_geometry_field_introspection: self.assertIn('geom = models.PolygonField()', output) self.assertIn('point = models.PointField()', output) else: self.assertIn('geom = models.GeometryField(', output) self.assertIn('point = models.GeometryField(', output)
self.set_attrs()
output[:, i:i + width, j:j + width] = dense_input[ :, idx][:, None, None] idx += 1
if _is_convertible_to_td(item): try: item = Timedelta(item) except: pass
mock_course_module = MagicMock(id=self.course.id, position=None) mock_chapter = MagicMock() mock_chapter.url_name = 'chapter_url_name' mock_course_module.get_display_items.return_value = [mock_chapter] mock_section = MagicMock() mock_section.url_name = 'section_url_name' mock_section.display_name_with_default_escaped = 'Test Section Display Name' mock_chapter.get_display_items.return_value = [mock_section] mock_section.get_display_items.return_value = [MagicMock()] self.assertEqual(helpers.get_course_position(mock_course_module), { 'display_name': 'Test Section Display Name', 'url': '/courses/{}/courseware/chapter_url_name/section_url_name/'.format(self.course.id), })
(pk_obj, 640, EmailPKData, "hovercraft@example.com"), (pk_obj, 660, FilePathPKData, "/foo/bar/whiz.txt"), (pk_obj, 670, DecimalPKData, decimal.Decimal('12.345')), (pk_obj, 671, DecimalPKData, decimal.Decimal('-12.345')), (pk_obj, 672, DecimalPKData, decimal.Decimal('0.0')), (pk_obj, 673, FloatPKData, 12.345), (pk_obj, 674, FloatPKData, -12.345), (pk_obj, 675, FloatPKData, 0.0), (pk_obj, 680, IntegerPKData, 123456789), (pk_obj, 681, IntegerPKData, -123456789), (pk_obj, 682, IntegerPKData, 0), (pk_obj, 695, GenericIPAddressPKData, "fe80:1424:2223:6cff:fe8a:2e8a:2151:abcd"), (pk_obj, 720, PositiveIntegerPKData, 123456789), (pk_obj, 730, PositiveSmallIntegerPKData, 12), (pk_obj, 740, SlugPKData, "this-is-a-slug"), (pk_obj, 750, SmallPKData, 12), (pk_obj, 751, SmallPKData, -12), (pk_obj, 752, SmallPKData, 0), (pk_obj, 791, UUIDData, uuid_obj), (fk_obj, 792, FKToUUID, uuid_obj),
is_1d = X.ndim == 1 X = np.atleast_2d(X) X = check_array(X, dtype=np.float64) n_samples, n_features = X.shape if out is None: out = np.empty_like(X) _log_logistic_sigmoid(n_samples, n_features, X, out) if is_1d: return np.squeeze(out) return out
try:
eigvals = np.maximum(eigvals.real, 0)
('ALIGN', (0, 0), (-1, -1), 'LEFT'), ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'), ('TEXTCOLOR', (0, 0), (-1, -1), colors.black), ('FONTSIZE', (0, 0), (-1, -1), 9), ('TEXTCOLOR', (0, 0), (-1, -1), '#AAAAAA'),
if master_pub: root_cmd('rm -f \'{0}/master.pub\''.format(tmp_dir), tty, sudo, **ssh_kwargs) log.debug('Removed {0}/master.pub'.format(tmp_dir)) if master_pem: root_cmd('rm -f \'{0}/master.pem\''.format(tmp_dir), tty, sudo, **ssh_kwargs) log.debug('Removed {0}/master.pem'.format(tmp_dir)) if master_conf: root_cmd('rm -f \'{0}/master\''.format(tmp_dir), tty, sudo, **ssh_kwargs) log.debug('Removed {0}/master'.format(tmp_dir))
return (self.equals(other) and all((getattr(self, c, None) == getattr(other, c, None) for c in self._comparables)) and type(self) == type(other))
from salt.modules import s6
__salt__
return default_pprint
self.assertEquals(bad_pwd_resp.status_code, 200) obj = json.loads(bad_pwd_resp.content) self.assertEquals(obj, { 'success': True, 'value': "('registration/password_reset_done.html', [])", }) self.assert_no_events_were_emitted()
__salt__['raid.save_config']()
valid_country = Country(name='Germany', iso_two_letter='DE') invalid_country = Country(id=0, name='Poland', iso_two_letter='PL') with self.assertRaises(ValueError): Country.objects.bulk_create([valid_country, invalid_country])
for i, sax in enumerate(sv): oax = ov[i] if sax != oax: raise ValueError( "invalid combinate of [%s] on appending data [%s] " "vs current table [%s]" % (c, sax, oax))
with connection.schema_editor() as editor: for table in tables[2:]: editor.execute(editor.sql_delete_table % {"table": table}) migration = executor.loader.get_migration("migrations", "0001_initial") self.assertEqual(executor.detect_soft_applied(None, migration)[0], False)
os.remove(tmp.name)
expc = Categorical.from_codes( np.arange(4).repeat(8), levels, ordered=True) exp = CategoricalIndex(expc) self.assert_index_equal(desc_result.index.get_level_values(0), exp) exp = Index(['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'] * 4) self.assert_index_equal(desc_result.index.get_level_values(1), exp)
changes[namespace] = { 'new': config, 'old': update_config, }
self.options.log_file_level = self.config.get( self._logfile_loglevel_config_setting_name_ )
pass
num_attempts = 99 with self.assertRaises(xmodule.exceptions.NotFoundError): (module, unused_result) = self.create_and_check( num_attempts=num_attempts, last_submission_time=datetime.datetime(2013, 12, 6, 0, 17, 36, tzinfo=UTC), submission_wait_seconds=180, considered_now=datetime.datetime(2013, 12, 6, 0, 24, 0, tzinfo=UTC) )
xx, yy = np.meshgrid(np.linspace(plt.xlim()[0], plt.xlim()[1], 100), np.linspace(plt.ylim()[0], plt.ylim()[1], 100)) zz = np.c_[xx.ravel(), yy.ravel()]
ps = tm.makePeriodSeries() shifted = ps.tshift(1) unshifted = shifted.tshift(-1)
data = np.random.randn(5, 100, 5) no_dup_panel = Panel(data, items=list("ABCDE")) panel = Panel(data, items=list("AACDE"))
comps = member.path.split('/') if len(comps) > 1 and comps[0] == comps[1]: member.path = '/'.join(comps[1:])
kwargs.setdefault("name", "Twitter") kwargs.setdefault("backend_name", "twitter") kwargs.setdefault("icon_class", "fa-twitter") kwargs.setdefault("key", "test") kwargs.setdefault("secret", "test") return cls.configure_oauth_provider(**kwargs)
unit = self.go_to_unit_page() verify_ordering(self, unit, [{"": ["Unit HTML", "Unit Problem"]}])
from __future__ import unicode_literals
self.assertAlmostEqual(self.ts[:15].cov(self.ts[5:]), self.ts[5:15].std() ** 2)
if args.update or args.test or args.compare or args.install or args.version: if args.update: do_update() run_all(test=args.test,compare=args.compare,install=args.install,version=args.version,build=args.build) exit(0)
has_changes = None if (is_xblock_unit or course_outline) and not is_library_block: has_changes = modulestore().has_changes(xblock)
_base_iterator_cls = None
cc_rendered_selector = self.get_element_selector(CSS_CLASS_NAMES['closed_captions']) self.wait_for_element_invisibility(cc_rendered_selector, 'Closed captions hidden')
for get_parameter in get_parameters: parameter_value = request.GET.get(get_parameter) if parameter_value is not None: cache_key = cache_key + '.' + urllib.urlencode({ get_parameter: unicode(parameter_value).encode('utf-8') })
gp = GaussianProcess(corr='absolute_exponential', theta0=1e-4, thetaL=1e-12, thetaU=1e-2, nugget=1e-2, optimizer='Welch', regr="linear", random_state=0)
user = User.objects.get(pk=user.pk)
geom_t = fromstr(g.wkt) self.assertEqual(geom_t.wkt, geom_h.wkt)
def __getattr__(self, name): return getattr(caches[DEFAULT_CACHE_ALIAS], name) def __setattr__(self, name, value): return setattr(caches[DEFAULT_CACHE_ALIAS], name, value) def __delattr__(self, name): return delattr(caches[DEFAULT_CACHE_ALIAS], name) def __contains__(self, key): return key in caches[DEFAULT_CACHE_ALIAS] def __eq__(self, other): return caches[DEFAULT_CACHE_ALIAS] == other def __ne__(self, other): return caches[DEFAULT_CACHE_ALIAS] != other
ip_cmd = run_all( name, 'ip link show', path=path, python_shell=False) if ip_cmd['retcode'] == 0: ip_data = ip_cmd['stdout'] ip_cmd = run_all( name, 'ip addr show', path=path, python_shell=False) ip_data += '\n' + ip_cmd['stdout'] ip_data = salt.utils.network._interfaces_ip(ip_data) else: ip_cmd = run_all( name, 'ifconfig', path=path, python_shell=False) if ip_cmd['retcode'] == 0: ip_data = \ salt.utils.network._interfaces_ifconfig( ip_cmd['stdout']) else: log.warning( 'Unable to run ip or ifconfig in container \'{0}\'' .format(name) ) ip_data = {}
pattern = klass(re) pattern.md = md pattern.ext = self md.inlinePatterns.add(name, pattern, "<reference")
retain_settings = opts.get('retain_settings', False) result = current if retain_settings else {}
response = self.fetch('/', headers={'Cookie': '{0}=foo'.format(saltnado.AUTH_COOKIE_NAME)}) token = json.loads(response.body)['token'] self.assertEqual(token, 'foo')
alt_class = BadgeClassFactory.create( slug=badge_class.slug, issuing_component=badge_class.issuing_component, course_id=CourseFactory.create().location.course_key ) BadgeAssertionFactory.create(user=self.user, badge_class=alt_class)
return __proxy__(args, kw)
return redirect(request.GET.get('next', 'dashboard'))
y_pred = base_estimator.predict(X)
if role is not None: role().add_users(user)
chunklet = self.index[-3:] chunklet.names = ['foo', 'baz'] result = first.difference(chunklet) self.assertEqual(result.names, (None, None))
try: return X.iloc[indices] except ValueError: warnings.warn("Copying input dataframe for slicing.", DataConversionWarning) return X.copy().iloc[indices]
if path_cached: path_hash = hash_file(path) path_cached_hash = hash_file(path_cached)
body.update(self.construct_managers())
ret.setdefault('updated branches', {})[ref_name] = \ {'old': old_sha, 'new': new_sha}
return { 'course_name': self.course.display_name, 'id': unicode(html_unit.location), 'content': {'html_content': '', 'display_name': html_unit.display_name}, 'course': unicode(self.course.id), 'location': [ self.chapter.display_name, self.sequential2.display_name, self.vertical3.display_name ], 'content_type': 'Text', 'org': self.course.org, 'content_groups': content_groups, 'start_date': datetime(2015, 4, 1, 0, 0, tzinfo=tzutc()) }
blocks = get_blocks(self.request, self.course.location, self.user, nav_depth=5, requested_fields=['nav_depth']) vertical_block = self.store.get_item(self.course.id.make_usage_key('vertical', 'vertical_x1a')) problem_block = self.store.get_item(self.course.id.make_usage_key('problem', 'problem_x1a_1')) vertical_descendants = blocks['blocks'][unicode(vertical_block.location)]['descendants'] self.assertIn(unicode(problem_block.location), vertical_descendants) self.assertNotIn(unicode(self.html_block.location), vertical_descendants)
from __future__ import absolute_import
parent = modulestore().get_item(locator) self.assertNotEqual(new_module.location.block_id, another_module.location.block_id) self.assertIn(new_module.location.version_agnostic(), version_agnostic(parent.children)) self.assertIn(another_module.location.version_agnostic(), version_agnostic(parent.children)) self.assertEqual(new_module.data, new_payload) self.assertEqual(another_module.data, another_payload) new_history = modulestore().get_definition_history_info(new_module.definition_locator) self.assertIsNone(new_history['previous_version']) self.assertEqual(new_history['original_version'], new_module.definition_locator.definition_id) self.assertEqual(new_history['edited_by'], "anotheruser") another_history = modulestore().get_definition_history_info(another_module.definition_locator) self.assertEqual(another_history['previous_version'], original.definition_locator.definition_id)
if env is not None and not isinstance(env, (list, dict)): ret['comment'] = ('Invalidly-formatted \'env\' parameter. See ' 'documentation.') return ret
path = path.replace('/', '_') return course_key.make_asset_key( 'asset' if not is_thumbnail else 'thumbnail', AssetLocator.clean_keeping_underscores(path) ).for_branch(None)
template = self.engine.get_template('index.html') self.assertEqual(template.origin.name, os.path.join(TEMPLATE_DIR, 'index.html')) self.assertEqual(template.origin.template_name, 'index.html') self.assertEqual(template.origin.loader, self.engine.template_loaders[0].loaders[0])
return service
import inspect import os
@setup({'i18n35': '{% load i18n %}{% trans "Page not found" as page_not_found %}{{ page_not_found }}'}) def test_i18n35(self): with translation.override('de'): output = self.engine.render_to_string('i18n35') self.assertEqual(output, 'Seite nicht gefunden')
if self.monitoring_dataset is not None: self.monitor.add_channel( name='ave_step_size', ipt=None, val=self.optimizer.ave_step_size, data_specs=(NullSpace(), ''), dataset=first_value(self.monitoring_dataset)) self.monitor.add_channel( name='ave_grad_size', ipt=None, val=self.optimizer.ave_grad_size, data_specs=(NullSpace(), ''), dataset=first_value(self.monitoring_dataset)) self.monitor.add_channel( name='ave_grad_mult', ipt=None, val=self.optimizer.ave_grad_mult, data_specs=(NullSpace(), ''), dataset=first_value(self.monitoring_dataset))
try: import sqlparse except ImportError: raise ImproperlyConfigured( "sqlparse is required if you don't split your SQL " "statements manually." ) else: return [sqlparse.format(statement, strip_comments=True) for statement in sqlparse.split(sql) if statement]
split_test_module.user_partitions = [ UserPartition( 10, 'incorrect_partition', 'Non Random Partition', [Group("0", 'alpha'), Group("2", 'gamma')], scheme=self.non_random_scheme ) ] split_test_module.user_partition_id = 10 validation = split_test_module.validate() self.assertEqual(len(validation.messages), 1) verify_validation_message( validation.messages[0], u"The experiment uses a group configuration that is not supported for experiments. " u"Select a valid group configuration or delete this experiment.", StudioValidationMessage.ERROR ) verify_summary_message( validation.summary, u"This content experiment has issues that affect content visibility.", StudioValidationMessage.ERROR )
qs = Person.objects.prefetch_related('houses__rooms', 'primary_house__occupants') [list(p.primary_house.occupants.all()) for p in qs]
self._change_student_enrollment(self.enrolled_student, self.course, 'enroll')
self.assertEqual(content.content_type, 'application/pdf')
self.assert_middleware_usage(pre_middleware, True, True, True, True, False) self.assert_middleware_usage(middleware, True, True, True, True, False) self.assert_middleware_usage(post_middleware, True, True, True, True, False)
poem = super(PoemForm, self).save(commit=False) poem.name = "Brooklyn Bridge" if commit: poem.save() return poem
if state.transposed: results = results[:, ::-1]
import logging
'description',
bf_errors = self.error_class([conditional_escape(error) for error in bf.errors]) if bf.is_hidden: if bf_errors: top_errors.extend( [_('(Hidden field %(name)s) %(error)s') % {'name': name, 'error': force_text(e)} for e in bf_errors]) hidden_fields.append(six.text_type(bf)) else: css_classes = bf.css_classes() if css_classes: html_class_attr = ' class="%s"' % css_classes
params.pop('svc') params.pop('anova') params2.pop('svc') params2.pop('anova') assert_equal(params, params2)
def _description(self): return _('Field of type: %(field_type)s') % { 'field_type': self.__class__.__name__ } description = property(_description)
if hasattr(lgeoip, 'GeoIP_lib_version'): GeoIP_lib_version = lgeoip.GeoIP_lib_version GeoIP_lib_version.argtypes = None GeoIP_lib_version.restype = c_char_p else: GeoIP_lib_version = None
self.create_and_submit("SuccessfulSally") with patch('lms.djangoapps.verify_student.models.requests.post', new=mock_software_secure_post_error): self.create_and_submit("RetryRoger") with patch('lms.djangoapps.verify_student.models.requests.post', new=mock_software_secure_post_error): self.create_and_submit("RetryRick") assert_equals(len(SoftwareSecurePhotoVerification.objects.filter(status="submitted")), 1) assert_equals(len(SoftwareSecurePhotoVerification.objects.filter(status='must_retry')), 2) call_command('retry_failed_photo_verifications') attempts_to_retry = SoftwareSecurePhotoVerification.objects.filter(status='must_retry') assert_equals(bool(attempts_to_retry), False)
import integration
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
return False
s1 = Series([1, 2, 3], index=[0, 1, 2]) s2 = Series([1, 3], index=[0, 2]) result = s1.expanding().cov(s2) expected = Series([None, None, 2.0]) tm.assert_series_equal(result, expected)
INETARRAY_OID = 1041 INETARRAY = psycopg2.extensions.new_array_type( (INETARRAY_OID,), 'INETARRAY', psycopg2.extensions.UNICODE, ) psycopg2.extensions.register_type(INETARRAY)
return reverse_course_url( 'textbooks_detail_handler', self.course.id, kwargs={'textbook_id': textbook_id} )
clf = GradientBoostingClassifier(n_estimators=10, random_state=1) clf.fit(iris.data, iris.target)
actual = self.get_comment_list(thread, endorsed=endorsed_arg, page=1, page_size=2).data self.assertEqual(actual["pagination"]["next"], "http://testserver/test_path?page=2") self.assertIsNone(actual["pagination"]["previous"])
user = BetaTesterFactory(course_key=course_descriptor.id) normal_student = UserFactory() instructor = InstructorFactory(course_key=course_descriptor.id)
from openpyxl.styles import Protection return Protection(**protection_dict)
grp = self.grouper if self.as_index and getattr(grp, 'groupings', None) is not None and \ self.obj.ndim > 1: ax = self.obj._info_axis groupers = [g.name for g in grp.groupings if g.level is None and g.in_axis] if len(groupers): self._group_selection = ax.difference(Index(groupers)).tolist()
warnings.simplefilter("always", DeprecationWarning) try: with warnings.catch_warnings(record=True) as w: value = getattr(self, key, None) if len(w) and w[0].category == DeprecationWarning: continue finally: warnings.filters.pop(0)
attempt.status = "must_retry" attempt.system_error("System error") attempt.approve() attempt.status = "must_retry" attempt.deny(DENY_ERROR_MSG)
self._assert_course_verification_status(VERIFY_STATUS_NEED_TO_VERIFY)
__virtualname__ = 'bluetooth'
response = self.client.get(reverse('admin:index')) self.assertContains(response, '<div class="app-admin_views module">') self.assertContains(response, '<tr class="model-actor">') self.assertContains(response, '<tr class="model-album">')
url(r'^Юникод/(\w+)/$', views.client2, name="метка_оператора"), url(r'^Юникод/(?P<tag>\S+)/$', views.client2, name="метка_оператора_2"),
MIN_DOCKER = (1, 4, 0) MIN_DOCKER_PY = (1, 4, 0)
table = SQLTable(table_name, self, index=index_col, schema=schema) return table.read(coerce_float=coerce_float, parse_dates=parse_dates, columns=columns, chunksize=chunksize)
text_content = 'Firstname Sürname is a great guy.' html_content = '<p>Firstname Sürname is a <strong>great</strong> guy.</p>' msg = EmailMultiAlternatives('Subject', text_content, 'from@example.com', ['to@example.com']) msg.encoding = 'iso-8859-1' msg.attach_alternative(html_content, "text/html") payload0 = msg.message().get_payload(0) self.assertMessageHasHeaders(payload0, { ('MIME-Version', '1.0'), ('Content-Type', 'text/plain; charset="iso-8859-1"'), ('Content-Transfer-Encoding', 'quoted-printable')}) self.assertTrue(payload0.as_bytes().endswith(b'\n\nFirstname S=FCrname is a great guy.')) payload1 = msg.message().get_payload(1) self.assertMessageHasHeaders(payload1, { ('MIME-Version', '1.0'), ('Content-Type', 'text/html; charset="iso-8859-1"'), ('Content-Transfer-Encoding', 'quoted-printable')}) self.assertTrue( payload1.as_bytes().endswith(b'\n\n<p>Firstname S=FCrname is a <strong>great</strong> guy.</p>') )
result = store.select('df', where=where) tm.assert_frame_equal(result, expected)
object_list = get_mors_with_properties(service_instance, object_type, property_list=[property_name], container_ref=container_ref)
from __future__ import unicode_literals
if '/' == asset_key_string[0]: asset_key_string = asset_key_string[1:] asset_key = AssetKey.from_string(asset_key_string) try: delete_asset(course_key, asset_key) except AssetNotFoundException: pass
continue
actual_events = self.wait_for_events(event_filter={'event_type': event_type}, number_of_matches=1) self.assert_events_match(event_data, actual_events)
guestfs.__salt__ = {}
logger.debug('Websocket already connected, returning') return
sorted_deps = serializers.sort_dependencies( [('fixtures_regress', [M2MSimpleA, M2MSimpleB])] ) self.assertEqual(sorted_deps, [M2MSimpleB, M2MSimpleA])
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
raise NotImplementedError('This method must be set by a subclass.')
api.set_credit_requirement_status(username, self.course_key, "grade", "grade") self.assert_grade_requirement_status('satisfied', 0)
with warnings.catch_warnings(record=True) as warning_list: warnings.simplefilter("always") management.call_command( 'loaddata', 'bad_fixture2', verbosity=0, ) warning = warning_list.pop() self.assertEqual(warning.category, RuntimeWarning) self.assertEqual( str(warning.message), "No fixture data found for 'bad_fixture2'. (File format may be invalid.)" )
if os.path.isdir(tmp_dir): shutil.rmtree(tmp_dir) elif os.path.isfile(tmp_dir): os.remove(tmp_dir)
random_state.randint(MAX_INT, size=len(self.estimators_))
vectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18, non_negative=True)
course_id = thread_data.get("course_id") user = request.user if not course_id: raise ValidationError({"course_id": ["This field is required."]}) try: course_key = CourseKey.from_string(course_id) course = _get_course(course_key, user) except InvalidKeyError: raise ValidationError({"course_id": ["Invalid value."]}) context = get_context(course, request) _check_initializable_thread_fields(thread_data, context) if ( "group_id" not in thread_data and is_commentable_cohorted(course_key, thread_data.get("topic_id")) ): thread_data = thread_data.copy() thread_data["group_id"] = get_cohort_id(user, course_key) serializer = ThreadSerializer(data=thread_data, context=context) actions_form = ThreadActionsForm(thread_data) if not (serializer.is_valid() and actions_form.is_valid()): raise ValidationError(dict(serializer.errors.items() + actions_form.errors.items())) serializer.save() cc_thread = serializer.instance thread_created.send(sender=None, user=user, post=cc_thread) api_thread = serializer.data _do_extra_actions(api_thread, cc_thread, thread_data.keys(), actions_form, context, request) track_thread_created_event(request, course, cc_thread, actions_form.cleaned_data["following"]) return api_thread
self.assertTrue(np.issubdtype(pan.values.dtype, np.floating))
self.user.is_active = False self.user.save()
class BandAdmin(admin.ModelAdmin): formfield_overrides = { CharField: {'widget': forms.TextInput(attrs={'size': '10'})} } ma = BandAdmin(models.Band, admin.site) f1 = ma.formfield_for_dbfield(models.Band._meta.get_field('name'), request=None) f2 = ma.formfield_for_dbfield(models.Band._meta.get_field('style'), request=None) self.assertNotEqual(f1.widget, f2.widget) self.assertEqual(f1.widget.attrs['maxlength'], '100') self.assertEqual(f2.widget.attrs['maxlength'], '20') self.assertEqual(f2.widget.attrs['size'], '10')
self.debugActions = (startAction or _defaultStartDebugAction, successAction or _defaultSuccessDebugAction, exceptionAction or _defaultExceptionDebugAction) self.debug = True return self
for cline in content.split('\n'): new_file.append(cline + '\n')
return library.location.library_key
return self.paginator_class( queryset, per_page, orphans=orphans, allow_empty_first_page=allow_empty_first_page, **kwargs)
for app_config in apps.get_app_configs(): if app_config.models_module is None: continue if verbosity >= 2: print("Running post-migrate handlers for application %s" % app_config.label) models.signals.post_migrate.send( sender=app_config, app_config=app_config, verbosity=verbosity, interactive=interactive, using=db, **kwargs )
from pandas import Index return Index(self).is_monotonic
truncator = text.Truncator('-----') self.assertEqual('---B\u030A', truncator.chars(4, 'B\u030A')) self.assertEqual('-----', truncator.chars(5, 'B\u030A'))
for kf in [cval.KFold(i, 5) for i in range(11, 17)]: sizes = [] for _, test in kf: sizes.append(len(test))
split_test = self._update_partition_id(1) self.assertEqual(5, len(split_test.children)) self.assertEqual(initial_vertical_0_location, split_test.children[0]) self.assertEqual(initial_vertical_1_location, split_test.children[1]) vertical_0 = self.get_item_from_modulestore(split_test.children[2], verify_is_draft=True) vertical_1 = self.get_item_from_modulestore(split_test.children[3], verify_is_draft=True) vertical_2 = self.get_item_from_modulestore(split_test.children[4], verify_is_draft=True)
elapsed_time = time.time() - self._start_time self._print('Done %3i out of %3i | elapsed: %s finished', (len(self._output), len(self._output), short_format_time(elapsed_time)))
send_mail(subject, message, from_email, [self.email], **kwargs)
self.assertEqual(len(mail.outbox), 1) self.assertEqual( mail.outbox[0].subject, 'You have been un-enrolled from {display_name}'.format(display_name=self.course.display_name,) ) self.assertEqual( mail.outbox[0].body, "Dear Student,\n\nYou have been un-enrolled from course {display_name} by a member of the course staff. " "Please disregard the invitation previously sent.\n\n----\n" "This email was automatically sent from edx.org to robot-allowed@robot.org".format( display_name=self.course.display_name, ) )
self._assert_steps_displayed( response, PayAndVerifyView.PAYMENT_STEPS, PayAndVerifyView.PAYMENT_CONFIRMATION_STEP, )
if current_backend is not None: pipeline_target = "student_account.views.third_party_auth.pipeline" with simulate_running_pipeline(pipeline_target, current_backend): response = self.client.get(reverse(url_name), params)
regions = {} price_json = json.loads(price_js) for region in price_json['config']['regions']: sizes = {} for itype in region['instanceTypes']: for size in itype['sizes']: sizes[size['size']] = size regions[region['region']] = sizes
resp = self.client.get( reverse("signin_user"), HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME ) self.assertContains(resp, "Log into your Test Microsite Account") self.assertContains(resp, "login-form")
nulls_order_largest = False
self.assertTrue(self.lib_page.are_previews_showing()) self.lib_page.toggle_previews() self.assertFalse(self.lib_page.are_previews_showing()) target = self.lib_page.xblocks[0] target.edit() target.save_settings() self.assertFalse(target.is_placeholder()) self.assertTrue(all([xblock.is_placeholder() for xblock in self.lib_page.xblocks[1:]]))
from salt.utils import fopen, is_darwin, vt
instructor = InstructorFactory(course_key=self.course.id) self.client.login(username=instructor.username, password='test') url = reverse('mark_student_can_skip_entrance_exam', kwargs={'course_id': unicode(self.course.id)}) response = self.client.post(url, { 'unique_student_identifier': self.request.user.email, }) self.assertEqual(response.status_code, 200)
if isinstance(inputs, tensor.Variable): return self._corrupt(inputs) else: return [self._corrupt(inp) for inp in inputs]
assert isinstance(key_or_alias, string_types) try: return super(alias_dict, self).__getitem__(key_or_alias) except KeyError: return super(alias_dict, self).__getitem__( self.__a2k__[key_or_alias])
self.process_manager.stop_restarting() self.process_manager.send_signal_to_processes(signum) self.process_manager.kill_children()
t = Telegram.objects.create(title="Frist Telegram") self.assertEqual(Telegram.objects.count(), 1) response = self.client.get(reverse('admin:admin_views_telegram_change', args=(t.pk,))) self.assertEqual(response.status_code, 200) post_data = { "title": "Telegram without typo", "_save": "Save", } response = self.client.post(reverse('admin:admin_views_telegram_change', args=(t.pk,)), post_data, follow=True) self.assertEqual(response.status_code, 200) self.assertEqual(Telegram.objects.count(), 1) self.assertContains( response, '<li class="success">The telegram "<a href="%s">' 'Telegram without typo</a>" was changed successfully.</li>' % reverse('admin:admin_views_telegram_change', args=(t.pk,)), html=True )
else: ret = _load_result(existing, ret)
for path in glob.glob(path): shutil.rmtree(path)
if expected_item.location.category == 'course': actual_item_location = actual_item_location.replace(name=actual_item_location.run) actual_item = actual_item_map.get(map_key(actual_item_location)) if actual_item is None and expected_item.location.category == 'course': actual_item_location = actual_item_location.replace(name='course') actual_item = actual_item_map.get(map_key(actual_item_location))
self.connection_reset = connection_reset
self.assertTrue(self.run_function('system.set_remote_login', [True])) self.assertTrue(self.run_function('system.get_remote_login')) self.assertTrue(self.run_function('system.set_remote_login', [False])) self.assertFalse(self.run_function('system.get_remote_login'))
resp = self.client.post('/password_reset_confirm/{0}-{1}/'.format(uidb36, token), { 'new_password1': 'foo', 'new_password2': 'foo' }, follow=True)
if not self.infer_axes(): return False
import salt.ext.six as six
_urlconfs = local()
if name[-1] == '/' and name != '/': name = name[:-1]
result = self.ts.reindex() self.assertFalse((result is self.ts))
X, y = make_classification(n_samples=200, n_features=20, n_informative=3, n_redundant=2, n_repeated=0, n_classes=8, n_clusters_per_class=1, flip_y=0.0, class_sep=10, shuffle=False, random_state=0)
field = models.ManyToManyField("auth.Permission") name, path, args, kwargs = field.deconstruct()
cache.delete(key)
MAX_SCREEN_LIST_LENGTH = 250
days=5 \ CN='My Little CA' \ C=US \ ST=Utah \ L=Salt Lake City \ O=Saltstack \ emailAddress=pleasedontemail@example.com
boston = load_boston() perm = rng.permutation(boston.target.size) boston.data = boston.data[perm] boston.target = boston.target[perm]
return self.get_values()[slicer]
env_path = os.environ.get(env_var, path) if not env_path or not os.path.isfile(env_path): env_path = path if path != default_path: env_path = path
data = [[_remove_whitespace(self._text_getter(col)) for col in self._parse_td(row)] for row in rows] return data
self.create_programs_config() self.mock_programs_api(data={'results': []}) actual = utils.get_programs_for_dashboard(self.user, self.COURSE_KEYS) self.assertEqual(actual, {})
DataFrame({'a': 'foo', 'b': s}, index=dr) DataFrame({'a': 'foo', 'b': s.values}, index=dr)
_mixin_prio_ = 100
from __future__ import absolute_import import os import logging import json import salt.utils.http from salt.exceptions import CommandExecutionError
df = DataFrame({"A": [1, 2, 3], "B": [2., 3., 4.], "C": pd.date_range('20130101', periods=3), "D": ['foo', 'bar', 'baz']}) result = df.quantile(.5, axis=1) expected = Series([1.5, 2.5, 3.5], name=0.5) assert_series_equal(result, expected)
attributes = self.q(css=self._bounded_selector('.cohort-management-assignment-type-settings')).attrs('class') if 'is-disabled' in attributes[0].split(): return True return False
self._asides.append(aside)
import dns.resolver
VALID_SERVICE_DIRS = [ '/service', '/var/service', '/etc/service', ] SERVICE_DIR = None for service_dir in VALID_SERVICE_DIRS: if os.path.exists(service_dir): SERVICE_DIR = service_dir break
def __init__(self, dataset_size, batch_size, num_batches, rng=None): self._rng = make_np_rng(rng, which_method=["random_integers", "shuffle"]) if batch_size is None: raise ValueError("batch_size cannot be None for random uniform " "iteration") elif num_batches is None: raise ValueError("num_batches cannot be None for random uniform " "iteration") self._dataset_size = dataset_size self._batch_size = batch_size self._num_batches = num_batches self._next_batch_no = 0 @wraps(SubsetIterator.next) def next(self): if self._next_batch_no >= self._num_batches: raise StopIteration() else: self._last = self._rng.random_integers(low=0, high=self._dataset_size - 1, size=(self._batch_size,)) self._next_batch_no += 1 return self._last def __next__(self): return self.next() fancy = True stochastic = True uniform_batch_size = True
print('Failed to save %s to .npy file:\n%s' % ( type(obj), traceback.format_exc()))
cat = pd.Categorical([0, 1, 2], categories=(x for x in [0, 1, 2])) tm.assert_categorical_equal(cat, exp) cat = pd.Categorical([0, 1, 2], categories=xrange(3)) tm.assert_categorical_equal(cat, exp)
ovr = OneVsRestClassifier(DecisionTreeRegressor()) pred = ovr.fit(iris.data, iris.target).predict(iris.data) assert_equal(len(ovr.estimators_), n_classes) assert_array_equal(np.unique(pred), [0, 1, 2]) assert_greater(np.mean(pred == iris.target), .9)
self.data = _set_tz(self.data, self.tz, coerce=True)
import logging
assert_equal(10 + 9, n_lines)
self._make_eligible() self._purchase_credit() CourseEnrollmentAttribute.objects.all().delete()
tsne = TSNE(metric="not available") assert_raises_regexp(ValueError, "Unknown metric not available.*", tsne.fit_transform, np.array([[0.0], [1.0]]))
class X(object):
instance = kwargs['instance'] instance.orig_state = instance.state
if set(salt_data['data'].get('lost', [])): dropped_minions = set(salt_data['data'].get('lost', [])) else: dropped_minions = set(self.minions) - set(salt_data['data'].get('present', []))
char = '\u0394' df = pd.DataFrame({'A': [char]}) with ensure_clean_store(self.path) as store: store.put('df', df, format='table', encoding='utf-8') result = store.get('df') tm.assert_frame_equal(result, df)
'ioflo_console_logdir': str,
inline_topics = self.q(css=self._bounded_selector('.check-discussion-subcategory-inline')) return all(topic.get_attribute('disabled') == 'true' for topic in inline_topics)
help = "Create a course in one of {}".format([ModuleStoreEnum.Type.mongo, ModuleStoreEnum.Type.split]) args = "modulestore user org course run"
for page_name in ["verify_student_start_flow", "verify_student_begin_flow", "verify_student_upgrade_and_verify"]: response = self._get_page(page_name, course.id) self.assertContains(response, "Upgrade Deadline Has Passed")
self.choose_new_seed()
return etree.parse(file_object, parser=EDX_XML_PARSER).getroot()
self.caption = caption return self
elif location is None: if __opts__['test']: ret['result'] = None ret['comment'] = 'No location to copy image from specified,\n' +\ 'glance.image_present would not create one' else: ret['result'] = False ret['comment'] = 'No location to copy image from specified,\n' +\ 'not creating a new image.' return ret
for feat in self: if feat.fid == feat_id: return feat
self.show_question_answer('p1')
with open(writer.path, "wb") as fh: fh.write(writer.as_string()) if self.verbosity > 0: self.stdout.write("\nCreated new merge migration %s" % writer.path)
item['location'] = item['_id'] del item['_id']
self.exp_env_info['host'] = socket.gethostname() self.exp_env_info['cpu'] = platform.processor() self.exp_env_info['os'] = platform.platform() if 'theano' in sys.modules: self.exp_env_info['theano_config'] = sys.modules['theano'].config else: self.exp_env_info['theano_config'] = None
conditional_params = self.mlp.fprop(X) if not type(conditional_params) == tuple: conditional_params = (conditional_params, ) return conditional_params
@setup({'autoescape-tag09': r'{{ unsafe }}'}) def test_autoescape_tag09(self): output = self.engine.render_to_string('autoescape-tag09', {'unsafe': UnsafeClass()}) self.assertEqual(output, 'you &amp; me')
settings.STATICFILES_DIRS.append( (u'themes/{}'.format(settings.THEME_NAME), theme_root / 'static') )
if not self._known_consolidated: self._consolidate_check() return self._is_consolidated
truncated = ts.truncate(after=self.ts.index[0] - offset) assert (len(truncated) == 0)
self.related_updates.setdefault(model, []).append((field, None, value))
return SlashSeparatedCourseKey(org, course, url_name)
with np.errstate(divide='ignore'): retval = 1. / dist return retval ** 2
kind = arr.dtype.kind if kind == 'M' or kind == 'm': return arr.dtype in _DATELIKE_DTYPES
pass
#latex_preamble = ''
try: return User.objects.get(username=requested_username) except User.DoesNotExist: raise Http404( "Requested user '{requested_username}' does not exist.".format(requested_username=requested_username) )
log.shutdown_multiprocessing_logging_listener()
centers[center_idx] /= counts[center_idx]
y = zca_dataset.mapback_for_viewer(zca_dataset.X) z = x/np.abs(x).max(axis=0) assert_allclose(z[start:stop], y, rtol=1e-2)
assert_raises(ValueError, sample_without_replacement, 0, 1) assert_raises(ValueError, sample_without_replacement, 1, 2)
raise NotImplementedError(str(type(self))+" does not implement plot.")
((), {}, '<label for="id_field">Field:</label>'),
alice_profile = UserProfile.objects.using('default').create(user=alice, flavor='chocolate') with self.assertRaises(ValueError): bob.userprofile = alice_profile
raise NotImplementedError
renderers = salt.loader.render(__opts__, __salt__) config = {}
if not cuda.cuda_available: convnet_available.compile_error = True _logger.debug('cuda unavailable') return False
return ['cuda_convnet', config.pthreads.lib] if config.pthreads.lib else ['cuda_convnet']
if "TRACKING_IGNORE_URL_PATTERNS" in ENV_TOKENS: TRACKING_IGNORE_URL_PATTERNS = ENV_TOKENS.get("TRACKING_IGNORE_URL_PATTERNS")
self.assertEqual(3, count_sessions()) management.call_command('clearsessions') self.assertEqual(1, count_sessions())
result = strings.str_cat(one) exp = 'aabbc' self.assertEqual(result, exp)
COURSE_CATALOG_VISIBILITY_PERMISSION = 'see_exists'
GENERAL_ASSET_TYPE = 'asset'
NOT_EDITABLE_HELPER_MESSAGE = "Contact your edX Partner Manager to update these settings." NOT_EDITABLE_DATE_WRAPPER = "<div class=\"field date is-not-editable\" id=\"field-enrollment-end-date\">" NOT_EDITABLE_TIME_WRAPPER = "<div class=\"field time is-not-editable\" id=\"field-enrollment-end-time\">" NOT_EDITABLE_DATE_FIELD = "<input type=\"text\" class=\"end-date date end\" \
student_counter += 1 TASK_LOG.info( u'%s, Task type: %s, Current step: %s, Grade calculation in-progress for students: %s/%s', task_info_string, action_name, current_step, student_counter, total_enrolled_students )
block_name = qualifiers.pop('name') block_ids = [] for block_id, block in course.structure['blocks'].iteritems(): if block_id.id in block_name and _block_matches_all(block): block_ids.append(block_id)
pipeline = preprocessing.Pipeline() pipeline.items.append(preprocessing.GlobalContrastNormalization(batch_size=5000)) pipeline.items.append(preprocessing.LeCunLCN((32,32)))
for minion in sorted(minions): digest = hashlib.sha256(str(minions[minion])).hexdigest() if digest not in ret: ret[digest] = {} ret[digest]['pool'] = [] ret[digest]['result'] = str(minions[minion])
_maybe_remove(store, 'df') store.append('df', df, data_columns=True) result = store.select('df') assert_frame_equal(result, df)
self.vw.default_parts.create(name='Windows') p6 = Part.objects.get(name='Windows') expected_messages.append({ 'instance': self.vw, 'action': 'pre_add', 'reverse': False, 'model': Part, 'objects': [p6], }) expected_messages.append({ 'instance': self.vw, 'action': 'post_add', 'reverse': False, 'model': Part, 'objects': [p6], }) self.assertEqual(self.m2m_changed_messages, expected_messages)
ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright) ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)
if db != 'read_replica': DATABASES[db].update(get_db_overrides(db))
import salt.utils
[(CourseMode.DEFAULT_SHOPPINGCART_MODE_SLUG, CourseMode.DEFAULT_SHOPPINGCART_MODE_SLUG)]
return mapping[key]
if referer.scheme != 'https': return self._reject(request, REASON_INSECURE_REFERER)
with self.assertNumQueries(1): bookmarks_data = api.get_bookmarks(user=self.user, course_key=course.id) self.assertEqual(len(bookmarks_data), count) self.assert_bookmark_data_is_valid(bookmarks[-1], bookmarks_data[0]) self.assert_bookmark_data_is_valid(bookmarks[0], bookmarks_data[-1])
ser = Series(['A', 'B'], [1, 2]) ser.sort_values()
from pandas.core.config import get_option if not isinstance(arg, compat.string_types): return arg from pandas.tseries.offsets import DateOffset if isinstance(freq, DateOffset): freq = freq.rule_code if dayfirst is None: dayfirst = get_option("display.date_dayfirst") if yearfirst is None: yearfirst = get_option("display.date_yearfirst") return tslib.parse_datetime_string_with_reso(arg, freq=freq, dayfirst=dayfirst, yearfirst=yearfirst)
random_state = check_random_state(0) y_true = random_state.randint(0, 4, size=(20, )) y_pred = random_state.randint(0, 4, size=(20, )) n_samples = y_true.shape[0]
if keepdb: return sys.stderr.write("Got an error creating the test database: %s\n" % e) if not autoclobber: confirm = input( "It appears the test database, %s, already exists. " "Type 'yes' to delete it, or 'no' to cancel: " % parameters['user']) if autoclobber or confirm == 'yes': if verbosity >= 1: print("Destroying old test database for alias '%s'..." % self.connection.alias) try: self._execute_test_db_destruction(cursor, parameters, verbosity) except DatabaseError as e: if 'ORA-29857' in str(e): self._handle_objects_preventing_db_destruction(cursor, parameters, verbosity, autoclobber) else: sys.stderr.write("Got an error destroying the old test database: %s\n" % e) sys.exit(2) except Exception as e: sys.stderr.write("Got an error destroying the old test database: %s\n" % e) sys.exit(2) try: self._execute_test_db_creation(cursor, parameters, verbosity, keepdb) except Exception as e: sys.stderr.write("Got an error recreating the test database: %s\n" % e) sys.exit(2) else: print("Tests cancelled.") sys.exit(1)
import salt.utils import salt.utils.find import salt.utils.filebuffer import salt.utils.files import salt.utils.atomicfile import salt.utils.url from salt.exceptions import CommandExecutionError, SaltInvocationError, get_error_message as _get_error_message
try: update_account_settings(user, {"name": full_name}) except UserNotFound: return HttpResponseBadRequest(_("No profile found for user")) except AccountValidationError: msg = _( "Name must be at least {min_length} characters long." ).format(min_length=NAME_MIN_LENGTH) return HttpResponseBadRequest(msg)
if salt.utils.is_windows(): fp_.close() try: os.unlink(fn_) except OSError: pass
self.login_page.visit()
self.assert_json_success_response_looks_correct(student_views.create_account(strategy.request)) created_user = self.get_user_by_email(strategy, email) self.assert_password_overridden_by_pipeline(overridden_password, created_user.username)
import logging
try: ret, req_opts = yield self.payload_handler(payload) except Exception as e: stream.send('Some exception handling minion payload') log.error('Some exception handling a payload from minion', exc_info=True) raise tornado.gen.Return()
plt.figure(figsize=(12, 8)) plt.subplot(221) plt.plot(X_train_r[:, 0], Y_train_r[:, 0], "ob", label="train") plt.plot(X_test_r[:, 0], Y_test_r[:, 0], "or", label="test") plt.xlabel("x scores") plt.ylabel("y scores") plt.title('Comp. 1: X vs Y (test corr = %.2f)' % np.corrcoef(X_test_r[:, 0], Y_test_r[:, 0])[0, 1]) plt.xticks(()) plt.yticks(()) plt.legend(loc="best")
if on_rtd: os.environ['DJANGO_SETTINGS_MODULE'] = 'lms' else: os.environ['DJANGO_SETTINGS_MODULE'] = 'lms'
response = organizations_helpers.get_organization_by_short_name(self.organization['short_name']) self.assertIsNone(response)
with salt.utils.fopen(conf_file, 'r') as ifile: for line in ifile: line = line.strip() if not line: continue if line.startswith('#'): continue splitline = line.split(' ', 1) ret[splitline[0]] = splitline[1] return ret
if x == 0: res = to_digits[0] else: res = '' while x > 0: digit = x % len(to_digits) res = to_digits[digit] + res x = int(x // len(to_digits)) return neg, res
self.assertIn( 'Invalid String Value for Enabled', self.run_function('system.set_remote_login', ['spongebob']))
app_module = import_module(app_name)
self.server.config['test_reset'] = 'This is a reset config test'
import salt.thorium
import salt.utils import salt.output from salt.utils.locales import sdecode
selector = self.prefix + ' .signatory-{}-view-{}'.format(self.mode, self.index) return ' '.join([selector, css])
assert_array_almost_equal(np.diag(K), np.ones(5))
return _("{currency_symbol}{price}").format(currency_symbol=currency_symbol, price=price)
if not added and not cmd_opt: raise CommandExecutionError( 'Specified arguments did not result in modification of repo' )
return self.opts['timeout']
buckets = 10 bucket_width = 1. / float(buckets) for i in xrange(buckets): lower_lim = i * bucket_width upper_lim = (i+1) * bucket_width
if com.in_qtconsole(): return None
prefix = self.idp_slug + ":" return self.backend_name == social_auth.provider and social_auth.uid.startswith(prefix)
distances.flat[::distances.shape[0] + 1] = 0.0
group1_id = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, self.user_partition)
store.select('df2', typ='legacy_frame')
data = None
return True
DATABASES = { 'default': { 'ENGINE': 'django.db.backends.sqlite3', 'NAME': TEST_ROOT / "db" / "test_edx.db", 'TEST_NAME': TEST_ROOT / "db" / "test_edx.db", 'OPTIONS': { 'timeout': 30, }, 'ATOMIC_REQUESTS': True, }, 'student_module_history': { 'ENGINE': 'django.db.backends.sqlite3', 'NAME': TEST_ROOT / "db" / "test_student_module_history.db", 'TEST_NAME': TEST_ROOT / "db" / "test_student_module_history.db", 'OPTIONS': { 'timeout': 30, }, } }
if re.search('[^a-zA-Z0-9]', force_text(token)): return _get_new_csrf_token() elif len(token) == CSRF_TOKEN_LENGTH: return token elif len(token) == CSRF_SECRET_LENGTH: return _salt_cipher_secret(token) return _get_new_csrf_token()
class CustomYearExact(YearExact): def as_custom_sql(self, compiler, connection): lhs_sql, lhs_params = self.process_lhs(compiler, connection, self.lhs.lhs) rhs_sql, rhs_params = self.process_rhs(compiler, connection) params = lhs_params + rhs_params + lhs_params + rhs_params return ("%(lhs)s >= str_to_date(CONCAT(%(rhs)s, '-01-01'), '%%%%Y-%%%%m-%%%%d') " "AND %(lhs)s <= str_to_date(CONCAT(%(rhs)s, '-12-31'), '%%%%Y-%%%%m-%%%%d')" % {'lhs': lhs_sql, 'rhs': rhs_sql}, params) setattr(CustomYearExact, 'as_' + connection.vendor, CustomYearExact.as_custom_sql) YearTransform.register_lookup(CustomYearExact) self.assertIn( 'CONCAT(', str(Author.objects.filter(birthdate__testyear=2012).query))
url = reverse('instructor_dashboard', kwargs={'course_id': self.course.id.to_deprecated_string()}) response = self.client.get(url)
httpretty.register_uri(httpretty.POST, urljoin(ZENDESK_URL, '/api/v2/tickets.json'), status=status, body='{}', content_type=JSON)
self.add_dropdown_to_section(vertical_1.location, 'H2P1_GROUP1', 1).location.html_id()
if not self.q(css="input.proctored_exam").present: return False
self.settings_page.visit() self.settings_page.wait_for_page() self.assertTrue(self.settings_page.is_browser_on_page()) self.settings_page.entrance_exam_field.click() self.settings_page.save_changes()
self.add_coupon(self.testing_course.id, True, self.coupon_code) resp = self.client.post(reverse('shoppingcart.views.use_code'), {'code': self.coupon_code}) self.assertEqual(resp.status_code, 200)
self._fields[key.field_name] = value
new_course_key = self.course_key.replace(course=self.course_key.course.upper()) resp = self._create_course_with_given_location(new_course_key) self.assertEqual(resp.status_code, 200)
if not is_okwarning: for w in ws: s = "\nWarning in %s at block ending on line %s\n" % (filename, lineno) s += "Specify :okwarning: as an option in the ipython:: block to suppress this message\n" sys.stdout.write('\n\n>>>' + ('-' * 73)) sys.stdout.write(s) sys.stdout.write('-' * 76 + '\n') s=warnings.formatwarning(w.message, w.category, w.filename, w.lineno, w.line) sys.stdout.write(s) sys.stdout.write('<<<' + ('-' * 73) + '\n')
urlpatterns = patterns( "edxnotes.views", url(r"^/$", "edxnotes", name="edxnotes"), url(r"^/notes/$", "notes", name="notes"), url(r"^/token/$", "get_token", name="get_token"), url(r"^/visibility/$", "edxnotes_visibility", name="edxnotes_visibility"), )
self.assertTrue('Activate Course Enrollment' in response.content)
self.assertEqual('2001:0000:5ef5:79fd:0000:059d:a0e5:0ba1', addr2.exploded) self.assertEqual('2001:0000:0000:0000:0000:0000:0000:0000/96', addr3.exploded) self.assertEqual('192.168.178.1', addr4.exploded)
@setup({'linenumbers01': '{{ a|linenumbers }} {{ b|linenumbers }}'}) def test_linenumbers01(self): output = self.engine.render_to_string( 'linenumbers01', {'a': 'one\n<two>\nthree', 'b': mark_safe('one\n&lt;two&gt;\nthree')}, ) self.assertEqual(output, '1. one\n2. &lt;two&gt;\n3. three 1. one\n2. &lt;two&gt;\n3. three') @setup({'linenumbers02': '{% autoescape off %}{{ a|linenumbers }} {{ b|linenumbers }}{% endautoescape %}'}) def test_linenumbers02(self): output = self.engine.render_to_string( 'linenumbers02', {'a': 'one\n<two>\nthree', 'b': mark_safe('one\n&lt;two&gt;\nthree')}, ) self.assertEqual(output, '1. one\n2. <two>\n3. three 1. one\n2. &lt;two&gt;\n3. three')
'poll_answer': self.poll_answer, 'poll_answers': self.poll_answers if self.voted else {}, 'total': sum(self.poll_answers.values()) if self.voted else 0, 'reset': str(self.descriptor.xml_attributes.get('reset', 'true')).lower()
course_key = CourseKey.from_string(course_id) user = _get_user(user_id) enrollment = CourseEnrollment.get_enrollment(user, course_key) return CourseEnrollmentAttribute.get_enrollment_attributes(enrollment)
s = Series( date_range('2000-01-01 09:00:00', periods=5, tz='US/Eastern'), name='foo') result = s.diff() assert_series_equal(result, Series( TimedeltaIndex(['NaT'] + ['1 days'] * 4), name='foo'))
self.set_group_access( block_specified, { self.animal_partition.id: [self.cat_group.id], self.color_partition.id: [self.red_group.id], }, ) self.check_access(self.red_cat, block_accessed, True) self.check_access(self.blue_dog, block_accessed, False) self.check_access(self.white_mouse, block_accessed, False) self.check_access(self.gray_worm, block_accessed, False) self.ensure_staff_access(block_accessed)
temp_path = salt.utils.mkstemp() with salt.utils.fopen(CONFIG, 'r') as org_conf: with salt.utils.fopen(temp_path, 'w') as temp_sysconf: for line in org_conf: temp_sysconf.write(line) return temp_path
problem = new_loncapa_problem(xml_str, capa_system=self.capa_system)
if not self._requested_xblock_fields: return for xblock_usage_key, xblock in self._xblock_map.iteritems(): for field_name in self._requested_xblock_fields: self._set_xblock_field(xblock_usage_key, xblock, field_name)
expected = { "action": "unenroll", "auto_enroll": False, "results": [ { "identifier": self.enrolled_student.email, "before": { "enrollment": True, "auto_enroll": False, "user": True, "allowed": False, }, "after": { "enrollment": False, "auto_enroll": False, "user": True, "allowed": False, } } ] }
import salt.utils
a_rows, a_cols, b_rows, b_cols = _check_rows_and_columns(a, b) n_a = a_rows.shape[0] n_b = b_rows.shape[0] result = np.array(list(list(similarity(a_rows[i], a_cols[i], b_rows[j], b_cols[j]) for j in range(n_b)) for i in range(n_a))) return result
self.assertOLXIsDraftOnly(block_list_to_unpublish) self.assertOLXIsDraftOnly(block_list_parent) self.publish(block_list_parent) self.assertOLXIsPublishedOnly(block_list_parent) self.assertOLXIsPublishedOnly(block_list_to_unpublish) self.unpublish(block_list_to_unpublish) self.assertOLXIsDraftOnly(block_list_to_unpublish) if self.is_split_modulestore: self.assertOLXIsDraftAndPublished(block_list_parent) elif self.is_old_mongo_modulestore: self.assertOLXIsPublishedOnly(block_list_parent) else: raise Exception("Must test either Old Mongo or Split modulestore!")
prec = -np.floor(np.log10(np.min( np.ediff1d(unique_pcts, to_begin=to_begin, to_end=to_end) ))).astype(int) prec = max(1, prec) out = np.empty_like(percentiles, dtype=object) out[int_idx] = percentiles[int_idx].astype(int).astype(str) out[~int_idx] = percentiles[~int_idx].round(prec).astype(str) return [i + '%' for i in out]
content = "<p><br><br></p>" payload = get_response(content, 'January 11, 2013') self.assertHTMLEqual(content, payload['content'])
for t in [r, g]: def f(): r[['A']].agg({'A': ['sum', 'std'], 'B': ['mean', 'std']})
pr_url = os.environ.get('CI_PULL_REQUEST') if not pr_url: exit("not a pull request")
mock_info = MagicMock(return_value={'Newvolume1': {'status': '1'}}) with patch.object(glusterfs, 'info', mock_info): mock_run = MagicMock(return_value=xml_command_success) with patch.dict(glusterfs.__salt__, {'cmd.run': mock_run}): self.assertEqual( glusterfs.start_volume('Newvolume1', force=True), True ) mock_run = MagicMock(return_value=xml_command_fail) with patch.dict(glusterfs.__salt__, {'cmd.run': mock_run}): self.assertEqual(glusterfs.start_volume('Newvolume1'), True) self.assertEqual( glusterfs.start_volume('Newvolume1', force=True), False )
filters = {'course_id': course_key, 'user': request.user} if uri != '': filters['uri'] = uri
def _checkindex(self, index): "Checks the given index." sz = self.size if (sz < 1) or (index < 0) or (index >= sz): raise IndexError('invalid GEOS Geometry index: %s' % str(index))
self.frame.reindex(columns=['A']).info(verbose=False, buf=buf) self.frame.reindex(columns=['A', 'B']).info(verbose=False, buf=buf)
from __future__ import absolute_import import os import pwd import grp import random
assert_equal(lfw_people.images.shape, (10, 62, 47)) assert_equal(lfw_people.data.shape, (10, 2914))
if target not in old: log.error('{0} {1} not installed'.format(target, version)) ret[target] = {'current': 'not installed'} continue else: if not version_num == old.get(target) \ and not old.get(target) == "Not Found" \ and version_num != 'latest': log.error('{0} {1} not installed'.format(target, version)) ret[target] = { 'current': '{0} not installed'.format(version_num) } continue
return re.sub( r"<!--.*-->", "", re.sub( r"<!\[CDATA\[.*\]\]>", "", re.sub( r"(\s|&nbsp;|//)+", " ", html_to_text(content) ) ) )
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import MagicMock, patch
AutoAuthPage(self.browser, course_id=self.course_id).visit()
second_result = SoftwareSecurePhotoVerification.get_initial_verification(user=user) self.assertIsNotNone(second_result) self.assertEqual(second_result, first_result)
if not settings.FEATURES.get('ENABLE_CORS_HEADERS'): raise MiddlewareNotUsed()
update_account_settings(self.user, {"name": "Mickey Mouse"}) account_settings = get_account_settings(self.default_request) self.assertEqual("Mickey Mouse", account_settings["name"]) update_account_settings(self.user, {"name": "Donald Duck"}, username=self.user.username) account_settings = get_account_settings(self.default_request) self.assertEqual("Donald Duck", account_settings["name"]) with self.assertRaises(UserNotAuthorized): update_account_settings(self.different_user, {"name": "Pluto"}, username=self.user.username)
from __future__ import unicode_literals
with TIMER.timer("find_ancestor_structures", course_context) as tagger: docs = [ structure_from_mongo(structure, course_context) for structure in self.structures.find({ 'original_version': original_version, 'blocks': { '$elemMatch': { 'block_id': block_key.id, 'block_type': block_key.type, 'edit_info.update_version': { '$exists': True, }, }, }, }) ] tagger.measure("structures", len(docs)) return docs
text_content = re.sub(r"(\s|&nbsp;|//)+", " ", html_to_text(html_content)) text_content = re.sub(r"<!\[CDATA\[.*\]\]>", "", text_content) text_content = re.sub(r"<!--.*-->", "", text_content)
return LibraryLocator.from_string(self.source_library_id)
with self.assertRaises(FieldError): Author.objects.filter(birthdate__testyear__junk=2012) self.assertEqual(TrackCallsYearTransform.call_order, ['lookup', 'transform']) TrackCallsYearTransform.call_order = [] with self.assertRaises(FieldError): Author.objects.filter(birthdate__testyear__junk__more_junk=2012) self.assertEqual(TrackCallsYearTransform.call_order, ['transform']) TrackCallsYearTransform.call_order = [] Author.objects.filter(birthdate__testyear=2012) self.assertEqual(TrackCallsYearTransform.call_order, ['lookup']) TrackCallsYearTransform.call_order = [] Author.objects.filter(birthdate__testyear__exact=2012) self.assertEqual(TrackCallsYearTransform.call_order, ['lookup'])
FEATURES['EMBARGO'] = True
return self.q(css="#prereq_min_score").visible
management.call_command( 'loaddata', 'sequence', verbosity=0, )
with open("{}/static/{}".format(DATA_DIR, asset_key.block_id), "rb") as f: content = StaticContent( asset_key, "Funky Pix", mimetypes.guess_type(asset_key.block_id)[0], f.read(), ) self.store.contentstore.save(content)
msg = ( "Cannot decorate 'prop' as it isn't a callable attribute of " "<class 'Test'> (1)" ) with self.assertRaisesMessage(TypeError, msg): @method_decorator(lambda: None, name="prop") class Test(object): prop = 1 @classmethod def __module__(cls): return "tests"
return self.s_rng.binomial(size=x.shape, p=x, dtype=theano.config.floatX)
all_models = [ (app_config.label, router.get_migratable_models(app_config, connection.alias, include_auto_created=False)) for app_config in apps.get_app_configs() if app_config.models_module is not None and app_config.label in app_labels ]
raise NotImplementedError()
raise NotImplementedError
self._add_prerequisite_course() add_prerequisite_course(self.course.id, self.prereq_course.id) fulfill_course_milestone(self.prereq_course.id, self.user) self.init_course_access() self.api_response()
from salt.states import zk_concurrency
self.admin_login(username='super', password='secret') self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder4_add')) inline_id = '#inner4stacked_set-group' def rows_length(): return len(self.selenium.find_elements_by_css_selector('%s .dynamic-inner4stacked_set' % inline_id)) self.assertEqual(rows_length(), 3) add_button = self.selenium.find_element_by_link_text( 'Add another Inner4 stacked') add_button.click() self.assertEqual(rows_length(), 4)
with self.assertRaises(Article.DoesNotExist): Article.objects.earliest()
self.assertTrue(act.called) expected_tasks = [ftask.to_dict() for ftask in self.tasks] actual_tasks = json.loads(response.content)['tasks'] for exp_task, act_task in zip(expected_tasks, actual_tasks): self.assertDictEqual(exp_task, act_task)
with patch.dict('django.conf.settings.FEATURES', {'ENABLE_LTI_PROVIDER': False}): request = build_launch_request() response = views.lti_launch(request, None, None) self.assertEqual(response.status_code, 403)
if os.path.isfile(session_cookie_jar): with salt.utils.fopen(session_cookie_jar, 'rb') as fh_: session_cookies = msgpack.load(fh_) if isinstance(session_cookies, dict): header_dict.update(session_cookies) else: with salt.utils.fopen(session_cookie_jar, 'wb') as fh_: msgpack.dump('', fh_)
raise NotImplementedError(str(self.__class__) + " does not implement " "sample_from_epsilon, which probably " "means it is not able to sample using the " "reparametrization trick.")
X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10])) y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_filtered)
source = String( help=_("The external URL to download the video."), display_name=_("Download Video"), scope=Scope.settings, default="" ) download_video = Boolean(
expected = DataFrame({'Quantity': [31], 'Date': [DT.datetime(2013, 10, 31, 0, 0) ]}).set_index('Date') result = df.groupby(pd.Grouper(freq='1M')).sum() assert_frame_equal(result, expected)
clf = LinearSVC(random_state=0) X, y = make_blobs(random_state=0, centers=2) Cs = [.1, 1, 10] for score in ['f1', 'roc_auc']: grid_search = GridSearchCV(clf, {'C': Cs}, scoring=score) grid_search.fit(X, y) cv = StratifiedKFold(n_folds=3) for C, scores in zip(Cs, grid_search.grid_scores_): clf.set_params(C=C)
import salt.utils import salt.utils.itertools from salt.exceptions import SaltInvocationError, CommandExecutionError
mac_address = ret.get('mac_address', None) if mac_address is None and 'macAddress' in ret['devices'][device]: ret['mac_address'] = ret['devices'][device]['macAddress']
DATE_INPUT_FORMATS = [ '%d/%m/%Y', '%d/%m/%y' ] DATETIME_INPUT_FORMATS = [ '%d/%m/%Y %H:%M:%S', '%d/%m/%Y %H:%M:%S.%f', '%d/%m/%Y %H:%M', '%d/%m/%y %H:%M:%S', '%d/%m/%y %H:%M:%S.%f', '%d/%m/%y %H:%M', ] DECIMAL_SEPARATOR = ',' THOUSAND_SEPARATOR = '.' NUMBER_GROUPING = 3
if imports:
e = pd.Series([0, 1, 2, 3.5, pd.Timestamp('20130101')]) tr, v = [3, 4], [3.5, pd.Timestamp('20130101')] check_replace(tr, v, e)
response = self.client.get(reverse('admin:admin_views_person_changelist'), {'o': '-2.1'}) self.assertContentBefore(response, link2, link3) self.assertContentBefore(response, link3, link1)
if six.PY2 and not isinstance(path, bytes): return path.encode(fs_encoding) return path
tm.assert_series_equal(s.transpose(), s)
if values.dtype == np.object_: if convert_numeric: try: new_values = lib.maybe_convert_numeric(values, set(), coerce_numeric=True)
wmi = types.ModuleType('wmi') sys.modules['wmi'] = wmi
assert_almost_equal(result._y.values.flat, [1, 4, 5], check_dtype=False) exp_x = DataFrame([[1., 6., 14., 1.], [1, 9, 17, 1], [0, 30, 48, 1]], index=result._x.index, columns=['FE_A', 'x1', 'x2', 'intercept'], dtype=float) tm.assert_frame_equal(result._x, exp_x.ix[:, result._x.columns])
pass
data = "a,b,c\n4,5,6\n\\" self.assertRaises(Exception, self.read_csv, StringIO(data), escapechar='\\')
from unittest import expectedFailure from django.db import connection return expectedFailure(func) if connection.vendor == 'oracle' and six.PY3 else func
ret['changes'] = image_update
environ = RequestFactory().get('/').environ environ['PATH_INFO'] = b'\xed' if six.PY2 else '\xed' handler = WSGIHandler() response = handler(environ, lambda *a, **k: None) self.assertEqual(response.status_code, 400)
self.assertEqual(len(mail.outbox), 1) self.assertEqual(mail.outbox[0].subject, 'Greetings from a function action')
result = df.drop_duplicates('C') expected = df.iloc[[0, 1, 5, 6]] tm.assert_frame_equal(result, expected)
X = csc_matrix(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
import logging import json import datetime
plt.figure(2, figsize=(3, 2.2)) plt.imshow(face_compressed, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)
assert_array_almost_equal(X_truth[:, 2], out_X_unstructured[4]) assert_array_almost_equal(X_truth[:, 2], out_X_structured[4])
new_key = [] for i, component in enumerate(key): if isinstance(component, compat.string_types) and \ labels.levels[i].is_all_dates: new_key.append(slice(component, component, None)) else: new_key.append(component) key = tuple(new_key)
by_columns = df.reset_index().groupby(idx_names).mean()
cache_key += '.%s' % getattr(request, 'LANGUAGE_CODE', get_language())
pass
idx = MultiIndex.from_product([['a', 'b'], ['a', 'b', 'c']], names=['missing', 'dense']) expected = DataFrame([0, 1, 2, np.nan, np.nan, np.nan], index=idx, columns=['values'])
if course is open_enroll_course or student is shib_student: self.assertEqual(response.status_code, 200) self.assertTrue(CourseEnrollment.is_enrolled(student, course.id)) else: self.assertEqual(response.status_code, 400) self.assertFalse(CourseEnrollment.is_enrolled(student, course.id))
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) _set_tcp_keepalive(sock, self.opts) stream = tornado.iostream.IOStream( sock, io_loop=self.io_loop, max_buffer_size=max_buffer_size) return stream.connect(addr)
resp = self.client.get('/copyright', HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME) self.assertEqual(resp.status_code, 200) self.assertContains(resp, 'This is a copyright page for an Open edX microsite.')
__func_alias__ = { 'set_': 'set' }
with translation.override('pl'): result = management.call_command('leave_locale_alone_false', stdout=StringIO()) self.assertIsNone(result)
self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_member_add'))
if installed_apps is not None: self.populate(installed_apps)
self.assertRaises(Exception, self.bseries.__getitem__, len(self.bseries) + 1)
self.assertTrue(hasattr(course_overview, 'image_set')) self.assertEqual(course_overview.image_set.small_url, '') self.assertEqual(course_overview.image_set.large_url, '')
] DATETIME_INPUT_FORMATS = [
@property def supports_collect_aggr(self): return aggregates.Collect not in self.connection.ops.disallowed_aggregates
def setUp(self): super(SignAndSendReplaceResultTest, self).setUp() self.course_key = CourseLocator( org='some_org', course='some_course', run='some_run' ) self.usage_key = BlockUsageLocator( course_key=self.course_key, block_type='problem', block_id='block_id' ) self.user = UserFactory.create() consumer = LtiConsumer( consumer_name='consumer', consumer_key='consumer_key', consumer_secret='secret' ) consumer.save() outcome = OutcomeService( lis_outcome_service_url='http://example.com/service_url', lti_consumer=consumer, ) outcome.save() self.assignment = GradedAssignment( user=self.user, course_key=self.course_key, usage_key=self.usage_key, outcome_service=outcome, lis_result_sourcedid='sourcedid', ) self.assignment.save() @patch('requests.post', return_value='response') def test_sign_and_send_replace_result(self, post_mock): response = outcomes.sign_and_send_replace_result(self.assignment, 'xml') post_mock.assert_called_with( 'http://example.com/service_url', data='xml', auth=ANY, headers={'content-type': 'application/xml'} ) self.assertEqual(response, 'response')
try: block = self.make_block(transf(values.astype(value.dtype))) return block.setitem(indexer=indexer, value=value, mgr=mgr)
if self._inplace: raise ValueError("%s instance is already inplace, can't convert" % self.__class__.__name__) return self.__class__(self._size_f, self._add_scale, self._pow_scale, self._blocked, inplace=True)
return (name, kwargs)
self.unapply_operations("test_rmflmm", with_field_state, operations=operations) self.assertTableExists("test_rmflmm_pony_stables")
clf = GradientBoostingRegressor(n_estimators=10, random_state=1) clf.fit(boston.data, boston.target)
self.reset_tracker()
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
name = models.CharField(blank=False, max_length=80) def __str__(self): return self.name
td.astype('int64')
sel_fmt = connection.ops.select % sel_fmt
g.precisions_init = rand_data.precisions[covar_type] g.fit(X) assert_array_equal(rand_data.precisions[covar_type], g.precisions_init)
if self[0] <= other[0]: left, right = self, other else: left, right = other, self
return "https://{}{}".format(settings.SITE_NAME, reverse( 'courseware.module_render.handle_xblock_callback_noauth', args=[ self.course.id.to_deprecated_string(), quote_slashes(unicode(self.lti_published.scope_ids.usage_id.to_deprecated_string()).encode('utf-8')), handler ] ))
nonstaff_user.is_active = True nonstaff_user.is_staff = False nonstaff_user.save() return nonstaff_user, password
),
dti = DatetimeIndex(start=datetime(2005, 1, 1), end=datetime(2005, 1, 10), freq='D') ser = Series(np.random.rand(len(dti)), dti)
for st in (4, 7, 1000): clear_counties() lm.save(step=st, strict=True) self.county_helper(county_feat=False)
from sklearn.externals.six.moves import cStringIO as StringIO import sys
from __future__ import absolute_import import logging import uuid import re
clf = MockClassifier() grid_search = GridSearchCV(clf, {'foo_param': [1]}) grid_search.fit(X, y) assert_true(hasattr(grid_search, "grid_scores_"))
self.assertRaises(KeyError, self.frame.ix.__setitem__, ['foo', 'bar', 'baz'], 1) self.assertRaises(KeyError, self.frame.ix.__setitem__, (slice(None, None), ['E']), 1)
dpkg.__grains__ = {} dpkg.__salt__ = {} dpkg.__context__ = {}
response = self.session.put( "{}/xblock/{}".format(STUDIO_BASE_URL, locator), data=json.dumps(data), headers=self.headers, )
if not isinstance(values, type(self)): try: values = type(self)(values) except ValueError: return self.asobject.isin(values) return algorithms.isin(self.asi8, values.asi8)
return None, mods, errors
from __future__ import absolute_import
p.mugshot.save("mug", self.file2) self.check_dimensions(p, 8, 4)
user1_new_email = "valid_user1_email@example.com" user2_new_email = "valid_user2_email@example.com"
tempfile = NamedTemporaryFile() self.assertTrue(hasattr(tempfile, "closed")) self.assertFalse(tempfile.closed) tempfile.close() self.assertTrue(tempfile.closed)
self.page.q(css=input_selector).fill(time) self.page.q(css=input_selector).results[0].send_keys(Keys.ENTER)
actual_decorator = user_passes_test( lambda u: u.is_authenticated, login_url=login_url, redirect_field_name=redirect_field_name ) if function: return actual_decorator(function) return actual_decorator
if success: status = 200 response = {self.UID_FIELD: self.social_uid} if email: response.update({'email': email}) body = json.dumps(response) else: status = 400 body = json.dumps({}) self._setup_provider_response_with_body(status, body)
return self.encode(inputs)
users = [UserFactory() for _ in range(5)] cohort = CohortFactory(course_id=self.course.id, users=users) response = self.request_users_in_cohort(cohort, self.course, 0) self.verify_users_in_cohort_and_response( cohort, response, expected_users=[], expected_page=0, expected_num_pages=1 ) response = self.request_users_in_cohort(cohort, self.course, 2) self.verify_users_in_cohort_and_response( cohort, response, expected_users=[], expected_page=2, expected_num_pages=1 )
return transform(X)
batch = _undo_op(batch, 'Cast') if self.sparse != is_sparse(batch): if space.sparse: batch = _undo_op(batch, 'SparseFromDense', strict=True) elif isinstance(batch, theano.sparse.SparseVariable): batch = _undo_op(batch, 'DenseFromSparse', strict=True) else: assert False, ("Unplanned-for branch in if-elif " "chain. This is a bug in the code.")
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.approve()
if name == 'cybersource_config_key': return 'test_microsite' else: return None
SIMPLE_CHILDREN_MAP = [[1, 2], [3, 4], [], [], []]
AsyncChannel._config_resolver()
context = self.mod.student_view({}).content for key in ['display_name', 'tag', 'source', 'instructions_html', 'content_html', 'annotation_storage', 'token', 'diacritic_marks', 'default_tab', 'annotation_mode', 'is_course_staff']: self.assertIn(key, context)
import tornado.testing import tornado.gen from tornado.testing import AsyncTestCase
stripped, ccx = strip_ccx(to_strip) yield stripped, partial(restore_ccx_collection, ccx_id=ccx)
[n.raise_error() for n in self.node_map.values() if isinstance(n, DummyNode)]
course_fixture.add_children( XBlockFixtureDesc('chapter', 'Test Section').add_children( XBlockFixtureDesc('sequential', 'Test Subsection').add_children( vertical ) ) ) course_fixture.install()
scripts.extend(['salt = salt.scripts:salt_main', 'salt-api = salt.scripts:salt_api', 'salt-cloud = salt.scripts:salt_cloud', 'salt-cp = salt.scripts:salt_cp', 'salt-key = salt.scripts:salt_key', 'salt-master = salt.scripts:salt_master', 'salt-minion = salt.scripts:salt_minion', 'salt-ssh = salt.scripts:salt_ssh', 'salt-syndic = salt.scripts:salt_syndic', 'salt-unity = salt.scripts:salt_unity', 'spm = salt.scripts:salt_spm']) return {'console_scripts': scripts}
super(RandomizeModuleTestCase, self).setUp() self.system = DummyImportSystem(load_error_modules=True) self.system.seed = None self.course = self.get_dummy_course() self.modulestore = self.system.modulestore
path = self._check_proto(path).rstrip('/') separated = path.rsplit('/', 1) if len(separated) != 2: prefix = '' else: prefix = separated[0]
from salt.states import group
return self.q(css='section.banner-user')
df = DataFrame(dict(A=lrange(5), B=lrange(5))) df.to_hdf(path, 'table', append=True) result = read_hdf(path, 'table', where=['index>2']) assert_frame_equal(df[df.index > 2], result)
deploy = config.get_cloud_config_value( 'deploy', vm_, __opts__, default=True ) win_password = config.get_cloud_config_value( 'win_password', vm_, __opts__, default='' ) key_filename = config.get_cloud_config_value( 'private_key', vm_, __opts__, search_global=False, default=None ) if deploy or (deploy and win_password == 'auto'): if key_filename is None: raise SaltCloudSystemExit( 'The required \'private_key\' configuration setting is missing from the ' '\'ec2\' driver.' )
with tm.assert_produces_warning(FutureWarning, check_stacklevel=False): result = self.series.resample('H')[0] expected = self.series.resample('H').mean()[0] self.assertEqual(result, expected)
result = ExampleCertificateSet.latest_status(self.COURSE_KEY) self.assertIs(result, None)
self.minion.process_manager.stop_restarting() self.minion.process_manager.send_signal_to_processes(signum) self.minion.process_manager.kill_children() super(Minion, self)._handle_signals(signum, sigframe)
course.position = 2 course.save()
s2 = s.copy()
if query_interval is not None: command.QUERY_INTERVAL = query_interval
if local_file and method == 'GET': log.debug('Saving to local file: {0}'.format(local_file)) with salt.utils.fopen(local_file, 'wb') as out: for chunk in result.iter_content(chunk_size=chunk_size): out.write(chunk) return 'Saved to local file: {0}'.format(local_file)
if not isinstance(self.input_space, Conv2DSpace): raise NotImplementedError() desired = self.W.get_value().T ipt = self.desired_space.format_as(desired, self.input_space) rval = Conv2DSpace.convert_numpy(ipt, self.input_space.axes, ('b', 0, 1, 'c')) return rval
run_all(name, 'sh -c \'if [ -f "{0}" ];then rm -f "{0}";fi\''.format(script), path=path, python_shell=True) if result['retcode'] != 0: error = ('Unable to write to /etc/resolv.conf in container \'{0}\'' .format(name)) if result['stderr']: error += ': {0}'.format(result['stderr']) raise CommandExecutionError(error) return True
X_sparse_pruned = sparse.csr_matrix(X_dense)
if not getattr(settings, 'CORS_ALLOW_INSECURE', False): if not request.is_secure(): log.debug( u"Request is not secure, so we cannot send the CSRF token. " u"For testing purposes, you can disable this check by setting " u"`CORS_ALLOW_INSECURE` to True in the settings" ) return False
return self.default is not NOT_PROVIDED
response = requests.Response() response.status_code = 400 return response
sorted_kwarg = sorted(six.iteritems(kwargs), key=operator.itemgetter(0)) return {"result": "inclusion_unlimited_args_kwargs - Expected result: %s / %s" % ( ', '.join(six.text_type(arg) for arg in [one, two] + list(args)), ', '.join('%s=%s' % (k, v) for (k, v) in sorted_kwarg) )}
STATICFILES_FINDERS = [ 'openedx.core.djangoapps.theming.finders.ComprehensiveThemeFinder', 'django.contrib.staticfiles.finders.FileSystemFinder', 'django.contrib.staticfiles.finders.AppDirectoriesFinder', 'openedx.core.lib.xblock_pipeline.finder.XBlockPipelineFinder', 'pipeline.finders.PipelineFinder', ]
X = np.array([[0.6, 0.8, 0.7]]) y = np.array([0]) mlp = MLPClassifier(algorithm='sgd', learning_rate_init=0.1, alpha=0.1, activation='logistic', random_state=1, max_iter=1, hidden_layer_sizes=2, momentum=0) mlp.coefs_ = [0] * 2 mlp.intercepts_ = [0] * 2 mlp.classes_ = [0, 1] mlp.n_outputs_ = 1 mlp.coefs_[0] = np.array([[0.1, 0.2], [0.3, 0.1], [0.5, 0]]) mlp.coefs_[1] = np.array([[0.1], [0.2]]) mlp.intercepts_[0] = np.array([0.1, 0.1]) mlp.intercepts_[1] = np.array([1.0]) mlp._coef_grads = [] * 2 mlp._intercept_grads = [] * 2
with warnings.catch_warnings(record=True) as warns: warnings.simplefilter('always') self.assertFalse(u.is_anonymous()) self.assertEqual(len(warns), 1) self.assertEqual(str(warns[0].message), deprecation_message)
if isinstance(values, (list, tuple)): values = np.array(values, dtype=np.object_) if not hasattr(values, 'dtype'): values = np.array([values], dtype=np.object_)
shutil.rmtree(TMP_DIR)
upload_start_time = datetime.utcnow().replace(microsecond=0, second=0) self.import_page.upload_tarball(self.tarball_name) self.import_page.wait_for_upload()
self.user.profile.profile_image_uploaded_at = TEST_PROFILE_IMAGE_UPLOAD_DT self.user.profile.save() expected_name = hashlib.md5('secret' + self.user.username).hexdigest() actual_urls = get_profile_image_urls_for_user(self.user) self.verify_urls(actual_urls, expected_name, is_default=False) self.user.profile.profile_image_uploaded_at = None self.user.profile.save() self.verify_urls(get_profile_image_urls_for_user(self.user), 'default', is_default=True)
self.assertEquals(self.middleware.process_request(self.request), None)
X = np.arange(200, dtype=np.float64).reshape(10, -1) X[2, :] = np.nan y = np.repeat([0, 1], X.shape[0] / 2) cval.train_test_split(X, y, test_size=0.2, random_state=42)
self.currency = settings.PAID_COURSE_REGISTRATION_CURRENCY[1] self.logo_path = microsite.get_value("PDF_RECEIPT_LOGO_PATH", settings.PDF_RECEIPT_LOGO_PATH) self.cobrand_logo_path = microsite.get_value( "PDF_RECEIPT_COBRAND_LOGO_PATH", settings.PDF_RECEIPT_COBRAND_LOGO_PATH ) self.tax_label = microsite.get_value("PDF_RECEIPT_TAX_ID_LABEL", settings.PDF_RECEIPT_TAX_ID_LABEL) self.tax_id = microsite.get_value("PDF_RECEIPT_TAX_ID", settings.PDF_RECEIPT_TAX_ID) self.footer_text = microsite.get_value("PDF_RECEIPT_FOOTER_TEXT", settings.PDF_RECEIPT_FOOTER_TEXT) self.disclaimer_text = microsite.get_value("PDF_RECEIPT_DISCLAIMER_TEXT", settings.PDF_RECEIPT_DISCLAIMER_TEXT) self.billing_address_text = microsite.get_value( "PDF_RECEIPT_BILLING_ADDRESS", settings.PDF_RECEIPT_BILLING_ADDRESS ) self.terms_conditions_text = microsite.get_value( "PDF_RECEIPT_TERMS_AND_CONDITIONS", settings.PDF_RECEIPT_TERMS_AND_CONDITIONS ) self.brand_logo_height = microsite.get_value( "PDF_RECEIPT_LOGO_HEIGHT_MM", settings.PDF_RECEIPT_LOGO_HEIGHT_MM ) * mm self.cobrand_logo_height = microsite.get_value( "PDF_RECEIPT_COBRAND_LOGO_HEIGHT_MM", settings.PDF_RECEIPT_COBRAND_LOGO_HEIGHT_MM ) * mm
import salt.ext.six as six
from __future__ import unicode_literals
if 'permutation' not in event_info: event_info['permutation'] = {} event_info['permutation'][response.answer_id] = (permutation_option, response.unmask_order())
summary += line.rstrip() + space2 + '...' line = space2
if issubclass(values.dtype.type, compat.string_types): values = np.array(values, dtype=object, copy=True)
if 'service.disable' not in __salt__ or 'service.disabled' not in __salt__: if started is True: ret['comment'] = ('Disable is not available on this minion,' ' service {0} started').format(name) elif started is None: ret['comment'] = ('Disable is not available on this minion,' ' service {0} is in the desired state' ).format(name) else: ret['comment'] = ('Disable is not available on this minion,' ' service {0} is dead').format(name) return ret
os.environ['SALT_MASTER_CONFIG'] = env_fpath config = sconfig.master_config(fpath) self.assertEqual(config['log_file'], fpath) os.environ.clear() os.environ.update(original_environ)
password = generate_unique_password([], 12) self.assertEquals(len(password), 12) for letter in password: self.assertNotIn(letter, 'aAeEiIoOuU1l')
self.assertFalse(res_json['success']) self.assertEqual( res_json['message'], u'Invalid data, generate_for must be "new" or "all".' )
if hasattr(zmq, 'RECONNECT_IVL_MAX'): self.socket.setsockopt( zmq.RECONNECT_IVL_MAX, 5000 )
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
self._cache.delete(key)
if not os.path.exists(dest_dir): os.makedirs(dest_dir)
from __future__ import unicode_literals
self.q(css='.wrapper-create-library .new-library-save').click()
is_transposed = False if hasattr(other, 'ndim') and hasattr(values, 'ndim'): if values.ndim != other.ndim: is_transposed = True else: if values.shape == other.shape[::-1]: is_transposed = True elif values.shape[0] == other.shape[-1]: is_transposed = True else: raise ValueError("cannot broadcast shape [%s] with block " "values [%s]" % (values.T.shape, other.shape))
problem = self.build_problem(sample_dict=sample_dict, num_samples=10, tolerance=0.01, answer="x+2*y", hints=hints)
s = Series([np.nan, np.nan, 5, 7, 9, np.nan])
if response and not (200 <= response.status_code < 300): if_none_match = None if_match = None
mock = MagicMock(return_value={'retcode': 0, 'stdout': ''}) with patch.dict(pip.__salt__, {'cmd.run_all': mock}): pip.install(requirements=requirements) mock.assert_called_once_with( expected, saltenv='base', runas=None, use_vt=False, python_shell=False, )
return self.find_css('.signatory-panel-body .signatory-title-value').first.text[0]
return False
from salt.states import reg
'text/plain': json.loads
return files, symlinks
return cls.objects.get_or_create( course_id=course_id, group_type=group_type, name=name )
if values is not None: course_ids = list(set(course_ids) & set(values))
result = df.iloc[4:8] expected = df.ix[8:14] assert_frame_equal(result, expected)
return "'%s'%s" % ( self.connection.alias, (" ('%s')" % database_name) if verbosity >= 2 else '', )
nona = self.series.dropna() self.assertEqual(nona[nona.idxmin()], nona.min()) self.assertEqual(nona.index.values.tolist().index(nona.idxmin()), nona.values.argmin())
for _ in range(num_students): random_id = uuid4().hex[:8] self.create_student(username='student{0}'.format(random_id))
if deploy_d_from_conf_file not in deploy_scripts_search_paths: deploy_scripts_search_paths.append( (deploy_d_from_conf_file, True) ) if deploy_d_from_syspaths not in deploy_scripts_search_paths: deploy_scripts_search_paths.append( (deploy_d_from_syspaths, True) )
curried.__name__ = curried_with_axis.__name__ = name
self.assertTrue(filecmp.cmp(path_in, path_test))
dependency_dict = defaultdict(lambda: defaultdict(set))
return ''
versions[BRANCH_NAME_PUBLISHED] = versions[BRANCH_NAME_DRAFT] modulestore().update_course_index(None, course_info) course = modulestore().get_course(locator.for_branch(BRANCH_NAME_PUBLISHED)) self.assertEqual(course.location.version_guid, versions[BRANCH_NAME_DRAFT])
from courseware.views.views import render_xblock return render_xblock(request, unicode(usage_key), check_if_enrolled=False)
app.connect('env-before-read-docs', add_documenter)
import os import logging import json
text = f.widget.format_value(result) self.assertEqual(text, "01:30:05 PM")
os.kill(initial_pid, signal.SIGKILL) time.sleep(0.1) process_manager.check_children() try: assert initial_pid != next(six.iterkeys(process_manager._process_map)) finally: process_manager.stop_restarting() process_manager.kill_children() time.sleep(0.5) if process_manager._process_map.keys(): process_manager.send_signal_to_processes(signal.SIGILL) process_manager.stop_restarting() process_manager.kill_children()
order_item = OrderItem.objects.get(order=registration_code_redemption.registration_code.order, courseregcodeitem__course_id=course_id) coupon_redemption = CouponRedemption.objects.select_related('coupon').filter( order_id=registration_code_redemption.registration_code.order) coupon_codes = [redemption.coupon.code for redemption in coupon_redemption] coupon_codes = ", ".join(coupon_codes) list_price = order_item.get_list_price() payment_amount = order_item.unit_cost coupon_codes_used = coupon_codes payment_status = order_item.status transaction_reference_number = order_item.order_id return list_price, payment_amount, coupon_codes_used, payment_status, transaction_reference_number
s = Series([-1, 0, 1])
def f(): cat.reorder_categories(["a", "b", "d"])
expected = { "action": "enroll", 'auto_enroll': False, "results": [ { "identifier": 'percivaloctavius@', "invalidIdentifier": True, } ] }
pv = PatchViewer(grid_shape, vis_chains.shape[1:3], is_color=vis_chains.shape[-1] == 3) for i in xrange(m): pv.add_patch(vis_chains[i, :], rescale=False) return pv
try: import sqlite3 value = sqlite3.adapt(value) except ImportError: pass except sqlite3.ProgrammingError: pass if isinstance(value, type(True)): return str(int(value)) elif isinstance(value, (Decimal, float)): return str(value) elif isinstance(value, six.integer_types): return str(value) elif isinstance(value, six.string_types): return "'%s'" % six.text_type(value).replace("\'", "\'\'") elif value is None: return "NULL" elif isinstance(value, (bytes, bytearray, six.memoryview)): value = bytes(value) hex_encoder = codecs.getencoder('hex_codec') value_hex, _length = hex_encoder(value) return "X'%s'" % value_hex.decode('ascii') else: raise ValueError("Cannot quote parameter value %r of type %s" % (value, type(value)))
message = ( "Calling modelformset_factory without defining 'fields' or 'exclude' " "explicitly is prohibited." ) with self.assertRaisesMessage(ImproperlyConfigured, message): modelformset_factory(Author)
self.client.force_login(self.changeuser) response = self.client.get(article_changelist_url) self.assertEqual(response.status_code, 200) response = self.client.get(article_change_url) self.assertEqual(response.status_code, 200) post = self.client.post(article_change_url, change_dict) self.assertRedirects(post, article_changelist_url) self.assertEqual(Article.objects.get(pk=self.a1.pk).content, '<p>edited article</p>')
autocommits_when_autocommit_is_off = False
if len(names) == 1: ret = {names[0]: ret} origins = __context__.get('pkg.origin', {}) return dict([ (x, {'origin': origins.get(x, ''), 'version': y}) for x, y in six.iteritems(ret) ])
if len(ndims) > 1: current_column = 0 max_ndim = sample.ndim self.objs, objs = [], self.objs for obj in objs:
addr = 'viewx3dtextx26qx3d@yahoo.comx26latlngx3d15854521645943074058' self.assertEqual(addr, f.clean(addr))
assert_warns_message(DeprecationWarning, "class_weight='auto' heuristic is deprecated", model.fit, X, y)
from salt.utils import configcomparer
while (svd_restarts < max_svd_restarts) and not has_converged:
if isinstance(err, DataFrame):
context.update(configuration.get(user_certificate.mode, {}))
continue
self.assertChildNodeContent(chan, { 'title': 'My blog', 'link': 'http://example.com/blog/', }) self.assertCategories(chan, ['python', 'django'])
return True
messages = { 'missing_keys': _('Some keys were missing: %(keys)s'), 'extra_keys': _('Some unknown keys were provided: %(keys)s'), } strict = False def __init__(self, keys, strict=False, messages=None): self.keys = set(keys) self.strict = strict if messages is not None: self.messages = copy.copy(self.messages) self.messages.update(messages) def __call__(self, value): keys = set(value.keys()) missing_keys = self.keys - keys if missing_keys: raise ValidationError( self.messages['missing_keys'], code='missing_keys', params={'keys': ', '.join(missing_keys)}, ) if self.strict: extra_keys = keys - self.keys if extra_keys: raise ValidationError( self.messages['extra_keys'], code='extra_keys', params={'keys': ', '.join(extra_keys)}, ) def __eq__(self, other): return ( isinstance(other, self.__class__) and self.keys == other.keys and self.messages == other.messages and self.strict == other.strict ) def __ne__(self, other): return not (self == other)
self.store.delete_item(component.location, self.user_id) vertical = self.store.get_item(vertical.location) self.assertTrue(self._has_changes(vertical.location))
return self.q(css=TOPIC_CARD_CSS).results
self.descriptor.youtube_id_0_75 = 'izygArpw-Qo' self.descriptor.youtube_id_1_0 = 'p2Q6BrNhdh8' self.descriptor.youtube_id_1_25 = '1EeWXzPdhSA' self.descriptor.youtube_id_1_5 = 'rABDYkeK0x8' expected = "0.75:izygArpw-Qo,1.00:p2Q6BrNhdh8,1.25:1EeWXzPdhSA,1.50:rABDYkeK0x8" self.assertEqual(create_youtube_string(self.descriptor), expected)
_ = self.capa_system.i18n.ugettext submitted_msg = _("Your answer has been submitted. As soon as your submission is" " graded, this message will be replaced with the grader's feedback.") self.submitted_msg = submitted_msg self.setup_code_response_rendering()
return cls(entry, module)
df = DataFrame({'A': [1, 2], 'B': [3, 4]}) red_rgba = [1.0, 0.0, 0.0, 1.0] green_rgba = [0.0, 1.0, 0.0, 1.0] rgba_array = np.array([red_rgba, green_rgba]) ax = df.plot.scatter(x='A', y='B', c=rgba_array) self.assertTrue(np.array_equal(ax.collections[0].get_facecolor(), rgba_array)) float_array = np.array([0.0, 1.0]) df.plot.scatter(x='A', y='B', c=float_array, cmap='spring')
self.first_checkpoint.add_verification_attempt(SoftwareSecurePhotoVerification.objects.create(user=self.user))
_tzname_re = re.compile(r'^[\w/:+-]+$')
elif revision == ModuleStoreEnum.RevisionOption.draft_only: return get_draft()
result = p.select(lambda x: x in ('ItemA', 'ItemC'), axis='items') expected = p.reindex(items=['ItemA', 'ItemC']) self.assert_panel_equal(result, expected)
self._dtype = super(SimplyTypedSpace, self)._clean_dtype_arg(new_dtype)
raise SyntaxError('encoding problem: utf-8')
result = sparse[orig % 2 == 1] exp = orig[orig % 2 == 1].to_sparse(fill_value=0) tm.assert_sp_series_equal(result, exp)
response = self.get_response() list(storage) storage.update(response) session_storing = self.stored_session_messages_count(storage, response) self.assertEqual(session_storing, 0)
raise NotImplementedError()
key1 = ['bar', 'bar', 'bar', 'foo', 'foo', 'baz', 'baz', 'qux', 'qux', 'snap'] key2 = ['two', 'one', 'three', 'one', 'two', 'one', 'two', 'two', 'three', 'one']
course = CourseFactory.create( org='cstX', number='cst_22', display_name='custom template course' ) self._add_course_certificates(count=1, signatory_count=2) self._create_custom_template(org_id=1, mode='honor') self._create_custom_template(org_id=1, mode='honor', course_key=course.id) test_url = get_certificate_url( user_id=self.user.id, course_id=unicode(self.course.id) ) with patch('certificates.api.get_course_organizations') as mock_get_orgs: mock_get_orgs.side_effect = [ [{"id": 1, "name": "organization name"}], ] response = self.client.get(test_url) self.assertEqual(response.status_code, 200) self.assertContains(response, 'course name: course_title_0')
try: beacon_module = __import__('salt.beacons.' + name, fromlist=['validate']) log.debug('Successfully imported beacon.') except ImportError: ret['comment'] = 'Beacon {0} does not exist'.format(name) return ret
res = df.loc[['a', 'a', 'b']] exp = DataFrame({'A': [1, 3, 1, 3, 2], 'B': [5, 7, 5, 7, 6]}, index=pd.CategoricalIndex(['a', 'a', 'a', 'a', 'b'], categories=list('abcde'))) tm.assert_frame_equal(res, exp, check_index_type=True)
with self.assertRaises(ImproperlyConfigured): self.client.get('/template/no_template/')
text = 'a,b,c\n1,"test \r\n",3\n' df = pd.read_csv(StringIO(text)) buf = StringIO() df.to_csv(buf, encoding='utf-8', index=False) self.assertEqual(buf.getvalue(), text)
'dark_lang',
s = string_at(result) free(result) return s
if draft is None or not draft: with self.store.branch_setting(ModuleStoreEnum.Branch.published_only): parent_block = self.store.get_item( parent_usage_key, ) self.assertIn(child_usage_key, parent_block.children) if draft is None or draft: with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred): parent_block = self.store.get_item( parent_usage_key, ) self.assertIn(child_usage_key, parent_block.children)
if sir_id is not None: params = {'Action': 'CancelSpotInstanceRequests', 'SpotInstanceRequestId.1': sir_id} result = aws.query(params, location=location, provider=provider, opts=__opts__, sigver='4') ret['spotInstance'] = result[0]
from __future__ import absolute_import
self.assertEqual(errors[0].id, 'postgres.E001') self.assertIn('max_length', errors[0].msg)
expected_event = { 'username': username, 'event': { 'setting': setting, 'user_id': int(user_id), 'table': 'user_api_userpreference', 'truncated': [] } } expected_event['event'].update(kwargs) event_filter = { 'event_type': self.USER_SETTINGS_CHANGED_EVENT_NAME, 'username': username, } with self.assert_events_match_during(event_filter=event_filter, expected_events=[expected_event]): yield
pnt_wo_srid = Point(1, 1) pnt_wo_srid.srid = pnt_wo_srid.srid
salt.utils.fopen(os.path.join(name, '36'), 'w').close() os.makedirs(os.path.join(name, 'scene33')) ret = self.run_state( 'file.recurse', name=name, source='salt://grail', clean=True) try: self.assertSaltTrueReturn(ret) self.assertFalse(os.path.exists(strayfile)) self.assertTrue(os.path.isfile(os.path.join(name, '36', 'scene'))) self.assertTrue(os.path.isfile(os.path.join(name, 'scene33'))) finally: shutil.rmtree(name, ignore_errors=True)
self.assertEqual(Article.objects.latest('expire_date'), a1) self.assertEqual( Article.objects.filter(pub_date__gt=datetime(2005, 7, 26)).latest('expire_date'), a3, )
name = os.path.join(integration.TMP, 'issue_27401', fname) try: ret = self.run_state( 'file.prepend', name=name, text='cheese', makedirs=True ) self.assertSaltTrueReturn(ret) finally: if os.path.isfile(name): os.remove(name)
(osfullname, _) = osinfo.Name.split('|', 1) osfullname = osfullname.strip()
cached_pkg = cached_pkg.replace('/', '\\') cache_path, _ = os.path.split(cached_pkg)
os_family = __grains__['os_family'] if os_family == 'RedHat': return 'apachectl' elif os_family == 'Debian' or os_family == 'SUSE': return 'apache2ctl' else: return 'apachectl'
link = CourseUserGroupPartitionGroup( course_user_group=cohort, partition_id=partition_id, group_id=group_id, ) link.save() return link
if not self.obj.columns.is_unique and engine == 'python': raise NotImplementedError("columns.is_unique == False not " "supported with engine='python'")
import logging
if start_date is None: start_date = datetime.datetime(1970, 1, 1)
self.panel['ItemQ'] = 'foo' self.assertEqual(self.panel['ItemQ'].values.dtype, np.object_)
courses_list_by_groups, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(courses_list_by_groups, [])
self.wait_for( lambda: self.team_capacity_text == self.format_capacity_text(num_members, max_size), description="Team capacity text is not correct" )
pnt = Point(0, 0) nullcity = City(name='NullCity', point=pnt) nullcity.save()
assert_raises(ValueError, check_symmetric, arr_bad)
if ( self.chapter.url_name != self.original_chapter_url_name or (self.original_section_url_name and self.section.url_name != self.original_section_url_name) ): raise Redirect( reverse( 'courseware_section', kwargs={ 'course_id': unicode(self.course_key), 'chapter': self.chapter.url_name, 'section': self.section.url_name, }, ) )
key = safe_key('key', 'prefix', 'a' * 300) self.assertTrue(self._is_valid_key(key))
ret['changes']['summary'] = _summary(result['stdout']) ret['result'] = True if not __opts__['test'] else None
import numpy as np
self.assertEqual(cl.result_count, 1)
import logging
#------------------------------------------------------------------------------
if before_toggle_disable_status: if started is True: ret['comment'] = ('Service {0} is already disabled,' ' and is running').format(name) elif started is None: ret['changes'] = {} ret['comment'] = ('Service {0} is already disabled,' ' and is in the desired state').format(name) else: ret['comment'] = ('Service {0} is already disabled,' ' and is dead').format(name) return ret
index = PeriodIndex(['NaT', '2011-01', '2011-02'], freq='M', name='idx')
import pyrax import pyrax.exceptions
from salt.states import mac_assistive as assistive
expr = stripXML(self.mathml_start + expr + self.mathml_end) expected = stripXML(self.mathml_start + expected + self.mathml_end)
return False
dummy_v = T.tensor4() dummy_v.name = 'dummy_v'
if timezone.is_aware(value): raise ValueError("MySQL backend does not support timezone-aware times.")
if __opts__['test'] is True:
ret = self.run_function( 'mysql.db_remove', name=dbname, connection_user=self.user, connection_pass=self.password ) self.assertEqual(True, ret)
raise NotImplementedError
position = get_index(unicode(item.location), section.children) + 1 item_dict['url'] = reverse('courseware_position', kwargs={ 'course_id': unicode(course.id), 'chapter': chapter.url_name, 'section': section.url_name, 'position': position, })
class MyEmailBackend(smtp.EmailBackend): def __init__(self, *args, **kwargs): kwargs.setdefault('timeout', 42) super(MyEmailBackend, self).__init__(*args, **kwargs) myemailbackend = MyEmailBackend() myemailbackend.open() self.assertEqual(myemailbackend.timeout, 42) self.assertEqual(myemailbackend.connection.timeout, 42) myemailbackend.close()
reindexed_int = int_ts.reindex(int_ts.index[::2]) self.assertEqual(reindexed_int.dtype, np.int_)
if self.uses_datetime_field: value = datetime.datetime.combine(value, datetime.time.min) if settings.USE_TZ: value = timezone.make_aware(value, timezone.get_current_timezone()) return value
def obj_func(theta, eval_gradient=True): if eval_gradient: lml, grad = self.log_marginal_likelihood( theta, eval_gradient=True) return -lml, -grad else: return -self.log_marginal_likelihood(theta)
self.cart = Order.get_cart_for_user(self.first_refund_user) CertificateItem.add_to_order(self.cart, self.course_key, self.cost, 'verified') self.cart.purchase() CourseEnrollment.unenroll(self.first_refund_user, self.course_key)
retval = self.service.set_credit_requirement_status( self.user.id, self.course.id, 'grade', 'grade' ) self.assertIsNone(retval)
kwargs.setdefault('widget', super(IntegerField, self).widget)
settingsfile = upath(sys.modules[settings.__module__].__file__) localedir = os.path.join(os.path.dirname(settingsfile), 'locale') translation = self._new_gnu_trans(localedir) self.merge(translation)
from ioflo.aid.odicting import odict
values = self.values mask = isnull(values)
print("There are objects in the old test database which prevent its destruction.") print("If they belong to the test user, deleting the user will allow the test " "database to be recreated.") print("Otherwise, you will need to find and remove each of these objects, " "or use a different tablespace.\n") if self._test_user_create(): if not autoclobber: confirm = input("Type 'yes' to delete user %s: " % parameters['user']) if autoclobber or confirm == 'yes': try: if verbosity >= 1: print("Destroying old test user...") self._destroy_test_user(cursor, parameters, verbosity) except Exception as e: sys.stderr.write("Got an error destroying the test user: %s\n" % e) sys.exit(2) try: if verbosity >= 1: print("Destroying old test database for alias '%s'..." % self.connection.alias) self._execute_test_db_destruction(cursor, parameters, verbosity) except Exception as e: sys.stderr.write("Got an error destroying the test database: %s\n" % e) sys.exit(2) else: print("Tests cancelled -- test database cannot be recreated.") sys.exit(1) else: print("Django is configured to use pre-existing test user '%s'," " and will not attempt to delete it.\n" % parameters['user']) print("Tests cancelled -- test database cannot be recreated.") sys.exit(1)
if self_referential: seen_self = sum( from_model == getattr(field.remote_field, 'model', None) for field in self.remote_field.through._meta.fields )
env: /path/to/virtualenv/
ItemFactory.create( parent=parent, category='discussion', display_name='released', start=self.now, )
if field_type == 'CharField' and row[3]: field_params['max_length'] = int(row[3])
assert_true(abs(len(l_test_unique) - round(test_size * len(l_unique))) <= 1) assert_true(abs(len(l_train_unique) - round((1.0 - test_size) * len(l_unique))) <= 1)
try: Book.objects.using('other').get(title="Dive into Python") except Book.DoesNotExist: self.fail('"Dive into Python" should exist on other database')
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
with self._mock_geoip('US'): result = embargo_api.check_course_access(self.course.id, user=self.user, ip_address='0.0.0.0')
if LooseVersion(zabbix_version) > LooseVersion("2.5"): if not groupid: groupid = None if not name: name = None ret = hostgroup_get(name, groupid, **connection_args) return bool(ret) else: params = {} method = 'hostgroup.exists' if groupid: params['groupid'] = groupid if name: params['name'] = name if LooseVersion(zabbix_version) < LooseVersion("2.4"): if node: params['node'] = node if nodeids: params['nodeids'] = nodeids if not groupid and not name and not node and not nodeids: return {'result': False, 'comment': 'Please submit groupid, name, node or nodeids parameter to' 'check if at least one host group that matches the given filter' ' criteria exists.'} ret = _query(method, params, conn_args['url'], conn_args['auth']) return ret['result']
monitoring_dataset = DenseDesignMatrix(X=X)
sku = models.CharField( max_length=255, null=True, blank=True, verbose_name="SKU", help_text=_( u"OPTIONAL: This is the SKU (stock keeping unit) of this mode in the external ecommerce service. " u"Leave this blank if the course has not yet been migrated to the ecommerce service." ) )
request_info = xmodule_instance_args.get('request_info', {}) if xmodule_instance_args is not None else {} task_info = {"student": student.username, "task_id": _get_task_id_from_xmodule_args(xmodule_instance_args)}
return start(name)
import salt.utils from salt.exceptions import CommandExecutionError
staff_user = UserFactory(username='test_staff_user', email='test_staff_user@openedx.org', password='test') CourseStaffRole(self.master_course_key).add_users(staff_user)
self.mean_square_grads[param.name] = mean_square_grad
result = p.ix[:, -1, :] expected = p.ix[:, p.major_axis[-1], :] assert_frame_equal(result, expected)
if sys.stderr.isatty(): output = sys.stderr _makepretty(output, stack) else: filename = 'salt-debug-{0}.log'.format(int(time.time())) destfile = os.path.join(tempfile.gettempdir(), filename) with salt.utils.fopen(destfile, 'w') as output: _makepretty(output, stack)
import dateutil yearfirst = dateutil.__version__ >= LooseVersion('2.5.0')
log = logging.getLogger(__name__)
from __future__ import absolute_import
visible_layer = BinaryVector(nvis) hidden_layer = BinaryVectorMaxPool(detector_layer_dim=nhid, pool_size=1, layer_name='h', irange=0.1) rbm = DBM(100, visible_layer, [hidden_layer], 1) rbm.visible_layer.set_biases(b_vis) rbm.hidden_layers[0].set_weights(w_hid) rbm.hidden_layers[0].set_biases(b_hid) rbm.nvis = nvis rbm.nhid = nhid
kwargs_for_reverse = {key_name: unicode(key_value)} if key_name else None if kwargs: kwargs_for_reverse.update(kwargs) return reverse('contentstore.views.' + handler_name, kwargs=kwargs_for_reverse)
from __future__ import unicode_literals
with tm.assert_produces_warning(FutureWarning): df.sort_index(by='a')
ensure generic email service exists: pagerduty_service.present: - name: my email service - service: description: "email service controlled by salt" escalation_policy_id: "my escalation policy" type: "generic_email" service_key: "test-email"
if spatialite: qs = qs.exclude(name='Texas') else: qs = qs.annotate(intersection=functions.Intersection('mpoly', geom))
result = df.drop_duplicates('AAA') expected = df.iloc[[0, 1, 2, 6]] tm.assert_frame_equal(result, expected)
def custom_score(y_true, y_pred): return (((y_true == y_pred).sum() - (y_true != y_pred).sum()) / y_true.shape[0])
except (InvalidKeyError, ItemNotFoundError, User.DoesNotExist) as exception: error_str = ( "Invalid cert: error finding course %s or user with id " "%d. Specific error: %s" ) log.info(error_str, course_id, user_id, str(exception)) return render_to_response(invalid_template_path, context)
state_schedule = schedule if schedule else {} schedule = { 'minute': 0, 'hour': 0, 'day': 0, 'month': 0, 'year': 0, } for hold in state_schedule.keys(): if hold not in schedule: del state_schedule[hold] schedule.update(state_schedule) if name not in __salt__['zfs.list'](name, **{'type': 'filesystem'}) and name not in __salt__['zfs.list'](name, **{'type': 'volume'}): ret['comment'] = '{0} is not a filesystem or a volume or does not exist'.format(name) ret['result'] = False if not prefix or len(prefix) < 4: ret['comment'] = 'prefix ({0}) must be at least 4 long'.format(prefix) ret['result'] = False snap_count = 0 for hold in schedule.keys(): if not isinstance(schedule[hold], int): ret['comment'] = 'schedule values must be integers' ret['result'] = False break snap_count += schedule[hold] if ret['result'] and snap_count == 0: ret['comment'] = 'at least one snapshot must be schedule' ret['result'] = False
self.len_indices = dict() self.len_counts = dict() for ll in self.len_unique: self.len_indices[ll] = np.where(self.lengths == ll)[0] self.len_counts[ll] = len(self.len_indices[ll])
def _response(self, filepath): return self.client.get( posixpath.join(settings.STATIC_URL, filepath)) def assertFileContains(self, filepath, text): self.assertContains(self._response(filepath), text) def assertFileNotFound(self, filepath): self.assertEqual(self._response(filepath).status_code, 404)
if self.mapper[which_set] == 4: set_indices = (data['folds'][:, fold] == 1) + \ (data['folds'][:, fold] == 2) else: set_indices = data['folds'][:, fold] == self.mapper[which_set] assert set_indices.sum() > 0
found_formset = False for i, context in enumerate(contexts): if formset not in context: continue found_formset = True for err in errors: if field is not None: if field in context[formset].forms[form_index].errors: field_errors = context[formset].forms[form_index].errors[field] self.assertTrue( err in field_errors, msg_prefix + "The field '%s' on formset '%s', " "form %d in context %d does not contain the " "error '%s' (actual errors: %s)" % (field, formset, form_index, i, err, repr(field_errors)) ) elif field in context[formset].forms[form_index].fields: self.fail( msg_prefix + "The field '%s' on formset '%s', form %d in context %d contains no errors" % (field, formset, form_index, i) ) else: self.fail( msg_prefix + "The formset '%s', form %d in context %d does not contain the field '%s'" % (formset, form_index, i, field) ) elif form_index is not None: non_field_errors = context[formset].forms[form_index].non_field_errors() self.assertFalse( len(non_field_errors) == 0, msg_prefix + "The formset '%s', form %d in context %d " "does not contain any non-field errors." % (formset, form_index, i) ) self.assertTrue( err in non_field_errors, msg_prefix + "The formset '%s', form %d in context %d " "does not contain the non-field error '%s' (actual errors: %s)" % (formset, form_index, i, err, repr(non_field_errors)) ) else: non_form_errors = context[formset].non_form_errors() self.assertFalse( len(non_form_errors) == 0, msg_prefix + "The formset '%s' in context %d does not " "contain any non-form errors." % (formset, i) ) self.assertTrue( err in non_form_errors, msg_prefix + "The formset '%s' in context %d does not " "contain the non-form error '%s' (actual errors: %s)" % (formset, i, err, repr(non_form_errors)) ) if not found_formset: self.fail(msg_prefix + "The formset '%s' was not used to render the response" % formset)
return path.startswith(self.base_url[2]) and not self.base_url[1]
X = np.atleast_2d(np.random.uniform(0, 10.0, size=100)).T X = X.astype(np.float32)
name = com._maybe_match_name(self, delta)
self.check_ordering_of_field_choices([self.b1, self.b2])
course.cohort_config = {'cohorted': False} self.assertFalse(course.is_cohorted)
self.timeout_map = {}
original_function.__name__, inspect.formatargspec( *salt.utils.args.get_function_argspec(original_function) )[1:-1], inspect.formatargspec( formatvalue=lambda val: '', *salt.utils.args.get_function_argspec(original_function) )[1:-1]
for new_name, colspec in compat.iteritems(parse_spec): if new_name in data_dict: raise ValueError('Date column %s already in dict' % new_name)
self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course_key)) self.assertTrue(self.user.roles.filter(name="Student", course_id=self.course_key))
try: app_list = resolver.app_dict[ns] if current_ns and current_ns in app_list: ns = current_ns elif ns not in app_list: ns = app_list[0] except KeyError: pass
visible_serialized_account = {} for field_name in field_whitelist: visible_serialized_account[field_name] = serialized_account.get(field_name, None) return visible_serialized_account
test_field = List def test_deserialize(self): self.assertDeserializeEqual(['foo', 'bar'], '["foo", "bar"]') self.assertDeserializeEqual([3.5, 5.6], '[3.5, 5.6]') self.assertDeserializeEqual([], '[]') def test_deserialize_unsupported_types(self): self.assertDeserializeEqual('3.4', '3.4') self.assertDeserializeEqual('false', 'false') self.assertDeserializeEqual('2', '2') self.assertDeserializeNonString()
c1 = Child.objects.create(name="c1", value=42) c2 = Child.objects.create(name="c2", value=37) Leaf.objects.create(name="l1", child=c1, second_child=c2)
self.assertFalse(CourseEnrollment.is_enrolled(self.instructor, self.course.id)) CourseEnrollment.enroll(self.instructor, course_1.id) CourseEnrollment.enroll(self.instructor, course_2.id) self.test_send_to_all()
return T.dot(x, self._W.T)
problem = self.build_problem(script=script, cfn="check_func")
LAMBDAS = np.array([0.41, 0.4, 0.37, 0.44, 0.39])[:, np.newaxis] COEFS = np.array([-1854.8214151, 3516.89893646, 221.29346712, 128.12323805, -2010.49422654])[:, np.newaxis]
def __init__(self): pass
ret['Success'] = False ret['Result'] = format(error)
for X, y in datasets: X = StandardScaler().fit_transform(X) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)
salt '*' win_wua.list_updates categories=['Critical Updates','Drivers']
self.assertFalse(self.page.has_add_button) for user in self.page.users: self.assertFalse(user.can_promote) self.assertFalse(user.can_demote) self.assertFalse(user.can_delete)
self.problem.display_name = "--changed in library--" store.update_item(self.problem, self.user.id) ItemFactory.create( category="problem", parent_location=self.library.location, user_id=self.user.id, publish_item=False, )
_list = self.run_function('beacons.list', return_yaml=False) self.assertIn('ps', _list)
mixed = Series(['a', NA, 'b', True, datetime.today(), 'foo', None, 1, 2.])
class Person(Form): first_name = CharField() last_name = CharField() birthday = DateField()
SuperWeightDoubling = WeightDoubling
if six.PY2 and sys.version_info[0] == 2: py_shell_cmd = ( python3_bin + ' -c \'import sys; import json; import salt.utils.thin; ' 'print(json.dumps(salt.utils.thin.get_tops(**(json.loads(sys.argv[1]))))); exit(0);\' ' '\'{0}\''.format(json.dumps({'extra_mods': extra_mods, 'so_mods': so_mods})) ) cmd = subprocess.Popen(py_shell_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True) stdout, stderr = cmd.communicate() if cmd.returncode == 0: try: tops = json.loads(stdout) tops_py_version_mapping['3'] = tops except ValueError: pass if six.PY3 and sys.version_info[0] == 3: py_shell_cmd = ( python2_bin + ' -c \'from __future__ import print_function; ' 'import sys; import json; import salt.utils.thin; ' 'print(json.dumps(salt.utils.thin.get_tops(**(json.loads(sys.argv[1]))))); exit(0);\' ' '\'{0}\''.format(json.dumps({'extra_mods': extra_mods, 'so_mods': so_mods})) ) cmd = subprocess.Popen(py_shell_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True) stdout, stderr = cmd.communicate() if cmd.returncode == 0: try: tops = json.loads(stdout.decode('utf-8')) tops_py_version_mapping['2'] = tops except ValueError: pass
raise RuntimeError("Infinite loop detected")
if not multi_output and sparse.isspmatrix(X): if 'X_offset' in params: X_sparse_scaling = params['X_offset'] / params['X_scale'] else: X_sparse_scaling = np.zeros(n_features)
teams_per_topic = 10 topics = self.setup_topics(num_topics=self.NUM_TOPICS, teams_per_topic=teams_per_topic) self.assert_serializer_output(topics, num_teams_per_topic=teams_per_topic, num_queries=1)
def setUp(self): super(TestTeamGradeReport, self).setUp() self.course = CourseFactory.create(teams_configuration={ 'max_size': 2, 'topics': [{'topic-id': 'topic', 'name': 'Topic', 'description': 'A Topic'}] }) self.student1 = UserFactory.create() CourseEnrollment.enroll(self.student1, self.course.id) self.student2 = UserFactory.create() CourseEnrollment.enroll(self.student2, self.course.id) def test_team_in_grade_report(self): self._verify_cell_data_for_user(self.student1.username, self.course.id, 'Team Name', '') def test_correct_team_name_in_grade_report(self): team1 = CourseTeamFactory.create(course_id=self.course.id) CourseTeamMembershipFactory.create(team=team1, user=self.student1) team2 = CourseTeamFactory.create(course_id=self.course.id) CourseTeamMembershipFactory.create(team=team2, user=self.student2) self._verify_cell_data_for_user(self.student1.username, self.course.id, 'Team Name', team1.name) self._verify_cell_data_for_user(self.student2.username, self.course.id, 'Team Name', team2.name) def test_team_deleted(self): team1 = CourseTeamFactory.create(course_id=self.course.id) membership1 = CourseTeamMembershipFactory.create(team=team1, user=self.student1) team2 = CourseTeamFactory.create(course_id=self.course.id) CourseTeamMembershipFactory.create(team=team2, user=self.student2) team1.delete() membership1.delete() self._verify_cell_data_for_user(self.student1.username, self.course.id, 'Team Name', '') self._verify_cell_data_for_user(self.student2.username, self.course.id, 'Team Name', team2.name)
if has_access(user, 'load', course_descriptor): world.enroll_user(user, course_descriptor.id)
msg = "save() prohibited to prevent data loss due to unsaved related object 'place'." with self.assertNumQueries(0): with self.assertRaisesMessage(ValueError, msg): b.save()
self.course_outline_page.visit() self.course_outline_page.add_section_from_top_button() self.course_outline_page.section_at(1).set_staff_lock(True) self.course_outline_page.view_live() courseware = CoursewarePage(self.browser, self.course_id) courseware.wait_for_page() self.assertEqual(courseware.num_sections, 2) StaffPage(self.browser, self.course_id).set_staff_view_mode('Student') self.assertEqual(courseware.num_sections, 1)
validation_1 = StudioValidation("id") validation_1.set_summary(StudioValidationMessage(StudioValidationMessage.WARNING, u"Summary message")) validation_1.add(StudioValidationMessage(StudioValidationMessage.ERROR, u"Error message")) validation_2 = StudioValidation("id") validation_2.set_summary(StudioValidationMessage(StudioValidationMessage.ERROR, u"Summary 2 message")) validation_2.add(StudioValidationMessage(StudioValidationMessage.NOT_CONFIGURED, u"Not configured")) validation_1.add_messages(validation_2) self.assertEqual(2, len(validation_1.messages)) self.assertEqual(StudioValidationMessage.ERROR, validation_1.messages[0].type) self.assertEqual(u"Error message", validation_1.messages[0].text) self.assertEqual(StudioValidationMessage.NOT_CONFIGURED, validation_1.messages[1].type) self.assertEqual(u"Not configured", validation_1.messages[1].text) self.assertEqual(StudioValidationMessage.WARNING, validation_1.summary.type) self.assertEqual(u"Summary message", validation_1.summary.text)
assert_raises(ValueError, ir.fit, x, y)
raise NotImplementedError('subclasses of BaseDatabaseOperations may require a quote_name() method')
rng = np.random.RandomState(0) X = rng.rand(5, 5)
#'preamble': '',
pass
mixed = drop.tolist() + ['foo'] dropped = self.strIndex.drop(mixed, errors='ignore') expected = self.strIndex[lrange(5) + lrange(10, n)] self.assert_index_equal(dropped, expected)
self.assertContains(response, 'class="form-row field-title"') self.assertContains(response, 'class="form-row field-content"') self.assertContains(response, 'class="form-row field-public"') self.assertContains(response, 'class="form-row field-awesomeness_level"') self.assertContains(response, 'class="form-row field-coolness"') self.assertContains(response, 'class="form-row field-value"')
return self.pdf.current_page_count() == 1
if dtype == np.object_: return v.tolist()
_make_books(10, base_date=datetime.date(2011, 12, 25)) res = self.client.get('/dates/books/2011/') self.assertEqual(list(res.context['date_list']), list(sorted(res.context['date_list'])))
log = logging.getLogger(__name__)
user = UserFactory.create(is_staff=True, is_superuser=True) user.save() self.client.login(username=user.username, password='test')
return self.render_to_response(self.get_context_data())
from __future__ import absolute_import import copy
from openedx.core.djangoapps.ccxcon import tasks tasks.update_ccxcon.delay(unicode(course_key))
pass
pkgs_str = ','.join([state_name for _, state_name in target_pkgs])
if LooseVersion(version) < '0.16.0': tm.assert_frame_equal(result, expected, check_categorical=False) else: tm.assert_frame_equal(result, expected)
main.keep.saltRaetKey.delete_key(match=other.local.role) remote = main.remotes[2] path = os.path.join(main.keep.remotedirpath, "{0}.{1}.{2}".format(main.keep.prefix, remote.name, main.keep.ext)) self.assertFalse(os.path.exists(path))
user_answer = json.loads(user_answer)
from __future__ import absolute_import import logging import salt.utils.vault
self.assertTrue(gentoo_service.enabled('name')) self.assertTrue(gentoo_service.enabled('name', runlevels='default')) self.assertFalse(gentoo_service.enabled('name', runlevels='boot'))
s = Series(list('abc'), dtype='category') s2 = Series(list('abd'), dtype='category')
result = ser.copy() result.loc[sel] = 1 expected = pd.Series(1, index=index) assert_series_equal(result, expected)
try: certificate = GeneratedCertificate.eligible_certificates.get(user=student, course_id=course_id)
draft_preferred = 'draft-preferred' published_only = 'published-only'
self._check_is_numeric(batch) return space.np_format_as(batch=batch, space=self)
with pd.option_context('max_rows', None):
import integration
updated_metadata = {'entrance_exam_id': '123afsdfsad90f87'} CourseMetadata.update_from_dict( updated_metadata, self.course, self.user, ) self.course = modulestore().get_course(self.course.id) resp = self.client.get(self.exam_url) self.assertEqual(resp.status_code, 404)
for bucket_name in _get_buckets(): s3_meta = __get_s3_meta(bucket_name)
return np.fromstring(values, dtype=dtype)
x = numpy.array(test_x[i:numpy.minimum(test_x.shape[0], i + batch_size), :], dtype=floatX) batch_size0 = len(x) if len(x) < batch_size: x = numpy.concatenate((x, numpy.zeros((batch_size-len(x),x.shape[1]), dtype=floatX)), axis=0)
if file_access_rights(local_dst,os.W_OK,check_above=True):
if register_to: register_to._registry = before_import_registry
for name, lookup in gis_lookups.items(): combo_keys = [ field + name for field in [ 'rast__', 'rast__', 'rastprojected__0__', 'rast__', 'rastprojected__', 'geom__', 'rast__', ] ] if issubclass(lookup, DistanceLookupBase): combo_values = [ (rast, 50, 'spheroid'), (rast, 0, 50, 'spheroid'), (rast, 0, D(km=1)), (stx_pnt, 0, 500), (stx_pnt, D(km=1000)), (rast, 500), (json.loads(JSON_RASTER), 500), ] elif name == 'relate': combo_values = [ (rast, 'T*T***FF*'), (rast, 0, 'T*T***FF*'), (rast, 0, 'T*T***FF*'), (stx_pnt, 0, 'T*T***FF*'), (stx_pnt, 'T*T***FF*'), (rast, 'T*T***FF*'), (json.loads(JSON_RASTER), 'T*T***FF*'), ] elif name == 'isvalid': continue elif PostGISOperations.gis_operators[name].func: combo_values = [ rast, (rast, 0), (rast, 0), (stx_pnt, 0), stx_pnt, rast, rast, json.loads(JSON_RASTER) ] else: combo_keys[2] = 'rastprojected__' + name combo_values = [rast, rast, rast, stx_pnt, stx_pnt, rast, rast, json.loads(JSON_RASTER)]
sensors.__salt__ = {}
adj = _get_adjustment()
response = self._load_dashboard() self.assertContains(response, "credit-request-rejected-msg")
lw = LedoitWolf() lw.fit(X) assert_almost_equal(lw.shrinkage_, shrinkage_, 4) assert_almost_equal(lw.shrinkage_, ledoit_wolf_shrinkage(X)) assert_almost_equal(lw.shrinkage_, ledoit_wolf(X)[1]) assert_almost_equal(lw.score(X), score_, 4) lw_cov_from_mle, lw_shinkrage_from_mle = ledoit_wolf(X) assert_array_almost_equal(lw_cov_from_mle, lw.covariance_, 4) assert_almost_equal(lw_shinkrage_from_mle, lw.shrinkage_) scov = ShrunkCovariance(shrinkage=lw.shrinkage_) scov.fit(X) assert_array_almost_equal(scov.covariance_, lw.covariance_, 4)
try: interfaces = salt.utils.network.interfaces() interface_status = False if name in interfaces: interface_status = interfaces[name].get('up') else: for iface in interfaces: if 'secondary' in interfaces[iface]: for second in interfaces[iface]['secondary']: if second.get('label', '') == 'name': interface_status = True if enabled: if 'noifupdown' not in kwargs: if interface_status: if ret['changes']: __salt__['ip.down'](name, type) __salt__['ip.up'](name, type) ret['changes']['status'] = 'Interface {0} restart to validate'.format(name) return ret else: __salt__['ip.up'](name, type) ret['changes']['status'] = 'Interface {0} is up'.format(name) else: if 'noifupdown' not in kwargs: if interface_status: __salt__['ip.down'](name, type) ret['changes']['status'] = 'Interface {0} down'.format(name) except Exception as error: ret['result'] = False ret['comment'] = str(error) return ret
ret = self.process_request("get", "/some/url?query=string") self.assertEqual(ret.status_code, 301) self.assertEqual( ret["Location"], "https://testserver/some/url?query=string")
agc = AgglomerativeClustering(n_clusters=2, connectivity=connectivity) agc.fit(X) n_samples = X.shape[0] n_nodes = agc.children_.shape[0] assert_equal(n_nodes, n_samples - 1)
return course_metadata_utils.clean_course_key(self.location.course_key, padding_char)
depth = len(b_list)
return max(0, -(-(self._stop - self._start) // self._step))
result = df.to_csv(path, index=index, sep=sep, na_rep=na_rep, float_format=float_format, header=header, index_label=index_label, mode=mode, nanRep=nanRep, encoding=encoding, date_format=date_format, decimal=decimal) if path is None: return result
self.save() orderitems = OrderItem.objects.filter(order=self).select_subclasses() site_name = microsite.get_value('SITE_NAME', settings.SITE_NAME)
import warnings
execute = False if not force: if not os.path.exists(winrepo_dir): ret['result'] = False ret['comment'] = '{0} is missing'.format(winrepo_dir) return ret elif not os.path.exists(winrepo_cachefile): execute = True ret['comment'] = '{0} is missing'.format(winrepo_cachefile) else: winrepo_cachefile_mtime = os.stat(winrepo_cachefile)[stat.ST_MTIME] for root, dirs, files in os.walk(winrepo_dir): for name in itertools.chain(files, dirs): full_path = os.path.join(root, name) if os.stat(full_path)[stat.ST_MTIME] > winrepo_cachefile_mtime: ret['comment'] = 'mtime({0}) < mtime({1})'.format(winrepo_cachefile, full_path) execute = True break
self.video.hide_closed_captions() self.video.wait_for_closed_captions_to_be_hidden() self.video.reload_page() self.video.wait_for_closed_captions_to_be_hidden()
def __init__(self, alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=1000, tol=1e-4, warm_start=False, random_state=None, selection='cyclic'): self.alpha = alpha self.coef_ = None self.fit_intercept = fit_intercept self.normalize = normalize self.max_iter = max_iter self.copy_X = copy_X self.tol = tol self.warm_start = warm_start self.l1_ratio = 1.0 self.random_state = random_state self.selection = selection
pass
parent = np.arange(n_nodes, dtype=np.intp) used_node = np.ones(n_nodes, dtype=np.intp) children = []
patcher = mock.patch('terrain.stubs.xqueue.Timer') timer = patcher.start() timer.side_effect = FakeTimer self.addCleanup(patcher.stop)
from .tasks import update_search_index
html5 = Book.objects.using('other').create(title="Dive into HTML5", published=datetime.date(2010, 3, 15)) self.assertEqual( list(Person.objects.using('other').filter(edited__title='Dive into HTML5').values_list('name', flat=True)), [] )
grouper = df.floats.apply(lambda x: np.round(x, -1)) grouped = df.groupby(grouper) old_way = df[grouped.letters. transform(lambda x: len(x) < N / 10).astype('bool')] new_way = grouped.filter(lambda x: len(x.letters) < N / 10) assert_frame_equal(new_way, old_way)
self.assertTrue(Index(['a', 'b', 'c']).equals(Index(['a', 'b', 'c'])))
url = reverse('team_membership_detail', args=[self.solar_team.team_id, self.users['student_enrolled'].username]) self.make_call(url, 204, 'delete', user='student_enrolled') self.assert_event_emitted( 'edx.team.learner_removed', team_id=self.solar_team.team_id, user_id=self.users['student_enrolled'].id, remove_method='self_removal' )
def put(self, *args, **kwargs): return self.post(*args, **kwargs)
if len(block) == 0: block = [np.array([])] elif len(block) != 1: raise ValueError('Cannot create SingleBlockManager with ' 'more than 1 block') block = block[0]
results = np.array(np.where(state.marked == 1)).T
tm._skip_if_no_pytz() import pytz
with self.assertNumQueries(5): spooky = Pet.objects.prefetch_related('people__houses__rooms__fleas').get(name='Spooky') with self.assertNumQueries(0): self.assertEqual(3, len(spooky.people.all()[0].houses.all()[0].rooms.all()[0].fleas.all()))
import json
return "mobile_api.app_version_upgrade.{}.{}".format(field, key)
try: import boto import boto.ec2 logging.getLogger('boto').setLevel(logging.CRITICAL) HAS_BOTO = True except ImportError: HAS_BOTO = False
setattr(c, "parent", None)
self.assertEqual(repr(i), exp)
with TIMER.timer("insert_structure", course_context) as tagger: tagger.measure("blocks", len(structure["blocks"])) self.structures.insert(structure_to_mongo(structure, course_context))
test_item = test_func test_item.__unittest_skip__ = CheckCondition(condition)
if kwargs.get('position') is None: parent.children.append(xblock.location) else: parent.children.insert(kwargs.get('position'), xblock.location)
for tree, dataset in product(REG_TREES, ["boston", "reg_small"]): if tree in SPARSE_TREES: yield (check_sparse_input, tree, dataset, 2)
pass
import sys import nose import itertools import warnings from datetime import datetime
content = service_instance.content.propertyCollector.RetrieveContents([filter_spec])
if os.path.isdir(dest): salt.utils.rm_rf(dest) fn_ = salt.utils.fopen(dest, 'wb+')
def __init__(self, targets, coeffs, supervised=False): self.__dict__.update(locals()) del self.self self.base_cost = MF_L2_ActCost(targets=targets, coeffs=coeffs, supervised=supervised) @wraps(Cost.expr) def expr(self, model, data, return_locals=False, **kwargs): self.get_data_specs(model)[0].validate(data) return self.base_cost.expr(model, data, return_locals=return_locals, **kwargs) @wraps(Cost.get_gradients) def get_gradients(self, model, data, **kwargs): self.get_data_specs(model)[0].validate(data) obj, scratch = self.base_cost.expr(model, data, return_locals=True, **kwargs) if self.supervised: assert isinstance(data, (list, tuple)) assert len(data) == 2 (X, Y) = data else: X = data H_hat = scratch['H_hat'] terms = scratch['terms'] hidden_layers = scratch['hidden_layers'] grads = OrderedDict() assert len(H_hat) == len(terms) assert len(terms) == len(hidden_layers) num_layers = len(hidden_layers) for i in xrange(num_layers): state = H_hat[i] layer = model.hidden_layers[i] term = terms[i] if term == 0.: continue else: logger.info('term is {0}'.format(term)) if i == 0: state_below = X layer_below = model.visible_layer else: layer_below = model.hidden_layers[i - 1] state_below = H_hat[i - 1] state_below = layer_below.upward_state(state_below) components = flatten(state) real_grads = T.grad(term, components) fake_state = layer.linear_feed_forward_approximation(state_below) fake_components = flatten(fake_state) real_grads = OrderedDict(safe_zip(fake_components, real_grads)) params = list(layer.get_params()) fake_grads = pylearn2.utils.grad( cost=None, consider_constant=flatten(state_below), wrt=params, known_grads=real_grads ) for param, grad in safe_zip(params, fake_grads): if param in grads: grads[param] = grads[param] + grad else: grads[param] = grad return grads, OrderedDict() @wraps(Cost.get_data_specs) def get_data_specs(self, model): return self.base_cost.get_data_specs(model)
self.assertNotContains(resp, '<section class="university-partners university-partners2x6">')
warnings.simplefilter('ignore', category=RemovedInDjango20Warning) obj = mark_for_escaping(new_obj) escape_isnt_last_filter = False
CourseEnrollment.enroll(self.user, self.course_1.id) self.client.login(username="jack", password="test") with patch('student.views.get_programs_for_dashboard') as mock_method: mock_method.return_value = self._create_program_data([]) response = self.client.get(reverse('dashboard')) self.assertEquals(response.status_code, 200) self.assertIn('Pursue a Certificate of Achievement to highlight', response.content) self._assert_responses(response, 0)
response = self.ajax_request('No', {})
output_shape = [self.num_patches] for dim in self.patch_shape: output_shape.append(dim) output_shape.append(X.shape[-1]) output = numpy.zeros(output_shape, dtype=X.dtype) channel_slice = slice(0, X.shape[-1]) for i in xrange(self.num_patches): args = [] args.append(rng.randint(X.shape[0]))
self._verify_non_staff_cannot_access(course_cohort_settings_handler, "GET", [unicode(self.course.id)]) self._verify_non_staff_cannot_access(course_cohort_settings_handler, "PATCH", [unicode(self.course.id)])
ret['master'] = ip_port[0]
layer_to_state = dbm.make_layer_to_state(1) v_state = layer_to_state[v] h1_state = layer_to_state[h1] h2_state = layer_to_state[h2]
setattr(p, 'restaurant', None)
for ip in addresses: if isinstance(ip, _BaseAddress): if ips and ips[-1]._version != ip._version: raise TypeError("%s and %s are not of the same version" % ( ip, ips[-1])) ips.append(ip) elif ip._prefixlen == ip._max_prefixlen: if ips and ips[-1]._version != ip._version: raise TypeError("%s and %s are not of the same version" % ( ip, ips[-1])) try: ips.append(ip.ip) except AttributeError: ips.append(ip.network_address) else: if nets and nets[-1]._version != ip._version: raise TypeError("%s and %s are not of the same version" % ( ip, nets[-1])) nets.append(ip)
USE_I18N = True
_check_statistics(X, X_true, "most_frequent", [np.nan, 2, 3, 3], -1)
MIN_PRICE = 1438
if i == DOT: return '... ' elif i == cl.page_num: return format_html('<span class="this-page">{}</span> ', i + 1) else: return format_html('<a href="{}"{}>{}</a> ', cl.get_query_string({PAGE_VAR: i}), mark_safe(' class="end"' if i == cl.paginator.num_pages - 1 else ''), i + 1)
from pandas.core.nanops import unique1d values = self.values if hasattr(values, 'unique'): return values.unique() return unique1d(values)
values = self.as_matrix()
if p_state == 10: stat['4505/new'] += 1 if packet['ip']['s_addr'] not in ips_auth: ips_auth.append(packet['ip']['s_addr']) elif p_state == 12: stat['4505/fin'] += 1
try: item_index.update(location_info) item_index.update(item_index_dictionary) item_index['id'] = item_id if item.start: item_index['start_date'] = item.start item_index['content_groups'] = item_content_groups if item_content_groups else None item_index.update(cls.supplemental_fields(item)) items_index.append(item_index) indexed_count["count"] += 1 return item_content_groups
import os
return (self.network_address.is_private and self.broadcast_address.is_private)
clf = ElasticNet() brc3 = Birch(n_clusters=clf) assert_raises(ValueError, brc3.fit, X)
self.lift = 1 if -1 in self.index.labels[self.level] else 0
uint_data = DataFrame({ 'u08': Series(np.random.randint(0, high=255, size=5), dtype=np.uint8), 'u16': Series(np.random.randint(0, high=65535, size=5), dtype=np.uint16), 'u32': Series(np.random.randint(0, high=2**30, size=5), dtype=np.uint32), 'u64': Series([2**58, 2**59, 2**60, 2**61, 2**62], dtype=np.uint64)}, index=np.arange(5)) _maybe_remove(store, 'uints') store.append('uints', uint_data) tm.assert_frame_equal(store['uints'], uint_data)
return BASE_URL + "/courses/" + self.course_id + "/" + self.url_path
choice = Choice.objects.create(choice=None) response = self.client.get(reverse('admin:admin_views_choice_change', args=(choice.pk,))) self.assertContains(response, '<p>No opinion</p>', html=True) self.assertNotContains(response, '<p>(None)</p>')
raise RuntimeError( 'The syslog facility \'{0}\' is not known'.format( facility_name ) )
pass
year = random.randint(1950, 2000) month = random.randint(1, 12) day = random.randint(1, 28) hour = random.randint(0, 23) minute = random.randint(0, 59) self.created = datetime.datetime(year, month, day, hour, minute, tzinfo=utc) self.targets = FakeEmail.FakeTargetGroup()
class Person(Form): first_name = CharField() last_name = CharField() birthday = DateField()
func.restype = c_int
return None
if com.is_datetimelike(self.categories): return self.categories.take(self._codes, fill_value=np.nan) return np.array(self)
if self.with_centering: check_is_fitted(self, 'center_') if self.with_scaling: check_is_fitted(self, 'scale_') X = self._check_array(X, self.copy) if X.ndim == 1: warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning) if sparse.issparse(X): if self.with_scaling: inplace_column_scale(X, self.scale_) else: if self.with_scaling: X *= self.scale_ if self.with_centering: X += self.center_ return X
try: course_key = CourseKey.from_string(course_id) except InvalidKeyError: raise Http404
query = queries[[rng.randint(0, n_queries)]]
if os.path.isdir(target): if force_clone: if __opts__['test']: return _neutral_test( ret, 'Target directory {0} exists. Since force_clone=True, ' 'the contents of {0} would be deleted, and {1} would ' 'be cloned into this directory.'.format(target, name) ) log.debug( 'Removing contents of {0} to clone repository {1} in its ' 'place (force_clone=True set in git.detached state)' .format(target, name) ) try: if os.path.islink(target): os.unlink(target) else: salt.utils.rm_rf(target) except OSError as exc: return _fail( ret, 'Unable to remove {0}: {1}'.format(target, exc), comments ) else: ret['changes']['forced clone'] = True elif os.listdir(target): return _fail( ret, 'Target \'{0}\' exists, is non-empty and is not a git ' 'repository. Set the \'force_clone\' option to True to ' 'remove this directory\'s contents and proceed with ' 'cloning the remote repository'.format(target) )
return None
if total_recipients <= settings.BULK_EMAIL_JOB_SIZE_THRESHOLD: routing_key = settings.BULK_EMAIL_ROUTING_KEY_SMALL_JOBS
return constant_time_compare( _unsalt_cipher_token(request_csrf_token), _unsalt_cipher_token(csrf_token), )
self.r.article_set.set([new_article]) self.assertQuerysetEqual( self.r.article_set.all(), ["<Article: John's second story>", "<Article: This is a test>"] ) self.assertQuerysetEqual(self.r2.article_set.all(), ["<Article: Paul's story>"]) self.assertFalse(hasattr(self.r2.article_set, 'remove')) self.assertFalse(hasattr(self.r2.article_set, 'clear'))
store = HDFStore(path, mode='a', driver='H5FD_CORE', driver_core_backing_store=0) store['df'] = df store.append('df2', df)
return store.db_connection.structures
log = logging.getLogger(__name__)
MESSAGE = _('The underlying module store does not support import.')
os.remove(hashdir) os.makedirs(hashdir)
from .test_auth_backends import ImportedModelBackend
result = __salt__['splunk.update_user']( email, profile, **kwargs )
DEFAULT_EXCEPTION_REPORTER_FILTER = 'django.views.debug.SafeExceptionReporterFilter'
has_content = True optional_arguments = 1 option_spec = {'filename': directives.unchanged_required} def run(self): code = '\n'.join(self.content) literal = snippet_with_filename(code, code) if self.arguments: literal['language'] = self.arguments[0] literal['filename'] = self.options['filename'] set_source_info(self, literal) return [literal]
updates = _filter_list_by_category(updates=updates, categories=categories)
for ret in salt.utils.boto.paged_call(conn.get_response, 'ListAttachedGroupPolicies', params, list_marker='AttachedPolicies'): policies.extend(ret.get('list_attached_group_policies_response', {}).get('list_attached_group_policies_result', {} ).get('attached_policies', [])) return policies
assert_series_equal(s10_2, r10) assert_series_equal(s10_2, r10_2) assert_series_equal(s10_2, rl)
return super(PythonSerializer, self).getvalue()
if tables: sql = ['%s %s %s;' % ( style.SQL_KEYWORD('DELETE'), style.SQL_KEYWORD('FROM'), style.SQL_FIELD(self.quote_name(table)) ) for table in tables] sql.extend(self.sequence_reset_by_name_sql(style, sequences)) return sql else: return []
from __future__ import unicode_literals
data = load("{0}extra_32x32.mat".format(path)) valid_index = [] for i in xrange(1, 11): index = numpy.nonzero(data['y'] == i)[0] index.flags.writeable = 1 rng.shuffle(index) valid_index.append(index[:num_valid_extra])
rng = np.random.RandomState([2014, 11, 4]) start = 0 stop = 990 num_examples = 1000 num_feat = 5 num_classes = 2
self._compare(o.head(len(o) + 1), o) self._compare(o.tail(len(o) + 1), o)
try: file_result = file_result[next(six.iterkeys(file_result))] except AttributeError: pass
for k, (train, test) in enumerate(cv): valid_k = k + 1 if valid_k == len(cv): valid_k = 0 valid = cv[valid_k][1] train = np.setdiff1d(train, valid) yield train, valid, test
expected = date_range('1/1/2000 09:00', periods=7, freq='H', tz=tz, name='idx') for d in [pd.Timestamp('2000-01-01 15:00', tz=tz), pytz.timezone(tz).localize(datetime(2000, 1, 1, 15))]:
course_key = course.id section_data = { 'section_key': 'special_exams', 'section_display_name': _('Special Exams'), 'access': access, 'course_id': unicode(course_key) } return section_data
res = s.where(cond, True) tm.assert_series_equal(res, pd.Series([1 + 1j, 1, 3 + 3j, 1])) self.assertEqual(res.dtype, np.complex128) res = s.where(cond, pd.Series([True, False, True, True])) tm.assert_series_equal(res, pd.Series([1 + 1j, 0, 3 + 3j, 1])) self.assertEqual(res.dtype, np.complex128)
return self.visbiasX + tensor.dot( self._factorsY(inputs) * self._factorsH(inputs), self.wxf.T)
tm.assert_equal(0, len(results))
user = UserFactory.create() badge_class = BadgeClassFactory.create() badge_class.award(user, evidence_url='http://example.com/evidence') self.assertTrue(mock_award.called) mock_award.assert_called_with(badge_class, user, evidence_url='http://example.com/evidence')
self._assert_no_redirect(self.course_with_bogus_survey)
HAS_PSUTIL = False try: import salt.utils.psutil_compat as psutil HAS_PSUTIL = True except ImportError: pass
self.assertEqual(len(self.lc_block.children), 1) self.assertEqual(self.problem_in_course.location, self.lc_block.children[0]) self.assertEqual(self.problem_in_course.display_name, self.original_display_name)
if os.path.isfile(cache_file): os.remove(cache_file)
server.ping()
obj = RelatedModel.objects.create(name="xyzzy") OneToOneRestrictedModel.objects.create(name="foo", is_public=False, related=obj) obj = RelatedModel.objects.get(name="xyzzy") obj.delete() self.assertEqual(len(OneToOneRestrictedModel.plain_manager.all()), 0)
engines = 'python', 'python-fwf' for engine in engines: for default in py_unsupported: msg = ('The %r option is not supported ' 'with the %r engine' % (default, engine)) kwargs = {default: object()} with tm.assertRaisesRegexp(ValueError, msg): read_csv(StringIO(data), engine=engine, **kwargs) if __name__ == '__main__': nose.runmodule(argv=[__file__, '-vvs', '-x', '--pdb', '--pdb-failure'], exit=False)
centers = np.array([ [0.0, 5.0, 0.0, 0.0, 0.0], [0.0, 0.0, 4.0, 0.0, 0.0], [1.0, 0.0, 0.0, 5.0, 1.0], ]) n_samples = 1000 n_clusters, n_features = centers.shape S, true_labels = make_blobs(n_samples=n_samples, centers=centers, cluster_std=1., random_state=42)
if ((not com.is_categorical_dtype(result)) and isinstance(result, np.ndarray)): result = _block_shape(result, ndim=self.ndim)
cat_unorderd = cat.set_ordered(False) self.assertFalse((cat > cat).any())
new_sum_squared_grad = ( sum_square_grad + T.sqr(grads[param]) )
client = salt.client.get_local_client(__opts__['conf_file']) alive = False try: if client.cmd(host, 'test.ping', timeout=20).get(host, None): alive = True except (TypeError, KeyError): pass if not alive: ret['comment'] = 'Host {0} is not reachable'.format(host) ret['result'] = False return ret
mask = isnull(self) if not self.is_object() and not quoting: values = np.asarray(self).astype(str) else: values = np.array(self, dtype=object, copy=True) values[mask] = na_rep return values
try: gc.collect(2) if not gc.get_referents(self.is_copy()): self.is_copy = None return except: pass
self.user = User.objects.get_by_natural_key(self.user.natural_key()[0])
tm._skip_if_no_scipy() import scipy.sparse
DEBUG = True
expected_grades = [self._format_user_grade(header_row, **grade) for grade in user_grades] self.verify_rows_in_csv(expected_grades)
self._verify_unit_warning( self.UnitState(is_released=False, publish_state=self.PublishState.PUBLISHED, is_locked=False), None )
return ( u"certificate-template-{key.org}-{key.course}-verified.pdf".format(key=course_key) if mode_slug == 'verified' else u"certificate-template-{key.org}-{key.course}.pdf".format(key=course_key) )
self.cursor().execute("BEGIN")
with six.assertRaisesRegex(self, TypeError, "Variable must be a string or number, got <(class|type) 'dict'>"): Variable({})
from __future__ import unicode_literals
self.assertFalse(self.storage.exists('test.file')) f = ContentFile('custom contents') f.name = 'test.file' storage_f_name = self.storage.save(None, f) self.assertEqual(storage_f_name, f.name) self.assertTrue(os.path.exists(os.path.join(self.temp_dir, f.name))) self.storage.delete(storage_f_name)
from pandas import date_range s = Series(date_range('20130102', periods=6)) result = s.idxmin() self.assertEqual(result, 0)
if not SignatureValidator(lti_consumer).verify(request): return HttpResponseForbidden()
if block.is_categorical: return self.set_atom_categorical(block, items=block_items, info=info) elif block.is_datetimetz: return self.set_atom_datetime64tz(block, info=info) elif block.is_datetime: return self.set_atom_datetime64(block) elif block.is_timedelta: return self.set_atom_timedelta64(block) elif block.is_complex: return self.set_atom_complex(block)
User.objects.filter(username='testclient').update(is_staff=True, is_superuser=True) self.login() self.admin = User.objects.get(pk=self.u1.pk)
plt.figure(2) theta0 = np.logspace(-2, 3, 49) theta1 = np.logspace(-2, 0, 50) Theta0, Theta1 = np.meshgrid(theta0, theta1) LML = [[gp.log_marginal_likelihood(np.log([0.36, Theta0[i, j], Theta1[i, j]])) for i in range(Theta0.shape[0])] for j in range(Theta0.shape[1])] LML = np.array(LML).T
for pattern in self.ALLOW_URL_PATTERNS: if pattern.match(request.path) is not None: return None
if msg:
msg = "Penalty term must be positive" assert_raise_message(ValueError, msg, LogisticRegression(C=-1).fit, X, Y1) assert_raise_message(ValueError, msg, LogisticRegression(C="test").fit, X, Y1)
plt.figure() plt.xlabel("Number of features selected") plt.ylabel("Cross validation score (nb of correct classifications)") plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_) plt.show()
self.teardown()
for axes in all_axes: axes.get_xaxis().set_visible(False) axes.get_yaxis().set_visible(False)
def get_executor(name): executor_class = minion_instance.executors.get(name) if executor_class is None: raise SaltInvocationError("Executor '{0}' is not available".format(name)) return executor_class executor = get_executor(executors.pop())(opts, data, func, args, kwargs) for executor_name in reversed(executors): executor = get_executor(executor_name)(opts, data, executor) return_data = executor.execute()
if ssh_key_changed: if not __opts__['test']: response = __salt__[esxi_cmd]('upload_ssh_key', ssh_key=ssh_key, ssh_key_file=ssh_key_file, certificate_verify=certificate_verify) error = response.get('Error') if error: ret['comment'] = 'Error: {0}'.format(error) return ret ret['changes'].update({'SSH Key': {'old': current_ssh_key, 'new': ssh_key if ssh_key else ssh_key_file}})
noconvert_columns = set()
sections = [' '.join(random.sample(WORDS, random.randint(3, 12))) for i in range(random.randint(1, 5))] s = ', '.join(sections) return '%s%s%s' % (s[0].upper(), s[1:], random.choice('?.'))
this_img = scale_to_unit_interval( this_x.reshape(img_shape))
self.assertRaises(IndexError, idx.__getitem__, empty_farr)
X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] assert_equal(check_array(X).dtype.kind, "i") assert_equal(check_array(X, ensure_2d=False).dtype.kind, "i")
orphans = self.store.get_orphans(course_key) self.assertIn(orphan, orphans) self.assertEqual(len(orphans), 1)
Book.authors.through.objects.using('default').delete()
assert code is not None if isinstance(code, offsets.DateOffset): code = code.rule_code return code.upper()
if block_key not in missing_blocks: self.assertEquals( set(block_structure.get_children(block_key)), set(children), )
self.factory(learning_rate="<unknown>")
DEBUG_TOOLBAR_MONGO_STACKTRACES = True
grains.update(_hw_data(grains))
validated_data["modes"] = self._new_course_mode_models(validated_data["modes"]) instance.update(validated_data) instance.save() return instance
self.assertNotEqual(t1.render(Context({})), t2.render(Context({})))
cohort1 = self._create_cohort(course.id, "TestCohort1", CourseCohort.RANDOM) self._create_cohort(course.id, "TestCohort2", CourseCohort.RANDOM) cohort3 = self._create_cohort(course.id, "TestCohort3", CourseCohort.MANUAL)
if LooseVersion(xlrd.__VERSION__) >= LooseVersion("0.9.3"): xlrd_0_9_3 = True else: xlrd_0_9_3 = False
if endpoint in ['update_forum_role_membership', 'list_forum_members']: continue elif endpoint == 'get_problem_responses': self._access_problem_responses_endpoint( "Staff member should be allowed to access endpoint " + endpoint ) continue self._access_endpoint( endpoint, args, expected_status, "Staff member should be allowed to access endpoint " + endpoint )
result = self[item] del self[item] try: result._reset_cacher() except AttributeError: pass return result
num_braces = 0
if os.path.exists(pid_path): with salt.utils.fopen(pid_path) as fhr: try: os.kill(int(fhr.read()), signal.SIGKILL) except OSError: pass try: self.assertFalse(os.path.isdir(os.path.join(config_dir, 'file:'))) self.assertIn( 'Failed to setup the Syslog logging handler', '\n'.join(ret[1]) ) self.assertEqual(ret[2], 2) finally: self.chdir(old_cwd) if os.path.isdir(config_dir): shutil.rmtree(config_dir)
for f in ['year', 'month', 'day', 'hour', 'minute', 'second', 'week', 'dayofyear', 'quarter', 'days_in_month']: self.assertTrue(np.isnan(getattr(p_nat, f))) self.assertTrue(np.isnan(getattr(t_nat, f)))
import salt.loader
if cyg_arch == 'x86_64': return 'cygwin64' elif cyg_arch == 'x86': return 'cygwin' raise SaltInvocationError( 'Invalid architecture {arch}'.format(arch=cyg_arch))
path = os.path.join(integration.FILES, 'conf', 'cloud.providers.d', provider + '.conf') config = cloud_providers_config(path)
result = normalize_date(result)
return list(itertools.chain(self.default_validators, self._validators))
self.assertRaises(ValueError, df.iloc.__getitem__, tuple(['j', 'D']))
if self.rc > 0 and other.rc <= 0: noc_info = list(self.noc_info) noc_info[3] = -1 return method(tuple(noc_info), other.noc_info)
if not self._state.adding and self.pk is not None: qs = qs.exclude(pk=self.pk)
loader = MigrationLoader(connection, ignore_no_migrations=True) graph = loader.graph if app_names: invalid_apps = [] for app_name in app_names: if app_name not in loader.migrated_apps: invalid_apps.append(app_name) if invalid_apps: raise CommandError("No migrations present for: %s" % (", ".join(invalid_apps))) else: app_names = sorted(loader.migrated_apps) for app_name in app_names: self.stdout.write(app_name, self.style.MIGRATE_LABEL) shown = set() for node in graph.leaf_nodes(app_name): for plan_node in graph.forwards_plan(node): if plan_node not in shown and plan_node[0] == app_name: title = plan_node[1] if graph.nodes[plan_node].replaces: title += " (%s squashed migrations)" % len(graph.nodes[plan_node].replaces) if plan_node in loader.applied_migrations: self.stdout.write(" [X] %s" % title) else: self.stdout.write(" [ ] %s" % title) shown.add(plan_node) if not shown: self.stdout.write(" (no migrations)", self.style.ERROR)
pca = PCA(n_components=5, random_state=0) Y = X[:10, :] pca.fit(Y) pca_test = PCA(n_components=5, svd_solver='full', random_state=0) pca_test.fit(Y) assert_array_almost_equal(pca.components_, pca_test.components_)
user = User.objects.create_user(username, email, password) reg = Registration() reg.register(user) profile = UserProfile(user=user) profile.name = name profile.country = country profile.save() return user
result = {} for key in keys: if key in d: result[key] = d[key] return result
self._encoding = val if hasattr(self, '_get'): del self._get if hasattr(self, '_post'): del self._post
raise ImportError("Oops")
self.group_a_problem = 'GROUP A CONTENT' self.group_b_problem = 'GROUP B CONTENT' self.group_a_and_b_problem = 'GROUP A AND B CONTENT' self.visible_to_all_problem = 'VISIBLE TO ALL CONTENT' course_fixture.add_children( XBlockFixtureDesc('chapter', 'Test Section').add_children( XBlockFixtureDesc('sequential', 'Test Subsection').add_children( XBlockFixtureDesc('vertical', 'Test Unit').add_children( XBlockFixtureDesc('problem', self.group_a_problem, data='<problem></problem>'), XBlockFixtureDesc('problem', self.group_b_problem, data='<problem></problem>'), XBlockFixtureDesc('problem', self.group_a_and_b_problem, data='<problem></problem>'), XBlockFixtureDesc('problem', self.visible_to_all_problem, data='<problem></problem>') ) ) ) )
X = arg1 + 0 * arg2 Y = arg2 + 0 * arg1
assert k in _offset_map self.assertEqual(k, (get_offset(k) * 3).rule_code)
ext_user = User.objects.get(email=self.ext_user.email) self.assertTrue(auth.user_has_role(ext_user, CourseInstructorRole(self.course.id)))
with patch.dict(postgres_user.__opts__, {'test': True}): ret = postgres_user.present('foo') self.assertEqual( ret, {'comment': 'User foo is set to be created', 'changes': {}, 'name': 'foo', 'result': None} ) self.assertEqual(SALT_STUB['postgres.user_create'].call_count, 0)
parent = self.store.create_item( self.user_id, self.course.id, 'vertical', block_id='parent', ) parent.children += [self.course.id.make_usage_key('vertical', 'does_not_exist')] parent = self.store.update_item(parent, self.user_id)
s1 = Series(arr, name='x') df = DataFrame([s1, arr]).T expected = DataFrame({'x': s1, 'Unnamed 0': arr}, columns=['x', 'Unnamed 0']) tm.assert_frame_equal(df, expected)
if valid_size < 1.0: valid_size /= 1.0 - np.true_divide(self.n_test, self.n) self.valid_size = valid_size
if 'beacons' not in opts: opts['beacons'] = {}
fks_dropped = set() if old_field.remote_field and old_field.db_constraint: fk_names = self._constraint_names(model, [old_field.column], foreign_key=True) if strict and len(fk_names) != 1: raise ValueError("Found wrong number (%s) of foreign key constraints for %s.%s" % ( len(fk_names), model._meta.db_table, old_field.column, )) for fk_name in fk_names: fks_dropped.add((old_field.column,)) self.execute(self._delete_constraint_sql(self.sql_delete_fk, model, fk_name)) if old_field.unique and (not new_field.unique or (not old_field.primary_key and new_field.primary_key)): constraint_names = self._constraint_names(model, [old_field.column], unique=True) if strict and len(constraint_names) != 1: raise ValueError("Found wrong number (%s) of unique constraints for %s.%s" % ( len(constraint_names), model._meta.db_table, old_field.column, )) for constraint_name in constraint_names: self.execute(self._delete_constraint_sql(self.sql_delete_unique, model, constraint_name)) if old_field.primary_key and new_field.primary_key and old_type != new_type: for _old_rel, new_rel in _related_non_m2m_objects(old_field, new_field): rel_fk_names = self._constraint_names( new_rel.related_model, [new_rel.field.column], foreign_key=True ) for fk_name in rel_fk_names: self.execute(self._delete_constraint_sql(self.sql_delete_fk, new_rel.related_model, fk_name)) if (old_field.db_index and not new_field.db_index and not old_field.unique and not (not new_field.unique and old_field.unique)): index_names = self._constraint_names(model, [old_field.column], index=True) for index_name in index_names: self.execute(self._delete_constraint_sql(self.sql_delete_index, model, index_name)) if old_db_params['check'] != new_db_params['check'] and old_db_params['check']: constraint_names = self._constraint_names(model, [old_field.column], check=True) if strict and len(constraint_names) != 1: raise ValueError("Found wrong number (%s) of check constraints for %s.%s" % ( len(constraint_names), model._meta.db_table, old_field.column, )) for constraint_name in constraint_names: self.execute(self._delete_constraint_sql(self.sql_delete_check, model, constraint_name)) if old_field.column != new_field.column: self.execute(self._rename_field_sql(model._meta.db_table, old_field, new_field, new_type)) actions = [] null_actions = [] post_actions = [] if old_type != new_type: fragment, other_actions = self._alter_column_type_sql( model._meta.db_table, old_field, new_field, new_type ) actions.append(fragment) post_actions.extend(other_actions) old_default = self.effective_default(old_field) new_default = self.effective_default(new_field) needs_database_default = ( old_default != new_default and new_default is not None and not self.skip_default(new_field) ) if needs_database_default: if self.connection.features.requires_literal_defaults: actions.append(( self.sql_alter_column_default % { "column": self.quote_name(new_field.column), "type": new_type, "default": self.prepare_default(new_default), }, [], )) else: actions.append(( self.sql_alter_column_default % { "column": self.quote_name(new_field.column), "type": new_type, "default": "%s", }, [new_default], )) if old_field.null != new_field.null: if (self.connection.features.interprets_empty_strings_as_nulls and new_field.get_internal_type() in ("CharField", "TextField")): pass elif new_field.null: null_actions.append(( self.sql_alter_column_null % { "column": self.quote_name(new_field.column), "type": new_type, }, [], )) else: null_actions.append(( self.sql_alter_column_not_null % { "column": self.quote_name(new_field.column), "type": new_type, }, [], )) four_way_default_alteration = ( new_field.has_default() and (old_field.null and not new_field.null) ) if actions or null_actions: if not four_way_default_alteration: actions = actions + null_actions if self.connection.features.supports_combined_alters and actions: sql, params = tuple(zip(*actions)) actions = [(", ".join(sql), sum(params, []))] for sql, params in actions: self.execute( self.sql_alter_column % { "table": self.quote_name(model._meta.db_table), "changes": sql, }, params, ) if four_way_default_alteration: self.execute( self.sql_update_with_default % { "table": self.quote_name(model._meta.db_table), "column": self.quote_name(new_field.column), "default": "%s", }, [new_default], ) for sql, params in null_actions: self.execute( self.sql_alter_column % { "table": self.quote_name(model._meta.db_table), "changes": sql, }, params, ) if post_actions: for sql, params in post_actions: self.execute(sql, params) if (not old_field.unique and new_field.unique) or ( old_field.primary_key and not new_field.primary_key and new_field.unique ): self.execute(self._create_unique_sql(model, [new_field.column])) if (not old_field.db_index and new_field.db_index and not new_field.unique and not (not old_field.unique and new_field.unique)): self.execute(self._create_index_sql(model, [new_field], suffix="_uniq")) rels_to_update = [] if old_field.primary_key and new_field.primary_key and old_type != new_type: rels_to_update.extend(_related_non_m2m_objects(old_field, new_field)) if not old_field.primary_key and new_field.primary_key: constraint_names = self._constraint_names(model, primary_key=True) if strict and len(constraint_names) != 1: raise ValueError("Found wrong number (%s) of PK constraints for %s" % ( len(constraint_names), model._meta.db_table, )) for constraint_name in constraint_names: self.execute(self._delete_constraint_sql(self.sql_delete_pk, model, constraint_name)) self.execute( self.sql_create_pk % { "table": self.quote_name(model._meta.db_table), "name": self.quote_name(self._create_index_name(model, [new_field.column], suffix="_pk")), "columns": self.quote_name(new_field.column), } ) rels_to_update.extend(_related_non_m2m_objects(old_field, new_field)) for old_rel, new_rel in rels_to_update: rel_db_params = new_rel.field.db_parameters(connection=self.connection) rel_type = rel_db_params['type'] fragment, other_actions = self._alter_column_type_sql( new_rel.related_model._meta.db_table, old_rel.field, new_rel.field, rel_type ) self.execute( self.sql_alter_column % { "table": self.quote_name(new_rel.related_model._meta.db_table), "changes": fragment[0], }, fragment[1], ) for sql, params in other_actions: self.execute(sql, params) if (new_field.remote_field and (fks_dropped or not old_field.remote_field or not old_field.db_constraint) and new_field.db_constraint): self.execute(self._create_fk_sql(model, new_field, "_fk_%(to_table)s_%(to_column)s")) if old_field.primary_key and new_field.primary_key and old_type != new_type: for rel in new_field.model._meta.related_objects: if not rel.many_to_many: self.execute(self._create_fk_sql(rel.related_model, rel.field, "_fk")) if old_db_params['check'] != new_db_params['check'] and new_db_params['check']: self.execute( self.sql_create_check % { "table": self.quote_name(model._meta.db_table), "name": self.quote_name(self._create_index_name(model, [new_field.column], suffix="_check")), "column": self.quote_name(new_field.column), "check": new_db_params['check'], } ) if needs_database_default: sql = self.sql_alter_column % { "table": self.quote_name(model._meta.db_table), "changes": self.sql_alter_column_no_default % { "column": self.quote_name(new_field.column), "type": new_type, } } self.execute(sql) if self.connection.features.connection_persists_old_columns: self.connection.close()
self.assertIsNone(subq._result_cache)
klass = obj.__class__ obj = (klass, ('HASHED', obj.descr))
def setUp(self): urlconf = 'urlpatterns_reverse.urls_error_handlers' urlconf_callables = 'urlpatterns_reverse.urls_error_handlers_callables' self.resolver = RegexURLResolver(r'^$', urlconf) self.callable_resolver = RegexURLResolver(r'^$', urlconf_callables) def test_named_handlers(self): handler = (empty_view, {}) self.assertEqual(self.resolver.resolve_error_handler(400), handler) self.assertEqual(self.resolver.resolve_error_handler(404), handler) self.assertEqual(self.resolver.resolve_error_handler(500), handler) def test_callable_handlers(self): handler = (empty_view, {}) self.assertEqual(self.callable_resolver.resolve_error_handler(400), handler) self.assertEqual(self.callable_resolver.resolve_error_handler(404), handler) self.assertEqual(self.callable_resolver.resolve_error_handler(500), handler)
assert_raises(ValueError, next, StratifiedShuffleSplit(train_size=2).split(X, y)) assert_raises(ValueError, next, StratifiedShuffleSplit(test_size=2).split(X, y))
if not mask.any(): return self if inplace else self.copy()
symbol = 'courseware.grades.grade' with patch(symbol) as mock_grade: mock_grade.return_value = {'grade': 'Pass', 'percent': 0.75} yield
result1 = s[[0.0, 5, 10]] result2 = s.loc[[0.0, 5, 10]] result3 = s.ix[[0.0, 5, 10]] result4 = s.iloc[[0, 2, 4]] assert_series_equal(result1, result2) assert_series_equal(result1, result3) assert_series_equal(result1, result4)
from pandas import Series return Series(self._to_embed(keep_tz), index=self, name=self.name)
if isinstance(convert, ndarray) or convert is None: args = (convert,) + args convert = True validate_take(args, kwargs, max_fname_arg_count=3, method='both') return convert
urls = microsite.get_value("urls", default={}) return urls.get(name) or EMPTY_URL
output = self.engine.render_to_string('autoescape-literals01') self.assertEqual(output, 'this & that')
task_entry = self._create_input_entry(course_id=SlashSeparatedCourseKey("bogus", "course", "id")) with self.assertRaises(ValueError): self._run_task_with_mock_celery(send_bulk_course_email, task_entry.id, task_entry.task_id)
for item in structure.get_children(): prepare_item_index(item, groups_usage_info=groups_usage_info) searcher.index(cls.DOCUMENT_TYPE, items_index) cls.remove_deleted_items(searcher, structure_key, indexed_items)
self._create_courses_and_enrollments( (self.TEST_ORG, True), ("other_org", True) )
import salt.utils from salt.exceptions import CommandExecutionError try: import salt.utils.pycrypto HAS_CRYPT = True except ImportError: HAS_CRYPT = False
X2 = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0], [1.0, 1.0], [1.0, 0.0]]) y2 = [1, 1, 1, -1, -1]
X = generator.multivariate_normal(mean, cov * np.identity(n_features), (n_samples,))
return self.find_css("#due_time").present
fields = self.q(css='div.problem div.capa_inputtype.textline input') fields = fields.nth(input_num) if input_num is not None else fields fields.fill(text)
return ['Test Section {}'.format(index), 'Test Subsection {}'.format(index), 'Test Problem {}'.format(index)]
X = X_orig.copy() imputer = Imputer(missing_values=0, strategy="mean", copy=False, axis=1) Xt = imputer.fit(X).transform(X) assert_false(sparse.issparse(Xt))
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.bio_book, self.djangonaut_book])
verify_date_or_time(COURSE_START_DATE_CSS, '12/20/2013') verify_date_or_time(COURSE_START_TIME_CSS, DUMMY_TIME)
video_block_key = self.course_key.make_usage_key('video', 'sample_video') self.assertIsNotNone( self.block_structure.get_transformer_block_field( video_block_key, StudentViewTransformer, StudentViewTransformer.STUDENT_VIEW_DATA, ) ) self.assertFalse( self.block_structure.get_transformer_block_field( video_block_key, StudentViewTransformer, StudentViewTransformer.STUDENT_VIEW_MULTI_DEVICE, ) )
try: assert counter.value == 4 finally: process_manager.stop_restarting() process_manager.kill_children() time.sleep(0.5) if process_manager._process_map.keys(): process_manager.send_signal_to_processes(signal.SIGILL) process_manager.stop_restarting() process_manager.kill_children()
self._params.update(l._params)
def check_for_staff(): return _has_staff_access_to_descriptor(user, descriptor, course_key) checkers = { 'load': check_for_staff, 'staff': check_for_staff, 'instructor': lambda: _has_instructor_access_to_descriptor(user, descriptor, course_key) } return _dispatch(checkers, action, user, descriptor)
return self.q(css=self.xblock_component_selector).attrs('data-block-type')[index]
response = self.client.get(self.url) self.assertIn('D: 0.5, C: 0.57, B: 0.63, A: 0.75', response.content)
cr_ec2 = MockEC2() cr_ec2.profile = profile cr_ec2.access = access cr_ec2.user_id = user_id cr_ec2.connection_args = connection_args return cr_ec2
try: ExternalAuthMap.objects.get(external_id=self.USER_EMAIL) except ExternalAuthMap.DoesNotExist, ex: self.fail('User did not get properly added to external auth map, exception was {0}'.format(str(ex))) try: User.objects.get(email=self.USER_EMAIL) except ExternalAuthMap.DoesNotExist, ex: self.fail('User did not get properly added to internal users, exception was {0}'.format(str(ex))) self.assertEqual(1, len(ExternalAuthMap.objects.all()))
from pylearn2.utils.rng import make_np_rng
prior_class = prior.DiagonalGaussianPrior posterior_class = conditional.DiagonalGaussian @wraps(KLIntegrator.kl_divergence) def kl_divergence(self, phi, theta, prior, posterior): return self.per_component_kl_divergence( phi=phi, theta=theta, prior=prior, posterior=posterior ).sum(axis=1) @wraps(KLIntegrator.per_component_kl_divergence) def per_component_kl_divergence(self, phi, theta, prior, posterior): self._validate_prior_posterior(prior, posterior) (posterior_mu, posterior_log_sigma) = phi (prior_mu, prior_log_sigma) = theta return ( prior_log_sigma - posterior_log_sigma + 0.5 * (T.exp(2 * posterior_log_sigma) + (posterior_mu - prior_mu) ** 2) / T.exp(2 * prior_log_sigma) - 0.5 )
nobs = self._nobs factors = (nobs - 1) / (nobs - self._df_raw) return 1 - (1 - self._r2_raw) * factors
for data in generated_certificates: data['report_run_date'] = report_run_date
input_dict = {'1_2_1': '-1', '1_2_2': '2', '1_2_3': '3'} correct_map = problem.grade_answers(input_dict)
D_fixed = ricker_matrix(width=width, resolution=resolution, n_components=n_components) D_multi = np.r_[tuple(ricker_matrix(width=w, resolution=resolution, n_components=np.floor(n_components / 5)) for w in (10, 50, 100, 500, 1000))]
microsite.set_by_domain(self.microsite_subdomain) self.assertTrue(microsite.is_request_in_microsite())
try: while True: observer.join(2) except KeyboardInterrupt: observer.stop() print("\nStopped asset watcher.")
TASK_LOG.info( "Task %s: creating %s subtasks to process %s items.", task_id, total_num_subtasks, total_num_items, ) num_subtasks = 0 for item_list in item_list_generator: subtask_id = subtask_id_list[num_subtasks] num_subtasks += 1 subtask_status = SubtaskStatus.create(subtask_id) new_subtask = create_subtask_fcn(item_list, subtask_status) new_subtask.apply_async()
return self.visible_layer.space
source_suffix = '.rst'
if course is None: course = self.course cs_thread_data = make_minimal_cs_thread({ "id": "test_thread", "course_id": unicode(course.id) }) cs_thread_data.update(thread_overrides or {}) self.register_get_thread_response(cs_thread_data) cs_comment_data = make_minimal_cs_comment({ "id": "test_comment", "course_id": cs_thread_data["course_id"], "thread_id": cs_thread_data["id"], "username": self.user.username, "user_id": str(self.user.id), "created_at": "2015-06-03T00:00:00Z", "updated_at": "2015-06-03T00:00:00Z", "body": "Original body", }) cs_comment_data.update(overrides or {}) self.register_get_comment_response(cs_comment_data) self.register_put_comment_response(cs_comment_data)
('default', base_mgr), ('food_mgr2', mgr2), (b'food_mgr1', mgr1),
url(r'^api/enrollment/v1/', include('enrollment.urls')),
return self.lcp.get_score()
'ENABLE_MOBILE_REST_API': False,
X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1) for Cls in [GradientBoostingRegressor, GradientBoostingClassifier]: est = Cls(n_estimators=100, max_depth=1, warm_start=True) est.fit(X, y) est.set_params(n_estimators=110, max_depth=2) est.fit(X, y)
try: cached_mapping = CourseStructure.objects.get(course_id=course.id).discussion_id_map if not cached_mapping: raise DiscussionIdMapIsNotCached() return cached_mapping.get(discussion_id) except CourseStructure.DoesNotExist: raise DiscussionIdMapIsNotCached()
s = Series(['A', 'B', 'C', 'Aaba', 'Baca', '', NA, 'CABA', 'dog', 'cat'])
if not isinstance(template, string_types): log.error('Template was specified incorrectly: {0}'.format(template)) return ret if not os.path.isfile(template): log.error('Template does not exist: {0}'.format(template)) return ret if salt.utils.is_empty(template): log.warning('Template is an empty file: {0}'.format(template)) return ret
comps = weight_cdf.searchsorted(rand) for comp in range(self.n_components): comp_in_X = (comp == comps) num_comp_in_X = comp_in_X.sum() if num_comp_in_X > 0: if self.covariance_type == 'tied': cv = self.covars_ elif self.covariance_type == 'spherical': cv = self.covars_[comp][0] else: cv = self.covars_[comp] X[comp_in_X] = sample_gaussian( self.means_[comp], cv, self.covariance_type, num_comp_in_X, random_state=random_state).T return X
return module.seed
file1 = open(filename) file2 = open(filename) r = HttpResponse(file1) r.content = file2 self.assertTrue(file1.closed) self.assertTrue(file2.closed)
result = td.fillna(1) expected = Series([timedelta(seconds=1), timedelta(0), timedelta(1), timedelta(days=1, seconds=9 * 3600 + 60 + 1)]) assert_series_equal(result, expected)
df2 = self.read_csv(StringIO(data2)) tm.assert_frame_equal(df2, df)
self._test_view('sitemap_xml', 'application/xml')
from urllib2 import HTTPError from urllib2 import quote from urllib2 import urlopen
reviewing_user = models.ForeignKey( User, db_index=True, default=None, null=True, related_name="photo_verifications_reviewed" )
with patch('xmodule.modulestore.split_mongo.mongo_connection.MongoConnection.insert_course_index', Mock(side_effect=Exception)): split_course4_id = CourseLocator(org="edx3", course="split3", run="rerun_fail") fields = {'display_name': 'total failure'} CourseRerunState.objects.initiated(split_course3_id, split_course4_id, self.user, fields['display_name']) result = rerun_course.delay(unicode(split_course3_id), unicode(split_course4_id), self.user.id, json.dumps(fields, cls=EdxJSONEncoder)) self.assertIn("exception: ", result.get()) self.assertIsNone(self.store.get_course(split_course4_id), "Didn't delete course after error") CourseRerunState.objects.find_first( course_key=split_course4_id, state=CourseRerunUIStateManager.State.FAILED )
egg_name = '%s/omelet.egg' % self.egg_dir with extend_sys_path(egg_name): with self.settings(INSTALLED_APPS=['omelet.app_with_models']): models_module = apps.get_app_config('app_with_models').models_module self.assertIsNotNone(models_module) del apps.all_models['app_with_models']
title = models.CharField(max_length=200) content_type = models.ForeignKey(ContentType, models.CASCADE, null=True) object_id = models.PositiveIntegerField(null=True) parent = GenericForeignKey() children = GenericRelation('Post') class Meta: order_with_respect_to = 'parent' def __str__(self): return self.title
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
'ALLOW_WIKI_ROOT_ACCESS': True,
self.lc_block.max_count = 2 del self.lc_block._xmodule._selected_set children = self.lc_block.get_child_descriptors() self.assertEqual(len(children), 2) child, new_child = children if children[0].location == child.location else reversed(children) event_data = self._assert_event_was_published("assigned") self.assertEqual(event_data["added"][0]["usage_key"], unicode(new_child.location)) self.assertEqual(len(event_data["result"]), 2) self.assertEqual(event_data["previous_count"], 1) self.assertEqual(event_data["max_count"], 2)
key.set_contents_from_string( data, headers={ "Content-Encoding": content_encoding, "Content-Length": len(data), "Content-Type": content_type, } )
templates = { os.path.normcase('templates/y.html'): six.StringIO("y"), } with self.create_egg('egg', templates): with self.assertRaises(TemplateDoesNotExist): self.engine.get_template('y.html')
if hasattr(estimator, "feature_importances_"): importances = estimator.feature_importances_ elif hasattr(estimator, "coef_"): if estimator.coef_.ndim == 1: importances = np.abs(estimator.coef_) else: importances = np.sum(np.abs(estimator.coef_), axis=0) else: raise ValueError( "The underlying estimator %s has no `coef_` or " "`feature_importances_` attribute. Either pass a fitted estimator" " to SelectFromModel or call fit before calling transform." % estimator.__class__.__name__) return importances
current_step = {'step': 'Uploading executive summary report HTML file'} task_progress.update_task_state(extra_meta=current_step) TASK_LOG.info(u'%s, Task type: %s, Current step: %s', task_info_string, action_name, current_step)
assert_panel4d_equal(panel4d.ix[:, items, :, :], panel4d.reindex(items=items))
all_assets, __ = contentstore('trashcan').get_all_content_for_course(self.course.id) self.assertGreater(len(all_assets), 0)
with ensure_clean_store(self.path) as store:
year = self.get_year() month = self.get_month() day = self.get_day() date = _date_from_string(year, self.get_year_format(), month, self.get_month_format(), day, self.get_day_format()) return self._get_dated_items(date)
self.assertFalse(check_user('nouser')) sys.stderr = stderr if writer.output != 'CRITICAL: User not found: "nouser"\n': sys.stderr.write(writer.output)
raise AbstractMethodError(self)
df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) df.columns = pd.MultiIndex.from_tuples([(0, 1), (1, 1), (2, 1)])
LogoutPage(self.browser).visit() AutoAuthPage(self.browser, course_id=self.course_id, staff=True).visit()
_maybe_remove(store, 'df') df = tm.makeTimeDataFrame() df['string'] = 'foo' df.ix[1:4, 'string'] = np.nan df['string2'] = 'bar' df.ix[4:8, 'string2'] = np.nan df['string3'] = 'bah' df.ix[1:, 'string3'] = np.nan store.append('df', df) result = store.select('df') tm.assert_frame_equal(result, df)
#print >> sys.stderr, "WARN: tile_raster_images sucks, use tile_slices_to_image" if len(img_shape)==3 and img_shape[2]==3: if scale_rows_to_unit_interval: logger.warning("tile_raster_images' scaling routine " "messes up colour - try tile_slices_to_image") return tile_raster_images( (X[:,0::3], X[:,1::3], X[:,2::3], None), img_shape=img_shape[:2], tile_shape=tile_shape, tile_spacing=tile_spacing, scale_rows_to_unit_interval=scale_rows_to_unit_interval, output_pixel_vals=output_pixel_vals, min_dynamic_range=min_dynamic_range)
Logger.__init__(self) self._verbose = verbose self.mmap_mode = mmap_mode self.timestamp = time.time() self.compress = compress if compress and mmap_mode is not None: warnings.warn('Compressed results cannot be memmapped', stacklevel=2) if cachedir is None: self.cachedir = None else: self.cachedir = os.path.join(cachedir, 'joblib') mkdirp(self.cachedir)
return errors
estimator_error = (sample_weight * error_vect).sum()
for i in range(9): i += 13 registration_code_redemption = RegistrationCodeRedemption( registration_code_id=i, redeemed_by=self.instructor ) registration_code_redemption.save()
if not os.path.isdir(GPG_KEYDIR): os.makedirs(GPG_KEYDIR)
import integration import salt.utils
kwargs = {'course_id': self.course_id.to_deprecated_string(), 'note_id': str(self.pk)} return reverse('notes_api_note', kwargs=kwargs)
'gating.apps.GatingConfig',
pass
SSL_AUTH_EMAIL_DOMAIN = "MIT.EDU" SSL_AUTH_DN_FORMAT_STRING = "/C=US/ST=Massachusetts/O=Massachusetts Institute of Technology/OU=Client CA v1/CN={0}/emailAddress={1}"
if not isinstance(self, ClassifierMixin): self.out_activation_ = 'identity' elif self.label_binarizer_.y_type_ == 'multiclass': self.out_activation_ = 'softmax' else: self.out_activation_ = 'logistic' if self.loss == 'log_loss': self.loss = 'binary_log_loss'
destroy(vm_['name'])
iris = datasets.load_iris() X = iris.data y = iris.target n_classes = np.unique(y).size
with patch(settings.SEARCH_ENGINE + '.index') as mock_index: self.reindex_course(self.store) self.assertTrue(mock_index.called) indexed_content = self._get_index_values_from_call_args(mock_index) self.assertIn(self._html_nogroup_result(self.html_unit1), indexed_content) mock_index.reset_mock()
return self.q(css="#register-username").attrs('value')[0]
self.assertIsNone(get_cache_data) response = HttpResponse() content = 'Check for cache with QUERY_STRING' response.content = content UpdateCacheMiddleware().process_response(request, response) get_cache_data = FetchFromCacheMiddleware().process_request(request) self.assertIsNotNone(get_cache_data) self.assertEqual(get_cache_data.content, content.encode()) request = self.factory.get(self.path, {'foo': 'bar', 'somethingelse': 'true'}) request._cache_update_cache = True get_cache_data = FetchFromCacheMiddleware().process_request(request) self.assertIsNone(get_cache_data)
sparse_results = sparse_classifier.predict_log_proba(X_test_sparse) dense_results = dense_classifier.predict_log_proba(X_test) assert_array_equal(sparse_results, dense_results)
config = sconfig.master_config(master_config)
ctx = self.cf.option_context(option_name, context_value) self.assertEqual(self.cf.get_option(option_name), original_value)
cs = CourseStructure.objects.get(course_id=self.course.id) self.assertEqual(cs.structure_json, structure_json)
new_module_store_setting = { "default": { "ENGINE": "xmodule.modulestore.mixed.MixedModuleStore", "OPTIONS": { "mappings": {}, "stores": [] } } }
action = False check_name = None if name != 'allprofiles': check_name = True
self.assertTrue(self.video.select_language('zh'))
answer_objs = SurveyAnswer.objects.filter( user=self.student, form=self.survey )
filterspec = changelist.get_filters(request)[0][0] self.assertEqual(force_text(filterspec.title), 'book') choice = select_by(filterspec.choices(changelist), "display", self.bio_book.title) self.assertEqual(choice['selected'], True) self.assertEqual(choice['query_string'], '?books_authored__id__exact=%d' % self.bio_book.pk)
mlp = MLP(layers=[Linear(layer_name='h', dim=5, irange=0.01)]) conditional = DummyConditional(mlp=mlp, name='conditional') vae = DummyVAE() conditional.set_vae(vae) conditional.initialize_parameters(input_space=VectorSpace(dim=5), ndim=5) numpy.testing.assert_equal(conditional.get_weights(), mlp.get_weights())
def root_attributes(self): attrs = super(MyCustomAtom1Feed, self).root_attributes() attrs['django'] = 'rocks' return attrs def add_root_elements(self, handler): super(MyCustomAtom1Feed, self).add_root_elements(handler) handler.addQuickElement('spam', 'eggs') def item_attributes(self, item): attrs = super(MyCustomAtom1Feed, self).item_attributes(item) attrs['bacon'] = 'yum' return attrs def add_item_elements(self, handler, item): super(MyCustomAtom1Feed, self).add_item_elements(handler, item) handler.addQuickElement('ministry', 'silly walks')
if 'profile' in __opts__: __opts__['internal_lxc_profile'] = __opts__['profile'] del __opts__['profile']
return self._data.view(np.ndarray)
l = len(tokens) mapped_tokens = [] i = 0 while i < l: token = tokens[i] if token == "is" and i + 1 < l and tokens[i + 1] == "not": token = "is not"
try: from libcloud.compute.types import Provider from libcloud.compute.providers import get_driver from libcloud.loadbalancer.types import Provider as Provider_lb from libcloud.loadbalancer.providers import get_driver as get_driver_lb from libcloud.common.google import ( ResourceInUseError, ResourceNotFoundError, ) import libcloud.security libcloud.security.CA_CERTS_PATH.append('/etc/ssl/certs/YaST-CA.pem') HAS_LIBCLOUD = True except ImportError: HAS_LIBCLOUD = False
w = clf.coef_[0] a = -w[0] / w[1] xx = np.linspace(-5, 5) yy = a * xx - (clf.intercept_[0]) / w[1]
self._verify_unit_warning( self.UnitState(is_released=True, publish_state=self.PublishState.NEVER_PUBLISHED, is_locked=False), self.NEVER_PUBLISHED_WARNING )
mock_get_scores.assert_called_with( self.course.id.to_deprecated_string(), anonymous_id_for_user(self.student_user, self.course.id) )
for idxr, getitem in [(lambda x: x.ix, False), (lambda x: x.iloc, False), (lambda x: x, True)]:
X = iris.data[45:100, :] y = iris.target[45:100] solvers = ("lbfgs", "newton-cg", "liblinear") class_weight_dict = _compute_class_weight_dictionary(y)
soup = BeautifulSoup(response.content) pay_and_verify_div = soup.find(id="pay-and-verify-container") self.assertIsNot( pay_and_verify_div, None, msg=( "Could not load pay and verify flow data. " "Maybe this isn't the pay and verify page?" ) ) return { 'full_name': pay_and_verify_div['data-full-name'], 'course_key': pay_and_verify_div['data-course-key'], 'course_name': pay_and_verify_div['data-course-name'], 'course_start_date': pay_and_verify_div['data-course-start-date'], 'courseware_url': pay_and_verify_div['data-courseware-url'], 'course_mode_name': pay_and_verify_div['data-course-mode-name'], 'course_mode_slug': pay_and_verify_div['data-course-mode-slug'], 'display_steps': json.loads(pay_and_verify_div['data-display-steps']), 'current_step': pay_and_verify_div['data-current-step'], 'requirements': json.loads(pay_and_verify_div['data-requirements']), 'message_key': pay_and_verify_div['data-msg-key'], 'contribution_amount': pay_and_verify_div['data-contribution-amount'], 'verification_deadline': pay_and_verify_div['data-verification-deadline'] }
self.assertEqual(len(self.ts1), 30) self.assertEqual(len(self.ts2), 25)
self.assertSetEqual( set(course_action_state.course_key for course_action_state in expected), set(course_action_state.course_key for course_action_state in found))
url = reverse('reset_student_attempts_for_entrance_exam', kwargs={'course_id': unicode(self.course.id)}) response = self.client.get(url, { 'unique_student_identifier': self.student.email, 'all_students': True, }) self.assertEqual(response.status_code, 400)
self.steps = tosequence(steps) transforms = estimators[:-1] estimator = estimators[-1]
pass
from __future__ import unicode_literals
super(ViewInStudioTest, self).setUp() self.staff_user = GlobalStaffFactory.create() self.request = RequestFactory().get('/') self.request.user = self.staff_user self.request.session = {} self.module = None self.default_context = {'bookmarked': False, 'username': self.user.username}
('started', 'started'),
return prlctl('snapshot', args, runas=runas)
ret['ping_status'] = bool(len(done))
_write_file_network(network, _RH_NETWORK_FILE)
if np.isneginf(max_log_prob) and self.n_iter: raise RuntimeError( "EM algorithm was never able to compute a valid likelihood " + "given initial parameters. Try different init parameters " + "(or increasing n_init) or check for degenerate data.")
msg = u"Enrollment mode mismatch: active mode={}, requested mode={}. Won't deactivate.".format( enrollment["mode"], mode ) log.warning(msg) return Response(status=status.HTTP_400_BAD_REQUEST, data={"message": msg})
self.verify_page_header( title='Edit Team', description='If you make significant changes, make sure you notify ' 'members of the team before making these changes.', breadcrumbs='All Topics {topic_name} {team_name}'.format( topic_name=self.topic['name'], team_name=self.team['name'] ) )
corrupted = self.corruptor(inputs) return (self.reconstructX(corrupted), self.reconstructY(corrupted))
V = rng.rand(n_features, n_features) VI = np.dot(V, V.T)
if dtype is not None: for col, my_type in dtype.items(): if not isinstance(my_type, str): raise ValueError('%s (%s) not a string' % ( col, str(my_type))) table = SQLiteTable(name, self, frame=frame, index=index, if_exists=if_exists, index_label=index_label, dtype=dtype) table.create() table.insert(chunksize)
axes = ('c', 0, 1, 'b') input_space = Conv2DSpace((3, 3), 16, axes=axes) output_space = Conv2DSpace((3, 3), 16, axes=axes) num_nonzero = 2 kernel_shape = (2, 2) conv2d = make_sparse_random_conv2D(num_nonzero, input_space, output_space, kernel_shape) f = theano.function([self.image_tensor], conv2d.lmul(self.image_tensor)) assert f(self.image).shape == (16, 2, 2, 1) assert conv2d.output_axes == axes assert numpy.count_nonzero(conv2d._filters.get_value()) >= 32
return self.kernel.is_stationary()
for seg in xrange(count): name = 'random_{0:002}'.format(seg) seg_id = mailchimp.listStaticSegmentAdd(id=list_id, name=name) for batch in chunk(chunks[seg], BATCH_SIZE): mailchimp.listStaticSegmentMembersAdd( id=list_id, seg_id=seg_id, batch=batch )
from salttesting.helpers import ensure_in_syspath
cv = (rng.rand() + 1.0) ** 2 samples = mixture.sample_gaussian( mu, cv, covariance_type='spherical', n_samples=n_samples)
username = "test_{uuid}".format(uuid=self.unique_id[0:6]) auto_auth_page = AutoAuthPage(self.browser, username=username).visit() user_id = auto_auth_page.get_user_id() return username, user_id
self.selenium.find_element_by_id('id_relatedprepopulated_set-0-pubdate').send_keys('2011-12-17') self.get_select_option('#id_relatedprepopulated_set-0-status', 'option one').click() self.selenium.find_element_by_id('id_relatedprepopulated_set-0-name').send_keys( ' here is a sŤāÇkeð inline ! ' ) slug1 = self.selenium.find_element_by_id('id_relatedprepopulated_set-0-slug1').get_attribute('value') slug2 = self.selenium.find_element_by_id('id_relatedprepopulated_set-0-slug2').get_attribute('value') self.assertEqual(slug1, 'here-stacked-inline-2011-12-17') self.assertEqual(slug2, 'option-one-here-stacked-inline')
cshp = self.col_shape() assert type(cshp) == tuple if T: ss = len(cshp) RR, CC = xshp[ss:], xshp[:ss] else: ss = len(xshp) - len(cshp) RR, CC = xshp[:ss], xshp[ss:] if len(CC) != len(cshp) or ( not all((isinstance(cc, theano.Variable) or cc == ci) for cc, ci in zip(CC, cshp))): raise ValueError('invalid left shape', dict(xshp=xshp, col_shape=cshp, xcols=CC, T=T)) if T: return CC, RR else: return RR, CC
if seed is None or seed is np.random: return np.random.mtrand._rand if isinstance(seed, (numbers.Integral, np.integer)): return np.random.RandomState(seed) if isinstance(seed, np.random.RandomState): return seed raise ValueError('%r cannot be used to seed a numpy.random.RandomState' ' instance' % seed)
try: socket.inet_aton(ip_str) except Exception: try: socket.inet_pton(socket.AF_INET6, ip_str) except Exception: self.fail("{0} is not a valid IP address".format(ip_str))
pass
self.make_course(pdf_textbooks=[PDF_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('pdf_book', book_index=0, chapter='fooey', page='xyzzy')
on_course_publish(course_key)
value = value.toPyObject()
'openedx.core.djangoapps.common_views',
import yaml
df = pd.DataFrame({'a': [pd.Timestamp('2011-01-01'), pd.Timestamp('2011-01-02')], 'b': [pd.Timestamp('2011-01-01', tz='US/Eastern'), pd.Timestamp('2011-01-02', tz='US/Eastern')], 'c': [pd.Timedelta('1 days'), pd.Timedelta('2 days')], 'd': [pd.Period('2011-01-01', freq='M'), pd.Period('2011-01-02', freq='M')]})
if name in self.quote_cache: return self.quote_cache[name] if ((name in self.query.alias_map and name not in self.query.table_map) or name in self.query.extra_select or ( name in self.query.external_aliases and name not in self.query.table_map)): self.quote_cache[name] = name return name r = self.connection.ops.quote_name(name) self.quote_cache[name] = r return r
import integration import salt.utils
from salt.exceptions import CommandExecutionError import salt.utils
if microsite_org: return courses
memcacheoptions = (host, port)
from __future__ import absolute_import
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'gce', vm_['profile'], vm_=vm_) is False: return False
def argpartition(a, kth, axis=-1, kind='introselect', order=None): return np.argsort(a, axis=axis, order=order)
self.assertFalse((var_x > 0).any().any()) expected = x * np.nan expected[count_x >= max(min_periods, 1)] = 0. if var is var_unbiased: expected[count_x < 2] = np.nan assert_equal(var_x, expected)
recons_data = DataFrame(test_data).to_dict("i")
myiterator = d.iterator(mode=i, batch_size=b, num_batches=n, data_specs=self._flat_data_specs, return_tuple=True, rng=sd)
assert_not_none( self.draft_store.get_item(Location('edX', 'test_unicode', '2012_Fall', 'video', 'Welcome')), ) assert_not_none( self.draft_store.get_item(Location('edX', 'test_unicode', '2012_Fall', 'video', 'Welcome')), ) assert_not_none( self.draft_store.get_item(Location('edX', 'test_unicode', '2012_Fall', 'chapter', 'Overview')), )
values = ['2014', '2013/02', '2013/01/02', '2013/02/01 9H', '2013/02/01 09:00'] for v in values: if _np_version_under1p9: with tm.assertRaises(ValueError): idx[v] else: continue
mixed = Series(['aBAD_BAD', NA, 'BAD_b_BAD', True, datetime.today(), 'foo', None, 1, 2.])
iris = load_iris() perm = rng.permutation(iris.target.size) iris.data = iris.data[perm] iris.target = iris.target[perm]
for element in t.nodes[:-1]: self._dispatch(element) self._write(", ")
CourseFactory.create(emit_signals=True)
return 1 / numpy.sin(arg)
pass
for library in NPM_INSTALLED_LIBRARIES: sh('/bin/cp -rf node_modules/{library} {vendor_dir}'.format( library=library, vendor_dir=NPM_VENDOR_DIRECTORY, ))
#today = '' #today_fmt = '%B %d, %Y'
self.assertContains(self.response, "<h3>Methods with arguments</h3>") self.assertContains(self.response, "<td>rename_company</td>") self.assertContains(self.response, "<td>dummy_function</td>") self.assertContains(self.response, "<td>suffix_company_name</td>")
super(TestCourseListing, self).setUp() self.student = UserFactory() self.teacher = UserFactory() GlobalStaff().add_users(self.teacher) self.client = Client() self.client.login(username=self.teacher.username, password='test')
df = self.tsframe.fillna(0).astype(int) df.cumprod(0) df.cumprod(1)
try: path, res = self._choose_path(fast_path, slow_path, group) except TypeError: return self._transform_item_by_item(obj, fast_path) except ValueError: msg = 'transform must return a scalar value for each group' raise ValueError(msg)
return True
rs = np.random.RandomState(1999) n_samples = 20 n_features = 10 X = rs.randn(n_samples, n_features)
if not __opts__.get('state_verbose', False) and \ ret['result'] and not schanged: continue
content_type = models.ForeignKey(ContentType, models.CASCADE) object_id = models.PositiveIntegerField() content_object = GenericForeignKey() url = models.URLField() description = models.CharField(max_length=100, blank=True) keywords = models.CharField(max_length=100, blank=True) def __str__(self): return self.url
request_task_id = _get_current_task().request.id if task_id != request_task_id: fmt = u'{task_info}, Requested task did not match actual task "{actual_id}"' message = fmt.format(task_info=task_info_string, actual_id=request_task_id) TASK_LOG.error(message) raise ValueError(message)
is_enrolled = CourseEnrollment.is_enrolled(self.user, self.course.id) self.assertTrue(is_enrolled)
self.assertEqual(apps.get_model("apps", "TotallyNormal"), TotallyNormal) with self.assertRaises(LookupError): apps.get_model("apps", "SoAlternative") with self.assertRaises(LookupError): new_apps.get_model("apps", "TotallyNormal") self.assertEqual(new_apps.get_model("apps", "SoAlternative"), SoAlternative)
boto_secgroup.create(group_name, group_description, vpc_id=vpc_id, **conn_parameters) conn = boto.ec2.connect_to_region(region, **boto_conn_parameters) group_filter = {'group-name': group_name, 'vpc-id': vpc_id} secgroup_created_group = conn.get_all_security_groups(filters=group_filter) expected_create_result = [group_name, group_description, vpc_id] secgroup_create_result = [secgroup_created_group[0].name, secgroup_created_group[0].description, secgroup_created_group[0].vpc_id] self.assertEqual(expected_create_result, secgroup_create_result)
if refresh: refresh_db()
self.assertEqual(sanitize_separators('10.10'), '10.10')
if is_justify:
response = self.post_with_bearer_token('/oauth2-test/', token=self.refresh_token.token) self.check_error_codes( response, status_code=status.HTTP_401_UNAUTHORIZED, error_code=authentication.OAUTH2_TOKEN_ERROR_NONEXISTENT )
definition, children = cls.load_definition(definition_xml, runtime, def_id, id_generator)
import salt.utils from salt import utils, exceptions
add_url = reverse('admin_custom_urls:admin_custom_urls_action_add') self.assertTrue(add_url.endswith('/!add/')) response = self.client.get(add_url) self.assertIsInstance(response, TemplateResponse) self.assertEqual(response.status_code, 200)
rval = [] for i, component in enumerate(self.components): if self.routing_needed and i in self.components_to_inputs: cur_state_below =self.input_space.restrict_batch(state_below, self.components_to_inputs[i]) else: cur_state_below = state_below class RoutingLayer(object): def __init__(self, idx, layer): self.__dict__.update(locals()) del self.self self.layer_name = 'route_'+str(idx)+'_'+layer.layer_name def downward_message(self, state): return self.layer.downward_message(state)[self.idx] if layer_above is not None: cur_layer_above = RoutingLayer(i, layer_above) else: cur_layer_above = None sample = component.sample(state_below = cur_state_below, state_above = state_above, layer_above = cur_layer_above, theano_rng = theano_rng) rval.append(sample) return tuple(rval)
profile = UserProfile.objects.get(user=user) sn_empty = not identity.get('sn') given_name_empty = not identity.get('givenName') displayname_empty = not identity.get('displayName')
def natural_key(self): return (self.name,) natural_key.dependencies = ['fixtures_regress.store']
parse_coverage( Env.BOK_CHOY_REPORT_DIR, Env.BOK_CHOY_COVERAGERC )
data = { 'student-action': 'add', 'student-id': students[0].email, } response = self.client.post(url, data=data, follow=True) self.assertEqual(response.status_code, 200) self.assertTrue( CourseEnrollment.objects.filter(course_id=ccx_course_key, user=students[0]).exists() )
if directory is None and action != 'selfupdate': raise SaltInvocationError( 'The \'directory\' argument is required for composer.{0}'.format(action) )
module = ''
from __future__ import absolute_import, division, print_function import contextlib import copy import collections import datetime
with tm.assert_produces_warning(FutureWarning): assert_series_equal(s.duplicated(take_last=True), expected) with tm.assert_produces_warning(FutureWarning): assert_series_equal( s.drop_duplicates(take_last=True), s[~expected]) sc = s.copy() with tm.assert_produces_warning(FutureWarning): sc.drop_duplicates(take_last=True, inplace=True) assert_series_equal(sc, s[~expected])
return {}
url( r'^user/(?P<user_id>[^/]*)/course/{course_id}'.format(course_id=settings.COURSE_ID_PATTERN), views.render_html_view, name='html_view' ),
DEFAULTS = {'mongo.db': 'salt', 'mongo.host': 'salt', 'mongo.password': '', 'mongo.port': 27017, 'mongo.user': '', 'redis.db': '0', 'redis.host': 'salt', 'redis.port': 6379, 'test.foo': 'unconfigured', 'ca.cert_base_path': '/etc/pki', 'solr.cores': [], 'solr.host': 'localhost', 'solr.port': '8983', 'solr.baseurl': '/solr', 'solr.type': 'master', 'solr.request_timeout': None, 'solr.init_script': '/etc/rc.d/solr', 'solr.dih.import_options': {'clean': False, 'optimize': True, 'commit': True, 'verbose': False}, 'solr.backup_path': None, 'solr.num_backups': 1, 'poudriere.config': '/usr/local/etc/poudriere.conf', 'poudriere.config_dir': '/usr/local/etc/poudriere.d', 'ldap.uri': '', 'ldap.server': 'localhost', 'ldap.port': '389', 'ldap.tls': False, 'ldap.no_verify': False, 'ldap.anonymous': True, 'ldap.scope': 2, 'ldap.attrs': None, 'ldap.binddn': '', 'ldap.bindpw': '', 'hosts.file': '/etc/hosts', 'aliases.file': '/etc/aliases', 'virt.images': os.path.join(syspaths.SRV_ROOT_DIR, 'salt-images'), 'virt.tunnel': False, }
self.conn.put_method.return_value = method_ret self.conn.put_integration.return_value = method_integration_ret self.conn.put_method_response.side_effect = ClientError(error_content, 'put_method_response')
rng = np.random.RandomState(0) X1 = rng.normal(size=(10, 3)) y1 = (rng.normal(size=(10)) > 0).astype(np.int)
sha1 = hashlib.sha1() sha1.update(body) oauth_body_hash = unicode(base64.b64encode(sha1.digest())) params = client.get_oauth_params(None) params.append((u'oauth_body_hash', oauth_body_hash)) mock_request = mock.Mock( uri=unicode(urllib.unquote(url)), headers=headers, body=u"", decoded_body=u"", oauth_params=params, http_method=unicode(method), ) sig = client.get_oauth_signature(mock_request) mock_request.oauth_params.append((u'oauth_signature', sig)) new_headers = parameters.prepare_headers(mock_request.oauth_params, headers, realm=None) return new_headers['Authorization']
result = self.df.loc[['c', 'a']] expected = self.df.iloc[[4, 0, 1, 5]] assert_frame_equal(result, expected, check_index_type=True)
assert_panel_equal(result.to_pandas(), p)
if isinstance(e, SystemCheckError): self.stderr.write(str(e), lambda x: x) else: self.stderr.write('%s: %s' % (e.__class__.__name__, e)) sys.exit(1)
try: import pymongo version = pymongo.version version = '.'.join(version.split('.')[:2]) HAS_PYMONGO = True except ImportError: HAS_PYMONGO = False
salt '*' vsphere.vmotion_disable my.esxi.host root bad-password
transport = 'ssh' address = self.url
indexer = np.sort(np.concatenate([b.mgr_locs.as_array for b in blocks])) inv_indexer = lib.get_reverse_indexer(indexer, self.shape[0])
for s in [Series(np.arange(10)), Series(np.arange(10.))]: self.assertRaises(TypeError, lambda: frequencies.infer_freq(s))
df = tm.makeDataFrame() self.assertRaises(ValueError, df.to_hdf, path, 'df', append=True, format='f') self.assertRaises(ValueError, df.to_hdf, path, 'df', append=True, format='fixed')
request_json = { "role": "student", "user_partition_id": self.user_partition.id, "group_id": group.id if group is not None else None } request = self._create_mock_json_request( self.test_user, body=json.dumps(request_json), session=self.session ) handle_ajax(request, unicode(self.course.id))
self.event = event self.action = action
if ':' in path: module_path, _, method_path = path.rpartition(':') module = import_module(module_path) class_name, method_name = method_path.split('.') _class = getattr(module, class_name) function = getattr(_class, method_name) else: module_path, _, name = path.rpartition('.') function = getattr(import_module(module_path), name) return function
cache_add.return_value = False self.assertEqual(cache.get_or_set('key', 'default'), 'default')
self.check_html(self.widget, 'time', '13:12:11', html=( '<input type="text" name="time" value="13:12:11" />' ))
for i, s in enumerate(series): f = lambda a, b: getattr(a, op)(b, axis='index') _compare_to_dense(frame, s, frame.to_dense(), s.to_dense(), f)
correction = np.median(self.dist_) / chi2(data.shape[1]).isf(0.5) covariance_corrected = self.raw_covariance_ * correction self.dist_ /= correction return covariance_corrected
response = requests.get(self._get_url("api/v1/search"), params={ "user": "dummy-user-id", "usage_id": "dummy-usage-id", "course_id": "dummy-course-id", "text": "world war 2" })
"form-1-id": str(fd2.id), "form-1-reference": "456", "form-1-driver": "bill", "form-1-restaurant": "thai",
@xframe_options_sameorigin def a_view(request): return HttpResponse() r = a_view(HttpRequest()) self.assertEqual(r['X-Frame-Options'], 'SAMEORIGIN')
for user_partition in self.user_partitions: if user_partition.id == user_partition_id: return user_partition raise NoSuchUserPartitionError("could not find a UserPartition with ID [{}]".format(user_partition_id))
'CERTIFICATES_INSTRUCTOR_GENERATION': False,
def test_course_license_export(self): content_store = contentstore() root_dir = path(mkdtemp_clean()) self.course.license = "creative-commons: BY SA" self.store.update_item(self.course, None) export_course_to_xml(self.store, content_store, self.course.id, root_dir, 'test_license') fname = "{block}.xml".format(block=self.course.scope_ids.usage_id.block_id) run_file_path = root_dir / "test_license" / "course" / fname run_xml = etree.parse(run_file_path.open()) self.assertEqual(run_xml.getroot().get("license"), "creative-commons: BY SA") def test_video_license_export(self): content_store = contentstore() root_dir = path(mkdtemp_clean()) video_descriptor = ItemFactory.create( parent_location=self.course.location, category='video', license="all-rights-reserved" ) export_course_to_xml(self.store, content_store, self.course.id, root_dir, 'test_license') fname = "{block}.xml".format(block=video_descriptor.scope_ids.usage_id.block_id) video_file_path = root_dir / "test_license" / "video" / fname video_xml = etree.parse(video_file_path.open()) self.assertEqual(video_xml.getroot().get("license"), "all-rights-reserved") def test_license_import(self): course_items = import_course_from_xml( self.store, self.user.id, TEST_DATA_DIR, ['toy'], create_if_not_present=True ) course = course_items[0] self.assertEqual(course.license, "creative-commons: BY") videos = self.store.get_items(course.id, qualifiers={'category': 'video'}) self.assertEqual(videos[0].license, "all-rights-reserved")
user_partititons = self.course.user_partitions self.assertEqual(len(user_partititons), 1) self.assertEqual(len(user_partititons[0].groups), 3) self.assertEqual(user_partititons[0].groups[1].name, 'Group B')
self.xml_data = "about page 463139"
submissions_score_reset_handler(None, **SUBMISSION_RESET_KWARGS) self.get_user_mock.assert_called_once_with('anonymous_id')
if self.backward: self.setup_response_backward() return
print('Loading van Hateren images') n_images = 50 vh = skdata.vanhateren.dataset.Calibrated(n_images) patches = vh.raw_patches((self.n_patches,) + self.imshp, items=vh.meta[:n_images], rng=np.random.RandomState(123), ) patches = patches.astype('float32') patches /= patches.reshape(self.n_patches, self.imshp[0] * self.imshp[1])\ .max(axis=1)[:, None, None]
_check_all_orients(self.categorical, sort='sort', raise_ok=ValueError)
import os import logging from salt.ext.six import string_types import salt.ext.six as six from salt.ext.six.moves import zip
LogoutPage(self.browser).visit()
mock_snap = MagicMock(return_value='') with patch.object(parallels, 'prlctl', mock_snap): parallels.snapshot(name) mock_snap.assert_called_once_with('snapshot', [name], runas=None)
return self.q(css='#upload_error').text[0]
for param in self.params: value = param.get_value(borrow=True) if not isfinite(value): raise RuntimeError("NaN in " + param.name)
import os import logging
score = sub_api.get_score(student_item) self.assertIs(score, None)
raise PermissionDenied()
ssh_args.extend([ '-oPasswordAuthentication=no', '-oChallengeResponseAuthentication=no', '-oPubkeyAuthentication=yes', '-oIdentitiesOnly=yes', '-oKbdInteractiveAuthentication=no', '-i {0}'.format(kwargs['key_filename']) ])
self.staff_level_endpoints = [ ('students_update_enrollment', {'identifiers': 'foo@example.org', 'action': 'enroll'}), ('get_grading_config', {}), ('get_students_features', {}), ('get_student_progress_url', {'unique_student_identifier': self.user.username}), ('reset_student_attempts', {'problem_to_reset': self.problem_urlname, 'unique_student_identifier': self.user.email}), ('update_forum_role_membership', {'unique_student_identifier': self.user.email, 'rolename': 'Moderator', 'action': 'allow'}), ('list_forum_members', {'rolename': FORUM_ROLE_COMMUNITY_TA}), ('send_email', {'send_to': '["staff"]', 'subject': 'test', 'message': 'asdf'}), ('list_instructor_tasks', {}), ('list_background_email_tasks', {}), ('list_report_downloads', {}), ('list_financial_report_downloads', {}), ('calculate_grades_csv', {}), ('get_students_features', {}), ('get_enrollment_report', {}), ('get_students_who_may_enroll', {}), ('get_exec_summary_report', {}), ('get_proctored_exam_results', {}), ('get_problem_responses', {}), ('export_ora2_data', {}), ] self.instructor_level_endpoints = [ ('bulk_beta_modify_access', {'identifiers': 'foo@example.org', 'action': 'add'}), ('modify_access', {'unique_student_identifier': self.user.email, 'rolename': 'beta', 'action': 'allow'}), ('list_course_role_members', {'rolename': 'beta'}), ('rescore_problem', {'problem_to_reset': self.problem_urlname, 'unique_student_identifier': self.user.email}), ]
yaml_parse = None control = None cuda = None
L = np.zeros((max_features, max_features), dtype=Gram.dtype)
id_range = get_object_range(page_number, page_size) db_objects = [build_mock_object(obj_id) for obj_id in id_range] self.mock_model.objects.filter = MagicMock(return_value=db_objects) page = paginate_search_results(self.mock_model, self.search_results, page_size, page_number) self.mock_model.objects.filter.assert_called_with(pk__in=id_range) self.assertEquals(db_objects, page.object_list) self.assertTrue(page.number, page_number) self.assertEquals(page.has_next(), has_next)
profile_match = \ __salt__['config.get']( 'lxc.{1}:{0}'.format(name, old_key), None) if profile_match is not None: salt.utils.warn_until( 'Carbon', 'lxc.{1} has been deprecated, please configure LXC ' 'container profiles under lxc.{0} instead'.format( key, old_key)) else: profile_match = {}
final_field = field targets = (field,) if fail_on_missing and pos + 1 != len(names): raise FieldError( "Cannot resolve keyword %r into field. Join on '%s'" " not permitted." % (names[pos + 1], name)) break
values = Series(['fooBAD__barBAD', NA, 'foo'])
expected_msg = u'Error: ȧƈƈḗƞŧḗḓ ŧḗẋŧ ƒǿř ŧḗşŧīƞɠ' self.assertEqual(expected_msg, result['success'])
if (ip in current['Network Settings']['IP_ADDRESS']['VALUE'] and netmask in current['Network Settings']['SUBNET_MASK']['VALUE'] and gateway in current['Network Settings']['GATEWAY_IP_ADDRESS']['VALUE']): return True
url(r'^gradebook$', 'instructor.views.gradebook_api.spoc_gradebook', name='spoc_gradebook'),
if source: cmd = '{0} -s {1}'.format(cmd, source)
pass
mock = MagicMock(return_value=True) with patch.object(composer, '_valid_composer', mock): mock = MagicMock(return_value={'retcode': 0, 'stderr': 'A'}) with patch.dict(composer.__salt__, {'cmd.run_all': mock}): self.assertTrue(composer.install('dir', None, None, None, None, None, None, None, None, None, True))
self.assertEqual(account_settings, { 'username': self.USERNAME, 'email': self.EMAIL, 'name': u'', 'gender': None, 'goals': None, 'is_active': False, 'level_of_education': None, 'mailing_address': None, 'year_of_birth': None, 'country': None, 'bio': None, 'profile_image': { 'has_image': False, 'image_url_full': request.build_absolute_uri('/static/default_50.png'), 'image_url_small': request.build_absolute_uri('/static/default_10.png'), }, 'requires_parental_consent': True, 'language_proficiencies': [], 'account_privacy': PRIVATE_VISIBILITY, 'accomplishments_shared': False, })
values[values == -1] = get_base_missing_value(dtype) data_formatted.append((col, values, index))
values = self.get_param_values() values = [value.reshape(value.size) for value in values] return np.concatenate(values, axis=0)
select_element = page.q(css=selector) self.assertTrue(select_element.is_present()) return [option.text for option in Select(select_element[0]).options]
result = f.clean('21-12-2010') self.assertEqual(result, date(2010, 12, 21))
model.fit(X, y) n_iter_cold_start = model.n_iter_ assert_equal(n_iter_cold_start, n_iter_reference)
with self.assertNumQueries(3): redwood = House.objects.prefetch_related('rooms__fleas').get(name='Redwood') with self.assertNumQueries(0): self.assertEqual(3, len(redwood.rooms.all()[0].fleas.all()))
if not name == '/': name = name.rstrip('/')
url = reverse('reset_due_date', kwargs={'course_id': self.course.id.to_deprecated_string()}) response = self.client.get(url, { 'student': self.user1.username, 'url': self.week1.location.to_deprecated_string(), }) self.assertEqual(response.status_code, 200, response.content) self.assertEqual( None, get_extended_due(self.course, self.week1, self.user1) )
from __future__ import absolute_import import logging
pred = lambda i: not _int64_overflow_possible(shape[:i]) nlev = next(filter(pred, range(len(shape), 0, -1)))
filename = self.capa_system.ajax_url.split('/')[-1] + '.js' self.display_filename = 'compiled/' + filename
if not self.remote_field.is_hidden() and not related.related_model._meta.swapped: setattr(cls, related.get_accessor_name(), ManyToManyDescriptor(self.remote_field, reverse=True))
from salt.modules import locate
df2.columns = ['B', 'B'] with tm.assertRaises(ValueError): df1.isin(df2)
if len(i8) and self.is_monotonic: if i8[0] != tslib.iNaT: return self._box_func(i8[0])
if field_name not in self.fields: raise GDALException('invalid field name: %s' % field_name) return [feat.get(field_name) for feat in self]
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, self.course.id): with check_mongo_calls(3): self.store.get_course(self.course.id, depth=2)
world.css_click(".mce-i-none")
return fnd
log.warn( "group not found in CohortPartitionScheme: %r", { "requested_partition_id": user_partition.id, "requested_group_id": group_id, "cohort_id": cohort.id, }, exc_info=True ) return None
new_state = project_state.clone() operation.state_forwards("test_crmomm", new_state) self.assertTableNotExists("test_crmomm_stable_ponies") with connection.schema_editor() as editor: operation.database_forwards("test_crmomm", editor, project_state, new_state) self.assertTableExists("test_crmomm_stable") self.assertTableExists("test_crmomm_stable_ponies") self.assertColumnNotExists("test_crmomm_stable", "ponies") with atomic(): Pony = new_state.apps.get_model("test_crmomm", "Pony") Stable = new_state.apps.get_model("test_crmomm", "Stable") stable = Stable.objects.create() p1 = Pony.objects.create(pink=False, weight=4.55) p2 = Pony.objects.create(pink=True, weight=5.43) stable.ponies.add(p1, p2) self.assertEqual(stable.ponies.count(), 2) stable.ponies.all().delete() with connection.schema_editor() as editor: operation.database_backwards("test_crmomm", editor, new_state, project_state) self.assertTableNotExists("test_crmomm_stable") self.assertTableNotExists("test_crmomm_stable_ponies")
return ModuleStoreEnum.Type.mongo
_SQL_WILDCARD = { 'mysql': '%s', 'sqlite': '?' }
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
if not url.startswith('/'): url = '/' + url site_id = get_current_site(request).id try: f = get_object_or_404(FlatPage, url=url, sites=site_id) except Http404: if not url.endswith('/') and settings.APPEND_SLASH: url += '/' f = get_object_or_404(FlatPage, url=url, sites=site_id) return HttpResponsePermanentRedirect('%s/' % request.path) else: raise return render_flatpage(request, f)
self.assertContains(response, self.MIN_GRADE_REQ_DISPLAY) self.assertContains(response, self.VERIFICATION_REQ_DISPLAY) self.assertContains(response, "Upcoming") self.assertContains( response, "{}, you have not yet met the requirements for credit".format(self.USER_FULL_NAME) )
rs = mi_int.iloc[0] xp = mi_int.ix[4].ix[8] assert_series_equal(rs, xp, check_names=False) self.assertEqual(rs.name, (4, 8)) self.assertEqual(xp.name, 8)
elif isinstance(err, string_types): evalues = self.data[err].values self.data = self.data[self.data.columns.drop(err)] err = np.atleast_2d(evalues) err = np.tile(err, (self.nseries, 1))
if __grains__['os'] == 'FreeBSD': return __virtualname__ return (False, 'The freebsdservice execution module cannot be loaded: only available on FreeBSD systems.')
for (suffix, mode, kind) in SUFFIXES: self.suffix_map[suffix] = (suffix, mode, kind) suffix_order.append(suffix)
self.Ut = numpy.dot(self.V[:,-self.n_eigen:].transpose(), self.Xt)
self.__restore_sysctl()
response = self._get_page("verify_student_verify_now", course.id) self.assertNotContains(response, "Verification is no longer available")
if is_nested_renamer: result = list(_agg(arg, _agg_1dim).values())
start, stop, step = key.start, key.stop, key.step
internal_result = self.check_formula(ans1, ans2, self.samples) return internal_result == "correct"
note = self.filter_by_id(self.notes, note_id) if note: note[0].update(note_info) return note else: return None
from __future__ import absolute_import
transform = AdditiveChi2Sampler(sample_steps=4) assert_raises(ValueError, transform.fit, X)
entry = InstructorTask.objects.get(id=task_entry.id) status = json.loads(entry.task_output) self.assertEquals(status.get('attempted'), succeeded + failed) self.assertEquals(status.get('succeeded'), succeeded) self.assertEquals(status.get('skipped'), skipped) self.assertEquals(status.get('failed'), failed) self.assertEquals(status.get('total'), total) self.assertEquals(status.get('action_name'), action_name) self.assertGreater(status.get('duration_ms'), 0) self.assertEquals(entry.task_state, SUCCESS) self._assert_single_subtask_status(entry, succeeded, failed, skipped, retried_nomax, retried_withmax) return entry
lower = node.lower if lower is not None: lower = self.visit(lower).value upper = node.upper if upper is not None: upper = self.visit(upper).value step = node.step if step is not None: step = self.visit(step).value return slice(lower, upper, step)
self.assertTrue(acid_block.init_fn_passed) self.assertTrue(acid_block.resource_url_passed) self.assertTrue(acid_block.scope_passed('user_state')) self.assertTrue(acid_block.scope_passed('user_state_summary')) self.assertTrue(acid_block.scope_passed('preferences')) self.assertTrue(acid_block.scope_passed('user_info'))
self.assertIsNone(df.is_copy) df['letters'] = df['letters'].apply(str.lower) self.assertIsNone(df.is_copy)
from __future__ import absolute_import
if url_params: url_params = urlencode(url_params) response = self.get_response(url_params) if expected_response_code == 200: self.assertContains(response, self.html_block.data, status_code=expected_response_code) for chrome_element in [self.COURSEWARE_CHROME_HTML_ELEMENTS + self.XBLOCK_REMOVED_HTML_ELEMENTS]: self.assertNotContains(response, chrome_element) else: self.assertNotContains(response, self.html_block.data, status_code=expected_response_code) return response
raise nose.SkipTest('unreliable test, receive partial components back for nasdaq_100')
add_master_course_staff_to_ccx(self.course, self.ccx_locator, self.ccx.display_name) self.assertEqual(len(outbox), len(list_staff_master_course) + len(list_instructor_master_course)) with ccx_course(self.ccx_locator) as course_ccx: list_staff_ccx_course = list_with_level(course_ccx, 'staff') list_instructor_ccx_course = list_with_level(course_ccx, 'instructor') self.assertEqual(len(list_staff_master_course), len(list_staff_ccx_course)) for user in list_staff_master_course: self.assertIn(user, list_staff_ccx_course) self.assertEqual(len(list_instructor_master_course), len(list_instructor_ccx_course)) for user in list_instructor_master_course: self.assertIn(user, list_instructor_ccx_course)
if which_norb == 'big': self.label_index_to_name = (self.label_index_to_name +
return self.nbytes
if os_family == 'FreeBSD': return
self._create_students(num_emails - 1)
lhs = DataFrame(randn(5, 2)) if self.engine == 'python': with tm.assertRaises(TypeError): result = pd.eval(expr, engine=self.engine, parser=self.parser) else: expect = lhs result = pd.eval(expr, engine=self.engine, parser=self.parser) assert_frame_equal(expect, result)
elif value["version"] >= 2: if "scheme" not in value: raise TypeError("UserPartition dict {0} missing value key 'scheme'".format(value))
for lhs in self.objects: for rhs in self.objects: if lhs is rhs: continue self.assertNotEqual(lhs, rhs)
try: return int(value) except (ValueError, TypeError): return 0
log.warning( "The required '{0}' configuration setting is missing " "from the '{1}' driver, which is configured under the " "'{2}' alias.".format(key, provider, alias) ) return False
return sum(self.maxpoints.values())
import integration
self.wait_for_field(field_id) query = self.q(css='.u-field-link-title-{}'.format(field_id)) return query.text[0] if query.present else None
exp_idx = Index([2, 2, 1, 1], dtype='int64', name='idx') expected = Series([0.2, 0.2, 0.1, 0.1], index=exp_idx, name='s') result = ser.iloc[[1, 1, 0, 0]] assert_series_equal(result, expected, check_index_type=True)
if not HAS_BOTO: return (False, 'The boto_s3_bucket module could not be loaded: ' 'boto libraries not found') elif _LooseVersion(boto3.__version__) < _LooseVersion(required_boto3_version): return (False, 'The boto_cognitoidentity module could not be loaded: ' 'boto version {0} or later must be installed.'.format(required_boto3_version)) else: return True
modified_cohort_name = "renamed random cohort" default_cohort.name = modified_cohort_name default_cohort.save()
patcher = patch('config_models.models.cache', Mock(get=Mock(return_value=None))) patcher.start() self.addCleanup(patcher.stop)
elif not __salt__['file.is_fifo'](name): if __opts__['test']: ret['comment'] = 'Fifo pipe {0} is set to be created'.format( name ) ret['result'] = None else: ret = __salt__['file.mknod'](name, ntype, major, minor, user, group, mode)
allowed_tags = ['div', 'p', 'audio', 'pre', 'span'] for tag in allowed_tags: queue_msg = "<{0}>Test message</{0}>".format(tag) state = { 'input_state': {'queue_msg': queue_msg}, 'status': 'queued', } elt = etree.fromstring(self.xml) the_input = self.input_class(test_capa_system(), elt, state) self.assertEqual(the_input.queue_msg, queue_msg)
est = FOREST_ESTIMATORS[name](oob_score=True, random_state=0, n_estimators=n_estimators, bootstrap=True) n_samples = X.shape[0] est.fit(X[:n_samples // 2, :], y[:n_samples // 2]) test_score = est.score(X[n_samples // 2:, :], y[n_samples // 2:])
df = getattr(self, method)(StringIO(data), sep=',') self.assertEqual(list(df.columns), ['A', 'A.1', 'B', 'B.1', 'B.2'])
self._click_button('advanced_tab')
def __get_s3_meta(): return s3.query( key=creds.key, keyid=creds.keyid, kms_keyid=creds.kms_keyid, bucket=creds.bucket, service_url=creds.service_url, verify_ssl=creds.verify_ssl, location=creds.location, return_bin=False, params={'prefix': prefix})
ret = salt.modules.vsphere.system_info(host=host, username=user, password=password)
spec_path = os.path.join(opts.build_dir, 'SPECS', 'salt.spec') with open(opts.spec_file, 'r') as spec: spec_lines = spec.read().splitlines() with open(spec_path, 'w') as fp_: for line in spec_lines: if line.startswith('%global srcver '): line = '%global srcver {0}'.format(salt_srcver) elif line.startswith('Version: '): line = 'Version: {0}'.format(salt_pkgver) fp_.write(line + '\n')
return ['cuda_convnet', config.pthreads.lib] if config.pthreads.lib else ['cuda_convnet']
comment_host_created = 'Host {0} created.'.format(host) comment_host_updated = 'Host {0} updated.'.format(host) comment_host_notcreated = 'Unable to create host: {0}. '.format(host) comment_host_exists = 'Host {0} already exists.'.format(host) changes_host_created = {host: {'old': 'Host {0} does not exist.'.format(host), 'new': 'Host {0} created.'.format(host), } }
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.approve()
return ( CourseKey.from_string(course_key_or_id) if isinstance(course_key_or_id, basestring) else course_key_or_id )
#'wiki.plugins.notifications', 'course_wiki.plugins.markdownedx',
#language = None
obj = slice(start, stop, step) f = lambda x: x[obj] return _na_map(f, arr)
small_width = models.IntegerField(default=375) small_height = models.IntegerField(default=200)
self.assertEqual(command_output, "Installed 0 object(s) (of 2) from 1 fixture(s)")
expected_str = ' '.join([operand2, op, operand1]) self.assertTrue(expected_str in getattr(klass, 'r' + op_name).__doc__)
try: fileserver.init() except FileserverConfigError as exc: critical_errors.append('{0}'.format(exc))
default_headers = { 'HTTP_AUTHORIZATION': 'Bearer ' + self.access_token } default_headers.update(headers) response = self.client.get(uri, follow=True, **default_headers) return response
] DATETIME_INPUT_FORMATS = [
html_use_modindex = True
return StaticContent.compute_location(location.course_key, filename)
self.session._session_key = '' self.assertIsNone(self.session.session_key)
import logging
classes = [np.array([0, 1]), np.array([0, 1, 2])] class_probabilites = [np.array([1.0, 0.0]), np.array([0.0, 1.0, 0.0])]
return {key: val for key, val in self.session.cookies.items()}
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import destructiveTest
result = sql.read_sql_table('test_nan', self.conn) tm.assert_frame_equal(result, df)
float_items = [] complex_items = [] int_items = [] bool_items = [] object_items = [] sparse_items = [] datetime_items = [] datetime_tz_items = [] cat_items = [] extra_locs = []
from .numeric import Int64Index, Float64Index try: res = data.astype('i8') if (res == data).all(): return Int64Index(res, copy=copy, name=name) except (TypeError, ValueError): pass
locator = BlockUsageLocator( CourseLocator(org='testx', course='GreekHero', run="run", branch=BRANCH_NAME_DRAFT), 'course', 'head12345' ) block = modulestore().get_item(locator) children = block.get_children() expected_ids = [ "chapter1", "chapter2", "chapter3" ] for child in children: self.assertEqual(child.category, "chapter") self.assertIn(child.location.block_id, expected_ids) expected_ids.remove(child.location.block_id) self.assertEqual(len(expected_ids), 0)
import logging
cc_thread, context = _get_thread_and_context(request, thread_id) if can_delete(cc_thread, context): cc_thread.delete() thread_deleted.send(sender=None, user=request.user, post=cc_thread) else: raise PermissionDenied
for item in good_out: self.assertIn(item, ret_output)
html_static_path = ['images']
shiftedFrame = self.tsframe.shift(5) self.assert_index_equal(shiftedFrame.index, self.tsframe.index)
log.debug('Initializing new AsyncZeroMQReqChannel due to GC for {0}'.format(key)) new_obj = object.__new__(cls) new_obj.__singleton_init__(opts, **kwargs) loop_instance_map[key] = new_obj return loop_instance_map[key]
offset = len(parts) - 5 if len(parts) == 6: ret[vhost][name]['apply_to'] = parts[2] ret[vhost][name].update({ 'pattern': parts[offset+2], 'definition': parts[offset+3], 'priority': parts[offset+4] })
filtered_list = list(cls.FILTERED_LIST)
dist_scripts = self.distribution.scripts for script in self.filelist.files[:]: if not script.startswith('scripts/'): continue if script not in dist_scripts: self.filelist.files.remove(script) return Sdist.write_manifest(self)
ret_val = [] if hasattr(obj_iter, 'all'): obj_iter = obj_iter.all() try: iter(obj_iter) except TypeError: obj_iter = [obj_iter] for obj in obj_iter: rel_objs = [] for part in path: if not part: continue try: related = getattr(obj, part[0]) except ObjectDoesNotExist: continue if related is not None: rel_objs.extend(cls.traverse_qs(related, [part[1:]])) ret_val.append((obj, rel_objs)) return ret_val
res2 = clear_password(name, runas)
response = self.send_get( client=self.client, url=reverse('bookmarks') ) bookmarks_data = response.data['results'] self.assertEqual(len(bookmarks_data), 3) self.assert_bookmark_data_is_valid(self.other_bookmark_1, bookmarks_data[0]) self.assert_bookmark_data_is_valid(self.bookmark_2, bookmarks_data[1]) self.assert_bookmark_data_is_valid(self.bookmark_1, bookmarks_data[2])
import logging
from __future__ import absolute_import
if not request.user.is_authenticated(): raise PermissionDenied qs = UserPreference.objects.filter( user=request.user, key=NOTIFICATION_PREF_KEY ) return HttpResponse(json.dumps({"status": len(qs)}), content_type="application/json")
self.availables.value.add('alpha') self.availables.value.add('beta') self.aliveds.value['alpha'] = createStack('1.1.1.1') self.aliveds.value['beta'] = createStack('1.2.3.4') testStack = self.event_stack.value presenceReq = self.presence_req.value ryn = 'manor' presenceReq.append({'route': {'dst': (None, ryn, 'presence_req'), 'src': (None, testStack.local.name, None)}, 'data': {'state': 'available'}}) presenceReq.append({'route': {'dst': (None, ryn, 'presence_req'), 'src': (None, testStack.local.name, None)}}) presenceReq.append({'route': {'dst': (None, ryn, 'presence_req'), 'src': (None, testStack.local.name, None)}, 'data': {}}) presenceReq.append({'route': {'dst': (None, ryn, 'presence_req'), 'src': (None, testStack.local.name, None)}, 'data': {'state': None}}) presenceReq.append({'route': {'dst': (None, ryn, 'presence_req'), 'src': (None, testStack.local.name, None)}, 'data': {'state': 'present'}})
if maybeStart < 0: lines = (['\n'] * -maybeStart) + lines if len(lines) < context: lines += ['\n'] * (context - len(lines)) buf = list(records[i]) buf[LNUM_POS] = lnum buf[INDEX_POS] = lnum - 1 - start buf[LINES_POS] = lines records[i] = tuple(buf)
ret = postgres_user.present('foo', login=False, replication=False) self.assertEqual( ret, {'comment': 'User foo is already present', 'changes': {}, 'name': 'foo', 'result': True} ) self.assertEqual(SALT_STUB['postgres.user_update'].call_count, 0)
self.assertQuerysetEqual( ItalianRestaurant.objects.values("name", "rating"), [ {"rating": 4, "name": "Ristorante Miron"}, ], lambda o: o )
response = self.client.get(reverse('admin:admin_views_person_changelist')) self.assertNotEqual(response.context['cl'].list_editable, ()) response = self.client.get(reverse('admin:admin_views_person_changelist') + '?%s' % IS_POPUP_VAR) self.assertEqual(response.context['cl'].list_editable, ())
if value != value or value in (Decimal('Inf'), Decimal('-Inf')): raise ValidationError(self.error_messages['invalid'], code='invalid')
from django.conf import urls callback = getattr(urls, 'handler%s' % view_type)
self.assertEqual(self.loader._dict, {}) func = self.loader['test.ping'] func.__globals__['__context__']['foo'] = 'bar' self.assertEqual(self.loader['test.echo'].__globals__['__context__']['foo'], 'bar') self.assertEqual(self.loader['grains.get'].__globals__['__context__']['foo'], 'bar')
import contextlib import logging import hashlib import os import shutil import ftplib from tornado.httputil import parse_response_start_line, HTTPInputError
footer_style.append(('BACKGROUND', (1, 6), (1, 6), '#EEEEEE'))
other = _ensure_datetimelike_to_i8(other) values = _ensure_datetimelike_to_i8(self) result = np.where(cond, values, other).astype('i8') result = self._ensure_localized(result) return self._shallow_copy(result, **self._get_attributes_dict())
self.create_cohorted_discussions()
X = check_array(X) n_eval, _ = X.shape n_samples, n_features = self.X.shape n_samples_y, n_targets = self.y.shape
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, MagicMock, NO_MOCK, NO_MOCK_REASON
kwds.setdefault('c', plt.rcParams['patch.facecolor'])
operation = migrations.AlterIndexTogether("Pony", [("pink", "weight")]) self.assertEqual(operation.describe(), "Alter index_together for Pony (1 constraint(s))") new_state = project_state.clone() operation.state_forwards("test_alinto", new_state) self.assertEqual(len(project_state.models["test_alinto", "pony"].options.get("index_together", set())), 0) self.assertEqual(len(new_state.models["test_alinto", "pony"].options.get("index_together", set())), 1) self.assertIndexNotExists("test_alinto_pony", ["pink", "weight"]) with connection.schema_editor() as editor: operation.database_forwards("test_alinto", editor, project_state, new_state) self.assertIndexExists("test_alinto_pony", ["pink", "weight"]) with connection.schema_editor() as editor: operation.database_backwards("test_alinto", editor, new_state, project_state) self.assertIndexNotExists("test_alinto_pony", ["pink", "weight"]) definition = operation.deconstruct() self.assertEqual(definition[0], "AlterIndexTogether") self.assertEqual(definition[1], []) self.assertEqual(definition[2], {'name': "Pony", 'index_together': {("pink", "weight")}})
if hasattr(variable, 'name') and variable.name is not None: return variable.name return anon
expected = DataFrame(dict({'A': [0, 2, 4, 4], 'B': [1, 3, 5, 5]})) df = df_orig.copy() df.loc[3] = df.loc[2] assert_frame_equal(df, expected)
for ping_ret in self.ping_gen: if ping_ret is None: break m = next(ping_ret.iterkeys()) if m not in self.minions: self.minions.append(m) to_run.append(m)
target_user = self.get_user(email) target_user.click_delete() self.wait_for_page()
([], 'honor'),
tm.assertIsInstance(res, np.ndarray)
field_errors = self._validate_patch(request.data) if field_errors: return Response({'field_errors': field_errors}, status=status.HTTP_400_BAD_REQUEST) return self.partial_update(request, *args, **kwargs)
data = 'a b c\n1 2 3' msg = 'is not supported'
cart = Order.get_cart_for_user(user=self.user) self.assertFalse(CourseEnrollment.is_enrolled(self.user, self.course_key)) item = CertificateItem.add_to_order(cart, self.course_key, self.cost, 'honor') self.assertFalse(CourseEnrollment.is_enrolled(self.user, self.course_key)) with patch('sys.stderr', sys.stdout.write): cart.purchase() self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course_key))
USE_TZ = False
module = CapaFactory.create(max_attempts=0, done=False) self.assertTrue(module.should_show_save_button())
def test_account_register(self): with translation.override('en'): self.assertEqual(reverse('account:register'), '/en/account/register/') with translation.override('nl'): self.assertEqual(reverse('account:register'), '/nl/profiel/registeren/')
'grains': dict,
self.assertEqual(ShortMessage.objects.count(), 0) post_data = { "content": "What's this SMS thing?", "_save": "Save", } response = self.client.post(reverse('admin:admin_views_shortmessage_add'), post_data, follow=True) self.assertEqual(response.status_code, 200) self.assertEqual(ShortMessage.objects.count(), 1) pk = ShortMessage.objects.all()[0].pk self.assertContains( response, '<li class="success">The short message "<a href="%s">' 'ShortMessage object</a>" was added successfully.</li>' % reverse('admin:admin_views_shortmessage_change', args=(pk,)), html=True )
for topic_id in range(self.NUM_TOPICS): team = CourseTeamFactory.create( name=u"Team for topic {}".format(topic_id), course_id=self.course.id, topic_id=topic_id, )
if not salt.utils.is_proxy(): self.opts['grains'] = salt.loader.grains(opts)
f = File(BytesIO(b'one\rtwo\rthree')) self.assertEqual(list(f), [b'one\r', b'two\r', b'three'])
store = self._verify_modulestore_support(dest_key.course_key, 'copy_from_template') return store.copy_from_template(source_keys, dest_key, user_id)
grouped = self.tsframe.groupby([lambda x: x.weekday(), lambda x: x.year ])
import numpy as np import matplotlib.pyplot as plt from sklearn import neighbors
os.unlink(self._clear_filename) self.run_collectstatic(clear=True)
self.assertTrue( True not in [len(buildout.LOG.by_level[a]) > 0 for a in buildout.LOG.by_level]) buildout.LOG.info('foo') self.assertTrue( True in [len(buildout.LOG.by_level[a]) > 0 for a in buildout.LOG.by_level]) buildout.LOG.clear() self.assertTrue( True not in [len(buildout.LOG.by_level[a]) > 0 for a in buildout.LOG.by_level])
win_dns_client.__salt__ = {} win_dns_client.__context__ = {}
span_element = rendered_html.find('span') self.assertEqual(span_element.get('attr'), "TEST")
return self.q(css="div.problem span.message").text[0]
extra_post_params = {"enrollment_action": "enroll"} with patch('student.views.change_enrollment') as mock_change_enrollment: mock_change_enrollment.return_value = HttpResponse() response, _ = self._login_response( 'test@edx.org', 'test_password', extra_post_params=extra_post_params, ) response_content = json.loads(response.content) self.assertIsNone(response_content["redirect_url"]) self._assert_response(response, success=True)
archive.__salt__ = {} archive.__pillar__ = {} archive.__grains__ = {"id": "0"} archive.__opts__ = {}
self.q(css="#verify_now_button").click() PaymentAndVerificationFlow(self.browser, self._course_id, entry_point='verify-now').wait_for_page()
if not request.data or not getattr(request.data, "keys", None): error_message = _("No data provided for user preference update") return Response( { "developer_message": error_message, "user_message": error_message }, status=status.HTTP_400_BAD_REQUEST ) try: with transaction.atomic(): update_user_preferences(request.user, request.data, user=username) except UserNotAuthorized: return Response(status=status.HTTP_403_FORBIDDEN) except UserNotFound: return Response(status=status.HTTP_404_NOT_FOUND) except PreferenceValidationError as error: return Response( {"field_errors": error.preference_errors}, status=status.HTTP_400_BAD_REQUEST ) except PreferenceUpdateError as error: return Response( { "developer_message": error.developer_message, "user_message": error.user_message }, status=status.HTTP_400_BAD_REQUEST ) return Response(status=status.HTTP_204_NO_CONTENT)
if item["author_name"] and item["author_email"]: handler.addQuickElement("author", "%s (%s)" % (item['author_email'], item['author_name'])) elif item["author_email"]: handler.addQuickElement("author", item["author_email"]) elif item["author_name"]: handler.addQuickElement( "dc:creator", item["author_name"], {"xmlns:dc": "http://purl.org/dc/elements/1.1/"} )
from __future__ import absolute_import
import salt.fileserver import salt.utils import salt.utils.url
X_sparse = csr_matrix(X) Y_sparse = csr_matrix(Y) if metric in ["chi2", "additive_chi2"]: assert_raises(ValueError, pairwise_kernels, X_sparse, Y=Y_sparse, metric=metric) continue K1 = pairwise_kernels(X_sparse, Y=Y_sparse, metric=metric) assert_array_almost_equal(K1, K2)
test_invalid_token("AAAAAAAAAAAAAAAAAAAAANRGw8HDEmlcLVFawgY9wI8=", "padding")
self.input_space.validate(state_below) if self.requires_reformat: if not isinstance(state_below, tuple): for sb in get_debug_values(state_below): if sb.shape[0] != self.dbm.batch_size: raise ValueError("self.dbm.batch_size is %d but got shape of %d" % (self.dbm.batch_size, sb.shape[0])) assert reduce(operator.mul, sb.shape[1:]) == self.input_dim state_below = self.input_space.format_as(state_below, self.desired_space) if iter_name is None: iter_name = 'anon' if state_above is not None: assert layer_above is not None msg = layer_above.downward_message(state_above) msg.name = 'msg_from_'+layer_above.layer_name+'_to_'+self.layer_name+'['+iter_name+']' else: msg = None if double_weights: state_below = 2. * state_below state_below.name = self.layer_name + '_'+iter_name + '_2state' z = self.transformer.lmul(state_below) + self.b if self.layer_name is not None and iter_name is not None: z.name = self.layer_name + '_' + iter_name + '_z' p,h = max_pool_channels(z, self.pool_size, msg) p.name = self.layer_name + '_p_' + iter_name h.name = self.layer_name + '_h_' + iter_name return p, h
return None
X, y = datasets.make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=10, random_state=42)
with tm.assert_produces_warning(FutureWarning): df.icol(1)
self.clipping_values = {} clip_names_seen = set() for parameter in self.params: clip_name = '%s_clip' % parameter.name if clip_name in kwargs: if clip_name in clip_names_seen: logger.warning('In SGDOptimizer, at least two parameters ' 'have the same name. Both will be affected ' 'by the keyword argument ' '{0}.'.format(clip_name)) clip_names_seen.add(clip_name) p_min, p_max = kwargs[clip_name] assert p_min <= p_max self.clipping_values[parameter] = (p_min, p_max)
'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.sites', 'django.contrib.messages', 'django.contrib.staticfiles', 'djcelery', 'method_override',
if field in self._dirty_fields: del self._dirty_fields[field]
grains = {} if 'NUMBER_OF_PROCESSORS' in os.environ: try: grains['num_cpus'] = int(os.environ['NUMBER_OF_PROCESSORS']) except ValueError: grains['num_cpus'] = 1 grains['cpu_model'] = platform.processor() return grains
log.warning( 'Minion id {0} is not who it says it is!'.format( load['id'] ) ) return {}
'' if '.' in field_type else 'models.', field_type,
user = request.user course_key = CourseKey.from_string(course_id) with modulestore().bulk_operations(course_key): course = get_course_with_access(user, 'load', course_key) if not registered_for_course(course, user): context = { 'course': course, 'csrftoken': csrf(request)["csrf_token"] } return render_to_response(self.template_name, context)
if self.n_components > n_samples: n_components = n_samples warnings.warn("n_components > n_samples. This is not possible.\n" "n_components was set to n_samples, which results" " in inefficient evaluation of the full kernel.")
tm._skip_if_no_scipy() s = Series([1, 2, 3]) result = s.interpolate(method='polynomial', order=1) assert_series_equal(result, s)
pass
new_item = modulestore().create_child( 'leech_master', new_draft.location, 'chapter', fields={'display_name': 'new chapter'} ) new_draft_locator = new_draft_locator.course_key.version_agnostic() new_index = modulestore().get_course_index_info(new_draft_locator) self.assertNotEqual(new_index['versions'][BRANCH_NAME_DRAFT], original_index['versions'][BRANCH_NAME_DRAFT]) new_draft = modulestore().get_course(new_draft_locator) self.assertEqual(new_item.edited_by, 'leech_master') self.assertNotEqual(new_item.location.version_guid, original_index['versions'][BRANCH_NAME_DRAFT]) self.assertNotEqual(new_draft.location.version_guid, original_index['versions'][BRANCH_NAME_DRAFT]) structure_info = modulestore().get_course_history_info(new_draft_locator) self.assertEqual(structure_info['edited_by'], 'leech_master')
provider = profile.get('provider', '0:0') comps = provider.split(':') if len(comps) < 2 or comps[1] != 'gce': return {'Error': 'The requested profile does not belong to GCE'}
should_visit_children = include_child_info and (course_outline and not is_xblock_unit or not course_outline) if should_visit_children and xblock.has_children: child_info = _create_xblock_child_info( xblock, course_outline, graders, include_children_predicate=include_children_predicate, user=user, course=course ) else: child_info = None
s = Series(range(10)).astype(float) s[8] = None result = s[8] self.assertTrue(isnull(result))
self.test_loader._top_level_dir = None
return self._descriptors[location.to_deprecated_string()]
with self.assertRaises(GeoIP2Exception): cntry_g.city('tmc.edu') with self.assertRaises(GeoIP2Exception): cntry_g.coords('tmc.edu')
initial = [{'choice': 'Calexico', 'votes': 100}] ChoiceFormSet = formset_factory(Choice, extra=3) formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices') form_output = []
bytes = bytearray
if sock_dir is None: sock_dir = os.path.join(syspaths.SOCK_DIR, 'master') event = salt.utils.event.get_event( 'master', sock_dir, transport, listen=False) try: event.fire_event(msg, tag) except ValueError: if isinstance(args, dict): args[key] = msg else: args = {key: msg} event.fire_event(args, tag)
xerr = kwds.pop('xerr', None) yerr = kwds.pop('yerr', None) self.errors = {} for kw, err in zip(['xerr', 'yerr'], [xerr, yerr]): self.errors[kw] = self._parse_errorbars(kw, err)
for l in [slice(-10, 10), slice(-10.0, 10.0)]:
STUDIO_EDIT_ROLES = 8 STUDIO_VIEW_USERS = 4 STUDIO_EDIT_CONTENT = 2 STUDIO_VIEW_CONTENT = 1
self.assertEqual(test, expected)
def as_custom_sql(self, compiler, connection): lhs_sql, lhs_params = self.process_lhs(compiler, connection, self.lhs.lhs) rhs_sql, rhs_params = self.process_rhs(compiler, connection) params = lhs_params + rhs_params + lhs_params + rhs_params return ("%(lhs)s >= str_to_date(concat(%(rhs)s, '-01-01'), '%%%%Y-%%%%m-%%%%d') " "AND %(lhs)s <= str_to_date(concat(%(rhs)s, '-12-31'), '%%%%Y-%%%%m-%%%%d')" % {'lhs': lhs_sql, 'rhs': rhs_sql}, params) setattr(YearExact, 'as_' + connection.vendor, as_custom_sql) self.assertIn( 'concat(', str(Author.objects.filter(birthdate__testyear=2012).query))
with warnings.catch_warnings(record=True): import pandas.io.ga as ga
if tab.type == 'single_textbook': book_type, book_index = tab.tab_id.split("/", 1) expected_link = self.reverse( type_to_reverse_name[book_type], args=[self.course.id.to_deprecated_string(), book_index] ) self.assertEqual(tab.link_func(self.course, self.reverse), expected_link) self.assertTrue(tab.name.startswith('Book{0}'.format(book_index))) num_textbooks_found = num_textbooks_found + 1
self.expected_settings_change_initiated_event( 'email', email, 'you@there.com', username=username, user_id=user_id),
new_inner = LinearRing((30, 30), (30, 70), (70, 70), (70, 30), (30, 30)) ns.poly[1] = new_inner ply[1] = new_inner self.assertEqual(4326, ns.poly.srid) ns.save() self.assertEqual(ply, State.objects.get(name='NullState').poly) ns.delete()
return ~self.isnull()
Person.objects.create(name='John Doe') self.assertEqual(Person.objects.count(), 1) person = Person.objects.all()[0] post_url = reverse('admin_custom_urls:admin_custom_urls_person_change', args=[person.pk]) response = self.client.post(post_url, {'name': 'Jack Doe'}) self.assertRedirects(response, reverse('admin_custom_urls:admin_custom_urls_person_delete', args=[person.pk]))
'clone']
_globals['include'] = Registry.include _globals['extend'] = Registry.make_extend
n_components, n_features = means.shape n_samples = X.shape[0] bound = np.empty((n_samples, n_components)) bound[:] = initial_bound if covariance_type in ['diag', 'spherical']: for k in range(n_components): d = X - means[k] bound[:, k] -= 0.5 * np.sum(d * d * precs[k], axis=1) elif covariance_type == 'tied': for k in range(n_components): bound[:, k] -= 0.5 * _sym_quad_form(X, means[k], precs) elif covariance_type == 'full': for k in range(n_components): bound[:, k] -= 0.5 * _sym_quad_form(X, means[k], precs[k]) return bound
appen_kind = isinstance(key, salt.key.RaetKey)
handle_scm = win32service.OpenSCManager( None, None, win32service.SC_MANAGER_ALL_ACCESS)
for prop in 'cpus', 'disk', 'ip_address', 'nameserver', 'password', 'swap', 'poolid', 'storage':
p = linear_response * (linear_response > 0.) + self.left_slope *\ linear_response * (linear_response < 0.) return p
self.assert_grade(problem, 'choice_0', 'incorrect') self.assert_grade(problem, 'choice_1', 'correct') self.assert_grade(problem, 'choice_2', 'partially-correct')
world.css_click(subsection_css)
related_models = get_related_models_recursive(old_model)
test_ssh_host = host test_ssh_port = port
request_info = xmodule_instance_args.get('request_info', {}) if xmodule_instance_args is not None else {} task_info = {'student': student.username, 'task_id': _get_task_id_from_xmodule_args(xmodule_instance_args)}
decorator `django.utils.decorators.decorator_from_middleware(middleware_class)`
transcripts_info = video_descriptor.get_transcripts_info() transcript_langs = video_descriptor.available_translations(transcripts_info, verify_assets=False)
if not exists(zpool): ret[zpool] = 'storage pool does not exist' return ret
self.assertFalse(CourseEnrollment.is_enrolled(self.user, non_existent_course_key))
only_net = bool(only_net) if not conf_tuples: conf_tuples = [] kwargs = copy.deepcopy(kwargs) ret = [] if not only_net: default_data = _get_lxc_default_data(**kwargs) for k, val in six.iteritems(default_data): ret.append({k: val}) net_datas = _network_conf(conf_tuples=conf_tuples, **kwargs) ret.extend(net_datas) return ret
_check_diff('core', os.path.join(os.getcwd(), 'conf', 'locale')) for name, dir_ in contrib_dirs: _check_diff(name, dir_)
repr(slo)
rng = np.random.RandomState(random_state) X = 2 * rng.rand(n_samples, n_features) - 1 y = ((X ** 2).sum(axis=1) < .5).astype(np.int) y_str = y.astype(str)
random_state = check_random_state(0) X_ = random_state.randint(0, 200, [10, 1]) y_ = np.ndarray.flatten(0.2 * X_ + 2) sample_weight = random_state.randint(0, 10, 10) outlier_X = random_state.randint(0, 1000, [1, 1]) outlier_weight = random_state.randint(0, 10, 1) outlier_y = random_state.randint(-1000, 0, 1)
return (tenant_id, subnet, router, network, floatingip, port, security_group, security_group_rule)
selector = Index(df.ts[0:100].values) result = store.select('df', [Term('ts=selector')]) expected = df[df.ts.isin(selector.values)] tm.assert_frame_equal(expected, result) self.assertEqual(len(result), 100)
self.q(css=self._bounded_selector(".annotator-save")).first.click() self.wait_for_notes_invisibility("Note is saved.") self.wait_for_ajax() return self
values = Series([u('a_b_c'), u('c_d_e'), np.nan, u('f_g_h')])
UserPartition.scheme_extensions = None super(GroupAccessTestCase, self).tearDown()
rng = np.random.RandomState(4) X = rng.uniform(0, 5, 10)[:, np.newaxis] y = np.sin((X[:, 0] - 2.5) ** 2) gp.fit(X, y)
if 'transport' in opts: ttype = opts['transport'] elif 'transport' in opts.get('pillar', {}).get('master', {}): ttype = opts['pillar']['master']['transport']
ret[zpool] = {} devs = ' '.join(vdevs) zpool_cmd = _check_zpool() cmd = '{zpool_cmd} offline {temp}{zpool} {devs}'.format( zpool_cmd=zpool_cmd, temp='-t ' if kwargs.get('temporary', False) else '', zpool=zpool, devs=devs ) res = __salt__['cmd.run_all'](cmd, python_shell=False) if res['retcode'] != 0: ret[zpool] = res['stderr'] if 'stderr' in res else res['stdout'] else: ret[zpool] = {} for device in vdevs: if device not in ['mirror', 'log', 'cache', 'raidz1', 'raidz2', 'raidz3', 'spare']: ret[zpool][device] = 'offlined' return ret
target[model] = {f.attname for f in fields}
df = DataFrame(np.random.randn(10, 2)) store.append('df2', df) result = store.select('df2', where=[0, 3, 5]) expected = df.iloc[[0, 3, 5]] tm.assert_frame_equal(result, expected)
def check_cs_op(result, func, cargs): "Checks the status code of a coordinate sequence operation." if result == 0: raise GEOSException('Could not set value on coordinate sequence') else: return result
X = iris.data[:, pair] y = iris.target
if self.meta: meta_attrs = self.meta.__dict__.copy() for name in self.meta.__dict__: if name.startswith('_'): del meta_attrs[name] for attr_name in DEFAULT_NAMES: if attr_name in meta_attrs: setattr(self, attr_name, meta_attrs.pop(attr_name)) self.original_attrs[attr_name] = getattr(self, attr_name) elif hasattr(self.meta, attr_name): setattr(self, attr_name, getattr(self.meta, attr_name)) self.original_attrs[attr_name] = getattr(self, attr_name)
mock_plain = MagicMock() with patch.object(parallels, 'prlctl', mock_plain): parallels.list_vms(runas=runas) mock_plain.assert_called_once_with('list', [], runas=runas)
return os.getcwd()
if k.lower() == 'name': continue if k.lower() == 'roles': if isinstance(v, str): v = v.split(',') if set(roles) != set(v): kwargs['roles'] = list(set(v)) elif resource_value != v: kwargs[k] = v
self.course.start = _LAST_WEEK self.assertTrue(self.course.has_started()) self.course.start = _NEXT_WEEK self.assertFalse(self.course.has_started())
fig, ax = self.plt.subplots() d = df.boxplot(ax=ax, return_type='dict') lines = list(itertools.chain.from_iterable(d.values())) self.assertEqual(len(ax.get_lines()), len(lines))
path = list() n_alphas = self.alphas inner_verbose = max(0, self.verbose - 1)
xblock_family = child.attrib.pop('xblock-family', None) if xblock_family: xblock_family = self._family_id_to_superclass(xblock_family) if issubclass(xblock_family, XBlockAside): aside_children.append(child)
return ( is_mobile_available_for_user(user, course) and ( _has_staff_access_to_descriptor(user, course, course.id) or _has_fulfilled_all_milestones(user, course.id) ) )
grid_search.scoring = 'sklearn' assert_raises(ValueError, grid_search.fit, X, y)
for role in six.iterkeys(ret): rdata = ret[role] groups = rdata.setdefault('groups', []) query = ( 'select rolname' ' from pg_user' ' join pg_auth_members' ' on (pg_user.usesysid=pg_auth_members.member)' ' join pg_roles ' ' on (pg_roles.oid=pg_auth_members.roleid)' ' where pg_user.usename=\'{0}\'' ).format(role) try: rows = psql_query(query, runas=runas, host=host, user=user, port=port, maintenance_db=maintenance_db, password=password) for row in rows: if row['rolname'] not in groups: groups.append(row['rolname']) except Exception: continue return ret
dog_stats_api.increment( 'DjangoXBlockUserStateClient.{}'.format(evt_name), timestamp=evt_time, sample_rate=self.API_DATADOG_SAMPLE_RATE, )
fields = (f for f in fields if f != '?')
self.addCleanup(self._mock_paver_needs.stop) self.addCleanup(os.remove, self.f.name)
initialize_permissions(destination_course_key, User.objects.get(id=user_id))
s = Series(np.arange(4)) result, bins = cut(s, 2, retbins=True) tm.assert_numpy_array_equal(result.cat.codes.values, np.array([0, 0, 1, 1], dtype=np.int8)) tm.assert_numpy_array_equal(bins, np.array([-0.003, 1.5, 3]))
self.assertContains(response, "<h2>admin_list</h2>", count=2, html=True)
leaves = est.apply(X) for est_id in range(leaves.shape[1]): leave_indicator = [indicator[i, n_nodes_ptr[est_id] + j] for i, j in enumerate(leaves[:, est_id])] assert_array_almost_equal(leave_indicator, np.ones(shape=n_samples))
extra_attrs = {'class': 'special'}
block_structure = BlockStructureModulestoreData(root_block_usage_key=0) for block in blocks: block_structure._add_xblock(block.location, block)
DECIMAL_SEPARATOR = ','
return self.q(css='span.filter-column').visible
self.assertTrue(set(['course', 'about']).issubset(set([item.location.block_type for item in items_in_tree]))) self.assertEqual(orphan in [item.location for item in items_in_tree], orphan_in_items) self.assertEqual(len(items_in_tree), expected_items_in_tree)
orphan = course_key.make_usage_key('chapter', 'OrphanChapter') self.store.create_item( self.user_id, orphan.course_key, orphan.block_type, block_id=orphan.block_id )
REQUIRE_JS = "js/vendor/requirejs/require.js"
text = f.widget.format_value(result) self.assertEqual(text, "13:30:00")
template = slug.lower().replace('-', '_') + ".html" try: resp = render_to_response('static_templates/press_releases/' + template, {}) except TopLevelLookupException: raise Http404 else: return resp
pass
y_pred3 = clf.fit(X, y3).predict(X) assert_true(np.any(y_pred3 != y3), 'solver %s' % solver)
child_vertical = ItemFactory.create(parent_location=vertical.location, category='vertical', display_name='Child Vertical') self.assertEqual(xblock_studio_url(child_vertical), u'/container/{}'.format(child_vertical.location))
_UA_PRODUCT = 'salt-cloud' _UA_VERSION = '0.2.0'
m = __salt__['boto_apigateway.create_api_method'](restApiId=self.restApiId, resourcePath=resource_path, httpMethod=method_name.upper(), authorizationType=authorization_type, apiKeyRequired=api_key_required, requestParameters=method.get('params'), requestModels=method.get('models'), **self._common_aws_args) if not m.get('created'): ret = _log_error_and_abort(ret, m) return ret
GlobalStaff().add_users(self.user) self.client.login(username=self.user.username, password='foo')
self.generate_renamed_models()
return False
homework_grader = graders.AssignmentFormatGrader("Homework", 12, 2) homework_grader2 = graders.grader_from_conf(homework_grader)
pw_group.__grains__ = {} pw_group.__salt__ = {} pw_group.__context__ = {} pw_group.grinfo = {}
inshp, kshp = shape out_indices, out_indptr, spmat_shape = out indices, indptr, spmatshp, outshp = self.evaluate(inshp, kshp) out_indices[0] = indices out_indptr[0] = indptr spmat_shape[0] = numpy.asarray(spmatshp)
if interval not in ['d', 'w', 'm', 'v']: raise ValueError("Invalid interval: valid values are 'd', 'w', 'm' and 'v'") return _get_data_from(symbols, start, end, interval, retry_count, pause, adjust_price, ret_index, chunksize, 'yahoo')
return models.Client.objects.get(**filters)
PIPELINE_CSS_COMPRESSOR = None PIPELINE_JS_COMPRESSOR = None
request = HTTPRequest(url, headers={'Origin': 'http://foo.bar', 'Host': 'example.com'}) try: ws = yield websocket_connect(request) except HTTPError as error: self.assertEqual(error.code, 403)
return True
ForestEstimator = FOREST_ESTIMATORS[name] forest = ForestEstimator(n_estimators=10, n_jobs=3, random_state=0) forest.fit(X, y) assert_equal(len(forest), 10) forest.set_params(n_jobs=1) y1 = forest.predict(X) forest.set_params(n_jobs=2) y2 = forest.predict(X) assert_array_almost_equal(y1, y2, 3)
if not isinstance(config, dict): return False, ('Configuration for twilio_txt_msg beacon ' 'must be a dictionary.') return True, 'Valid beacon configuration'
initial_description = "A" + " really" * 50 + " long description" self.set_team_configuration( {u"max_team_size": 1, u"topics": [{"name": "", "id": "", "description": initial_description}]} ) self.topics_page.visit() truncated_description = self.topics_page.topic_descriptions[0] self.assertLess(len(truncated_description), len(initial_description)) self.assertTrue(truncated_description.endswith('...')) self.assertIn(truncated_description.split('...')[0], initial_description)
if self.opts.get('ipc_mode') == 'tcp' and ( (not isinstance(self.opts['tcp_pub_port'], list) and self.opts['tcp_pub_port'] != 4510) or (not isinstance(self.opts['tcp_pull_port'], list) and self.opts['tcp_pull_port'] != 4511) ): raise SaltException('For multi-master, tcp_(pub/pull)_port ' 'settings must be lists of ports, or the ' 'default 4510 and 4511') masternumber = 0 for master in set(self.opts['master']): s_opts = copy.deepcopy(self.opts) s_opts['master'] = master s_opts['multimaster'] = True s_opts['auth_timeout'] = self.MINION_CONNECT_TIMEOUT if self.opts.get('ipc_mode') == 'tcp': if isinstance(self.opts['tcp_pub_port'], list): s_opts['tcp_pub_port'] = self.opts['tcp_pub_port'][masternumber] s_opts['tcp_pull_port'] = self.opts['tcp_pull_port'][masternumber] else: s_opts['tcp_pub_port'] = self.opts['tcp_pub_port'] + (masternumber * 2) s_opts['tcp_pull_port'] = self.opts['tcp_pull_port'] + (masternumber * 2) self.io_loop.spawn_callback(self._connect_minion, s_opts) masternumber += 1
return self.bias.get_value()
g_on = tensor.inc_subtensor( state_below[:, self.dim:], tensor.dot(state_before, U[:, self.dim:]) ) r_on = tensor.nnet.sigmoid(g_on[:, self.dim:2*self.dim]) u_on = tensor.nnet.sigmoid(g_on[:, 2*self.dim:]) z_t = tensor.tanh( g_on[:, :self.dim] + tensor.dot(r_on * state_before, U[:, :self.dim]) ) z_t = u_on * state_before + (1. - u_on) * z_t z_t = mask[:, None] * z_t + (1 - mask[:, None]) * state_before return z_t
user_id = user_service.get_current_user().opt_attrs.get('edx-platform.user_id', None)
import integration
rng = np.random.RandomState(0) sigmoid_nonlin = SigmoidConvNonlinearity(monitor_style="detection") (rows, cols) = (10, 10) axes = ('c', 0, 1, 'b') nchs = 1 space_shp = (nchs, rows, cols, 1) X_vals = np.random.uniform(-0.01, 0.01, size=space_shp).astype(config.floatX) X = theano.shared(X_vals, name="X") Y_vals = (np.random.uniform(-0.01, 0.01, size=(rows, cols)) > 0.005).astype("uint8") Y = theano.shared(Y_vals, name="y_vals") conv_elemwise = ConvElemwise(layer_name="h0", output_channels=1, irange=.005, kernel_shape=(1, 1), max_kernel_norm=0.9, nonlinearity=sigmoid_nonlin) input_space = pylearn2.space.Conv2DSpace(shape=(rows, cols), num_channels=nchs, axes=axes) model = MLP(batch_size=1, layers=[conv_elemwise], input_space=input_space) Y_hat = model.fprop(X) cost = model.cost(Y, Y_hat).eval() assert not(np.isnan(cost) or np.isinf(cost) or (cost < 0.0) or (cost is None)), ("cost returns illegal " "value.")
args = [_sdecode(name)] args.extend(_normalize_args(command))
'RUN_AS_ANALYTICS_SERVER_ENABLED': False,
mode = CourseMode.objects.get(course_id=course_key, mode_slug=mode_slug) mode.expiration_datetime = upgrade_deadline mode.save()
ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright) ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)
request = RequestFactory().request() request.user = user tabs = get_course_tab_list(request, course) return len([tab for tab in tabs if tab.type == 'edxnotes']) == 1
@setup({'if-tag-filter01': '{% if foo|length == 5 %}yes{% else %}no{% endif %}'}) def test_if_tag_filter01(self): output = self.engine.render_to_string('if-tag-filter01', {'foo': 'abcde'}) self.assertEqual(output, 'yes')
if len(args) != 1: raise CommandError("export requires one argument: <output path>") output_path = args[0] courses, failed_export_courses = export_courses_to_output_path(output_path) print "=" * 80 print u"=" * 30 + u"> Export summary" print u"Total number of courses to export: {0}".format(len(courses)) print u"Total number of courses which failed to export: {0}".format(len(failed_export_courses)) print u"List of export failed courses ids:" print u"\n".join(failed_export_courses) print "=" * 80
if isinstance(space, CompositeSpace): return tuple(make_dtype_tree(dtype, component) for component in space.components) else: return super_self._clean_dtype_arg(dtype)
import logging import subprocess
self.assertEqual(response.status_code, 200)
return self.as_matrix()
assert_greater(n_iter_reference, 2)
CourseEnrollment.enroll(user, course_id(course))
self.assertEqual(s.dropna().values.item(), 'l')
testStack = self.store.fetch('.salt.test.lane.stack').value statsReq = self.store.fetch('.salt.stats.event_req').value tag = tagify('road', 'stats') statsReq.append({'route': {'dst': (None, None, 'stats_req'), 'src': (None, testStack.local.name, None)}, 'tag': tag})
if transform: self.source_srs = self.check_srs(source_srs) self.transform = self.coord_transform() else: self.transform = transform
if bind_def.startswith('0.0.0.0:'): bind_def = bind_def.replace('0.0.0.0:', '') desired_binds.append(bind_def)
from __future__ import unicode_literals
self.user_preference.delete() self.assert_user_setting_event_emitted(setting=self.TEST_KEY, old=self.TEST_VALUE, new=None)
@setup({'basic-syntax28': "{{ a.b }}"}) def test_basic_syntax28(self): output = self.engine.render_to_string('basic-syntax28', {'a': SilentGetItemClass()}) if self.engine.string_if_invalid: self.assertEqual(output, 'INVALID') else: self.assertEqual(output, '')
return hash(self.callback)
options.c = 1 self.assertEqual(len(holder), 1)
self.store.publish(self.course.location, self.user_id) published_xblock = self.store.get_item( self.vertical_x1a, revision=ModuleStoreEnum.RevisionOption.published_only ) self.assertIsNotNone(published_xblock)
CMS_SEGMENT_KEY = None
_, res = SendMessageTimeout(HWND_BROADCAST, WM_SETTINGCHANGE, 0, 0, SMTO_ABORTIFHUNG, 5000) return not bool(res)
version = '%s' % (pandas.__version__)
should_reset_team_size = False if self.pk is None: should_reset_team_size = True if not self.last_activity_at: self.last_activity_at = datetime.utcnow().replace(tzinfo=pytz.utc) super(CourseTeamMembership, self).save(*args, **kwargs) if should_reset_team_size: self.team.reset_team_size()
if existing['code'] == 200:
if ((ascending and labels.is_monotonic_increasing) or (not ascending and labels.is_monotonic_decreasing)): if inplace: return else: return self.copy()
output_emails = [row["email"] for row in output] for email in output_emails: self.assertIn(email, output_emails)
query = self._course_key_to_son(location.course_key) query['definition.children'] = unicode(location)
IN_PROGRESS = "in_progress" FAILED = "failed" SUCCEEDED = "succeeded"
from __future__ import absolute_import import logging
if hasattr(self, 'process_manager'): self.process_manager.stop_restarting() self.process_manager.send_signal_to_processes(signum) self.process_manager.kill_children()
user = UserFactory.create() course = CourseFactory.create() tag = UserCourseTagFactory.create(user=user, course_id=course.id, key="testkey", value="foobar") self.assertEquals(tag.user, user) self.assertEquals(tag.course_id, course.id) self.assertEquals(tag.key, "testkey") self.assertEquals(tag.value, "foobar")
if convnet_available.compiled: _logger.debug('already compiled') return True
default_tolerance = '0.001%'
self.load_data()
index = DatetimeIndex(["2013-08-27", "2013-10-01", "2013-10-29", "2013-11-26"]) assert frequencies.infer_freq(index) != 'WOM-4TUE'
settings_dict = settings.DATABASES[self.connection.alias] user = settings_dict.get('SAVED_USER') or settings_dict['USER'] password = settings_dict.get('SAVED_PASSWORD') or settings_dict['PASSWORD'] settings_dict = settings_dict.copy() settings_dict.update(USER=user, PASSWORD=password) DatabaseWrapper = type(self.connection) return DatabaseWrapper(settings_dict, alias=self.connection.alias)
self.asset_collection.update( {'_id': course_assets.doc_id}, {'$set': {self._make_mongo_asset_key(asset_key.asset_type): all_asset_info}} ) return 1
pgettext = real_pgettext
msg += 'Found the following errors:\n' for profile_name, error in six.iteritems(dmap['errors']): msg += ' {0}: {1}\n'.format(profile_name, error) sys.stderr.write(msg) sys.stderr.flush()
cohorted_course_wide_discussions, cohorted_inline_discussions = get_cohorted_discussions( course, course_cohort_settings ) return { 'id': course_cohort_settings.id, 'is_cohorted': course_cohort_settings.is_cohorted, 'cohorted_inline_discussions': cohorted_inline_discussions, 'cohorted_course_wide_discussions': cohorted_course_wide_discussions, 'always_cohort_inline_discussions': course_cohort_settings.always_cohort_inline_discussions, }
self._assert_course_verification_status(VERIFY_STATUS_MISSED_DEADLINE)
block_types = ['notes', 'lti'] with override_settings(DEPRECATED_BLOCK_TYPES=block_types): course_module = modulestore().get_item(self.course.location) self._create_test_data(course_module, create_blocks=True, block_types=block_types) info = _deprecated_blocks_info(course_module, block_types) self._verify_deprecated_info(course_module.id, course_module.advanced_modules, info, block_types)
resp = self.client.get(course_wiki_page, follow=False, HTTP_REFERER=referer) self.assertEqual(resp.status_code, 302)
new = Point(5, 23) nullcity.point = new
pass
break
if return_type not in self._valid_return_types: raise ValueError( "return_type must be {None, 'axes', 'dict', 'both'}")
if os.path.exists(lxc_config): cmd += ' -f {0}'.format(pipes.quote(lxc_config)) cmd += ' -d' _ensure_exists(name, path=path) if kwargs.get('restart', False): salt.utils.warn_until( 'Carbon', 'The \'restart\' argument to \'lxc.start\' has been deprecated, ' 'please use \'lxc.restart\' instead.' ) return restart(name, path=path) if state(name, path=path) == 'frozen': raise CommandExecutionError( 'Container \'{0}\' is frozen, use lxc.unfreeze'.format(name) ) use_vt = kwargs.get('use_vt', None) with_communicate = kwargs.get('with_communicate', False) return _change_state(cmd, name, 'running', stdout=None, stderr=None, stdin=None, with_communicate=with_communicate, path=path, use_vt=use_vt)
last_element = t.nodes[-1] self._dispatch(last_element)
query_features.append('city') query_features_names['city'] = _('City') query_features.append('country') query_features_names['country'] = _('Country')
request = Request(self.environ) request.body = self.get_request_body() response = self.xmodule.grade_handler(request, '') real_response = self.get_response_values(response) expected_response = { 'action': None, 'code_major': 'failure', 'description': 'OAuth verification error: Malformed authorization header', 'messageIdentifier': self.defaults['messageIdentifier'], } self.assertEqual(response.status_code, 200) self.assertDictEqual(expected_response, real_response)
log.debug('formularesponse: error %s in formula', err) raise StudentInputError( _("Invalid input: Could not parse '{bad_input}' as a formula").format( bad_input=cgi.escape(answer) ) )
assert_array_almost_equal(grad_1, grad_2) assert_almost_equal(loss_1, loss_2)
reg_check = __salt__['reg.read_value'](hive=hive, key=key, vname=vname, use_32bit_registry=use_32bit_registry) if not reg_check['success'] or reg_check['vdata'] == '(value not set)': ret['comment'] = '{0} is already absent'.format(name) return ret
self.store.publish(unit.location, self.user_id) self.assertFalse(self._has_changes(vertical.location)) self.assertFalse(self._has_changes(unit.location))
try: self.repo = git.Repo(self.cachedir) except git.exc.InvalidGitRepositoryError: log.error(_INVALID_REPO.format(self.cachedir, self.url, self.role)) return new
qs = RasterModel.objects.filter(rast__contains=GEOSGeometry('POINT (-0.5 0.5)', 4326)) self.assertEqual(qs.count(), 1) qs = RasterModel.objects.filter(rast__contains=GEOSGeometry('POINT (0.5 0.5)', 4326)) self.assertEqual(qs.count(), 0) qs = RasterModel.objects.filter(rast__contains_properly=GEOSGeometry('POINT (0 0)', 4326)) self.assertEqual(qs.count(), 0) qs = RasterModel.objects.filter(rast__left=GEOSGeometry('POINT (1 0)', 4326)) self.assertEqual(qs.count(), 1)
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, mock_open, patch, NO_MOCK, NO_MOCK_REASON )
verbose_name_plural = "Microsite histories"
allow_quiet_fail = acceptable_ora_err is not None and len(acceptable_ora_err) > 0 self._execute_statements(cursor, statements, parameters, verbosity, allow_quiet_fail=allow_quiet_fail) return True
self.assertIn('Welcome to edX.', self.video.captions_text)
def test_not(self): var = IfParser(["not", False]).parse() self.assertEqual("(not (literal False))", repr(var)) self.assertTrue(var.eval({}))
raise NotImplementedError
item = Item.objects.create(name="first", value=42) o2o = OneToOneItem.objects.create(item=item, name="second") self.assertEqual(len(Item.objects.defer('one_to_one_item__name')), 1) self.assertEqual(len(Item.objects.select_related('one_to_one_item')), 1) self.assertEqual(len(Item.objects.select_related( 'one_to_one_item').defer('one_to_one_item__name')), 1) self.assertEqual(len(Item.objects.select_related('one_to_one_item').defer('value')), 1) self.assertEqual(len(Item.objects.only('one_to_one_item')), 1) with self.assertNumQueries(1): i = Item.objects.select_related('one_to_one_item')[0] self.assertEqual(i.one_to_one_item.pk, o2o.pk) self.assertEqual(i.one_to_one_item.name, "second") with self.assertNumQueries(1): i = Item.objects.select_related('one_to_one_item').defer( 'value', 'one_to_one_item__name')[0] self.assertEqual(i.one_to_one_item.pk, o2o.pk) self.assertEqual(i.name, "first") with self.assertNumQueries(1): self.assertEqual(i.one_to_one_item.name, "second") with self.assertNumQueries(1): self.assertEqual(i.value, 42)
path = url.path if not path.startswith('/'): path = urljoin(response.request['PATH_INFO'], path)
self.assertFalse(form.is_valid())
import salt.utils
images = as_cuda_ndarray_variable(images) top_down = as_cuda_ndarray_variable(top_down) assert images.ndim == 4 assert top_down.ndim == 4 channels_broadcastable = images.type.broadcastable[0] batch_broadcastable = images.type.broadcastable[3] rows_broadcastable = False cols_broadcastable = False houtput_broadcastable = (channels_broadcastable, rows_broadcastable, cols_broadcastable, batch_broadcastable) houtput_type = CudaNdarrayType(broadcastable=houtput_broadcastable) houtput = houtput_type() poutput_broadcastable = (channels_broadcastable, rows_broadcastable, cols_broadcastable, batch_broadcastable) poutput_type = CudaNdarrayType(broadcastable=poutput_broadcastable) poutput = poutput_type() return Apply(self, [images, top_down], [houtput, poutput])
with self.assertRaises(ValueError): with transaction.atomic(using='default'): marty.book_set.set([pro, dive])
if update_datetime: if not __opts__['test']: response = __salt__[esxi_cmd]('update_host_datetime').get(host) error = response.get('Error') if error: ret['comment'] = 'Error: {0}'.format(error) return ret ret['changes'].update({'update_datetime': {'old': '', 'new': 'Host datetime was updated.'}})
with tm.assert_produces_warning(FutureWarning): assert_series_equal(s.duplicated(take_last=True), expected) with tm.assert_produces_warning(FutureWarning): assert_series_equal( s.drop_duplicates(take_last=True), s[~expected]) sc = s.copy() with tm.assert_produces_warning(FutureWarning): sc.drop_duplicates(take_last=True, inplace=True) assert_series_equal(sc, s[~expected])
self.assertEquals( self.split_test_module.child_descriptor.url_name, self.split_test_module.child_descriptor.url_name )
return self._fit(X, y, alpha=self.alpha, C=1.0, loss=self.loss, learning_rate=self.learning_rate, coef_init=coef_init, intercept_init=intercept_init, sample_weight=sample_weight)
always_cohort_inline_discussions = models.BooleanField(default=True)
raise nose.SkipTest("skipping for now")
idx = pd.Int64Index([1, 2, 3], name='xxx') result = idx.take(np.array([1, 0, -1])) expected = pd.Int64Index([2, 1, 3], name='xxx') tm.assert_index_equal(result, expected)
REQUIRE_EXCLUDE = ("build.txt",)
for_update_sql = tested_connection.ops.for_update_sql(nowait) sql = tested_connection.queries[-1]['sql'] return bool(sql.find(for_update_sql) > -1)
return cls.css
distances, neighbors = lshf.radius_neighbors(query, radius=mean_dist, return_distance=True) assert_array_less(distances[0], mean_dist)
a = Article.objects.create( author=self.jane, headline="Nobody remembers the early years", pub_date=datetime(1, 2, 3, 4, 5, 6))
tg = TimeGrouper(**kwds) return tg._get_resampler(obj, kind=kind)
self.library = LibraryFactory.create(modulestore=store) self.html_unit1 = ItemFactory.create( parent_location=self.library.location, category="html", display_name="Html Content", modulestore=store, publish_item=False, ) self.html_unit2 = ItemFactory.create( parent_location=self.library.location, category="html", display_name="Html Content 2", modulestore=store, publish_item=False, )
compression='infer', thousands=None, decimal=b'.', lineterminator=None, quotechar='"', quoting=csv.QUOTE_MINIMAL, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=False,
export_extra_content( export_fs, self.modulestore, self.courselike_key, xml_centric_courselike_key, 'course_info', 'info', '.html' )
def _asarray_compat(x): if isinstance(x, ABCSeries): return x._values else: return np.asarray(x)
if len(axes) != self.ndim - 1: raise ValueError( "currently only support ndim-1 indexers in an AppendableTable")
def from_json(self, values): return [UserPartition.from_json(v) for v in values] def to_json(self, values): return [user_partition.to_json() for user_partition in values]
return self._enabled
return name + "\x00" * (length - len(name))
result = grouped.agg(OrderedDict([['A', 'var'], ['B', 'std'], ['C', 'mean'], ['D', 'sem']])) expected = DataFrame(OrderedDict([['A', grouped['A'].var( )], ['B', grouped['B'].std()], ['C', grouped['C'].mean()], ['D', grouped['D'].sem()]])) assert_frame_equal(result, expected)
value = "(%s)" if len(strings) != 1 else "(%s,)" return value % (", ".join(strings)), imports
azure.get_blob( storage_conn=storage_conn, container=env, name=blob, local_path=file_name, )
text_document = self._white_spaces.sub(" ", text_document)
while len(parts) > 1: p = parts.pop(0) popped.append(p) zone = '{0}.{1}'.format('.'.join(parts), 'in-addr.arpa.') name = '.'.join(popped) ptr = delete(zone, name, 'PTR', fqdn, nameserver=nameserver, **kwargs) if ptr: res = True
with self.assertRaises(PermissionDenied): self.get_form(expected_valid=False)
self.run_function('virtualenv.create', [self.venv_dir])
FACEBOOK_API_VERSION = AUTH_TOKENS.get("FACEBOOK_API_VERSION") FACEBOOK_APP_SECRET = AUTH_TOKENS.get("FACEBOOK_APP_SECRET") FACEBOOK_APP_ID = AUTH_TOKENS.get("FACEBOOK_APP_ID")
super(RoleBase, self).__init__() self.org = org self.course_key = course_key self._role_name = role_name
delete_indices = np.array([])
x_weights = np.dot(X.T, y_score) / np.dot(y_score.T, y_score)
new_connection.cursor() self.assertFalse(new_connection.get_autocommit())
if not isinstance(Y, np.ndarray): raise ValueError("y_pred should be an array of floats.")
with filesystem.open('policy.json', 'r') as course_policy: on_disk = loads(course_policy.read()) self.assertIn('course/2012_Fall', on_disk) self.assertEqual(on_disk['course/2012_Fall'], own_metadata(course))
self._mock_programs_api([]) enrollments = self._create_enrollments('org/course/run') meter = utils.ProgramProgressMeter(self.user, enrollments) self.assertEqual(meter.engaged_programs, []) self._assert_progress(meter)
try: manual_enrollment = cls.objects.filter(enrollment=enrollment).latest('time_stamp') except cls.DoesNotExist: manual_enrollment = None return manual_enrollment
active_users = get_user_model()._default_manager.filter( email__iexact=email, is_active=True) return (u for u in active_users if u.has_usable_password())
updates = OrderedDict() for (param, grad) in six.iteritems(grads): vel = sharedX(param.get_value() * 0.) assert param.dtype == vel.dtype assert grad.dtype == param.dtype if param.name is not None: vel.name = 'vel_' + param.name scaled_lr = learning_rate * lr_scalers.get(param, 1.) updates[vel] = self.momentum * vel - scaled_lr * grad inc = updates[vel] if self.nesterov_momentum: inc = self.momentum * inc - scaled_lr * grad assert inc.dtype == vel.dtype updates[param] = param + inc return updates
char_model_empty = PrimaryKeyCharModel.objects.create(string='') fk_model_empty = FkToChar.objects.create(out=char_model_empty) fk_model_empty = FkToChar.objects.select_related('out').get(id=fk_model_empty.pk) self.assertEqual(fk_model_empty.out, char_model_empty)
with check_mongo_calls_range(max_finds=4, max_sends=2): self._update_partitions(reload_items=False)
@property def tuple(self): "Returns a tuple of the point." return self._cs.tuple
from salttesting.helpers import ( ensure_in_syspath, ) ensure_in_syspath('../../')
with self.assertRaises(ValueError): c(span=0.5)
ground_truth = rng.normal(size=n_features) y = (np.dot(X_scaled, ground_truth) > 0.).astype(np.int32) assert_array_equal(np.unique(y), [0, 1])
jidstore_fstr = '{0}.prep_jid'.format(job_cache) try: mminion.returners[jidstore_fstr](False, passed_jid=load['jid']) except KeyError: emsg = "Returner '{0}' does not support function prep_jid".format(job_cache) log.error(emsg) raise KeyError(emsg)
vertical = self.store.create_item( self.user_id, test_course.id, 'vertical', block_id='test_vertical' ) component = self.store.create_child( self.user_id, vertical.location, 'html', block_id='html_component' )
user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw') p0 = PasswordResetTokenGenerator() tk1 = p0.make_token(user) self.assertTrue(p0.check_token(user, tk1))
class MyEstimator(BaseEstimator):
url( r'^reverify/{course_id}/{usage_id}/$'.format( course_id=settings.COURSE_ID_PATTERN, usage_id=settings.USAGE_ID_PATTERN ), views.InCourseReverifyView.as_view(), name="verify_student_incourse_reverify" ),
import salt.ext.six as six
input_element = etree.Element("imageinput") input_element.set("src", str(src)) input_element.set("width", str(width)) input_element.set("height", str(height))
with self.assertRaises(CommandError) as exc_context: call_command('manage_group', TEST_GROUP, '--permissions', 'auth:Group:nonexistent-perm') self.assertIn('invalid permission codename', str(exc_context.exception).lower()) self.assertIn('nonexistent-perm', str(exc_context.exception).lower()) self.check_group_permissions(initial_group_permissions)
from __future__ import absolute_import, print_function, generators import os import copy import glob import time import signal import logging import traceback import multiprocessing import sys from itertools import groupby
try: atomic_replace(temp_filename,local_dst) except Exception as e: raise IOError("[ac] %s %s --> %s" % (str(e),temp_filename,local_dst))
raise NotImplementedError()
second_checkpoint.photo_verification.remove(attempt) self.assertEqual(second_checkpoint.photo_verification.count(), 0)
self._assertion_errors.append(formatted_exc)
sql.to_sql(self.test_frame1, 'test_frame', self.conn)
_check_unary_op(operator.inv)
os.setsid()
from __future__ import unicode_literals
if not hasattr(settings, 'TEST_RUNNER'): settings.TEST_RUNNER = 'django.test.runner.DiscoverRunner' TestRunner = get_runner(settings)
print(file=ex_file) print('Examples using ``%s``' % backref, file=ex_file) print('-----------------%s--' % ('-' * len(backref)), file=ex_file) print(file=ex_file)
if N.size(imshp)==2: inshp = (1,)+imshp
return Fragment(self.FRAG_CONTENT)
X = np.ones((1, 10)) y = np.ones(1) msg = "1 sample(s) (shape=(1, 10)) while a minimum of 2 is required." assert_raise_message(ValueError, msg, check_X_y, X, y, ensure_min_samples=2)
request = RequestFactory().get("dummy_url", {"page": requested_page}) request.user = self.staff_user response = users_in_cohort(request, unicode(course.id), cohort.id) if should_return_bad_request: self.assertEqual(response.status_code, 400) return self.assertEqual(response.status_code, 200) return json.loads(response.content)
response = self.client.get(reverse('admin:admin_views_persona_add')) names = name_re.findall(response.content) self.assertEqual(len(names), len(set(names)))
mat2 = ma.copy(mat) mat2[0, 0] = 1 mat2[1, 2] = 2 frame = DataFrame(mat2, columns=['A', 'B', 'C'], index=[1, 2]) self.assertEqual(1, frame['A'].view('i8')[1]) self.assertEqual(2, frame['C'].view('i8')[2])
out_size = get_encoded_size(img_h, img_w, rings) output = numpy.zeros((batch_size, out_size * chans))
if self.loss not in ('linear', 'square', 'exponential'): raise ValueError( "loss must be 'linear', 'square', or 'exponential'")
pass
assert_equals(value, getattr(seq, attribute))
ret = self.run_function('state.sls', mods='issue-1879', timeout=120) self.assertSaltTrueReturn(ret)
self.srt_file.seek(0) _upload_file(self.srt_file, self.item_descriptor.location, u'塞.srt') self.srt_file.seek(0) request = Request.blank('translation/zh?filename={}'.format(u'塞.srt'.encode('utf8'))) response = self.item_descriptor.studio_transcript(request=request, dispatch='translation/zh') self.assertEqual(response.body, self.srt_file.read()) self.assertEqual(response.headers['Content-Type'], 'application/x-subrip; charset=utf-8') self.assertEqual(response.headers['Content-Disposition'], 'attachment; filename="塞.srt"') self.assertEqual(response.headers['Content-Language'], 'zh')
self.assertRaises(ValueError, Timestamp, max_ts_us + one_us)
os.system('rm source/html-styling.html') os.system('cd build; rm -f html/pandas.zip;')
ctag = _gen_tag(chunk) if ctag not in running: if ctag in self.active: if chunk.get('__prerequired__'): if tag not in self.pre: low['__prereq__'] = True self.pre[ctag] = self.call(low, chunks, running) return running else: return running elif ctag not in running: log.error('Recursive requisite found') running[tag] = { 'changes': {}, 'result': False, 'comment': 'Recursive requisite found', '__run_num__': self.__run_num, '__sls__': low['__sls__']} self.__run_num += 1 self.event(running[tag], len(chunks), fire_event=low.get('fire_event')) return running running = self.call_chunk(chunk, running, chunks) if self.check_failhard(chunk, running): running['__FAILHARD__'] = True return running
pro = Book.objects.create(title="Pro Django", published=datetime.date(2008, 12, 16))
try: pip_list = __salt__['pip.list'](prefix, bin_env=bin_env, user=user, cwd=cwd) prefix_realname = _find_key(prefix, pip_list) except (CommandNotFoundError, CommandExecutionError) as err: ret['result'] = None ret['comment'] = 'Error installing {0!r}: {1}'.format(state_pkg_name, err) return ret
from __future__ import division import warnings import numpy as np from scipy import linalg
return np.int64(key.value).view(_TD_DTYPE)
ts_irregular[5:].plot(secondary_y=True, ax=ax) ts_irregular[:5].plot(ax=ax)
from __future__ import absolute_import import logging import time
columns = ['a', 'b', ('a', ''), ('c', 'c1')] expected = DataFrame(columns=columns, data=[[1, 11, 0, 44], [0, 22, 1, 33]]) with tm.assert_produces_warning(UserWarning): result = df1.join(df2, on='a') tm.assert_frame_equal(result, expected)
import salt.utils import integration from salt.utils.verify import ( check_user, verify_env, verify_socket, zmq_version, check_max_open_files, valid_id )
if self.required: raise ValidationError(self.error_messages['required'], code='required')
if n < 0: raise ValueError("A negative number of rows requested. Please " "provide positive value.")
profile_str = 'virtualbox-config' providers = self.run_cloud('--list-providers') log.debug("providers: %s", providers)
data = bz2.decompress(f.read()) f = StringIO(data)
requiv = lambda x, y, op=op: getattr(operator, op)(y, x) pairings.append((lop, lequiv, fv)) pairings.append((rop, requiv, fv))
section = ItemFactory.create(parent=self.course, category='chapter', display_name='Test Section') subsection = ItemFactory.create(parent=section, category='sequential', display_name='Test Subsection') vertical = ItemFactory.create(parent=subsection, category='vertical', display_name='Test Unit')
ret = dockerng_state.volume_present('volume_foo', driver='local') self.assertEqual( { 'name': 'volume_foo', 'comment': ("Driver for existing volume 'volume_foo'" " ('dummy_default') does not match specified" " driver ('local') and force is False"), 'changes': {}, 'result': False, }, ret) self.assertEqual(orig_volumes, volumes)
resp = self.client.get( reverse("register_user"), HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME ) self.assertContains(resp, "Register for Test Microsite") self.assertContains(resp, "register-form")
rng = np.random.RandomState(123) lengths = rng.randint(1,10, 100) data = [['w']*l for l in lengths] batch_size = 5 my_iter = EvenSequencesSubsetIterator(data, batch_size) visited = [False] * len(data) for ind_list in my_iter: assert [len(data[i]) == len(data[ind_list[0]]) for i in ind_list] for i in ind_list: visited[i] = True assert all(visited)
self.rwork = _aligned_zeros(self.ncv, self.tp.lower())
self.assertIsNone(self.service.get_credit_state(self.user.id, self.course.id))
s_list = list('aaa') s_series = Series(s_list) s_series_index = Series(s_list, list('ABC'))
mask = os.umask(191) try: log.info('Rotating AES key') if os.path.isfile(dfn): log.info('AES key rotation already requested') return
@wraps(func, assigned=available_attrs(func)) def inner_func(*args, **kwargs): response = func(*args, **kwargs) patch_vary_headers(response, ('Cookie',)) return response return inner_func
return conn.validate_template(template_body, template_url)
ItemFactory.create(category='html', parent_location=nested_vertical1.location) nested_vertical2 = ItemFactory.create(category='vertical', parent_location=nested_section.location) module2 = ItemFactory.create(category='html', parent_location=nested_vertical2.location)
geom_transform = void_output(lgdal.OGR_G_Transform, [c_void_p, c_void_p]) geom_transform_to = void_output(lgdal.OGR_G_TransformTo, [c_void_p, c_void_p])
return (os.path.normcase(os.path.abspath(src)) == os.path.normcase(os.path.abspath(dst)))
sys.excepthook = __global_logging_exception_handler
codes = self.categories.get_indexer(keyarr) if (codes == -1).any(): raise KeyError("a list-indexer must only include values that are " "in the categories") return None
htpasswd.__salt__ = {}
if not diff_quality_percentage_pass: raise BuildFailure("Diff-quality failure(s).")
if self.remote_field.through is not None: return self.remote_field.through._meta.db_table elif self.db_table: return self.db_table else: return utils.truncate_name('%s_%s' % (opts.db_table, self.name), connection.ops.max_name_length())
x = Series(date_range('20130101 09:00:00', periods=5, freq='D')) x.iloc[1] = np.nan result = fmt.Datetime64Formatter(x).get_result() self.assertEqual(result[0].strip(), "2013-01-01 09:00:00") self.assertEqual(result[1].strip(), "NaT") self.assertEqual(result[4].strip(), "2013-01-05 09:00:00")
random_state = check_random_state(random_state) for i in range(10): n = 100 X = sparse.eye(n, n) beta = random_state.rand(n) y = X * beta[:, np.newaxis]
return xblock._edit_info.get('subtree_edited_by')
admin = OAuth2ProviderConfigAdmin(provider1, AdminSite()) update_url = reverse('admin:{}_{}_add'.format(admin.model._meta.app_label, admin.model._meta.model_name)) update_url += "?source={}".format(provider1.pk)
if exploded_grant[position_tracker + 1] == ',' \ or exploded_grant[position_tracker + 1] == 'ON': if multiword_statement: multiword_statement.append(token) grant_tokens.append(' '.join(multiword_statement)) multiword_statement = [] else: grant_tokens.append(token)
Number.objects.filter(pk=self.n.pk).update(integer=F('integer') / 2, float=F('float') / 42.7)
time.sleep(420) results = self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME)) try: self.assertIn( 'True', [i.strip() for i in results] ) except AssertionError: raise
draft_branch = ModuleStoreEnum.BranchName.library published_branch = ModuleStoreEnum.BranchName.library
if self.opts.get('ext_job_cache'): if ret: ret += ',{0}'.format(self.opts['ext_job_cache']) else: ret = self.opts['ext_job_cache']
batch_inertia /= model.batch_size centers_squared_diff /= model.batch_size
tmp *= 1 - damping R *= damping R += tmp
instructor = UserFactory.create(username="instructor", email="instructor@e.com", password="test") role = CourseInstructorRole(self.course_limited.id) role.add_users(instructor)
course_usage_main_vertical = self.lc_block.children[0] course_usage_inner_vertical = self.store.get_item(course_usage_main_vertical).children[0] inner_vertical_in_course = self.store.get_item(course_usage_inner_vertical) course_usage_html = inner_vertical_in_course.children[0] course_usage_problem = inner_vertical_in_course.children[1]
DATE_FORMAT = 'j F Y' TIME_FORMAT = 'H:i' DATETIME_FORMAT = 'j F Y, H:i' YEAR_MONTH_FORMAT = 'F Y' MONTH_DAY_FORMAT = 'j F' SHORT_DATE_FORMAT = 'd.m.Y' SHORT_DATETIME_FORMAT = 'd.m.Y, H:i'
return self._reject(request, REASON_NO_CSRF_COOKIE)
X, labels = check_X_y(X, labels) le = LabelEncoder() labels = le.fit_transform(labels) n_labels = len(le.classes_) n_samples = X.shape[0] if not 1 < n_labels < n_samples: raise ValueError("Number of labels is %d. Valid values are 2 " "to n_samples - 1 (inclusive)" % n_labels) if sample_size is not None: random_state = check_random_state(random_state) indices = random_state.permutation(X.shape[0])[:sample_size] if metric == "precomputed": X, labels = X[indices].T[indices].T, labels[indices] else: X, labels = X[indices], labels[indices] return np.mean(silhouette_samples(X, labels, metric=metric, **kwds))
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
unordered = frame.ix[:, ['D', 'B', 'C', 'A']] df = unordered.copy() df.sort_index(axis=1, inplace=True) expected = frame assert_frame_equal(df, expected)
pep8_checker = StyleGuide() files_to_check = [] for path in list_files(".py"): rel_path = os.path.relpath(path, pylearn2.__path__[0]) if rel_path in whitelist_pep8: continue else: files_to_check.append(path) report = pep8_checker.check_files(files_to_check) if report.total_errors > 0: raise AssertionError("PEP8 Format not respected")
res = self.client.post('/edit/author/%d/delete/' % a.pk) self.assertEqual(res.status_code, 302) self.assertRedirects(res, '/list/authors/') self.assertQuerysetEqual(Author.objects.all(), [])
expected = Series([2, 0], index=Float64Index([5.0, 0.0]))
with self.store.default_store(ModuleStoreEnum.Type.mongo): ClientFactory(name="edx-notes") self.course = CourseFactory.create() self.chapter = ItemFactory.create(category="chapter", parent_location=self.course.location) self.chapter_2 = ItemFactory.create(category="chapter", parent_location=self.course.location) self.sequential = ItemFactory.create(category="sequential", parent_location=self.chapter.location) self.vertical = ItemFactory.create(category="vertical", parent_location=self.sequential.location) self.html_module_1 = ItemFactory.create(category="html", parent_location=self.vertical.location) self.html_module_2 = ItemFactory.create(category="html", parent_location=self.vertical.location) self.vertical_with_container = ItemFactory.create( category='vertical', parent_location=self.sequential.location ) self.child_container = ItemFactory.create( category='split_test', parent_location=self.vertical_with_container.location) self.child_vertical = ItemFactory.create(category='vertical', parent_location=self.child_container.location) self.child_html_module = ItemFactory.create(category="html", parent_location=self.child_vertical.location)
c = self.factor.copy() indexer = np.zeros(len(c), dtype='bool') indexer[0] = True indexer[-1] = True c[indexer] = 'c' expected = Categorical.from_array(['c', 'b', 'b', 'a', 'a', 'c', 'c', 'c'], ordered=True)
self.assertNotContains(response, "How it Works") self.assertNotContains(response, "Find courses") self.assertNotContains(response, "Schools & Partners")
import salt.utils
self.assertEquals(destination_course.wiki_slug, destination_wiki_slug)
request = HttpRequest() request.META = { 'SERVER_NAME': 'internal.com', 'SERVER_PORT': 80, } self.assertEqual(request.get_host(), 'internal.com')
self._update_staff_locks(False, False, False) self.assertFalse(utils.ancestor_has_staff_lock(self.vertical)) self._update_staff_locks(False, False, True) self.assertFalse(utils.ancestor_has_staff_lock(self.vertical))
#except:
f = forms.FileField(required=True) self.assertEqual(f.clean(False, 'initial'), 'initial') with self.assertRaises(ValidationError): f.clean(False)
concrete_model = obj._meta.concrete_model for field in concrete_model._meta.local_fields: if field.serialize: if field.remote_field is None: if self.selected_fields is None or field.attname in self.selected_fields: self.handle_field(obj, field) else: if self.selected_fields is None or field.attname[:-3] in self.selected_fields: self.handle_fk_field(obj, field) for field in concrete_model._meta.many_to_many: if field.serialize: if self.selected_fields is None or field.attname in self.selected_fields: self.handle_m2m_field(obj, field) self.end_object(obj) progress_bar.update(count) if self.first: self.first = False
if keep_blank_values: nv.append('') else: continue
pairs = [('1988-Q2', '1988Q2'), ('2Q-1988', '2Q1988'), ]
name = name.split('.')[0]
project_state = ProjectState() project_state.add_model(ModelState.from_model(Author)) project_state.add_model(ModelState.from_model(Book)) self.assertEqual( [name for name, field in project_state.models["migrations", "book"].fields], ["id", "author"], )
if wildcard: return '*' else: return unicode(badge_class.course_id)
with self.assertRaises(TypeError): Polygon(0, [1, 2, 3]) with self.assertRaises(TypeError): Polygon('foo')
return ItemFactory.create( parent_location=parent_location, category=category, display_name=display_name, publish_item=False, user_id=self.user.id, **kwargs )
from salt.exceptions import SaltInvocationError from salt.utils import exactly_one
n = 20000 Series(np.random.randn(n)).rolling(window=2, center=False).median() Series(np.random.randn(n)).rolling(window=2, center=False).median()
def asgd(self, X, y, eta, alpha, weight_init=None, intercept_init=0.0): if weight_init is None: weights = np.zeros(X.shape[1]) else: weights = weight_init
result.status_code
try: while proc.has_unread_data: try: cstdout, cstderr = proc.recv() if cstdout: stdout += cstdout if cstderr: if output is None: stdout += cstderr else: stderr += cstderr time.sleep(0.5) except KeyboardInterrupt: break ret = stdout if output is None \ else {'retcode': proc.exitstatus, 'pid': 2, 'stdout': stdout, 'stderr': stderr} except vt.TerminalException: trace = traceback.format_exc() log.error(trace) ret = stdout if output is None \ else {'retcode': 127, 'pid': 2, 'stdout': stdout, 'stderr': stderr} finally: proc.terminate()
row = np.argmax(state.marked[:, path[count, 1]] == 1) if not state.marked[row, path[count, 1]] == 1: break else: count += 1 path[count, 0] = row path[count, 1] = path[count - 1, 1]
membership_country = models.ForeignKey(Country, models.CASCADE) date_joined = models.DateTimeField(default=datetime.datetime.now) invite_reason = models.CharField(max_length=64, null=True) person_id = models.IntegerField() group_id = models.IntegerField(blank=True, null=True)
self.assertFalse(self.advanced_settings.is_validation_modal_present())
reindex_course_and_check_access(self.course.id, self.user)
name = '{0}-{1}'.format(name, version)
alias = _weekday_rule_aliases[self.rep_stamp.weekday()] return _maybe_add_count('W-%s' % alias, days / 7)
if not reparse_data: return False
from __future__ import absolute_import
y_pred_proba = clf.predict_proba(X) y_pred_log_proba = clf.predict_log_proba(X) assert_array_almost_equal(np.log(y_pred_proba), y_pred_log_proba, 8)
actual = self.get_exceldf(basename, 'Sheet1', converters=converters) tm.assert_frame_equal(actual, expected)
"payment_url": get_purchase_endpoint(),
self._initialize_mixed(contentstore=contentstore, mappings={})
if not names: raise
models.Article.objects.create( headline="Test article", pub_date=datetime.datetime(2010, 9, 4), reporter=self.r, ) a = models.Article.objects.get(headline="Test article") a.reporter_id = 30 with connection.constraint_checks_disabled(): a.save() with self.assertRaises(IntegrityError): connection.check_constraints() transaction.set_rollback(True)
eps = 0.8 min_samples = 10 metric = distance.euclidean core_samples, labels = dbscan(X, metric=metric, eps=eps, min_samples=min_samples, algorithm='ball_tree')
self._each_parens(r'\left(x^y\right)', 'x^y', '(', tall=True)
email, password = self.STUDENT_INFO[0] self.login(email, password) self.enroll(self.course, True) self.enroll(self.test_course, True) resp = self.client.get(reverse('courseware', kwargs={'course_id': self.course.id.to_deprecated_string()})) self.assertRedirects(resp, reverse( 'courseware_section', kwargs={'course_id': self.course.id.to_deprecated_string(), 'chapter': 'Overview', 'section': 'Welcome'}))
_encoding = None _upload_handlers = []
assert_frame_equal(df.add(row, axis=None), df + row)
dirname = tempfile.mkdtemp() filename = os.path.join(dirname, 'test_syntax_error.py') self.addCleanup(shutil.rmtree, dirname) with open(filename, 'w') as f: f.write("Ceci n'est pas du Python.") with extend_sys_path(dirname): with self.assertRaises(SyntaxError): autoreload.check_errors(import_module)('test_syntax_error') self.assertFileFound(filename)
try: for token in generate_tokens(linereader): tokeneater(*token) except (IndexError, UnicodeDecodeError): pass except tokenize.TokenError as msg: _m = ("An unexpected error occurred while tokenizing input file %s\n" "The following traceback may be corrupted or invalid\n" "The error message is: %s\n" % (file, msg)) print(_m)
